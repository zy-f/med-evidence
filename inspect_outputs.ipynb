{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4971510e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "import functools\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.ticker import MaxNLocator, PercentFormatter\n",
    "from tqdm import tqdm\n",
    "from easydict import EasyDict as edict\n",
    "from transformers import GPT2Tokenizer\n",
    "import time\n",
    "from Bio import Entrez\n",
    "plt.style.use('seaborn-v0_8-deep')\n",
    "COLORS = plt.rcParams['axes.prop_cycle'].by_key()['color']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f2b6cd",
   "metadata": {},
   "source": [
    "### input statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf5c354",
   "metadata": {},
   "source": [
    "#### initial 250"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288934ea",
   "metadata": {},
   "source": [
    "##### make the setupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e9f604",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"manual_250\"\n",
    "with open(f\"dset_gen/{dataset_name}-setup.json\", 'r') as fh:\n",
    "    dssetup = json.load(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ac82b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for review_pmid, qs in dssetup.items():\n",
    "    for q in qs:\n",
    "        q['review_pmid'] = review_pmid\n",
    "        rows.append(q)\n",
    "setupdf = pd.DataFrame(rows)\n",
    "setupdf.loc[setupdf.evidence_quality == '', 'evidence_quality'] = 'n/a'\n",
    "setupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421048c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not 'sla' in setupdf.columns:\n",
    "    SOURCE_AGREEMENT_PCT_BINS = ['0%', '1-49%', '50%', '51-99%', '100%']\n",
    "    def bin_sla_pct(x):\n",
    "        if x < 0.1:\n",
    "            return \"0%\"\n",
    "        elif x < 0.5:\n",
    "            return \"1-49%\"\n",
    "        elif x == 0.5:\n",
    "            return \"50%\"\n",
    "        elif x < 1:\n",
    "            return \"51-99%\"\n",
    "        else:\n",
    "            return \"100%\"\n",
    "\n",
    "    ### source-level agreement calcs\n",
    "    with open('run-med-evidence/___res/deepseekV3_SLA.jsonl', 'r') as fh:\n",
    "        sla_data = [json.loads(l) for l in fh]\n",
    "\n",
    "    question_source_scores = defaultdict(dict)\n",
    "    for out in sla_data:\n",
    "        model_ans = out.get('parsed_answer')\n",
    "        ans = model_ans.get('answer', \"\").lower() if isinstance(model_ans, dict) else \"invalid\"\n",
    "        score = (out['gt_answer'].lower() in ans.lower())\n",
    "        question_id, source_pmid = out['question_id'].split('-')\n",
    "        question_source_scores[question_id][source_pmid] = score\n",
    "    sla_data = {}\n",
    "    sla_data_binned = {}\n",
    "    for qid, src_lbls in question_source_scores.items():\n",
    "        sla_pct = sum(src_lbls.values())/len(src_lbls.values())\n",
    "        sla_data[int(qid)] = sla_pct\n",
    "    sla_df = pd.Series(sla_data).to_frame(name='sla').reset_index(names='question_id')\n",
    "    setupdf = setupdf.merge(sla_df, on='question_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145c9dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not 'context_tokens' in setupdf.columns:\n",
    "    with open(f\"run-med-evidence/datasets/manual_250-fulltext.jsonl\", 'r') as fh:\n",
    "        dset = [edict(json.loads(l)) for l in fh]\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "    get_count = lambda s: len(tokenizer.encode(s, add_special_tokens=False, verbose=False))\n",
    "    output_data = []\n",
    "    for rd in tqdm(dset):\n",
    "        sources = {s.article_id: s.content for s in rd.sources}\n",
    "        for q in rd.question_data:\n",
    "            full_text = '\\n'.join(sources[pmid] for pmid in q.relevant_sources)\n",
    "            output_data.append({\n",
    "                'question_id': int(q.question_id),\n",
    "                'context_tokens': get_count(full_text)\n",
    "            })\n",
    "    token_data = pd.DataFrame(output_data)\n",
    "    setupdf = setupdf.merge(token_data, on='question_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f5f0e5",
   "metadata": {},
   "source": [
    "##### meshterms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c895423c",
   "metadata": {},
   "outputs": [],
   "source": [
    "outpath = \"___tmp/manual_250-reviews.xml\"\n",
    "if not os.path.exists(outpath):\n",
    "    Entrez.email = PRIVATE\n",
    "    pmids = setupdf.review_pmid.unique().tolist()\n",
    "    with Entrez.efetch(db=\"pubmed\", id=pmids, retmode=\"xml\") as handle:\n",
    "        res = handle.read()\n",
    "    with open(outpath, 'wb') as fh:\n",
    "        fh.write(res)\n",
    "review_xml = Entrez.read(outpath)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55a47c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "major = 'N'\n",
    "meshterms = {}\n",
    "for article_elem in review_xml['PubmedArticle']:\n",
    "    pmid = str(article_elem['MedlineCitation']['PMID'])\n",
    "    terms = []\n",
    "    for mesh in article_elem['MedlineCitation'].get('MeshHeadingList', []):\n",
    "        # grab the qualifier major/minor flag, if any\n",
    "        qualifiers = mesh['QualifierName']\n",
    "        # if len(qualifiers) > 0:\n",
    "        #     major = str(qualifiers[0].attributes.items()[0][1])\n",
    "        # grab descriptor name\n",
    "        descr = mesh['DescriptorName']\n",
    "        terms.append(descr.title())\n",
    "    meshterms[pmid] = list(set(terms))\n",
    "all_meshterms = set(functools.reduce(lambda x, y: x + y, meshterms.values()))\n",
    "print('\\n'.join(all_meshterms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ca7302",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('run-med-evidence/src')\n",
    "import queryllms\n",
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7718d9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = queryllms.QueryLLM(provider=\"together\", model=\"deepseek-ai/DeepSeek-R1-Distill-Llama-70B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a10e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_prompt = \"\"\"\n",
    "Consider the following categories:\n",
    "1. Internal Medicine & Subspecialties\n",
    "2. Surgery\n",
    "3. Pediatrics & Neonatology\n",
    "4. Obstetrics & Gynecology\n",
    "5. Psychiatry & Neurology\n",
    "6. Oncology & Hematology\n",
    "7. Emergency Medicine & Critical Care\n",
    "8. Family Medicine & Preventive Care\n",
    "9. Dentistry & Oral Health\n",
    "10. Public Health, Epidemiology & Health Systems\n",
    "11. Other\n",
    "\n",
    "Given the following QUESTION, categorize it into the best-fitting category.\n",
    "Use the following output format:\n",
    "**CATEGORY**: Surgery\n",
    "**REASONING**: your reasoning for your selection\n",
    "\n",
    "Think step by step.\n",
    "QUESTION: {q}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daae7e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(r):\n",
    "    pattern = r\"(-\\s*)?\\*\\*(.*?)\\*\\*:\\s*((.|\\n)*?)(?=\\n+(-\\s*)?\\*\\*|\\Z)\"\n",
    "    matches = re.findall(pattern, r , re.DOTALL) # Extract matches\n",
    "    if matches:\n",
    "        # Convert to a dictionary\n",
    "        parsed_data = {}\n",
    "        for (_,key,value,_,_) in matches:\n",
    "            formatted_key = key.strip().lower().replace(\" \",\"_\")\n",
    "            parsed_data[formatted_key] = value.strip()\n",
    "        return parsed_data\n",
    "    else:\n",
    "        return {'unknown': r}\n",
    "\n",
    "def ask(question_id, question):\n",
    "    prompt = base_prompt.format(q=question)\n",
    "    response = llm.simple_query('follow instruction', prompt)\n",
    "    parsed = parse(response)\n",
    "    return (question_id, parsed)\n",
    "\n",
    "responses = []\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=20) as executor:\n",
    "    futures = [executor.submit(ask, q.question_id, q.question) for _,q in setupdf.iterrows()]\n",
    "    with tqdm(total=len(setupdf), desc=\"answering questions\") as pbar:\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            responses.append(future.result())\n",
    "            pbar.update()\n",
    "print(responses[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db05bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorydata = []\n",
    "for (qid, r) in responses:\n",
    "    categorydata.append({\n",
    "        'question_id': qid,\n",
    "        'med_specialty': r['category']\n",
    "    })\n",
    "categorydf = pd.DataFrame(categorydata)\n",
    "setupdf = setupdf.merge(categorydf, on='question_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339276fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(setupdf.med_specialty.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14f879a",
   "metadata": {},
   "source": [
    "##### add dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051345cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import download_pubmed_data as dpd\n",
    "data = dpd.get_review_pmid_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a54a9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "year_map = {}\n",
    "for review_pmid in setupdf.review_pmid.unique():\n",
    "    year = int(data[review_pmid]['date'][:4])\n",
    "    year_map[review_pmid] = year\n",
    "setupdf['review_year'] = setupdf.review_pmid.map(year_map)\n",
    "setupdf.to_csv('run-med-evidence/___res/dataset_stats_250.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5061ca6",
   "metadata": {},
   "source": [
    "#### additional data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313d19bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "setupdf_250 = pd.read_csv('run-med-evidence/___res/dataset_stats_250.csv')\n",
    "setupdf_250.loc[:,'relevant_sources'] = setupdf_250['relevant_sources'].apply(ast.literal_eval)\n",
    "setupdf_250['review_pmid'] = setupdf_250['review_pmid'].astype(str)\n",
    "setupdf_250.loc[:, 'evidence_quality'] = setupdf_250['evidence_quality'].fillna('N/A')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2be4ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"manual_284\"\n",
    "with open(f\"dset_gen/{dataset_name}-setup.json\", 'r') as fh:\n",
    "    dssetup = json.load(fh)\n",
    "rows = []\n",
    "for review_pmid, qs in dssetup.items():\n",
    "    for q in qs:\n",
    "        q['review_pmid'] = review_pmid\n",
    "        rows.append(q)\n",
    "setupdf = pd.DataFrame(rows)\n",
    "setupdf.loc[setupdf.evidence_quality == '', 'evidence_quality'] = 'n/a'\n",
    "new_df = setupdf[setupdf.question_id > 249]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c6c3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE_AGREEMENT_PCT_BINS = ['0%', '1-49%', '50%', '51-99%', '100%']\n",
    "def bin_sla_pct(x):\n",
    "    if x < 0.1:\n",
    "        return \"0%\"\n",
    "    elif x < 0.5:\n",
    "        return \"1-49%\"\n",
    "    elif x == 0.5:\n",
    "        return \"50%\"\n",
    "    elif x < 1:\n",
    "        return \"51-99%\"\n",
    "    else:\n",
    "        return \"100%\"\n",
    "\n",
    "### source-level agreement calcs\n",
    "with open('run-med-evidence/___res/deepseekV3_SLA.jsonl', 'r') as fh:\n",
    "    sla_data = [json.loads(l) for l in fh]\n",
    "\n",
    "question_source_scores = defaultdict(dict)\n",
    "for out in sla_data:\n",
    "    model_ans = out.get('parsed_answer')\n",
    "    ans = model_ans.get('answer', \"\").lower() if isinstance(model_ans, dict) else \"invalid\"\n",
    "    score = (out['gt_answer'].lower() in ans.lower())\n",
    "    question_id, source_pmid = out['question_id'].split('-')\n",
    "    question_source_scores[question_id][source_pmid] = score\n",
    "sla_data = {}\n",
    "sla_data_binned = {}\n",
    "for qid, src_lbls in question_source_scores.items():\n",
    "    sla_pct = sum(src_lbls.values())/len(src_lbls.values())\n",
    "    sla_data[int(qid)] = sla_pct\n",
    "sla_df = pd.Series(sla_data).to_frame(name='sla').reset_index(names='question_id')\n",
    "new_df = new_df.merge(sla_df, on='question_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca84c080",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('run-med-evidence/src')\n",
    "import queryllms\n",
    "import concurrent.futures\n",
    "\n",
    "base_prompt = \"\"\"\n",
    "Consider the following categories:\n",
    "1. Internal Medicine & Subspecialties\n",
    "2. Surgery\n",
    "3. Pediatrics & Neonatology\n",
    "4. Obstetrics & Gynecology\n",
    "5. Psychiatry & Neurology\n",
    "6. Oncology & Hematology\n",
    "7. Emergency Medicine & Critical Care\n",
    "8. Family Medicine & Preventive Care\n",
    "9. Dentistry & Oral Health\n",
    "10. Public Health, Epidemiology & Health Systems\n",
    "11. Other\n",
    "\n",
    "Given the following QUESTION, categorize it into the best-fitting category.\n",
    "Use the following output format:\n",
    "**CATEGORY**: Surgery\n",
    "**REASONING**: your reasoning for your selection\n",
    "\n",
    "Think step by step.\n",
    "QUESTION: {q}\n",
    "\"\"\"\n",
    "llm = queryllms.QueryLLM(provider=\"together\", model=\"deepseek-ai/DeepSeek-R1-Distill-Llama-70B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b1322e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(r):\n",
    "    pattern = r\"(-\\s*)?\\*\\*(.*?)\\*\\*:\\s*((.|\\n)*?)(?=\\n+(-\\s*)?\\*\\*|\\Z)\"\n",
    "    matches = re.findall(pattern, r , re.DOTALL) # Extract matches\n",
    "    if matches:\n",
    "        # Convert to a dictionary\n",
    "        parsed_data = {}\n",
    "        for (_,key,value,_,_) in matches:\n",
    "            formatted_key = key.strip().lower().replace(\" \",\"_\")\n",
    "            parsed_data[formatted_key] = value.strip()\n",
    "        return parsed_data\n",
    "    else:\n",
    "        return {'unknown': r}\n",
    "\n",
    "def ask(question_id, question):\n",
    "    prompt = base_prompt.format(q=question)\n",
    "    response = llm.simple_query('follow instruction', prompt)\n",
    "    parsed = parse(response)\n",
    "    return (question_id, parsed)\n",
    "\n",
    "responses = []\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=20) as executor:\n",
    "    futures = [executor.submit(ask, q.question_id, q.question) for _,q in new_df.iterrows()]\n",
    "    with tqdm(total=len(new_df), desc=\"answering questions\") as pbar:\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            responses.append(future.result())\n",
    "            pbar.update()\n",
    "print(responses[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db94258a",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorydata = []\n",
    "for (qid, r) in responses:\n",
    "    categorydata.append({\n",
    "        'question_id': qid,\n",
    "        'med_specialty': r['category']\n",
    "    })\n",
    "categorydf = pd.DataFrame(categorydata)\n",
    "new_df = new_df.merge(categorydf, on='question_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540af6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = pd.concat([setupdf_250, new_df])\n",
    "full_df.to_csv('run-med-evidence/___res/dataset_stats.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b38afd3",
   "metadata": {},
   "source": [
    "#### plot stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936931a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "setupdf = pd.read_csv('run-med-evidence/___res/dataset_stats.csv')\n",
    "setupdf.loc[:,'relevant_sources'] = setupdf['relevant_sources'].apply(ast.literal_eval)\n",
    "setupdf['review_pmid'] = setupdf['review_pmid'].astype(str)\n",
    "setupdf.loc[:, 'evidence_quality'] = setupdf['evidence_quality'].fillna('N/A')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524f5272",
   "metadata": {},
   "outputs": [],
   "source": [
    "setupdf['nsrc'] = [len(x) if len(x) < 4 else 4 for x in setupdf.relevant_sources]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c972da76",
   "metadata": {},
   "outputs": [],
   "source": [
    "setupdf.groupby(['nsrc', 'answer']).agg({'question_id':'count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36afe6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "srcs = []\n",
    "for r in setup_data:\n",
    "    for q in setup_data[r]:\n",
    "        srcs += q['relevant_sources']\n",
    "print(len(set(srcs)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93df11ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = []\n",
    "for x in setupdf.relevant_sources.values:\n",
    "    a += x\n",
    "print(len(a))\n",
    "print(len(set(a)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a46e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def donut_plot(ax, data, labels, pal, **kwargs):\n",
    "    wedges, texts, autotexts = ax.pie(data, wedgeprops=dict(width=0.6), colors=pal, autopct=\"%1.1f%%\",\n",
    "        labels=labels, #textprops={'bbox': dict(boxstyle=\"square,pad=0.3\", fc=\"w\", ec=\"k\", lw=0.72)},\n",
    "        pctdistance=0.7, explode=[0.03]*len(data), **kwargs)\n",
    "    for i, a in enumerate(autotexts):\n",
    "        a.set_bbox(None)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7144a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=2, figsize=(12,5), dpi=200)\n",
    "pal = sns.color_palette('hls', n_colors=5)\n",
    "eq = setupdf.groupby('evidence_quality').agg({'question_id':'count'})\n",
    "donut_plot(ax[0], data=eq.values.flatten(), labels=eq.index, pal=pal, startangle=0, labeldistance=1.1)\n",
    "ax[0].set_title('(a) Dataset stratified by evidence certainty\\n')\n",
    "ax[1].hist([len(x) for x in setupdf.relevant_sources], bins=11, weights=np.ones(len(setupdf)) / len(setupdf))\n",
    "ax[1].yaxis.set_major_formatter(PercentFormatter(1, decimals=0))\n",
    "ax[1].set_title('(b) Dataset stratified by number of\\nrelevant sources')\n",
    "ax[1].set_xlabel('Number of relevant sources')\n",
    "ax[1].set_ylabel('Percentage of questions')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b0376e",
   "metadata": {},
   "outputs": [],
   "source": [
    "TITLE_SIZE = 14\n",
    "pal = sns.color_palette('hls', n_colors=12)\n",
    "fig, ax = plt.subplots(ncols=3, width_ratios=[2, 1, 1], figsize=(20,8), dpi=200)\n",
    "\n",
    "setupdf['_med'] = setupdf['med_specialty'].str.replace('& ', '&\\n')\n",
    "med_distrib = setupdf.groupby('_med').agg({'question_id': 'count'})#.drop('Other')\n",
    "donut_plot(ax[0], data=med_distrib.values.flatten(), labels=med_distrib.index, pal=pal, startangle=190, labeldistance=1.1)\n",
    "ax[0].set_title('(a) Dataset stratified by medical specialty\\n', fontsize=TITLE_SIZE)\n",
    "\n",
    "label_distrib = setupdf.groupby('answer').agg({'question_id': 'count'})\n",
    "pal = sns.color_palette('hls', n_colors=6)\n",
    "label_distrib = label_distrib.loc[['higher', 'lower', 'no difference', 'uncertain effect', 'insufficient data']]\n",
    "donut_plot(ax[1], data=label_distrib.values.flatten(), labels=label_distrib.index, pal=pal, labeldistance=1.05)\n",
    "ax[1].set_title('(b) Dataset stratified by\\ntreatment outcome effect', fontsize=TITLE_SIZE)\n",
    "\n",
    "sla_bins = [(setupdf.sla.values < 0.1).sum(),\n",
    "            ((setupdf.sla.values > 0.1) & (setupdf.sla.values < 0.99)).sum(),\n",
    "            (setupdf.sla.values > 0.99).sum()]\n",
    "pal = sns.color_palette('hls', n_colors=7)\n",
    "sla_labels = ['No agreement', 'Mixed agreement', 'Full agreement']\n",
    "donut_plot(ax[2], sla_bins, sla_labels, pal=pal, labeldistance=1.05, startangle=180)\n",
    "ax[2].set_title('(c) Dataset stratified by source\\nconcordance with correct answer', fontsize=TITLE_SIZE)\n",
    "\n",
    "fig.align_titles()\n",
    "# fig.suptitle('Question distributions\\n', fontsize=TITLE_SIZE+5)\n",
    "\n",
    "# ax[2].tick_params(axis='x', labelrotation=30)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93adca45",
   "metadata": {},
   "source": [
    "### outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fd9c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_data = dpd.get_review_pmid_data()\n",
    "\n",
    "for rev in review_data:\n",
    "    title_len = len(review_data[rev]['title'])\n",
    "    if title_len < 10:\n",
    "        print(rev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec8411f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANSWER_CLASSES = [\"higher\", \"lower\", \"no difference\", \"insufficient data\", \"uncertain effect\"]\n",
    "DEFAULT_ANS = \"invalid\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69f1580",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump(filepath, question_ids):\n",
    "    with open(filepath, 'r') as fh:\n",
    "        outputs = [json.loads(line) for line in fh]\n",
    "    outputs = sorted(outputs, key=lambda o: o['question_id'])\n",
    "    str_outputs = []\n",
    "    for out in outputs:\n",
    "        if not (out['question_id'] in question_ids):\n",
    "            continue\n",
    "        used_articles = [a['article_id'] for a in out[\"articles\"]]\n",
    "        str_out = f\"\"\"EXAMPLE {out['question_id']} [REVIEW_PMID = {out['original_review']}]\\n\"\"\" \\\n",
    "                + f\"\"\"QUESTION: {out['gt_question']}\\nRESPONSE:\\n{out['answer']}\\n\"\"\" \\\n",
    "                + f\"\"\"GT ANSWER: {out['gt_answer']}\\nPMIDS OF SOURCES USED: {used_articles}\\n\\nREASON:\"\"\"\n",
    "        str_outputs.append(str_out)\n",
    "    with open('___tmp/dump.txt', 'w') as fh:\n",
    "        fh.write(f'\\n\\n{\"-\"*10}\\n\\n'.join(str_outputs))\n",
    "    print('dumped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989a7ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dump('run-med-evidence/___res/deepseekV3_closed_barebones_fulltext.jsonl', eval_df.question_id[~eval_df.score].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6495cbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = grade('run-med-evidence/___res/deepseekV3_closed_barebones_fulltext.jsonl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d265ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\n",
    "    (\"run-med-evidence/___res/manual101ft_qwen32b_closed_basic.jsonl\", 'qwen_basic'),\n",
    "    (\"run-med-evidence/___res/manual101ft_qwen32b_closed_qualAware.jsonl\", 'qwen_aware'),\n",
    "    (\"run-med-evidence/___res/manual101ft_qwen32b_closed_qualPrio.jsonl\", 'qwen_priority'),\n",
    "    (\"run-med-evidence/___res/manual101ft_qwen32b_closed_cochraneAware.jsonl\", 'qwen_cochrane'),\n",
    "    (\"run-med-evidence/___res/manual101ft_qwen32b_filterClosed_qualAware.jsonl\", 'qwen_noisyclosed'),\n",
    "    (\"run-med-evidence/___res/manual101abs_qwen32b_closed_qualAware.jsonl\", 'qwen_abstract'),\n",
    "    # (\"run-med-evidence/___res/manual101abs_llamaR1_closed_basic.jsonl\", 'r1_abstract'),\n",
    "    # (\"run-med-evidence/___res/manual101ft_llamaR1_closed_qualAware.jsonl\", 'r1_aware'),\n",
    "    # (\"run-med-evidence/___res/manual101ft_llamaR1_closed_qualPrio.jsonl\", 'r1_priority'),\n",
    "    # (\"run-med-evidence/___res/manual101ft_llamaR1_filterClosed_basic.jsonl\", 'r1_noisyclosed'),\n",
    "    # (\"run-med-evidence/___res/manual101ft_llamaR1_closed_basic.jsonl\", 'r1_basic')\n",
    "]\n",
    "groupby_key = \"evidence_quality\"\n",
    "# joint_key = \"score\"\n",
    "# joint_basecols = [\"question_id\", \"review_id\", \"question\", \"fulltext_required\", \"correct_answer\"]\n",
    "joint_df = pd.DataFrame()\n",
    "for (filepath, shortname) in files:\n",
    "    print(\"\\n\\n\"+shortname)\n",
    "    eval_df = grade(filepath)\n",
    "    # print(eval_df.score.mean())\n",
    "    if joint_df.empty:\n",
    "        joint_df = eval_df[joint_basecols]\n",
    "    _subset = eval_df[[\"question_id\", joint_key]].rename(columns={joint_key: shortname})\n",
    "    joint_df = joint_df.merge(_subset, how='outer', on=[\"question_id\"])\n",
    "    grouped_df = eval_df.groupby(groupby_key).agg(\n",
    "        accuracy=pd.NamedAgg(column=\"score\", aggfunc=\"mean\"),\n",
    "        n_correct=pd.NamedAgg(column=\"score\", aggfunc=\"sum\"),\n",
    "        n_questions=pd.NamedAgg(column=\"question\", aggfunc=\"count\")\n",
    "    ).astype({\"n_correct\": int, \"n_questions\": int}).sort_values(\"accuracy\")\n",
    "    display(grouped_df.loc[['', 'very low', 'low', 'moderate', 'high']])\n",
    "    # conf_mat_data = []\n",
    "    # all_classes = ANSWER_CLASSES + ['invalid']\n",
    "    # for p_label in all_classes:\n",
    "    #     row = []\n",
    "    #     p_label_mask = eval_df.pred_answer.str.lower() == p_label\n",
    "    #     for c_label in all_classes:\n",
    "    #         c_label_mask = eval_df.correct_answer.str.lower() == c_label\n",
    "    #         row.append((c_label_mask & p_label_mask).sum())\n",
    "    #     conf_mat_data.append(row)\n",
    "    # conf_mat = pd.DataFrame(conf_mat_data,\n",
    "    #                         columns=[[\"correct_answer\"]*len(all_classes), all_classes],\n",
    "    #                         index=[[\"pred_answer\"]*len(all_classes), all_classes],\n",
    "    # )\n",
    "    # display(conf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28539b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_df.groupby('fulltext_required').mean(numeric_only = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435de4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fulltext_ids = joint_df.question_id[joint_df.fulltext_required == 'yes'].to_list()\n",
    "dump('run-med-evidence/___res/manual101ft_qwen32b_closed_cochraneAware.jsonl', fulltext_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a9a20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df[eval_df.fulltext_required == 'yes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e8ac98",
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_scores = joint_df[joint_basecols].copy()\n",
    "for (_, shortname) in files:\n",
    "    joint_scores.loc[:, shortname] = (joint_df[shortname] == joint_df.correct_answer)\n",
    "all_wrong = (~joint_scores.iloc[:,4:]).all(axis=1)\n",
    "all_wrong.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3aad99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_df[all_wrong]#.groupby('correct_answer').agg({'question_id':'count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14e1ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_df[all_wrong].question.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e15137f",
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_df[\"diff\"] = joint_df.default != joint_df.abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e804272",
   "metadata": {},
   "source": [
    "### final figure plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9595aa04",
   "metadata": {},
   "source": [
    "#### define funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d7aa29",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import ast\n",
    "setupdf = pd.read_csv('run-med-evidence/___res/dataset_stats.csv').drop(columns=['question', 'answer'])\n",
    "setupdf.loc[:,'relevant_sources'] = setupdf['relevant_sources'].apply(ast.literal_eval)\n",
    "setupdf['review_pmid'] = setupdf['review_pmid'].astype(str)\n",
    "setupdf.loc[:, 'evidence_quality'] = setupdf['evidence_quality'].fillna('N/A')\n",
    "setup_dict = setupdf.set_index('question_id').T.to_dict()\n",
    "setupdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0df608",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_n_source(n):\n",
    "    if n < 4:\n",
    "        return str(n)\n",
    "    else:\n",
    "        return \"4+\"\n",
    "\n",
    "def bin_sla_pct(x):\n",
    "    if x < 0.1:\n",
    "        return \"0%\"\n",
    "    elif x < 0.5:\n",
    "        return \"1-49%\"\n",
    "    elif x == 0.5:\n",
    "        return \"50%\"\n",
    "    elif x < 1:\n",
    "        return \"51-99%\"\n",
    "    else:\n",
    "        return \"100%\"\n",
    "\n",
    "NSOURCE_BINS = [\"1\", \"2\", '3', \"4+\"]\n",
    "ANSWER_CLASSES = [\"higher\", \"lower\", \"no difference\", \"insufficient data\", \"uncertain effect\"]\n",
    "EVIDENCE_QUALITIES = [\"very low\", \"low\", \"moderate\", \"high\"]\n",
    "DEFAULT_ANS = \"invalid\"\n",
    "SOURCE_AGREEMENT_PCT_BINS = ['0%', '1-49%', '50%', '51-99%', '100%']\n",
    "\n",
    "def grade(filepath):\n",
    "    with open(filepath, 'r') as fh:\n",
    "        outputs = [json.loads(line) for line in fh]\n",
    "    eval_data = []\n",
    "    \n",
    "    for out in outputs:\n",
    "        qid = out['question_id']\n",
    "        setup = setup_dict[qid]\n",
    "        model_ans = out.get('parsed_answer')\n",
    "        ans = model_ans.get('answer', \"\").lower().strip() if isinstance(model_ans, dict) else \"\"\n",
    "        # for ac in ANSWER_CLASSES:\n",
    "        #     if ans.startswith(ac):\n",
    "        #         ans = ac\n",
    "        #         break\n",
    "        score = (out['gt_answer'].lower() == ans)\n",
    "        n_src_bin = bin_n_source(len(out['gt_relevant_sources']))\n",
    "        sla_bin = bin_sla_pct(setup['sla'])\n",
    "        eval_data.append({\n",
    "            \"question_id\": qid,\n",
    "            \"review_id\": out['original_review'],\n",
    "            \"question\": out['question'],\n",
    "            \"score\": score,\n",
    "            \"pred_answer\": ans,\n",
    "            \"correct_answer\": out['gt_answer'],\n",
    "            \"evidence_quality\": out['gt_evidence_quality'],\n",
    "            \"review_pmid\": out['original_review'],\n",
    "            \"n_sources\": n_src_bin, # \"no\" if out['gt_answer'] == 'insufficient data' else \n",
    "            \"fulltext_required\": out['gt_fulltext_required'],\n",
    "            \"pred_reasoning\": model_ans,\n",
    "            \"sla_bin\": sla_bin,\n",
    "            \"review_year\": setup['review_year']\n",
    "        })\n",
    "    eval_df = pd.DataFrame(eval_data)\n",
    "    eval_df.loc[:, 'evidence_quality'] = eval_df.evidence_quality.replace('', \"n/a\")\n",
    "    eval_df.loc[~eval_df.pred_answer.str.lower().isin(ANSWER_CLASSES), 'pred_answer'] = DEFAULT_ANS\n",
    "    return eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9f6f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_df(basepath, output_files):\n",
    "    data = []\n",
    "    for (modelname, display_name, model_series, is_reasoning, model_size, is_medical) in output_files:\n",
    "        filepath = basepath.format(MODEL=modelname)\n",
    "        eval_df = grade(filepath)\n",
    "        row = {\n",
    "            \"Model\": display_name,\n",
    "            \"Reason Type\": \"Reasoning\" if is_reasoning else \"Non-Reasoning\",\n",
    "            \"is_medical\": is_medical,\n",
    "            \"Model Size\": model_size if model_size > 0 else None,\n",
    "            \"Model Series\": model_series or display_name,\n",
    "            \"Accuracy\": eval_df.score.mean()\n",
    "        }\n",
    "        \n",
    "        evlevel_df = eval_df.groupby(\"evidence_quality\").agg(\n",
    "            accuracy=pd.NamedAgg(column=\"score\", aggfunc=\"mean\"),\n",
    "            n_correct=pd.NamedAgg(column=\"score\", aggfunc=\"sum\"),\n",
    "            n_questions=pd.NamedAgg(column=\"question\", aggfunc=\"count\")\n",
    "        ).sort_values(\"accuracy\").astype({\"n_correct\": int, \"n_questions\": int})\n",
    "        for evq in EVIDENCE_QUALITIES:\n",
    "            row[evq] = evlevel_df.loc[evq, \"accuracy\"]\n",
    "        \n",
    "        correct_df = eval_df.groupby(\"correct_answer\").agg(\n",
    "            accuracy=pd.NamedAgg(column=\"score\", aggfunc=\"mean\"),\n",
    "            n_correct=pd.NamedAgg(column=\"score\", aggfunc=\"sum\"),\n",
    "            n_questions=pd.NamedAgg(column=\"question\", aggfunc=\"count\")\n",
    "        ).sort_values(\"accuracy\").astype({\"n_correct\": int, \"n_questions\": int})\n",
    "        for cor_ans in ANSWER_CLASSES:\n",
    "            row[cor_ans] = correct_df.loc[cor_ans, \"accuracy\"]\n",
    "        \n",
    "        preds = (eval_df.groupby(\"pred_answer\").agg({\"question\": \"count\"}) / len(eval_df))['question'].to_dict()\n",
    "        row['valid'] = 1 - preds.get('invalid', 0)\n",
    "        \n",
    "        nsrc_df = eval_df.groupby(\"n_sources\").agg(\n",
    "            accuracy=pd.NamedAgg(column=\"score\", aggfunc=\"mean\"),\n",
    "            n_correct=pd.NamedAgg(column=\"score\", aggfunc=\"sum\"),\n",
    "            n_questions=pd.NamedAgg(column=\"question\", aggfunc=\"count\")\n",
    "        ).sort_values(\"accuracy\").astype({\"n_correct\": int, \"n_questions\": int})\n",
    "        for nbin in NSOURCE_BINS:\n",
    "            row[nbin] = nsrc_df.loc[nbin, \"accuracy\"]\n",
    "        \n",
    "        sla_df = eval_df.groupby(\"sla_bin\").agg(\n",
    "            accuracy=pd.NamedAgg(column=\"score\", aggfunc=\"mean\"),\n",
    "            n_correct=pd.NamedAgg(column=\"score\", aggfunc=\"sum\"),\n",
    "            n_questions=pd.NamedAgg(column=\"question\", aggfunc=\"count\")\n",
    "        ).sort_values(\"accuracy\").astype({\"n_correct\": int, \"n_questions\": int})\n",
    "        for sla_bin in SOURCE_AGREEMENT_PCT_BINS:\n",
    "            row[sla_bin] = sla_df.loc[sla_bin, \"accuracy\"]\n",
    "        \n",
    "        data.append(row)\n",
    "\n",
    "    df = pd.DataFrame(data).sort_values(['Accuracy', 'Model'])\n",
    "    palette = sns.color_palette(\"Spectral\", n_colors=len(df))\n",
    "    return df, palette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3ec280",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_long_df(basepath, outfiles):\n",
    "    dfs = []\n",
    "    for (modelname, display_name, model_series, is_reasoning, model_size, is_medical) in outfiles:\n",
    "        filepath = basepath.format(MODEL=modelname)\n",
    "        eval_df = grade(filepath)\n",
    "        eval_df[\"Model\"] = display_name\n",
    "        eval_df[\"Reason Type\"] = \"Reasoning\" if is_reasoning else \"Non-Reasoning\"\n",
    "        eval_df['is_medical'] = is_medical\n",
    "        eval_df[\"Model Size\"] = model_size if model_size > 0 else None\n",
    "        eval_df[\"Model Series\"] = model_series or display_name\n",
    "        acc = eval_df.score.mean()\n",
    "        eval_df[\"Accuracy\"] = acc\n",
    "        dfs.append(eval_df)\n",
    "    long_df = pd.concat(dfs).sort_values(['Accuracy', 'Model'])\n",
    "    model_order = long_df[long_df['question_id'] == 0][['Model', 'is_medical', 'Reason Type']].reset_index(drop=True)\n",
    "    palette = sns.color_palette(\"Spectral\", n_colors=len(model_order))\n",
    "    _df_2 = long_df.copy(deep=True)\n",
    "    _df_2[\"Model\"] = \"Average\"\n",
    "    _df_2[\"is_medical\"] = 0\n",
    "    _df_2[\"Reason Type\"] = \"Average\"\n",
    "    catdf = pd.concat([long_df, _df_2])\n",
    "    avg_palette = palette + ['black']\n",
    "    return long_df, palette, model_order, catdf, avg_palette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ee3b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pub_year_acc(ax, _df, pal):\n",
    "    sns.lineplot(_df, x='review_year', y='score', hue='Model', style='Reason Type', palette=pal, \n",
    "                ax=ax, errorbar=None, markers='.', markersize=10)\n",
    "    ax.legend(loc='lower center', bbox_to_anchor=(0.5, -0.8), ncol=3)\n",
    "    ax.set_xlabel('Systematic Review Publication Year')\n",
    "    ax.set_ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fff49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def percent_valid_plot(ax, df, pal):\n",
    "    # percent valid\n",
    "    sns.barplot(df, x='valid', y='Model', hue='Model', orient='h', ax=ax, palette=pal)\n",
    "    for i, bar in enumerate(ax.patches):\n",
    "        if df.iloc[i].is_medical:\n",
    "            bar.set_edgecolor('black')\n",
    "            bar.set_linewidth(2)\n",
    "        if df.iloc[i]['Reason Type'] == 'Reasoning':\n",
    "            bar.set_hatch('//')\n",
    "    legend_elements = [\n",
    "        Patch(facecolor='grey', hatch='//', label=f'Reasoning'),\n",
    "        Patch(facecolor='grey', hatch='', label=f'Non-Reasoning'),\n",
    "        Patch(facecolor='grey', edgecolor='black', linewidth=2, hatch='', label=f'Medically Finetuned')\n",
    "    ]\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylabel('')\n",
    "    ax.set_xlabel('Percentage Parsable Outputs')\n",
    "\n",
    "    ax.legend(handles=legend_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb5c121",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evidence_quality_plot(ax, df, pal):\n",
    "    # by evidence quality\n",
    "    # evqual_df = df.set_index('Model')[EVIDENCE_QUALITIES]\n",
    "    ax.set_ylabel(\"Accuracy\")\n",
    "    ax.set_xlabel(\"Evidence Certainty Level\")\n",
    "    # sns.lineplot(evqual_df.T, ax=ax, palette=pal, dashes=False, markers='.', markersize=MSIZE)\n",
    "    # ax.plot(evqual_df.mean().values, label=\"Average\", color='k', linestyle='--', linewidth='2', marker='.', markersize=MSIZE-2)\n",
    "    df = df[df.evidence_quality.isin(EVIDENCE_QUALITIES)]\n",
    "    df['srt_evidence_quality'] = pd.Categorical(df['evidence_quality'], categories=EVIDENCE_QUALITIES, ordered=True)\n",
    "    sns.lineplot(df, x='srt_evidence_quality', y='score', hue='Model', style='Reason Type', palette=pal, \n",
    "                ax=ax, errorbar=None, markers='.', markersize=10)\n",
    "    ax.legend(loc='lower center', bbox_to_anchor=(0.5, -0.75), ncol=3)\n",
    "    ax.set_ylim(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebc91a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def percent_source_agreement_plot(ax, df, pal, plot_type='line'):\n",
    "    assert plot_type in ['line', 'bar']\n",
    "    # by % source-level agreement\n",
    "    xname = 'Percentage of Individual Sources Matching the Correct Answer (binned)'\n",
    "    ######## lineplot\n",
    "    # _df = df.set_index('Model')[SOURCE_AGREEMENT_PCT_BINS]\n",
    "    # ax.set_ylabel(\"Accuracy\")\n",
    "    ax.set_xlabel(xname)\n",
    "    # sns.lineplot(_df.T, ax=ax, palette=pal, dashes=False, markers='.', markersize=MSIZE)\n",
    "    # ax.plot(_df.mean().values, label=\"Average\", color='k', linewidth='2', marker='.', markersize=MSIZE-2)\n",
    "    df = df[df.sla_bin.isin(SOURCE_AGREEMENT_PCT_BINS)]\n",
    "    df['srt_sla_bin'] = pd.Categorical(df['sla_bin'], categories=SOURCE_AGREEMENT_PCT_BINS, ordered=True)\n",
    "    sns.lineplot(df, x='srt_sla_bin', y='score', hue='Model', style='Reason Type', palette=pal, \n",
    "            ax=ax, errorbar=None, markers='.', markersize=10)\n",
    "    ax.legend(loc='lower center', bbox_to_anchor=(0.5, -0.75), ncol=3)\n",
    "    ax.set_ylim(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39ef282",
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_sources_plot(ax, df, pal):\n",
    "    # by number of sources\n",
    "    nsource_df = df.set_index('Model')[NSOURCE_BINS]\n",
    "\n",
    "    ax.set_ylabel(\"Accuracy\")\n",
    "    ax.set_xlabel(\"Number of Sources\")\n",
    "    sns.lineplot(df, x='n_src_bin', y='score', hue='Model', style='Reason Type', palette=pal, \n",
    "        ax=ax, errorbar=None, markers='.', markersize=10)\n",
    "    # sns.lineplot(nsource_df.T, ax=ax, palette=pal, dashes=False)\n",
    "    # ax.plot(nsource_df.mean().values, label=\"Average\", color='k', linestyle='--', linewidth='2')\n",
    "    ax.legend(loc='lower center', bbox_to_anchor=(0.5, -0.75), ncol=3)\n",
    "    ax.set_ylim(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a038d322",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_size_plot(ax, df, pal='hls'):\n",
    "    pal = 'hls'\n",
    "    # this is hardcoded and wrong lol\n",
    "    # med_pal = [(0.86, 0.33999999999999997, 0.6207999999999999), (0.33999999999999997, 0.86, 0.5792000000000002)]\n",
    "    ## lines\n",
    "    for v in [7, 32, 70]:\n",
    "        ax.axvline(v, linestyle='--', color='black', linewidth=1, zorder=-10)\n",
    "        ax.annotate(str(v)+\"B\", xy=(v+3, 0.8), rotation=90)\n",
    "    ### plot\n",
    "    size_df = df.dropna().sort_values('Model Size', ascending=False)\n",
    "    sns.lineplot(size_df, x='Model Size', y='Accuracy', hue='Model Series', ax=ax, legend=False, palette=pal)\n",
    "    sns.scatterplot(size_df[size_df.is_medical >= 1], x='Model Size', y='Accuracy', hue='Model Series', palette=pal,\n",
    "                    style='Reason Type', style_order=['Non-Reasoning', 'Reasoning'], ec='black', linewidth=2,s=80)\n",
    "    sns.scatterplot(size_df[size_df.is_medical < 1], x='Model Size', y='Accuracy', hue='Model Series', palette=pal,\n",
    "                    style='Reason Type', style_order=['Non-Reasoning', 'Reasoning'], s=80, ax=ax)\n",
    "    ax.set_ylim(0,1)\n",
    "    ax.set_xlabel(\"Model Size (B)\")\n",
    "    ## legend\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    ax.legend(handles[:5]+handles[9:], labels[:5]+labels[9:], loc='center right', bbox_to_anchor=(1.5, 0.5), frameon=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a47793",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conf_mat(eval_df):\n",
    "    conf_mat_data = []\n",
    "    pred_cls = ANSWER_CLASSES + ['invalid']\n",
    "    gt_cls = ANSWER_CLASSES\n",
    "    for c_label in gt_cls:\n",
    "        row = []\n",
    "        c_label_mask = eval_df.correct_answer.str.lower() == c_label\n",
    "        for p_label in pred_cls:\n",
    "            p_label_mask = eval_df.pred_answer.str.lower() == p_label\n",
    "            row.append((c_label_mask & p_label_mask).sum())\n",
    "        conf_mat_data.append(row)\n",
    "    conf_mat = pd.DataFrame(conf_mat_data, columns=pred_cls, index=gt_cls)\n",
    "    return conf_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94cc860",
   "metadata": {},
   "source": [
    "#### question context size vs model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc305bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from easydict import EasyDict as edict\n",
    "from transformers import GPT2Tokenizer\n",
    "with open(f\"run-med-evidence/datasets/manual_250-fulltext.jsonl\", 'r') as fh:\n",
    "    dset = [edict(json.loads(l)) for l in fh]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d058fc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "get_count = lambda s: len(tokenizer.encode(s, add_special_tokens=False, verbose=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498899c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data = []\n",
    "for rd in tqdm(dset):\n",
    "    sources = {s.article_id: s.content for s in rd.sources}\n",
    "    for q in rd.question_data:\n",
    "        full_text = '\\n'.join(sources[pmid] for pmid in q.relevant_sources)\n",
    "        output_data.append({\n",
    "            'question_id': q.question_id,\n",
    "            'context_tokens': get_count(full_text)\n",
    "        })\n",
    "token_data = pd.DataFrame(output_data).sort_values('context_tokens')\n",
    "token_data['ix'] = np.arange(len(token_data))\n",
    "token_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72850db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_QUANTILES = 5\n",
    "token_data['token_bin'] = (token_data['ix'] * N_QUANTILES)//len(token_data)\n",
    "threshs = np.linspace(0,100,N_QUANTILES+1).astype(int)\n",
    "bin_names = [f\"{threshs[i]}-{threshs[i+1]}%\" for i in range(N_QUANTILES)]\n",
    "token_data['token_bin'] = token_data['token_bin'].map(dict(zip(range(N_QUANTILES), bin_names)))\n",
    "token_data['token_bin'] = pd.Categorical(token_data['token_bin'], categories=bin_names, ordered=True)\n",
    "\n",
    "\n",
    "outfiles = [\n",
    "    # format: (model, display name, model_series, is_reasoning, model_size in B, is_medical) [-1 = do not include]\n",
    "    (\"qwq32b\", \"QwQ-32B\", '', 1, 32, 0),\n",
    "    (\"qwen3_235b\", \"Qwen3-235B-A22B-FP8\", '', 1, 235, 0),\n",
    "    (\"llama70b_r1distill\", \"DeepSeek R1-Distill Llama 3.3 70B\", '', 1, 70, 0),\n",
    "    (\"llama70b_instruct\", \"Llama 3.3 70B-Instruct\", '', 0, 70, 0),\n",
    "    (\"llama4scout\", \"Llama 4 Scout\", 'Llama 4', 0, 109, 0),\n",
    "    (\"llama4mav\", \"Llama 4 Maverick\", 'Llama 4', 0, 400, 0),\n",
    "    (\"qwen7b\", \"Qwen2.5-7B-Instruct\", 'Qwen2.5', 0, 7, 0),\n",
    "    (\"qwen72b\", \"Qwen2.5-72B-Instruct\", 'Qwen2.5', 0, 72, 0),\n",
    "    (\"llama3_1_8b\", \"Llama 3.1 8B\", 'Llama 3.1', 0, 8, 0),\n",
    "    (\"llama3_1_70b\", \"Llama 3.1 70B\", 'Llama 3.1', 0, 70, 0),\n",
    "    (\"llama3_1_405b\", \"Llama 3.1 405B\", 'Llama 3.1', 0, 405, 0),\n",
    "    (\"llama3_8b\", \"Llama 3.0 8B\", 'Llama 3.0', 0, 8, 0),\n",
    "    (\"llama3_70b\", \"Llama 3.0 70B\", 'Llama 3.0', 0, 70, 0),\n",
    "    (\"deepseekR1\", \"DeepSeek R1\", '', 1, 671, 0),\n",
    "    (\"deepseekV3\", \"DeepSeek V3\", '', 0, 671, 0),\n",
    "    (\"gpto1\", \"GPT-o1\", '', 1, -1, 0),\n",
    "    (\"gpt4_1mini\", \"GPT-4.1 mini\", 'GPT-4.1', 0, -1, 0),\n",
    "    (\"gpt4_1\", \"GPT-4.1\", 'GPT-4.1', 0, -1, 0),\n",
    "    (\"openthinker2_32b\", \"OpenThinker2-32B\", '', 1, 32, 0),\n",
    "    (\"qwen32b\", \"Qwen2.5-32B-Instruct\", 'Qwen2.5', 0, 32, 0),\n",
    "    (\"huatuo7b\", \"HuatuoGPT-o1-7B\", '', 1, 6.99, 1),\n",
    "    (\"huatuo70b\", \"HuatuoGPT-o1-70B\", '', 1, 69.99, 1),\n",
    "    (\"openbiollm_8b\", \"OpenBioLLM 8B\", '', 0, 7.99, 1),\n",
    "    (\"openbiollm_70b\", \"OpenBioLLM 70B\", '', 0, 69.99, 1),\n",
    "]\n",
    "\n",
    "dfs = []\n",
    "for (modelname, display_name, model_series, is_reasoning, model_size, is_medical) in outfiles:\n",
    "    filepath = basepath.format(MODEL=modelname)\n",
    "    eval_df = grade(filepath)\n",
    "    eval_df = eval_df.merge(token_data[['question_id', 'token_bin']], on='question_id')\n",
    "    eval_df[\"Model\"] = display_name\n",
    "    eval_df[\"Reason Type\"] = \"Reasoning\" if is_reasoning else \"Non-Reasoning\"\n",
    "    eval_df['is_medical'] = is_medical\n",
    "    eval_df[\"Model Size\"] = model_size if model_size > 0 else None\n",
    "    eval_df[\"Model Series\"] = model_series or display_name\n",
    "    acc = eval_df.score.mean()\n",
    "    eval_df[\"Accuracy\"] = acc\n",
    "    dfs.append(eval_df)\n",
    "long_df = pd.concat(dfs).sort_values(['Accuracy', 'Model'])\n",
    "model_order = long_df[long_df['question_id'] == 0][['Model', 'is_medical', 'Reason Type']].reset_index(drop=True)\n",
    "palette = sns.color_palette(\"Spectral\", n_colors=len(model_order))\n",
    "print(model_order.Model.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e807f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "_df = long_df\n",
    "_df_2 = _df.copy(deep=True)\n",
    "_df_2[\"Model\"] = \"Average\"\n",
    "_df_2[\"is_medical\"] = 0\n",
    "_df_2[\"Reason Type\"] = \"Average\"\n",
    "catdf = pd.concat([_df, _df_2])\n",
    "avg_palette = palette + ['black']\n",
    "fake_ax = sns.lineplot(catdf, x='token_bin', y='score', hue='Model', style='Reason Type',\n",
    "                errorbar=None, palette=avg_palette)\n",
    "handles, labels = fake_ax.get_legend_handles_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be34a0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sets = [\n",
    "    (long_df['Model Size'] < 10, '7-8B'),\n",
    "    ((long_df['Model Size'] > 10) & (long_df['Model Size'] < 40), '32B'),\n",
    "    ((long_df['Model Size'] > 40) & (long_df['Model Size'] < 80), '70-72B'),\n",
    "    ((long_df['Model Size'] > 100), '100B+'),\n",
    "]\n",
    "fig, axs = plt.subplots(nrows=2, ncols=2, figsize=(12,12), dpi=200)\n",
    "for i, (mask, mtype) in enumerate(model_sets):\n",
    "    ax = axs[i//2, i%2]\n",
    "    _df = long_df[mask]\n",
    "    # pal = sns.color_palette('husl', n_colors=len(_df.Model.unique()))\n",
    "    colors = []\n",
    "    for m in _df.Model.unique():\n",
    "        ix = model_order.set_index('Model').index.get_loc(m)\n",
    "        colors.append(palette[ix])\n",
    "    _df_2 = _df.copy(deep=True)\n",
    "    _df_2[\"Model\"] = \"Average\"\n",
    "    _df_2[\"is_medical\"] = 0\n",
    "    _df_2[\"Reason Type\"] = \"Average\"\n",
    "\n",
    "    catdf = pd.concat([_df, _df_2])\n",
    "    avg_palette = colors + ['black']\n",
    "    sns.lineplot(catdf, ax=ax, x='token_bin', y='score', hue='Model', style='Reason Type',\n",
    "                errorbar=None, palette=avg_palette)\n",
    "    # ax.plot(evqual_df.mean().values, label=\"Average\", color='k', linestyle='--', linewidth='2')\n",
    "    # ax.legend(loc='lower center', bbox_to_anchor=(0.5, -0.5), ncol=3)\n",
    "    ax.set_xlabel('Total Number of Context Tokens (Quantile Bins)')\n",
    "    ax.legend('off')\n",
    "    if i > -1:\n",
    "        ax.set_ylabel('Accuracy')\n",
    "    ax.set_title(f'{mtype} models')\n",
    "    ax.set_ylim(0, 0.7)\n",
    "    ax.legend().set_visible(False)\n",
    "fig.legend(handles, labels, loc='lower center', bbox_to_anchor=(0.5,-0.1), ncol=4)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56213693",
   "metadata": {},
   "source": [
    "#### run stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f292fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_basic,_ = make_df(\"run-med-evidence/___res/{MODEL}_closed_barebones_fulltext.jsonl\", outfiles)\n",
    "# df_basic = df_basic[['Model', 'Accuracy']]\n",
    "# df_expguided,_ = make_df(\"run-med-evidence/___res/{MODEL}_closed.jsonl\", outfiles)\n",
    "# df_expguided = df_expguided[['Model', 'Accuracy']]\n",
    "\n",
    "# joint = df_basic.merge(df_expguided, on='Model', suffixes=(' (Basic)', ' (Expert Guided)'))\n",
    "# joint[\"Diff (expert-basic)\"] = joint[\"Accuracy (Expert Guided)\"] - joint['Accuracy (Basic)']\n",
    "# joint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59cd110",
   "metadata": {},
   "outputs": [],
   "source": [
    "outfiles = [\n",
    "    # format: (model, display name, model_series, is_reasoning, model_size in B, is_medical) [-1 = do not include]\n",
    "    (\"deepseekR1\", \"DeepSeek R1\", '', 1, 671, 0),\n",
    "    (\"deepseekV3\", \"DeepSeek V3\", '', 0, 671, 0),\n",
    "    (\"gpt4_1\", \"GPT-4.1\", 'GPT-4.1', 0, -1, 0),\n",
    "    (\"gpt4_1mini\", \"GPT-4.1 mini\", 'GPT-4.1', 0, -1, 0),\n",
    "    (\"gpto1\", \"GPT-o1\", '', 1, -1, 0),\n",
    "    (\"huatuo7b\", \"HuatuoGPT-o1-7B\", '', 1, 6.99, 1),\n",
    "    (\"huatuo70b\", \"HuatuoGPT-o1-70B\", '', 1, 69.99, 1),\n",
    "    (\"llama3_1_8b\", \"Llama 3.1 8B\", 'Llama 3.1', 0, 8, 0),\n",
    "    (\"llama3_1_70b\", \"Llama 3.1 70B\", 'Llama 3.1', 0, 70, 0),\n",
    "    (\"llama3_1_405b\", \"Llama 3.1 405B\", 'Llama 3.1', 0, 405, 0),\n",
    "    (\"llama3_8b\", \"Llama 3.0 8B\", 'Llama 3.0', 0, 8, 0),\n",
    "    (\"llama3_70b\", \"Llama 3.0 70B\", 'Llama 3.0', 0, 70, 0),\n",
    "    (\"llama4mav\", \"Llama 4 Maverick\", 'Llama 4', 0, 400, 0),\n",
    "    (\"llama4scout\", \"Llama 4 Scout\", 'Llama 4', 0, 109, 0),\n",
    "    (\"llama70b_instruct\", \"Llama 3.3 70B-Instruct\", '', 0, 70, 0),\n",
    "    (\"llama70b_r1distill\", \"Llama 3.3 70B (R1-Distill)\", '', 1, 70, 0),\n",
    "    (\"openbiollm_8b\", \"OpenBioLLM 8B\", '', 0, 7.99, 1),\n",
    "    (\"openbiollm_70b\", \"OpenBioLLM 70B\", '', 0, 69.99, 1),\n",
    "    (\"openthinker2_32b\", \"OpenThinker2-32B\", '', 1, 32, 0),\n",
    "    (\"qwen3_235b\", \"Qwen3-235B-A22B-FP8\", '', 1, 235, 0),\n",
    "    (\"qwen7b\", \"Qwen2.5-7B-Instruct\", 'Qwen2.5', 0, 7, 0),\n",
    "    (\"qwen32b\", \"Qwen2.5-32B-Instruct\", 'Qwen2.5', 0, 32, 0),\n",
    "    (\"qwen72b\", \"Qwen2.5-72B-Instruct\", 'Qwen2.5', 0, 72, 0),\n",
    "    (\"qwq32b\", \"QwQ-32B\", '', 1, 32, 0),\n",
    "]\n",
    "\n",
    "analysis_sets = [\n",
    "    (\"basic prompts\", \"run-med-evidence/___res/{MODEL}_closed_barebones_fulltext.jsonl\"),\n",
    "    # (\"expert-guided\", \"run-med-evidence/___res/{MODEL}_closed.jsonl\")\n",
    "]\n",
    "basepath = analysis_sets[0][1]\n",
    "long_df, palette, model_order, catdf, avg_palette = make_long_df(basepath, outfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216ad2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_acc_plot(ax, df):\n",
    "    sns.barplot(df, x='score', y='Model', hue='Model', errorbar=('ci', 95), n_boot=1000, palette=palette, ax=ax,\n",
    "                err_kws=dict(linewidth=1, color='#000000'), capsize=0.3)\n",
    "\n",
    "    for i, bar in enumerate(ax.patches):\n",
    "        if model_order.iloc[i].is_medical:\n",
    "            bar.set_edgecolor('black')\n",
    "            bar.set_linewidth(2)\n",
    "        if model_order.iloc[i]['Reason Type'] == 'Reasoning':\n",
    "            bar.set_hatch('//')\n",
    "    legend_elements = [\n",
    "        Patch(facecolor='grey', hatch='//', label=f'Reasoning'),\n",
    "        Patch(facecolor='grey', hatch='', label=f'Non-Reasoning'),\n",
    "        Patch(facecolor='grey', edgecolor='black', linewidth=2, hatch='', label=f'Medically Finetuned')\n",
    "    ]\n",
    "    # ax.set_xlim(0, 1)\n",
    "    ax.set_xlabel('Overall Accuracy')\n",
    "    ax.set_ylabel('')\n",
    "\n",
    "    # Add the legend\n",
    "    ax.legend(handles=legend_elements)\n",
    "\n",
    "def avg_recall_plot(ax, df):\n",
    "    avgs = df[df.Model=='Average']\n",
    "    sns.barplot(avgs, x='correct_answer', y='score', ax=ax, color='lightgrey', linewidth=1,\n",
    "                order=['uncertain effect', 'no difference', 'insufficient data', 'higher', 'lower'],\n",
    "                errorbar=('ci', 95), err_kws=dict(linewidth=1.5, color='#000000'), capsize=0.3)\n",
    "    ax.set_xlabel('Treatment Outcome Effect')\n",
    "    ax.set_xticklabels(['Uncertain\\nEffect', 'No Difference', 'Insufficient\\nData', 'higher', 'lower'], fontsize=8, rotation=30)\n",
    "    ax.set_ylabel('Average Recall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddeb308c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=2, figsize=(15,7), dpi=200, width_ratios=[2,1])\n",
    "main_acc_plot(axs[0], long_df)\n",
    "avg_recall_plot(axs[1], catdf)\n",
    "axs[0].set_title('(a)', loc='left', fontsize=15)\n",
    "axs[1].set_title('(b)', loc='left', fontsize=15)\n",
    "fig.align_xlabels()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8560e3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(dpi=200)\n",
    "evidence_quality_plot(ax, catdf, pal=avg_palette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5988c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "MSIZE = 10\n",
    "# df, pal = make_df(basepath, outfiles)\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(15,5), dpi=200)\n",
    "evidence_quality_plot(ax[0], catdf, pal=avg_palette)\n",
    "percent_source_agreement_plot(ax[1], catdf, pal=avg_palette)\n",
    "handles, labels = ax[0].get_legend_handles_labels()\n",
    "ax[0].legend().set_visible(False)\n",
    "ax[0].set_xlabel(\"Evidence Certainty Level\")\n",
    "ax[0].set_title('(a)', loc='left', fontsize=15)\n",
    "ax[1].set_xlabel(\"Percentage of Individual Sources Matching the Correct Answer (Binned)\")\n",
    "ax[1].set_ylabel(\"\")\n",
    "ax[1].set_title('(b)', loc='left', fontsize=15)\n",
    "ax[1].legend().set_visible(False)\n",
    "fig.legend(handles[1:-5]+handles[-3:], labels[1:-5]+labels[-3:], loc='lower center', bbox_to_anchor=(0.5, -0.25), ncol=6)\n",
    "fig.show()\n",
    "fig.savefig('figs/certainty_and_sla.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0e4e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "### APPENDIX VERSION OF PER-CLASS PERFORMANCE\n",
    "fig, ax = plt.subplots(figsize=(25,5), dpi=200)\n",
    "# AVG_COLOR = 'white'\n",
    "# long_df_2 = long_df.copy(deep=True)\n",
    "# long_df_2[\"Model\"] = \"Average\"\n",
    "# long_df_2[\"is_medical\"] = 0\n",
    "# long_df_2[\"Reason Type\"] = \"Non-Reasoning\"\n",
    "\n",
    "# catdf = pd.concat([long_df, long_df_2])\n",
    "mypal = list(palette) + ['white']\n",
    "\n",
    "sns.barplot(catdf, x='correct_answer', y='score', hue='Model', palette=mypal, ax=ax,\n",
    "            order=['uncertain effect', 'no difference', 'insufficient data', 'lower', 'higher'],\n",
    "            errorbar=('ci', 95), err_kws=dict(linewidth=1.5, color='#000000'), capsize=0.3)\n",
    "\n",
    "for i, bars in enumerate(ax.containers):\n",
    "    # Set a different hatch for each group of bars\n",
    "    for bar in bars:\n",
    "        if i >= len(model_order):\n",
    "            bar.set_edgecolor('black')\n",
    "            bar.set_linewidth(1)\n",
    "            continue\n",
    "        if model_order.iloc[i].is_medical:\n",
    "            bar.set_edgecolor('black')\n",
    "            bar.set_linewidth(2)\n",
    "        if model_order.iloc[i]['Reason Type'] == 'Reasoning':\n",
    "            bar.set_hatch('//')\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "# Modify the patch properties\n",
    "for i, handle in enumerate(handles):\n",
    "    if i >= len(model_order):\n",
    "        handle.set_edgecolor('black')\n",
    "        handle.set_linewidth(1)\n",
    "        continue\n",
    "    if model_order.iloc[i].is_medical:\n",
    "        handle.set_edgecolor('black')\n",
    "        handle.set_linewidth(2)\n",
    "    if model_order.iloc[i]['Reason Type'] == 'Reasoning':\n",
    "        handle.set_hatch('//')\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_xlabel('Correct Answer')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.legend(loc='lower center', bbox_to_anchor=(0.5, -0.4), ncol=7)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c8f5f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681c4c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "pal = palette\n",
    "fig, ax = plt.subplots(dpi=200, figsize=(4.1,5))\n",
    "pairs = [\n",
    "    \"OpenBioLLM 8B\", \"Llama 3.0 8B\",\n",
    "    \"HuatuoGPT-o1-7B\", \"Qwen2.5-7B-Instruct\",\n",
    "    \"OpenBioLLM 70B\", \"Llama 3.0 70B\",\n",
    "    \"HuatuoGPT-o1-70B\", \"Llama 3.1 70B\"\n",
    "]\n",
    "medpairdf = long_df.set_index('Model').loc[pairs, :].reset_index(names='Model')\n",
    "medmodelorder = model_order.set_index('Model').loc[pairs,:].reset_index()\n",
    "colors = []\n",
    "for p in pairs:\n",
    "    ix = model_order.set_index('Model').index.get_loc(p)\n",
    "    colors.append(pal[ix])\n",
    "sns.barplot(medpairdf, y='Model', x='score', hue='Model', palette=colors, legend=False, orient='h', ax=ax,\n",
    "            errorbar=('ci', 95), err_kws=dict(linewidth=1.5, color='#000000'), capsize=0.3)\n",
    "for i, bar in enumerate(ax.patches[-len(medpairdf):]):\n",
    "    if medmodelorder.iloc[i].is_medical:\n",
    "        bar.set_edgecolor('black')\n",
    "        bar.set_linewidth(2)\n",
    "    if medmodelorder.iloc[i]['Reason Type'] == 'Reasoning':\n",
    "        bar.set_hatch('//')\n",
    "for i in range(len(pairs)//2):\n",
    "    if i % 2 == 1:\n",
    "        ax.axhspan(2*i-0.5, 2*i+1.5, facecolor='lightgrey', alpha=0.5, edgecolor=None, zorder=0)\n",
    "legend_elements = [\n",
    "    Patch(facecolor='grey', hatch='//', label=f'Reasoning'),\n",
    "    Patch(facecolor='grey', hatch='', label=f'Non-Reasoning'),\n",
    "    Patch(facecolor='grey', edgecolor='black', linewidth=2, hatch='', label=f'Medically Finetuned')\n",
    "]\n",
    "ax.legend(handles=legend_elements)\n",
    "# ax.set_xlim(0,1)\n",
    "ax.set_xlabel('Accuracy')\n",
    "ax.set_ylabel('')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73e9dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "basepath = analysis_sets[0][1]\n",
    "all_mats = []\n",
    "for (modelname, display_name, model_series, is_reasoning, model_size, is_medical) in outfiles:\n",
    "    filepath = basepath.format(MODEL=modelname)\n",
    "    eval_df = grade(filepath)\n",
    "    acc = eval_df.score.mean()\n",
    "    if acc < 0.4:\n",
    "        continue\n",
    "    mat = conf_mat(eval_df)\n",
    "    all_mats.append(mat)\n",
    "    # fig, ax = plt.subplots()\n",
    "    # sns.heatmap(mat, annot=True, ax=ax)\n",
    "    # ax.set_xlabel('Correct')\n",
    "    # ax.set_ylabel('Predicted')\n",
    "    # ax.set_title(modelname)\n",
    "    # fig.show()\n",
    "\n",
    "avg_mat = functools.reduce(lambda x, y: x.add(y, fill_value=0), all_mats)  / len(all_mats)\n",
    "fig, ax = plt.subplots(dpi=200)\n",
    "sns.heatmap(avg_mat, annot=True, ax=ax)\n",
    "ax.set_xlabel('Predicted')\n",
    "ax.set_ylabel('Correct')\n",
    "# ax.set_title(\"Average\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cc2caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import matplotlib.colors as colors\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "joint_key = \"score\"\n",
    "joint_basecols = [\"question_id\"]\n",
    "joint_df = pd.DataFrame()\n",
    "cmap = colors.ListedColormap(['black', 'lightblue'])\n",
    "for (modelname, display_name, model_series, is_reasoning, model_size, is_medical) in outfiles:\n",
    "    filepath = basepath.format(MODEL=modelname)\n",
    "    eval_df = grade(filepath)\n",
    "    if joint_df.empty:\n",
    "        joint_df = eval_df[joint_basecols]\n",
    "    _subset = eval_df[[\"question_id\", joint_key]].rename(columns={joint_key: display_name})\n",
    "    joint_df = joint_df.merge(_subset, how='outer', on=[\"question_id\"])\n",
    "joint_df['sum'] = joint_df.iloc[:, 1:].sum(axis=1)\n",
    "joint_df = joint_df.sort_values('sum').reset_index(drop=True)\n",
    "fig, ax = plt.subplots(dpi=200)\n",
    "tickformat = lambda x, _: 'Incorrect' if x < 0.5 else 'Correct'\n",
    "sns.heatmap(joint_df.loc[:, model_order.Model.values], cmap=cmap, ax=ax, cbar_kws={'ticks': [0.25, .75], 'format': FuncFormatter(tickformat)})\n",
    "ax.set_yticks([])\n",
    "ax.set_ylabel('Question')\n",
    "# all_wrong_mask = ~joint_df.set_index('question_id').any(axis=1).values\n",
    "# Counter(eval_df[all_wrong_mask].correct_answer.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a159e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_funcs = [\n",
    "    evidence_quality_plot,\n",
    "    # percent_source_agreement_plot,\n",
    "    # num_sources_plot,\n",
    "    # model_size_plot,\n",
    "    # percent_valid_plot,\n",
    "]\n",
    "df, pal = make_df(basepath, outfiles)\n",
    "for plot_func in plot_funcs:\n",
    "    fig, ax = plt.subplots(dpi=200)\n",
    "    plot_func(ax, df, pal=pal)\n",
    "    # fig.suptitle(f\"{plotname} - {name}\")\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28d3dae",
   "metadata": {},
   "source": [
    "### random stats stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86dbce3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6514ac6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.quantile(setupdf.context_tokens.values, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b51cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "grade(basepath.format(MODEL='deepseekV3')).groupby('sla_bin').agg({'score':'mean'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ae2317",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "\n",
    "model_scores = {}\n",
    "basepath = \"run-med-evidence/___res/{MODEL}_closed_barebones_fulltext.jsonl\"\n",
    "for (modelname, display_name, model_series, is_reasoning, model_size, is_medical) in outfiles:\n",
    "    filepath = basepath.format(MODEL=modelname)\n",
    "    eval_df = grade(filepath)\n",
    "    model_scores[display_name] = eval_df.score.astype(int).tolist()\n",
    "scoredf = pd.DataFrame(model_scores)\n",
    "scoredf.to_csv('___tmp/model_raw_scores.csv')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eddd04b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "\n",
    "rng = np.random.default_rng(seed=0)\n",
    "def compute_ci(a):\n",
    "    M = 1000\n",
    "    N = len(a)\n",
    "    p_samples = rng.choice(a, size=(M, N), replace=True).mean(axis=1)\n",
    "    iv = scipy.stats.t.interval(0.95, len(a)-1, loc=np.mean(a), scale=scipy.stats.sem(a))\n",
    "    return iv # InterVal\n",
    "    \n",
    "\n",
    "data = []\n",
    "CLASS = 'no difference'\n",
    "for (modelname, display_name, model_series, is_reasoning, model_size, is_medical) in outfiles:\n",
    "    filepath = basepath.format(MODEL=modelname)\n",
    "    eval_df = grade(filepath)\n",
    "    eval_df = eval_df[eval_df.correct_answer == CLASS]\n",
    "    scores = eval_df.score.values\n",
    "    low, high = compute_ci(scores)\n",
    "    data.append({\n",
    "        'Model': display_name,\n",
    "        'Accuracy': scores.mean(),\n",
    "        'Lower CI': low,\n",
    "        'Upper CI': high\n",
    "    })\n",
    "pd.DataFrame(data).set_index('Model').loc[model_order.Model.values,:].to_csv('___tmp/CIs.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medev2",
   "language": "python",
   "name": "medev2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
