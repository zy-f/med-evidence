question_id,question,answer,evidence_certainty,fulltext_required,relevant_sources,original_review,review_year,sources,source_concordance,medical_specialty
0,"Is the long-term rate of overall lymphocyst formation higher, lower, or the same when comparing retroperitoneal drainage to no drainage?",no difference,high,no,['17466514'],28660687,2017,"{'17466514': {'article_id': '17466514', 'content': ""Drainage, following radical hysterectomy and pelvic lymph node dissection to prevent postoperative lymphocyst formation and surgical morbidity, is controversial. To study the clinical significance of drainage, 253 patients were registered and 234 patients were randomised into two arms. In one arm (n=117) postoperative drainage was performed, in the other arm (n=117) no drains were inserted. In both arms closure of the peritoneum of the operating field was omitted. The main exclusion criteria were blood loss of more than 3000 ml during surgery or persistent oozing at the end of the operation. Clinical and ultrasound or CT-scan evaluation was done at one and 12 months postoperatively. The median follow-up amounted to 13.3 months. No difference in the incidence of postoperative lymphocyst formation or postoperative complications was found between the two study arms. The late (12 months) incidence of symptomatic lymphocysts was 3.4% (drains: 5.9%; no drains: 0.9%). The difference showed a p-value of 0.06 in Fisher's Exact test. The operating time was related to the occurrence of postoperative lymphocyst formation. It was concluded that drains can be safely omitted following radical hysterectomy and pelvic node dissection without pelvic reperitonisation in patients without excessive bleeding during or oozing at the end of surgery."", 'title': 'Randomised trial of drains versus no drains following radical hysterectomy and pelvic lymph node dissection: a European Organisation for Research and Treatment of Cancer-Gynaecological Cancer Group (EORTC-GCG) study in 234 patients.', 'date': '2007-05-01'}}",1.0,Surgery
1,"Is the short-term rate of overall lymphocyst formation higher, lower, or the same when comparing retroperitoneal drainage to no drainage?",no difference,moderate,no,"['9190979', '12214830']",28660687,2017,"{'9190979': {'article_id': '9190979', 'content': 'To evaluate the clinical effectiveness of retroperitoneal drainage following lymphadenectomy in gynecologic surgery.\nOne hundred thirty-seven consecutive patients undergoing systematic lymphadenectomy for gynecologic malignancies were randomized to receive (Group A, 68) or not (Group B, 69) retroperitoneal drainage. The pelvic peritoneum and the paracolic gutters were not sutured after node dissection. Perioperative data and complications were recorded.\nClinical and surgical parameters were comparable in the two groups. Postoperative hospital stay was significantly shorter in Group B (P < 0.001), whereas the complication rate was significantly higher in Group A (P = 0.01). This was mainly due to a significant increase in lymphocyst and lymphocyst-related morbidity. Sonographic monitoring for lymphocyst showed free abdominal fluid in 18% of drained and 36% of not-drained patients (P = 0.03). Symptomatic ascites developed in 2 drained (3%) and 3 not-drained (4%) patients (NS), respectively.\nProphylactic drainage of the retroperitoneum seems to increase lymphadenectomy-related morbidity and postoperative stay. Therefore, routine drainage following lymphadenectomy seems to be no longer indicated when the retroperitoneum is left open.', 'title': 'A randomized study comparing retroperitoneal drainage with no drainage after lymphadenectomy in gynecologic malignancies.', 'date': '1997-06-01'}, '12214830': {'article_id': '12214830', 'content': 'To evaluate the postoperative morbidity and lymphocyst formation in invasive cervical cancer patients undergoing radical hysterectomy and pelvic lymphadenectomy (RHPL) with no drainage and no peritonization compared with retroperitoneal drainage and peritonization.\nBetween July 1999 and May 2000, 100 patients with stage IA-IIA cervical cancer undergoing RHPL in Chiang Mai University Hospital were prospectively randomized to receive either no peritonization and no drainage (Group A = 48 cases) or retroperitoneal drainage and peritonization (Group B = 52 cases). Perioperative data and morbidity were recorded. Transabdominal and transvaginal sonography were performed at 4, 8 and 12 weeks postoperatively to detect lymphocyst formation.\nBoth groups were similar regarding age, size and gross appearance of tumor, tumor histology and stage. There was no difference between groups in respect of operative time, need for blood transfusion, intraoperative complications, hospital stay, number of nodes removed, nodal metastases, and need for adjuvant radiation and chemotherapy. Asymptomatic lymphocysts were sonographically detected at 4, 8 and 12 weeks postoperatively in 3 (6.8%), 2 (4.6%), and 3 (7.7%) of 44, 43, and 39 patients, respectively in Group A, whereas none was found in Group B (P = 0.2). No significant difference was found in term of postoperative morbidity in the two groups.\nRoutine retroperitoneal drainage and peritonization after RHPL for invasive cervical cancer can be safely omitted.', 'title': 'A prospective randomized study comparing retroperitoneal drainage with no drainage and no peritonization following radical hysterectomy and pelvic lymphadenectomy for invasive cervical cancer.', 'date': '2002-09-07'}}",0.5,Surgery
2,"Is the rate of clinician impression of cognitive change higher, lower, or the same when comparing cholinase inhibitors to placebo in patients with MS?",higher,high,no,"['15534239', '21519001']",25734590,2015,"{'15534239': {'article_id': '15534239', 'content': 'To determine the effect of donepezil in treating memory and cognitive dysfunction in multiple sclerosis (MS).\nThis single-center double-blind placebo-controlled clinical trial evaluated 69 MS patients with cognitive impairment who were randomly assigned to receive a 24-week treatment course of either donepezil (10 mg daily) or placebo. Patients underwent neuropsychological assessment at baseline and after 24 weeks of treatment. The primary outcome was change in verbal learning and memory on the Selective Reminding Test (SRT). Secondary outcomes included other tests of cognitive function, patient-reported change in memory, and clinician-reported impression of cognitive change.\nDonepezil-treated patients showed significant improvement in memory performance on the SRT compared to placebo (p = 0.043). The benefit of donepezil remained significant after controlling for various covariates including age, Expanded Disability Status Scale, baseline SRT score, reading ability, MS subtype, and sex. Donepezil-treated patients did not show significant improvements on other cognitive tests, but were more than twice as likely to report memory improvement than those in the placebo group (p = 0.006). The clinician also reported cognitive improvement in almost twice as many donepezil vs placebo patients (p = 0.036). No serious adverse events related to study medication occurred, although more donepezil (34.3%) than placebo (8.8%) subjects reported unusual/abnormal dreams (p = 0.010).\nDonepezil improved memory in MS patients with initial cognitive impairment in a single center clinical trial. A larger multicenter investigation of donepezil in MS is warranted in order to more definitively assess the efficacy of this intervention.', 'title': 'Donepezil improved memory in multiple sclerosis in a randomized clinical trial.', 'date': '2004-11-10'}, '21519001': {'article_id': '21519001', 'content': ""The goal of this study was to determine if memory would be improved by donepezil as compared to placebo in a multicenter, double-blind, randomized clinical trial (RCT).\nDonepezil 10 mg daily was compared to placebo to treat memory impairment. Eligibility criteria included the following: age 18-59 years, clinically definite multiple sclerosis (MS), and performance ≤ ½ SD below published norms on the Rey Auditory Verbal Learning Test (RAVLT). Neuropsychological assessments were performed at baseline and 24 weeks. Primary outcomes were change on the Selective Reminding Test (SRT) of verbal memory and the participant's impression of memory change. Secondary outcomes included changes on other neuropsychological tests and the evaluating clinician's impression of memory change.\nA total of 120 participants were enrolled and randomized to either donepezil or placebo. No significant treatment effects were found between groups on either primary outcome of memory or any secondary cognitive outcomes. A trend was noted for the clinician's impression of memory change in favor of donepezil (37.7%) vs placebo (23.7%) (p = 0.097). No serious or unanticipated adverse events attributed to study medication developed.\nDonepezil did not improve memory as compared to placebo on either of the primary outcomes in this study.\nThis study provides Class I evidence which does not support the hypothesis that 10 mg of donepezil daily for 24 weeks is superior to placebo in improving cognition as measured by the SRT in people with MS whose baseline RAVLT score was 0.5 SD or more below average."", 'title': 'Multicenter randomized clinical trial of donepezil for memory impairment in multiple sclerosis.', 'date': '2011-04-27'}}",0.5,Psychiatry & Neurology
3,"Is the rate of patient self-reported impression of memory change higher, lower, or the same when comparing cholinase inhibitors to placebo in patients with MS?",uncertain effect,high,no,"['15534239', '21519001']",25734590,2015,"{'15534239': {'article_id': '15534239', 'content': 'To determine the effect of donepezil in treating memory and cognitive dysfunction in multiple sclerosis (MS).\nThis single-center double-blind placebo-controlled clinical trial evaluated 69 MS patients with cognitive impairment who were randomly assigned to receive a 24-week treatment course of either donepezil (10 mg daily) or placebo. Patients underwent neuropsychological assessment at baseline and after 24 weeks of treatment. The primary outcome was change in verbal learning and memory on the Selective Reminding Test (SRT). Secondary outcomes included other tests of cognitive function, patient-reported change in memory, and clinician-reported impression of cognitive change.\nDonepezil-treated patients showed significant improvement in memory performance on the SRT compared to placebo (p = 0.043). The benefit of donepezil remained significant after controlling for various covariates including age, Expanded Disability Status Scale, baseline SRT score, reading ability, MS subtype, and sex. Donepezil-treated patients did not show significant improvements on other cognitive tests, but were more than twice as likely to report memory improvement than those in the placebo group (p = 0.006). The clinician also reported cognitive improvement in almost twice as many donepezil vs placebo patients (p = 0.036). No serious adverse events related to study medication occurred, although more donepezil (34.3%) than placebo (8.8%) subjects reported unusual/abnormal dreams (p = 0.010).\nDonepezil improved memory in MS patients with initial cognitive impairment in a single center clinical trial. A larger multicenter investigation of donepezil in MS is warranted in order to more definitively assess the efficacy of this intervention.', 'title': 'Donepezil improved memory in multiple sclerosis in a randomized clinical trial.', 'date': '2004-11-10'}, '21519001': {'article_id': '21519001', 'content': ""The goal of this study was to determine if memory would be improved by donepezil as compared to placebo in a multicenter, double-blind, randomized clinical trial (RCT).\nDonepezil 10 mg daily was compared to placebo to treat memory impairment. Eligibility criteria included the following: age 18-59 years, clinically definite multiple sclerosis (MS), and performance ≤ ½ SD below published norms on the Rey Auditory Verbal Learning Test (RAVLT). Neuropsychological assessments were performed at baseline and 24 weeks. Primary outcomes were change on the Selective Reminding Test (SRT) of verbal memory and the participant's impression of memory change. Secondary outcomes included changes on other neuropsychological tests and the evaluating clinician's impression of memory change.\nA total of 120 participants were enrolled and randomized to either donepezil or placebo. No significant treatment effects were found between groups on either primary outcome of memory or any secondary cognitive outcomes. A trend was noted for the clinician's impression of memory change in favor of donepezil (37.7%) vs placebo (23.7%) (p = 0.097). No serious or unanticipated adverse events attributed to study medication developed.\nDonepezil did not improve memory as compared to placebo on either of the primary outcomes in this study.\nThis study provides Class I evidence which does not support the hypothesis that 10 mg of donepezil daily for 24 weeks is superior to placebo in improving cognition as measured by the SRT in people with MS whose baseline RAVLT score was 0.5 SD or more below average."", 'title': 'Multicenter randomized clinical trial of donepezil for memory impairment in multiple sclerosis.', 'date': '2011-04-27'}}",0.0,Psychiatry & Neurology
4,"Is the number of people with at least one neoplastic lesion detected higher, lower, or the same when comparing chromoscopy to conventional endoscopy?",higher,,no,"['12196768', '16527699', '21159889', '16767577']",27056645,2016,"{'12196768': {'article_id': '12196768', 'content': 'Small adenomas may be missed during colonoscopy, but chromoscopy has been reported to enhance detection. The aim of this randomized-controlled trial was to determine the effect of total colonic dye spray on adenoma detection during routine colonoscopy.\nConsecutive outpatients undergoing routine colonoscopy were randomized to a dye-spray group (0.1% indigo carmine used to coat the entire colon during withdrawal from the cecum) or control group (no dye).\nTwo hundred fifty-nine patients were randomized, 124 to the dye-spray and 135 to the control group; demographics, indication for colonoscopy, and quality of the preparation were similar between the groups. Extubation from the cecum took a median of 9:05 minutes (range: 2:48-24:44 min) in the dye-spray group versus 4:52 minutes (range: 1:42-15:21 min) in the control group (p < 0.0001). The proportion of patients with at least 1 adenoma and the total number of adenomas were not different between groups. However, in the dye-spray group significantly more diminutive adenomas (<5 mm) were detected proximal to the sigmoid colon (p = 0.026) and more patients were identified with 3 or more adenomas (p = 0.002). More non-neoplastic polyps were detected throughout the colon in the dye-spray group (p = 0.003). There were no complications.\nDye-spray increases the detection of small adenomas in the proximal colon and patients with multiple adenomas, but long-term outcomes should be studied to determine the clinical value of these findings.', 'title': 'Total colonic dye-spray increases the detection of diminutive adenomas during routine colonoscopy: a randomized controlled trial.', 'date': '2002-08-28'}, '16527699': {'article_id': '16527699', 'content': 'High-resolution colonoscopy with chromoscopy (HRC) is a technique designed to improve the detection of colonic neoplasias. We prospectively compared standard colonoscopy (SC) and HRC in a randomized multicenter trial.\nPatients (n = 203; age, 58 +/- 10 years; sex ratio, 1) were recruited according to the following criteria: (1) a history of either familial or personal colonic neoplasia or (2) alarm symptoms after the age of 60 years. After randomization, an SC was performed in 100 patients (resolution, < or = 410,000 pixels) and a HRC in 103 patients (Fujinon EC485ZW, 850,000 pixels). In the HRC group, each colonic segment was examined before and after spraying with indigo carmine 0.4%.\nTwo hundred seventy-six polyps were detected in 198 patients. One hundred sixty of them were hyperplastic polyps, 116 were adenomas, and 2 were carcinomas. The numbers of hyperplastic polyps and purely flat adenomas were significantly higher in the HRC group than in the SC group (1.1 +/- 1.6 vs 0.5 +/- 1.4 and 0.22 +/- 0.68 vs 0.07 +/- 0.29, respectively; P = .01 and P = .04), but there was no significant difference in the total number of adenomas per patient (primary end point) detected between the HRC and the SC groups (0.6 +/- 1.0 vs 0.5 +/- 0.9, respectively).\nAlthough HRC improves detection of purely flat adenomas and hyperplastic polyps, the overall detection of colonic adenomas in a population at increased risk of neoplasia is not significantly improved. These findings do not support the routine use of HRC in clinical practice.', 'title': 'High resolution colonoscopy with chromoscopy versus standard colonoscopy for the detection of colonic neoplasia: a randomized study.', 'date': '2006-03-11'}, '21159889': {'article_id': '21159889', 'content': ""Colonoscopy is the accepted gold standard for detecting colorectal adenomas, but the miss rate, especially for small and flat lesions, remains unacceptably high. The aim of this study was to determine whether enhanced mucosal contrast using pancolonic chromoendoscopy (PCC) allows higher rates of adenoma detection.\nIn a prospective, randomised two-centre trial, PCC (with 0.4% indigo carmine spraying during continuous extubation) was compared with standard colonoscopy (control group) in consecutive patients attending for routine colonoscopy. The histopathology of the lesions detected was confirmed by evaluating the endoscopic resection or biopsy specimens.\nA total of 1008 patients were included (496 in the PCC group, 512 in the control group). The patients' demographic characteristics and indications for colonoscopy were similar in the two groups. The proportion of patients with at least one adenoma was significantly higher in the PCC group (46.2%) than in the control group (36.3%; p = 0.002). Chromoendoscopy increased the overall detection rate for adenomas (0.95 vs 0.66 per patient), flat adenomas (0.56 vs 0.28 per patient) and serrated lesions (1.19 vs 0.49 per patient) (p < 0.001). There was a non-significant trend towards increased detection of advanced adenomas (103 vs 81; p = 0.067). Mean extubation times were slightly but significantly longer in the PCC group in comparison with the control group (11.6 ± 3.36 min vs 10.1 ± 2.03 min; p < 0.001).\nPancolonic chromoendoscopy markedly enhances adenoma detection rates in an average-risk population and is practicable enough for routine application."", 'title': 'Pancolonic chromoendoscopy with indigo carmine versus standard colonoscopy for detection of neoplastic lesions: a randomised two-centre trial.', 'date': '2010-12-17'}, '16767577': {'article_id': '16767577', 'content': 'Colonoscopy is still considered the standard investigation for the detection of colorectal adenomas, but the miss rate, especially for small and flat lesions, remains unacceptably high. Chromoscopy has been shown to increase the yield for lesion detection in inflammatory bowel disease. The aim of this randomized prospective study was to determine whether a combination of chromoscopy and structure enhancement could increase the adenoma detection rate in high-risk patients.\nAll patients included in the trial had a personal history of colorectal adenomas and/or a family history of colorectal cancer (but excluding genetic syndromes). They were randomized to one of two tandem colonoscopy groups, with the first pass consisting of conventional colonoscopy for both groups, followed by either chromoscopy and structure enhancement (the ""study"" group) or a second conventional colonoscopy (the control group) for the second-pass colonoscopy. All detected lesions was examined histopathologically after endoscopic resection or biopsy. The principal outcome parameter was the adenoma detection rate; the number, histopathology, and location of lesions was also recorded.\nA total of 292 patients were included in the study (146 patients in each group). The patients\' demographic characteristics, the indications for colonoscopy, and the quality of bowel preparation were similar in the two groups. There was a significant difference between the two groups with respect to the median duration of the examination (18.9 minutes in the control group vs. 27.1 minutes for the study group, P < 0.001). Although more hyperplastic lesions were detected throughout the colon in the study group ( P = 0.033), there was no difference between the two groups in either the proportion of patients with at least one adenoma or in the total number of adenomas detected. Chromoscopy and structure enhancement diagnosed significantly more diminutive adenomas (< 5mm) in the right colon, compared with controls ( P = 0.039).\nOn the basis of our results we cannot generally recommend the systematic use of chromoscopy and structure enhancement in a high-risk patient population, although the detection of small adenomas in the proximal colon was improved.', 'title': 'Does chromoendoscopy with structure enhancement improve the colonoscopic adenoma detection rate?', 'date': '2006-06-13'}}",0.25,Internal Medicine & Subspecialties
5,"Is total polyp detection higher, lower, or the same when comparing chromoscopy to conventional endoscopy?",higher,,no,"['12196768', '14960519', '20179689', '16527699', '21159889', '16767577', '19139000']",27056645,2016,"{'12196768': {'article_id': '12196768', 'content': 'Small adenomas may be missed during colonoscopy, but chromoscopy has been reported to enhance detection. The aim of this randomized-controlled trial was to determine the effect of total colonic dye spray on adenoma detection during routine colonoscopy.\nConsecutive outpatients undergoing routine colonoscopy were randomized to a dye-spray group (0.1% indigo carmine used to coat the entire colon during withdrawal from the cecum) or control group (no dye).\nTwo hundred fifty-nine patients were randomized, 124 to the dye-spray and 135 to the control group; demographics, indication for colonoscopy, and quality of the preparation were similar between the groups. Extubation from the cecum took a median of 9:05 minutes (range: 2:48-24:44 min) in the dye-spray group versus 4:52 minutes (range: 1:42-15:21 min) in the control group (p < 0.0001). The proportion of patients with at least 1 adenoma and the total number of adenomas were not different between groups. However, in the dye-spray group significantly more diminutive adenomas (<5 mm) were detected proximal to the sigmoid colon (p = 0.026) and more patients were identified with 3 or more adenomas (p = 0.002). More non-neoplastic polyps were detected throughout the colon in the dye-spray group (p = 0.003). There were no complications.\nDye-spray increases the detection of small adenomas in the proximal colon and patients with multiple adenomas, but long-term outcomes should be studied to determine the clinical value of these findings.', 'title': 'Total colonic dye-spray increases the detection of diminutive adenomas during routine colonoscopy: a randomized controlled trial.', 'date': '2002-08-28'}, '14960519': {'article_id': '14960519', 'content': 'Diminutive and flat colorectal lesions can be difficult to detect using conventional colonoscopic techniques. Previous data have suggested that pan-chromoscopy may improve detection rates. No randomised control trial has been performed examining detection rates of such lesions while controlling for extubation time and lavage effect.\nWe conducted a randomised controlled trial of pan-colonic chromoscopic colonoscopy for the detection of diminutive and flat colorectal lesions while controlling for extubation time and lavage effect.\nConsecutive patients attending for routine colonoscopy were randomised to either pan-chromoscopy using 0.5% indigo carmine (IC) or targeted chromoscopy (control group). A minimum diagnostic extubation time was set at eight minutes with controls undergoing a matched volume of saline wash.\nA total of 260 patients were randomised; 132 controls and 128 to pan-colonic chromoscopy. Extubation times did not differ significantly between the control (median 15 minutes (range 8-41)) and chromoscopy (median 17 minutes (range 8-39)) groups. The volume of IC used in the pan-chromoscopy group (median 68 ml (range 65-90)) and normal saline used in the control group (69 ml (range 60-93)) did not differ significantly. There was a statistically significant difference between the groups regarding the total number of adenomas detected (p<0.05) with significantly more diminutive (<4 mm) adenomas detected in the pan-chromoscopy group (p = 0.03). Pan-chromoscopy diagnosed more diminutive and flat lesions in the right colon compared with controls (p<0.05), with more patients with multiple adenomas (>3) detected using pan-chromoscopy (p<0.01). Hyperplastic lesions were more commonly detected in the pan-chromoscopy group compared with controls (p<0.001). More hyperplastic polyps were detected in the left colon (86% rectosigmoid) using chromoscopy compared with controls.\nChromoscopy improves the total number of adenomas detected and enhances the detection of diminutive and flat lesions. Importantly, eight diminutive lesions had foci of high grade dysplasia. Chromoscopy may benefit patients, assuming a high risk of colorectal cancer, and help in risk stratification and planning follow up colonoscopy intervals.', 'title': 'Detecting diminutive colorectal lesions at colonoscopy: a randomised controlled trial of pan-colonic versus targeted chromoscopy.', 'date': '2004-02-13'}, '20179689': {'article_id': '20179689', 'content': 'Flat and depressed colon neoplasms are an increasingly recognized precursor for colorectal cancer (CRC) in Western populations. High-definition chromoscopy is used to increase the yield of colonoscopy for flat and depressed neoplasms; however, its role in average-risk patients undergoing routine screening remains uncertain.\nAverage-risk patients referred for screening colonoscopy at four U.S. medical centers were randomized to high-definition chromocolonoscopy or high-definition white light colonoscopy. The primary outcomes, patients with at least one adenoma and the number of adenomas per patient, were compared between the two groups. The secondary outcome was patients with flat or depressed neoplasms, as defined by the Paris classification.\nA total of 660 patients were randomized (chromocolonoscopy: 321, white light: 339). Overall, the mean number of adenomas per patient was 1.2+/-2.1, the mean number of flat polyps per patient was 1.4+/-1.9, and the mean number of flat adenomas per patient was 0.5+/-1.0. The number of patients with at least one adenoma (55.5% vs. 48.4%, absolute difference 7.1%, 95% confidence interval (-0.5% to 14.7%), P=0.07), and the number of adenomas per patient (1.3+/-2.4 vs. 1.1+/-1.8, P=0.07) were marginally higher in the chromocolonoscopy group. There were no significant differences in the number of advanced adenomas per patient (0.06+/-0.37 vs. 0.04+/-0.25, P=0.3) and the number of advanced adenomas<10 mm per patient (0.02+/-0.26 vs. 0.01+/-0.14, P=0.4). Two invasive cancers were found, one in each group; neither was a flat neoplasm. Chromocolonoscopy detected significantly more flat adenomas per patient (0.6+/-1.2 vs. 0.4+/-0.9, P=0.01), adenomas<5 mm in diameter per patient (0.8+/-1.3 vs. 0.7+/-1.1, P=0.03), and non-neoplastic lesions per patient (1.8+/-2.3 vs. 1.0+/-1.3, P<0.0001).\nHigh-definition chromocolonoscopy marginally increased overall adenoma detection, and yielded a modest increase in flat adenoma and small adenoma detection, compared with high-definition white light colonoscopy. The yield for advanced neoplasms was similar for the two methods. Our findings do not support the routine use of high-definition chromocolonoscopy for CRC screening in average-risk patients. The high adenoma detection rates observed in this study may be due to the high-definition technology used in both groups.', 'title': 'High-definition chromocolonoscopy vs. high-definition white light colonoscopy for average-risk colorectal cancer screening.', 'date': '2010-02-25'}, '16527699': {'article_id': '16527699', 'content': 'High-resolution colonoscopy with chromoscopy (HRC) is a technique designed to improve the detection of colonic neoplasias. We prospectively compared standard colonoscopy (SC) and HRC in a randomized multicenter trial.\nPatients (n = 203; age, 58 +/- 10 years; sex ratio, 1) were recruited according to the following criteria: (1) a history of either familial or personal colonic neoplasia or (2) alarm symptoms after the age of 60 years. After randomization, an SC was performed in 100 patients (resolution, < or = 410,000 pixels) and a HRC in 103 patients (Fujinon EC485ZW, 850,000 pixels). In the HRC group, each colonic segment was examined before and after spraying with indigo carmine 0.4%.\nTwo hundred seventy-six polyps were detected in 198 patients. One hundred sixty of them were hyperplastic polyps, 116 were adenomas, and 2 were carcinomas. The numbers of hyperplastic polyps and purely flat adenomas were significantly higher in the HRC group than in the SC group (1.1 +/- 1.6 vs 0.5 +/- 1.4 and 0.22 +/- 0.68 vs 0.07 +/- 0.29, respectively; P = .01 and P = .04), but there was no significant difference in the total number of adenomas per patient (primary end point) detected between the HRC and the SC groups (0.6 +/- 1.0 vs 0.5 +/- 0.9, respectively).\nAlthough HRC improves detection of purely flat adenomas and hyperplastic polyps, the overall detection of colonic adenomas in a population at increased risk of neoplasia is not significantly improved. These findings do not support the routine use of HRC in clinical practice.', 'title': 'High resolution colonoscopy with chromoscopy versus standard colonoscopy for the detection of colonic neoplasia: a randomized study.', 'date': '2006-03-11'}, '21159889': {'article_id': '21159889', 'content': ""Colonoscopy is the accepted gold standard for detecting colorectal adenomas, but the miss rate, especially for small and flat lesions, remains unacceptably high. The aim of this study was to determine whether enhanced mucosal contrast using pancolonic chromoendoscopy (PCC) allows higher rates of adenoma detection.\nIn a prospective, randomised two-centre trial, PCC (with 0.4% indigo carmine spraying during continuous extubation) was compared with standard colonoscopy (control group) in consecutive patients attending for routine colonoscopy. The histopathology of the lesions detected was confirmed by evaluating the endoscopic resection or biopsy specimens.\nA total of 1008 patients were included (496 in the PCC group, 512 in the control group). The patients' demographic characteristics and indications for colonoscopy were similar in the two groups. The proportion of patients with at least one adenoma was significantly higher in the PCC group (46.2%) than in the control group (36.3%; p = 0.002). Chromoendoscopy increased the overall detection rate for adenomas (0.95 vs 0.66 per patient), flat adenomas (0.56 vs 0.28 per patient) and serrated lesions (1.19 vs 0.49 per patient) (p < 0.001). There was a non-significant trend towards increased detection of advanced adenomas (103 vs 81; p = 0.067). Mean extubation times were slightly but significantly longer in the PCC group in comparison with the control group (11.6 ± 3.36 min vs 10.1 ± 2.03 min; p < 0.001).\nPancolonic chromoendoscopy markedly enhances adenoma detection rates in an average-risk population and is practicable enough for routine application."", 'title': 'Pancolonic chromoendoscopy with indigo carmine versus standard colonoscopy for detection of neoplastic lesions: a randomised two-centre trial.', 'date': '2010-12-17'}, '16767577': {'article_id': '16767577', 'content': 'Colonoscopy is still considered the standard investigation for the detection of colorectal adenomas, but the miss rate, especially for small and flat lesions, remains unacceptably high. Chromoscopy has been shown to increase the yield for lesion detection in inflammatory bowel disease. The aim of this randomized prospective study was to determine whether a combination of chromoscopy and structure enhancement could increase the adenoma detection rate in high-risk patients.\nAll patients included in the trial had a personal history of colorectal adenomas and/or a family history of colorectal cancer (but excluding genetic syndromes). They were randomized to one of two tandem colonoscopy groups, with the first pass consisting of conventional colonoscopy for both groups, followed by either chromoscopy and structure enhancement (the ""study"" group) or a second conventional colonoscopy (the control group) for the second-pass colonoscopy. All detected lesions was examined histopathologically after endoscopic resection or biopsy. The principal outcome parameter was the adenoma detection rate; the number, histopathology, and location of lesions was also recorded.\nA total of 292 patients were included in the study (146 patients in each group). The patients\' demographic characteristics, the indications for colonoscopy, and the quality of bowel preparation were similar in the two groups. There was a significant difference between the two groups with respect to the median duration of the examination (18.9 minutes in the control group vs. 27.1 minutes for the study group, P < 0.001). Although more hyperplastic lesions were detected throughout the colon in the study group ( P = 0.033), there was no difference between the two groups in either the proportion of patients with at least one adenoma or in the total number of adenomas detected. Chromoscopy and structure enhancement diagnosed significantly more diminutive adenomas (< 5mm) in the right colon, compared with controls ( P = 0.039).\nOn the basis of our results we cannot generally recommend the systematic use of chromoscopy and structure enhancement in a high-risk patient population, although the detection of small adenomas in the proximal colon was improved.', 'title': 'Does chromoendoscopy with structure enhancement improve the colonoscopic adenoma detection rate?', 'date': '2006-06-13'}, '19139000': {'article_id': '19139000', 'content': 'Conventional colonoscopy misses some neoplastic lesions. We compared the sensitivity of chromoendoscopy and colonoscopy with intensive inspection for detecting adenomatous polyps missed by conventional colonoscopy. Fifty subjects with a history of colorectal cancer or adenomas underwent tandem colonoscopies at one of five centers of the Great Lakes New England Clinical Epidemiology and Validation Center of the Early Detection Research Network. The first exam was a conventional colonoscopy with removal of all visualized polyps. The second exam was randomly assigned as either pan-colonic indigocarmine chromoendoscopy or standard colonoscopy with intensive inspection lasting >20 minutes. Size, histology, and numbers of polyps detected on each exam were recorded. Twenty-seven subjects were randomized to a second exam with chromoendoscopy and 23 underwent intensive inspection. Forty adenomas were identified on the first standard colonoscopies. The second colonoscopies detected 24 additional adenomas: 19 were found using chromoendoscopy and 5 were found using intensive inspection. Chromoendoscopy found additional adenomas in more subjects than did intensive inspection (44% versus 17%) and identified significantly more missed adenomas per subject (0.7 versus 0.2, P < 0.01). Adenomas detected with chromoendoscopy were significantly smaller (mean size 2.66 +/- 0.97 mm) and were more often right-sided. Chromoendoscopy was associated with more normal tissue biopsies and longer procedure times than intensive inspection. After controlling for procedure time, chromoendoscopy detected more adenomas and hyperplastic polyps compared with colonoscopy using intensive inspection alone. Chromoendoscopy detected more polyps missed by standard colonoscopy than did intensive inspection. The clinical significance of these small missed lesions warrants further study.', 'title': 'Chromoendoscopy detects more adenomas than colonoscopy using intensive inspection without dye spraying.', 'date': '2009-01-14'}}",0.285714286,Internal Medicine & Subspecialties
219,"Is the risk of adverse events higher, lower, or the same when comparing chromoscopy to conventional endoscopy?",insufficient data,,no,"['12196768', '14960519', '20179689', '16527699', '21159889', '16767577', '19139000']",27056645,2016,"{'12196768': {'article_id': '12196768', 'content': 'Small adenomas may be missed during colonoscopy, but chromoscopy has been reported to enhance detection. The aim of this randomized-controlled trial was to determine the effect of total colonic dye spray on adenoma detection during routine colonoscopy.\nConsecutive outpatients undergoing routine colonoscopy were randomized to a dye-spray group (0.1% indigo carmine used to coat the entire colon during withdrawal from the cecum) or control group (no dye).\nTwo hundred fifty-nine patients were randomized, 124 to the dye-spray and 135 to the control group; demographics, indication for colonoscopy, and quality of the preparation were similar between the groups. Extubation from the cecum took a median of 9:05 minutes (range: 2:48-24:44 min) in the dye-spray group versus 4:52 minutes (range: 1:42-15:21 min) in the control group (p < 0.0001). The proportion of patients with at least 1 adenoma and the total number of adenomas were not different between groups. However, in the dye-spray group significantly more diminutive adenomas (<5 mm) were detected proximal to the sigmoid colon (p = 0.026) and more patients were identified with 3 or more adenomas (p = 0.002). More non-neoplastic polyps were detected throughout the colon in the dye-spray group (p = 0.003). There were no complications.\nDye-spray increases the detection of small adenomas in the proximal colon and patients with multiple adenomas, but long-term outcomes should be studied to determine the clinical value of these findings.', 'title': 'Total colonic dye-spray increases the detection of diminutive adenomas during routine colonoscopy: a randomized controlled trial.', 'date': '2002-08-28'}, '14960519': {'article_id': '14960519', 'content': 'Diminutive and flat colorectal lesions can be difficult to detect using conventional colonoscopic techniques. Previous data have suggested that pan-chromoscopy may improve detection rates. No randomised control trial has been performed examining detection rates of such lesions while controlling for extubation time and lavage effect.\nWe conducted a randomised controlled trial of pan-colonic chromoscopic colonoscopy for the detection of diminutive and flat colorectal lesions while controlling for extubation time and lavage effect.\nConsecutive patients attending for routine colonoscopy were randomised to either pan-chromoscopy using 0.5% indigo carmine (IC) or targeted chromoscopy (control group). A minimum diagnostic extubation time was set at eight minutes with controls undergoing a matched volume of saline wash.\nA total of 260 patients were randomised; 132 controls and 128 to pan-colonic chromoscopy. Extubation times did not differ significantly between the control (median 15 minutes (range 8-41)) and chromoscopy (median 17 minutes (range 8-39)) groups. The volume of IC used in the pan-chromoscopy group (median 68 ml (range 65-90)) and normal saline used in the control group (69 ml (range 60-93)) did not differ significantly. There was a statistically significant difference between the groups regarding the total number of adenomas detected (p<0.05) with significantly more diminutive (<4 mm) adenomas detected in the pan-chromoscopy group (p = 0.03). Pan-chromoscopy diagnosed more diminutive and flat lesions in the right colon compared with controls (p<0.05), with more patients with multiple adenomas (>3) detected using pan-chromoscopy (p<0.01). Hyperplastic lesions were more commonly detected in the pan-chromoscopy group compared with controls (p<0.001). More hyperplastic polyps were detected in the left colon (86% rectosigmoid) using chromoscopy compared with controls.\nChromoscopy improves the total number of adenomas detected and enhances the detection of diminutive and flat lesions. Importantly, eight diminutive lesions had foci of high grade dysplasia. Chromoscopy may benefit patients, assuming a high risk of colorectal cancer, and help in risk stratification and planning follow up colonoscopy intervals.', 'title': 'Detecting diminutive colorectal lesions at colonoscopy: a randomised controlled trial of pan-colonic versus targeted chromoscopy.', 'date': '2004-02-13'}, '20179689': {'article_id': '20179689', 'content': 'Flat and depressed colon neoplasms are an increasingly recognized precursor for colorectal cancer (CRC) in Western populations. High-definition chromoscopy is used to increase the yield of colonoscopy for flat and depressed neoplasms; however, its role in average-risk patients undergoing routine screening remains uncertain.\nAverage-risk patients referred for screening colonoscopy at four U.S. medical centers were randomized to high-definition chromocolonoscopy or high-definition white light colonoscopy. The primary outcomes, patients with at least one adenoma and the number of adenomas per patient, were compared between the two groups. The secondary outcome was patients with flat or depressed neoplasms, as defined by the Paris classification.\nA total of 660 patients were randomized (chromocolonoscopy: 321, white light: 339). Overall, the mean number of adenomas per patient was 1.2+/-2.1, the mean number of flat polyps per patient was 1.4+/-1.9, and the mean number of flat adenomas per patient was 0.5+/-1.0. The number of patients with at least one adenoma (55.5% vs. 48.4%, absolute difference 7.1%, 95% confidence interval (-0.5% to 14.7%), P=0.07), and the number of adenomas per patient (1.3+/-2.4 vs. 1.1+/-1.8, P=0.07) were marginally higher in the chromocolonoscopy group. There were no significant differences in the number of advanced adenomas per patient (0.06+/-0.37 vs. 0.04+/-0.25, P=0.3) and the number of advanced adenomas<10 mm per patient (0.02+/-0.26 vs. 0.01+/-0.14, P=0.4). Two invasive cancers were found, one in each group; neither was a flat neoplasm. Chromocolonoscopy detected significantly more flat adenomas per patient (0.6+/-1.2 vs. 0.4+/-0.9, P=0.01), adenomas<5 mm in diameter per patient (0.8+/-1.3 vs. 0.7+/-1.1, P=0.03), and non-neoplastic lesions per patient (1.8+/-2.3 vs. 1.0+/-1.3, P<0.0001).\nHigh-definition chromocolonoscopy marginally increased overall adenoma detection, and yielded a modest increase in flat adenoma and small adenoma detection, compared with high-definition white light colonoscopy. The yield for advanced neoplasms was similar for the two methods. Our findings do not support the routine use of high-definition chromocolonoscopy for CRC screening in average-risk patients. The high adenoma detection rates observed in this study may be due to the high-definition technology used in both groups.', 'title': 'High-definition chromocolonoscopy vs. high-definition white light colonoscopy for average-risk colorectal cancer screening.', 'date': '2010-02-25'}, '16527699': {'article_id': '16527699', 'content': 'High-resolution colonoscopy with chromoscopy (HRC) is a technique designed to improve the detection of colonic neoplasias. We prospectively compared standard colonoscopy (SC) and HRC in a randomized multicenter trial.\nPatients (n = 203; age, 58 +/- 10 years; sex ratio, 1) were recruited according to the following criteria: (1) a history of either familial or personal colonic neoplasia or (2) alarm symptoms after the age of 60 years. After randomization, an SC was performed in 100 patients (resolution, < or = 410,000 pixels) and a HRC in 103 patients (Fujinon EC485ZW, 850,000 pixels). In the HRC group, each colonic segment was examined before and after spraying with indigo carmine 0.4%.\nTwo hundred seventy-six polyps were detected in 198 patients. One hundred sixty of them were hyperplastic polyps, 116 were adenomas, and 2 were carcinomas. The numbers of hyperplastic polyps and purely flat adenomas were significantly higher in the HRC group than in the SC group (1.1 +/- 1.6 vs 0.5 +/- 1.4 and 0.22 +/- 0.68 vs 0.07 +/- 0.29, respectively; P = .01 and P = .04), but there was no significant difference in the total number of adenomas per patient (primary end point) detected between the HRC and the SC groups (0.6 +/- 1.0 vs 0.5 +/- 0.9, respectively).\nAlthough HRC improves detection of purely flat adenomas and hyperplastic polyps, the overall detection of colonic adenomas in a population at increased risk of neoplasia is not significantly improved. These findings do not support the routine use of HRC in clinical practice.', 'title': 'High resolution colonoscopy with chromoscopy versus standard colonoscopy for the detection of colonic neoplasia: a randomized study.', 'date': '2006-03-11'}, '21159889': {'article_id': '21159889', 'content': ""Colonoscopy is the accepted gold standard for detecting colorectal adenomas, but the miss rate, especially for small and flat lesions, remains unacceptably high. The aim of this study was to determine whether enhanced mucosal contrast using pancolonic chromoendoscopy (PCC) allows higher rates of adenoma detection.\nIn a prospective, randomised two-centre trial, PCC (with 0.4% indigo carmine spraying during continuous extubation) was compared with standard colonoscopy (control group) in consecutive patients attending for routine colonoscopy. The histopathology of the lesions detected was confirmed by evaluating the endoscopic resection or biopsy specimens.\nA total of 1008 patients were included (496 in the PCC group, 512 in the control group). The patients' demographic characteristics and indications for colonoscopy were similar in the two groups. The proportion of patients with at least one adenoma was significantly higher in the PCC group (46.2%) than in the control group (36.3%; p = 0.002). Chromoendoscopy increased the overall detection rate for adenomas (0.95 vs 0.66 per patient), flat adenomas (0.56 vs 0.28 per patient) and serrated lesions (1.19 vs 0.49 per patient) (p < 0.001). There was a non-significant trend towards increased detection of advanced adenomas (103 vs 81; p = 0.067). Mean extubation times were slightly but significantly longer in the PCC group in comparison with the control group (11.6 ± 3.36 min vs 10.1 ± 2.03 min; p < 0.001).\nPancolonic chromoendoscopy markedly enhances adenoma detection rates in an average-risk population and is practicable enough for routine application."", 'title': 'Pancolonic chromoendoscopy with indigo carmine versus standard colonoscopy for detection of neoplastic lesions: a randomised two-centre trial.', 'date': '2010-12-17'}, '16767577': {'article_id': '16767577', 'content': 'Colonoscopy is still considered the standard investigation for the detection of colorectal adenomas, but the miss rate, especially for small and flat lesions, remains unacceptably high. Chromoscopy has been shown to increase the yield for lesion detection in inflammatory bowel disease. The aim of this randomized prospective study was to determine whether a combination of chromoscopy and structure enhancement could increase the adenoma detection rate in high-risk patients.\nAll patients included in the trial had a personal history of colorectal adenomas and/or a family history of colorectal cancer (but excluding genetic syndromes). They were randomized to one of two tandem colonoscopy groups, with the first pass consisting of conventional colonoscopy for both groups, followed by either chromoscopy and structure enhancement (the ""study"" group) or a second conventional colonoscopy (the control group) for the second-pass colonoscopy. All detected lesions was examined histopathologically after endoscopic resection or biopsy. The principal outcome parameter was the adenoma detection rate; the number, histopathology, and location of lesions was also recorded.\nA total of 292 patients were included in the study (146 patients in each group). The patients\' demographic characteristics, the indications for colonoscopy, and the quality of bowel preparation were similar in the two groups. There was a significant difference between the two groups with respect to the median duration of the examination (18.9 minutes in the control group vs. 27.1 minutes for the study group, P < 0.001). Although more hyperplastic lesions were detected throughout the colon in the study group ( P = 0.033), there was no difference between the two groups in either the proportion of patients with at least one adenoma or in the total number of adenomas detected. Chromoscopy and structure enhancement diagnosed significantly more diminutive adenomas (< 5mm) in the right colon, compared with controls ( P = 0.039).\nOn the basis of our results we cannot generally recommend the systematic use of chromoscopy and structure enhancement in a high-risk patient population, although the detection of small adenomas in the proximal colon was improved.', 'title': 'Does chromoendoscopy with structure enhancement improve the colonoscopic adenoma detection rate?', 'date': '2006-06-13'}, '19139000': {'article_id': '19139000', 'content': 'Conventional colonoscopy misses some neoplastic lesions. We compared the sensitivity of chromoendoscopy and colonoscopy with intensive inspection for detecting adenomatous polyps missed by conventional colonoscopy. Fifty subjects with a history of colorectal cancer or adenomas underwent tandem colonoscopies at one of five centers of the Great Lakes New England Clinical Epidemiology and Validation Center of the Early Detection Research Network. The first exam was a conventional colonoscopy with removal of all visualized polyps. The second exam was randomly assigned as either pan-colonic indigocarmine chromoendoscopy or standard colonoscopy with intensive inspection lasting >20 minutes. Size, histology, and numbers of polyps detected on each exam were recorded. Twenty-seven subjects were randomized to a second exam with chromoendoscopy and 23 underwent intensive inspection. Forty adenomas were identified on the first standard colonoscopies. The second colonoscopies detected 24 additional adenomas: 19 were found using chromoendoscopy and 5 were found using intensive inspection. Chromoendoscopy found additional adenomas in more subjects than did intensive inspection (44% versus 17%) and identified significantly more missed adenomas per subject (0.7 versus 0.2, P < 0.01). Adenomas detected with chromoendoscopy were significantly smaller (mean size 2.66 +/- 0.97 mm) and were more often right-sided. Chromoendoscopy was associated with more normal tissue biopsies and longer procedure times than intensive inspection. After controlling for procedure time, chromoendoscopy detected more adenomas and hyperplastic polyps compared with colonoscopy using intensive inspection alone. Chromoendoscopy detected more polyps missed by standard colonoscopy than did intensive inspection. The clinical significance of these small missed lesions warrants further study.', 'title': 'Chromoendoscopy detects more adenomas than colonoscopy using intensive inspection without dye spraying.', 'date': '2009-01-14'}}",1.0,Internal Medicine & Subspecialties
6,"Is stroke prevention higher, lower, or the same when comparing Transcatheter Device Closure (TDC) to medical therapy?",no difference,,no,"['22417252', '23514285', '23514286']",26346232,2015,"{'22417252': {'article_id': '22417252', 'content': 'The prevalence of patent foramen ovale among patients with cryptogenic stroke is higher than that in the general population. Closure with a percutaneous device is often recommended in such patients, but it is not known whether this intervention reduces the risk of recurrent stroke.\nWe conducted a multicenter, randomized, open-label trial of closure with a percutaneous device, as compared with medical therapy alone, in patients between 18 and 60 years of age who presented with a cryptogenic stroke or transient ischemic attack (TIA) and had a patent foramen ovale. The primary end point was a composite of stroke or transient ischemic attack during 2 years of follow-up, death from any cause during the first 30 days, or death from neurologic causes between 31 days and 2 years.\nA total of 909 patients were enrolled in the trial. The cumulative incidence (Kaplan-Meier estimate) of the primary end point was 5.5% in the closure group (447 patients) as compared with 6.8% in the medical-therapy group (462 patients) (adjusted hazard ratio, 0.78; 95% confidence interval, 0.45 to 1.35; P=0.37). The respective rates were 2.9% and 3.1% for stroke (P=0.79) and 3.1% and 4.1% for TIA (P=0.44). No deaths occurred by 30 days in either group, and there were no deaths from neurologic causes during the 2-year follow-up period. A cause other than paradoxical embolism was usually apparent in patients with recurrent neurologic events.\nIn patients with cryptogenic stroke or TIA who had a patent foramen ovale, closure with a device did not offer a greater benefit than medical therapy alone for the prevention of recurrent stroke or TIA. (Funded by NMT Medical; ClinicalTrials.gov number, NCT00201461.).', 'title': 'Closure or medical therapy for cryptogenic stroke with patent foramen ovale.', 'date': '2012-03-16'}, '23514285': {'article_id': '23514285', 'content': 'The options for secondary prevention of cryptogenic embolism in patients with patent foramen ovale are administration of antithrombotic medications or percutaneous closure of the patent foramen ovale. We investigated whether closure is superior to medical therapy.\nWe performed a multicenter, superiority trial in 29 centers in Europe, Canada, Brazil, and Australia in which the assessors of end points were unaware of the study-group assignments. Patients with a patent foramen ovale and ischemic stroke, transient ischemic attack (TIA), or a peripheral thromboembolic event were randomly assigned to undergo closure of the patent foramen ovale with the Amplatzer PFO Occluder or to receive medical therapy. The primary end point was a composite of death, nonfatal stroke, TIA, or peripheral embolism. Analysis was performed on data for the intention-to-treat population.\nThe mean duration of follow-up was 4.1 years in the closure group and 4.0 years in the medical-therapy group. The primary end point occurred in 7 of the 204 patients (3.4%) in the closure group and in 11 of the 210 patients (5.2%) in the medical-therapy group (hazard ratio for closure vs. medical therapy, 0.63; 95% confidence interval [CI], 0.24 to 1.62; P=0.34). Nonfatal stroke occurred in 1 patient (0.5%) in the closure group and 5 patients (2.4%) in the medical-therapy group (hazard ratio, 0.20; 95% CI, 0.02 to 1.72; P=0.14), and TIA occurred in 5 patients (2.5%) and 7 patients (3.3%), respectively (hazard ratio, 0.71; 95% CI, 0.23 to 2.24; P=0.56).\nClosure of a patent foramen ovale for secondary prevention of cryptogenic embolism did not result in a significant reduction in the risk of recurrent embolic events or death as compared with medical therapy. (Funded by St. Jude Medical; ClinicalTrials.gov number, NCT00166257.).', 'title': 'Percutaneous closure of patent foramen ovale in cryptogenic embolism.', 'date': '2013-03-22'}, '23514286': {'article_id': '23514286', 'content': 'Whether closure of a patent foramen ovale is effective in the prevention of recurrent ischemic stroke in patients who have had a cryptogenic stroke is unknown. We conducted a trial to evaluate whether closure is superior to medical therapy alone in preventing recurrent ischemic stroke or early death in patients 18 to 60 years of age.\nIn this prospective, multicenter, randomized, event-driven trial, we randomly assigned patients, in a 1:1 ratio, to medical therapy alone or closure of the patent foramen ovale. The primary results of the trial were analyzed when the target of 25 primary end-point events had been observed and adjudicated.\nWe enrolled 980 patients (mean age, 45.9 years) at 69 sites. The medical-therapy group received one or more antiplatelet medications (74.8%) or warfarin (25.2%). Treatment exposure between the two groups was unequal (1375 patient-years in the closure group vs. 1184 patient-years in the medical-therapy group, P=0.009) owing to a higher dropout rate in the medical-therapy group. In the intention-to-treat cohort, 9 patients in the closure group and 16 in the medical-therapy group had a recurrence of stroke (hazard ratio with closure, 0.49; 95% confidence interval [CI], 0.22 to 1.11; P=0.08). The between-group difference in the rate of recurrent stroke was significant in the prespecified per-protocol cohort (6 events in the closure group vs. 14 events in the medical-therapy group; hazard ratio, 0.37; 95% CI, 0.14 to 0.96; P=0.03) and in the as-treated cohort (5 events vs. 16 events; hazard ratio, 0.27; 95% CI, 0.10 to 0.75; P=0.007). Serious adverse events occurred in 23.0% of the patients in the closure group and in 21.6% in the medical-therapy group (P=0.65). Procedure-related or device-related serious adverse events occurred in 21 of 499 patients in the closure group (4.2%), but the rate of atrial fibrillation or device thrombus was not increased.\nIn the primary intention-to-treat analysis, there was no significant benefit associated with closure of a patent foramen ovale in adults who had had a cryptogenic ischemic stroke. However, closure was superior to medical therapy alone in the prespecified per-protocol and as-treated analyses, with a low rate of associated risks. (Funded by St. Jude Medical; RESPECT ClinicalTrials.gov number, NCT00465270.).', 'title': 'Closure of patent foramen ovale versus medical therapy after cryptogenic stroke.', 'date': '2013-03-22'}}",1.0,Surgery
7,"Is the composite endpoint of recurrent stroke or TIA higher, lower, or the same when comparing Transcatheter Device Closure (TDC) to medical therapy?",no difference,,no,"['22417252', '23514285']",26346232,2015,"{'22417252': {'article_id': '22417252', 'content': 'The prevalence of patent foramen ovale among patients with cryptogenic stroke is higher than that in the general population. Closure with a percutaneous device is often recommended in such patients, but it is not known whether this intervention reduces the risk of recurrent stroke.\nWe conducted a multicenter, randomized, open-label trial of closure with a percutaneous device, as compared with medical therapy alone, in patients between 18 and 60 years of age who presented with a cryptogenic stroke or transient ischemic attack (TIA) and had a patent foramen ovale. The primary end point was a composite of stroke or transient ischemic attack during 2 years of follow-up, death from any cause during the first 30 days, or death from neurologic causes between 31 days and 2 years.\nA total of 909 patients were enrolled in the trial. The cumulative incidence (Kaplan-Meier estimate) of the primary end point was 5.5% in the closure group (447 patients) as compared with 6.8% in the medical-therapy group (462 patients) (adjusted hazard ratio, 0.78; 95% confidence interval, 0.45 to 1.35; P=0.37). The respective rates were 2.9% and 3.1% for stroke (P=0.79) and 3.1% and 4.1% for TIA (P=0.44). No deaths occurred by 30 days in either group, and there were no deaths from neurologic causes during the 2-year follow-up period. A cause other than paradoxical embolism was usually apparent in patients with recurrent neurologic events.\nIn patients with cryptogenic stroke or TIA who had a patent foramen ovale, closure with a device did not offer a greater benefit than medical therapy alone for the prevention of recurrent stroke or TIA. (Funded by NMT Medical; ClinicalTrials.gov number, NCT00201461.).', 'title': 'Closure or medical therapy for cryptogenic stroke with patent foramen ovale.', 'date': '2012-03-16'}, '23514285': {'article_id': '23514285', 'content': 'The options for secondary prevention of cryptogenic embolism in patients with patent foramen ovale are administration of antithrombotic medications or percutaneous closure of the patent foramen ovale. We investigated whether closure is superior to medical therapy.\nWe performed a multicenter, superiority trial in 29 centers in Europe, Canada, Brazil, and Australia in which the assessors of end points were unaware of the study-group assignments. Patients with a patent foramen ovale and ischemic stroke, transient ischemic attack (TIA), or a peripheral thromboembolic event were randomly assigned to undergo closure of the patent foramen ovale with the Amplatzer PFO Occluder or to receive medical therapy. The primary end point was a composite of death, nonfatal stroke, TIA, or peripheral embolism. Analysis was performed on data for the intention-to-treat population.\nThe mean duration of follow-up was 4.1 years in the closure group and 4.0 years in the medical-therapy group. The primary end point occurred in 7 of the 204 patients (3.4%) in the closure group and in 11 of the 210 patients (5.2%) in the medical-therapy group (hazard ratio for closure vs. medical therapy, 0.63; 95% confidence interval [CI], 0.24 to 1.62; P=0.34). Nonfatal stroke occurred in 1 patient (0.5%) in the closure group and 5 patients (2.4%) in the medical-therapy group (hazard ratio, 0.20; 95% CI, 0.02 to 1.72; P=0.14), and TIA occurred in 5 patients (2.5%) and 7 patients (3.3%), respectively (hazard ratio, 0.71; 95% CI, 0.23 to 2.24; P=0.56).\nClosure of a patent foramen ovale for secondary prevention of cryptogenic embolism did not result in a significant reduction in the risk of recurrent embolic events or death as compared with medical therapy. (Funded by St. Jude Medical; ClinicalTrials.gov number, NCT00166257.).', 'title': 'Percutaneous closure of patent foramen ovale in cryptogenic embolism.', 'date': '2013-03-22'}}",1.0,Surgery
8,"Is the length of hospital stay higher, lower, or the same when comparing Pancreatojejunostomy (PJ) to Pancreatogastrostomy (PG)?",no difference,low,no,"['16327486', '19092337', '7574936']",28898386,2017,"{'16327486': {'article_id': '16327486', 'content': 'To compare the results of pancreaticogastrostomy versus pancreaticojejunostomy following pancreaticoduodenectomy in a prospective and randomized setting.\nWhile several techniques have been proposed for reconstructing pancreatico-digestive continuity, only a limited number of randomized studies have been carried out.\nA total of 151 patients undergoing pancreaticoduodenectomy with soft residual tissue were randomized to receive either pancreaticogastrostomy (group PG) or end-to-side pancreaticojejunostomy (group PJ).\nThe 2 treatment groups showed no differences in vital statistics or underlying disease, mean duration of surgery, and need for intraoperative blood transfusion. Overall, the incidence of surgical complications was 34% (29% in PG, 39% in PJ, P = not significant). Patients receiving PG showed a significantly lower rate of multiple surgical complications (P = 0.002). Pancreatic fistula was the most frequent complication, occurring in 14.5% of patients (13% in PG and 16% in PJ, P = not significant). Five patients in each treatment arm required a second surgical intervention; the postoperative mortality rate was 0.6%. PG was favored over PJ due to significant differences in postoperative collections (P = 0.01), delayed gastric emptying (P = 0.03), and biliary fistula (P = 0.01). The mean postoperative hospitalization period stay was comparable in both groups.\nWhen compared with PJ, PG did not show any significant differences in the overall postoperative complication rate or incidence of pancreatic fistula. However, biliary fistula, postoperative collections and delayed gastric emptying are significantly reduced in patients treated by PG. In addition, pancreaticogastrostomy is associated with a significantly lower frequency of multiple surgical complications.', 'title': 'Reconstruction by pancreaticojejunostomy versus pancreaticogastrostomy following pancreatectomy: results of a comparative study.', 'date': '2005-12-06'}, '19092337': {'article_id': '19092337', 'content': 'To compare the results of postoperative morbidity rate of a new pancreatogastrostomy technique, pylorus-preserving pancreaticoduodenectomy (PPPD) with gastric partition (PPPD-GP) with the conventional technique of pancreaticojejunostomy (PJ).\nPancreatojejunostomy and pancreatogastrostomy (PG) are the commonly preferred methods of anastomosis after pancreatoduodenectomy (PD). All randomized controlled trials failed to show advantage of a particular technique, suggesting that both PJ and PG provide equally results. However, postoperative morbidity remains high. The best technique in pancreatic anastomosis is still debated.\nDescribed here is a new technique, PPPD-GP; in this technique the gastroepiploic arcade is preserved. Gastric partition was performed using 2 endo-Gia staplers along the greater curvature of the stomach, 3 cm from the border. This gastric segment, 10 to 12 cm in length is placed in close proximity to the cut edge of the pancreatic stump. An end-to-side, duct-to-mucosa anastomosis (with pancreatic duct stent) is constructed. One hundred eight patients undergoing PPPD for benign and malignant diseases of the pancreatic head and the periampullary region were randomized to receive PG (PPPD-GP) or end-to-side PJ (PPPD-PJ).\nThe two treatment groups showed no differences in preoperative parameters and intraoperative factors. The overall postoperative complications were 23% after PPPD-GP and 44% after PPPD-PJ (P < 0.01). The incidence of pancreatic fistula was 4% after PPPD-GP and 18% after PPPD-PJ (P < 0.01). The mean + SD hospital stay was 12 +/- 2 days after PPPD-GP and 16 +/- 3 days after PPPD-PJ.\nThis study shows that PPPD-GP can be performed safely and is associated with less complication than PPPD-PJ. The advantage of this technique over other PG techniques is that the anastomosis is outside the area of the stomach where the contents empty into the jejunum, but pancreatic juice drains directly into the stomach.', 'title': 'Pancreatogastrostomy with gastric partition after pylorus-preserving pancreatoduodenectomy versus conventional pancreatojejunostomy: a prospective randomized study.', 'date': '2008-12-19'}, '7574936': {'article_id': '7574936', 'content': 'The authors hypothesized that pancreaticogastrostomy is safer than pancreaticojejunostomy after pancreaticoduodenectomy and less likely to be associated with a postoperative pancreatic fistula.\nPancreatic fistula is a leading cause of morbidity and mortality after pancreaticoduodenectomy, occurring in 10% to 20% of patients. Nonrandomized reports have suggested that pancreaticogastrostomy is less likely than pancreaticojejunostomy to be associated with postoperative complications.\nBetween May 1993 and January 1995, the findings for 145 patients were analyzed in this prospective trial at The Johns Hopkins Hospital. After giving their appropriate preoperative informed consent, patients were randomly assigned to pancreaticogastrostomy or pancreaticojejunostomy after completion of the pancreaticoduodenal resection. All pancreatic anastomoses were performed in two layers without pancreatic duct stents and with closed suction drainage. Pancreatic fistula was defined as drainage of greater than 50 mL of amylase-rich fluid on or after postoperative day 10.\nThe pancreaticogastrostomy (n = 73) and pancreaticojejunostomy (n = 72) groups were comparable with regard to multiple parameters, including demographics, medical history, preoperative laboratory values, and intraoperative factors, such as operative time, blood transfusions, pancreatic texture, length of pancreatic remnant mobilized, and pancreatic duct diameter. The overall incidence of pancreatic fistula after pancreaticoduodenectomy was 11.7% (17/145). The incidence of pancreatic fistula was similar for the pancreaticogastrostomy (12.3%) and pancreaticojejunostomy (11.1%) groups. Pancreatic fistula was associated with a significant prolongation of postoperative hospital stay (36 +/- 5 vs. 15 +/- 1 days) (p < 0.001). Factors significantly increasing the risk of pancreatic fistula by univariate logistic regression analysis included ampullary or duodenal disease, soft pancreatic texture, longer operative time, greater intraoperative red blood cell transfusions, and lower surgical volume (p < 0.05). A multivariate logistic regression analysis revealed the factors most highly associated with pancreatic fistula to be lower surgical volume and ampullary or duodenal disease in the resected specimen.\nPancreatic fistula is a common complication after pancreaticoduodenectomy, with an incidence most strongly associated with surgical volume and underlying disease. These data do not support the hypothesis that pancreaticogastrostomy is safer than pancreaticojejunostomy or is associated with a lower incidence of pancreatic fistula.', 'title': 'A prospective randomized trial of pancreaticogastrostomy versus pancreaticojejunostomy after pancreaticoduodenectomy.', 'date': '1995-10-01'}}",0.333333333,Surgery
9,"Is the overall risk of postoperative pancreatic fistula higher, lower, or the same when comparing Pancreatojejunostomy (PJ) to Pancreatogastrostomy (PG)?",no difference,low,no,"['24467711', '16327486', '15910726', '19092337', '24264781', '25799130', '26135690', '23643139', '7574936']",28898386,2017,"{'24467711': {'article_id': '24467711', 'content': 'The optimal strategy for the reconstruction of the pancreas following pancreaticoduodenectomy (PD) is still debated. The aim of this study was to compare the outcomes of isolated Roux loop pancreaticojejunostomy (IRPJ) with those of pancreaticogastrostomy (PG) after PD.\nConsecutive patients submitted to PD were randomized to either method of reconstruction. The primary outcome measure was the rate of postoperative pancreatic fistula (POPF). Secondary outcomes included operative time, day to resumption of oral feeding, postoperative morbidity and mortality, and exocrine and endocrine pancreatic functions.\nNinety patients treated by PD were included in the study. The median total operative time was significantly longer in the IRPJ group (320\u2009min versus 300\u2009min; P = 0.047). Postoperative pancreatic fistula developed in nine of 45 patients in the IRPJ group and 10 of 45 patients in the PG group (P = 0.796). Seven IRPJ patients and four PG patients had POPF of type B or C (P = 0.710). Time to resumption of oral feeding was shorter in the IRPJ group (P = 0.03). Steatorrhea at 1 year was reported in nine of 42 IRPJ patients and 18 of 41 PG patients (P = 0.029). Albumin levels at 1 year were 3.6\u2009g/dl in the IRPJ group and 3.3\u2009g/dl in the PG group (P = 0.001).\nIsolated Roux loop PJ was not associated with a lower rate of POPF, but was associated with a decrease in the incidence of postoperative steatorrhea. The technique allowed for early oral feeding and the maintenance of oral feeding even if POPF developed.', 'title': 'Isolated Roux loop pancreaticojejunostomy versus pancreaticogastrostomy after pancreaticoduodenectomy: a prospective randomized study.', 'date': '2014-01-29'}, '16327486': {'article_id': '16327486', 'content': 'To compare the results of pancreaticogastrostomy versus pancreaticojejunostomy following pancreaticoduodenectomy in a prospective and randomized setting.\nWhile several techniques have been proposed for reconstructing pancreatico-digestive continuity, only a limited number of randomized studies have been carried out.\nA total of 151 patients undergoing pancreaticoduodenectomy with soft residual tissue were randomized to receive either pancreaticogastrostomy (group PG) or end-to-side pancreaticojejunostomy (group PJ).\nThe 2 treatment groups showed no differences in vital statistics or underlying disease, mean duration of surgery, and need for intraoperative blood transfusion. Overall, the incidence of surgical complications was 34% (29% in PG, 39% in PJ, P = not significant). Patients receiving PG showed a significantly lower rate of multiple surgical complications (P = 0.002). Pancreatic fistula was the most frequent complication, occurring in 14.5% of patients (13% in PG and 16% in PJ, P = not significant). Five patients in each treatment arm required a second surgical intervention; the postoperative mortality rate was 0.6%. PG was favored over PJ due to significant differences in postoperative collections (P = 0.01), delayed gastric emptying (P = 0.03), and biliary fistula (P = 0.01). The mean postoperative hospitalization period stay was comparable in both groups.\nWhen compared with PJ, PG did not show any significant differences in the overall postoperative complication rate or incidence of pancreatic fistula. However, biliary fistula, postoperative collections and delayed gastric emptying are significantly reduced in patients treated by PG. In addition, pancreaticogastrostomy is associated with a significantly lower frequency of multiple surgical complications.', 'title': 'Reconstruction by pancreaticojejunostomy versus pancreaticogastrostomy following pancreatectomy: results of a comparative study.', 'date': '2005-12-06'}, '15910726': {'article_id': '15910726', 'content': 'Only 2 large (more than 100 patients) prospective trials comparing pancreatogastrostomy (PG) with pancreatojejunostomy (PJ) after pancreatoduodenectomy (PD) have been reported until now. One nonrandomized study showed that there were less pancreatic and digestive tract fistula with PG, whereas the other, a randomized trial from a single high-volume center, found no significant differences between the two techniques.\nSingle blind, controlled randomized, multicenter trial. The main endpoint was intra-abdominal complications (IACs).\nOf 149 randomized patients, 81 underwent PG and 68 PJ. No significant difference was found between the two groups concerning pre- or intraoperative patient characteristics. The rate of patients with one or more IACs was 34% in each group. Twenty-seven patients sustained a pancreatoenteric fistula (18%), 13 in PG (16%; 95% confidence interval [CI] 8-24%) and 14 in PJ (20%; 95% CI 10.5-29.5%). No statistically significant difference was found between the 2 groups concerning the mortality rate (11% overall), the rate of reoperations and/or postoperative interventional radiology drainages (23%), or the length of hospital stay (median 20.5 days). Univariate analysis found the following risk factors: (1) age > or =70 years old, (2) extrapancreatic disease, (3) normal consistency of pancreas, (4) diameter of main pancreatic duct <3 mm, (5) duration of operation >6 hours, and (6) a center effect. Significantly more IAC, pancreatoenteric fistula, and deaths occurred in one center (that included the most patients) (P = .05), but there were significantly more high-risk patients in this center (normal pancreas consistency, extrapancreatic pathology, small pancreatic duct, higher transfusion requirements, and duration of operation >6 hours) compared with the other centers. In multivariate analysis, the center effect disappeared. Independent risk factors included duration of operation >6 hours for IAC and for pancreatoenteric fistula (P = .01), extrapancreatic disease for pancreatoenteric fistulas (P < .04), and age > or =70 years for mortality (P < .02).\nThe type of pancreatoenteric anastomosis (PJ or PG) after PD does not significantly influence the rate of patients with one or more IAC and/or pancreatic fistula or the severity of complications.', 'title': 'A controlled randomized multicenter trial of pancreatogastrostomy or pancreatojejunostomy after pancreatoduodenectomy.', 'date': '2005-05-25'}, '19092337': {'article_id': '19092337', 'content': 'To compare the results of postoperative morbidity rate of a new pancreatogastrostomy technique, pylorus-preserving pancreaticoduodenectomy (PPPD) with gastric partition (PPPD-GP) with the conventional technique of pancreaticojejunostomy (PJ).\nPancreatojejunostomy and pancreatogastrostomy (PG) are the commonly preferred methods of anastomosis after pancreatoduodenectomy (PD). All randomized controlled trials failed to show advantage of a particular technique, suggesting that both PJ and PG provide equally results. However, postoperative morbidity remains high. The best technique in pancreatic anastomosis is still debated.\nDescribed here is a new technique, PPPD-GP; in this technique the gastroepiploic arcade is preserved. Gastric partition was performed using 2 endo-Gia staplers along the greater curvature of the stomach, 3 cm from the border. This gastric segment, 10 to 12 cm in length is placed in close proximity to the cut edge of the pancreatic stump. An end-to-side, duct-to-mucosa anastomosis (with pancreatic duct stent) is constructed. One hundred eight patients undergoing PPPD for benign and malignant diseases of the pancreatic head and the periampullary region were randomized to receive PG (PPPD-GP) or end-to-side PJ (PPPD-PJ).\nThe two treatment groups showed no differences in preoperative parameters and intraoperative factors. The overall postoperative complications were 23% after PPPD-GP and 44% after PPPD-PJ (P < 0.01). The incidence of pancreatic fistula was 4% after PPPD-GP and 18% after PPPD-PJ (P < 0.01). The mean + SD hospital stay was 12 +/- 2 days after PPPD-GP and 16 +/- 3 days after PPPD-PJ.\nThis study shows that PPPD-GP can be performed safely and is associated with less complication than PPPD-PJ. The advantage of this technique over other PG techniques is that the anastomosis is outside the area of the stomach where the contents empty into the jejunum, but pancreatic juice drains directly into the stomach.', 'title': 'Pancreatogastrostomy with gastric partition after pylorus-preserving pancreatoduodenectomy versus conventional pancreatojejunostomy: a prospective randomized study.', 'date': '2008-12-19'}, '24264781': {'article_id': '24264781', 'content': 'Anastomotic leakage of pancreaticojejunostomy (PJ) remains the single most important source of morbidity after pancreaticoduodenectomy (PD). The primary aim of this randomized clinical trial comparing PG with PJ after PD was to test the hypothesis that invaginated PG would result in a lower rate and severity of pancreatic fistula.\nPatients undergoing PD were randomized to receive either a duct-to-duct PJ or a double-layer invaginated PG. The primary endpoint was the rate of pancreatic fistula, using the definition of the International Study Group on Pancreatic Fistula. Secondary endpoints were the evaluation of severe abdominal complications (Clavien-Dindo grade IIIa or above), endocrine and exocrine function.\nOf 123 patients randomized, 58 underwent PJ and 65 had PG. The incidence of pancreatic fistula was significantly higher following PJ than for PG (20 of 58 versus 10 of 65 respectively; P\u2009=\u20090.014), as was the severity of pancreatic fistula (grade A: 2 versus 5 per cent; grade B-C: 33 versus 11 per cent; P\u2009=\u20090.006). The hospital readmission rate for complications was significantly lower after PG (6 versus 24 per cent; P\u2009=\u20090.005), weight loss was lower (P\u2009=\u20090.025) and exocrine function better (P\u2009=\u20090.022).\nThe rate and severity of pancreatic fistula was significantly lower with this PG technique compared with that following PJ.\nISRCTN58328599 (http://www.controlled-trials.com).', 'title': 'Randomized clinical trial of pancreaticogastrostomy versus pancreaticojejunostomy on the rate and severity of pancreatic fistula after pancreaticoduodenectomy.', 'date': '2013-11-23'}, '25799130': {'article_id': '25799130', 'content': 'It has been suggested that pancreaticogastrostomy (PG) is a safer reconstruction than pancreaticojejunostomy (PJ), resulting in lower morbidity, including lower pancreatic leak rates and decreased postoperative mortality. We compared PJ and PG after pancreaticoduodenectomy (PD).\nA randomized clinical trial was designed. It was stopped with 50% accrual. Patients underwent either PG or PJ reconstruction. The primary outcome was the pancreatic fistula rate, and the secondary outcomes were overall morbidity and mortality. We used the Student t, Mann-Whitney U and χ(2) tests for intention to treat analysis. The effect of randomization, American Society of Anesthesiologists score, soft pancreatic texture and use of pancreatic stent on overall complications and fistula rates was calculated using logistic regression.\nOur trial included 98 patients. The rate of pancreatic fistula formation was 18% in the PJ and 25% in the PG groups (p = 0.40). Postoperative complications occurred in 48% of patients in the PJ and 58% in the PG groups (p = 0.31). There were no significant predictors of overall complications in the multivariate analysis. Only soft pancreatic gland predicted the occurrence of pancreatic fistula (odds ratio 5.89, p = 0.003).\nThere was no difference in the rates of pancreatic leak/fistula, overall complications or mortality between patients undergoing PG and and those undergoing PJ after PD.\nSelon certains, la pancréatogastrostomie (PG) est une technique de reconstruction plus sécuritaire que la pancréatojéjunostomie (PJ) et entraîne une morbidité moindre, y compris un taux moins élevé de fuites pancréatiques et une mortalité postopératoire diminuée. Nous avons comparé la PJ et la PG post-pancréatoduodénectomie.\nUn essai clinique randomisé a été conçu et cessé à l’atteinte d’un taux de participation de 50 %. Les patients ont subi une reconstruction par PG ou par PJ. Le paramètre principal était le taux de fistules pancréatiques et les paramètres secondaires étaient la morbidité et la mortalité globales. Nous avons utilisé les tests \nNotre essai a regroupé 98 patients. Le taux de fistules pancréatiques a été de 18 % dans le groupe soumis à la PJ et de 25 % dans le groupe soumis à la PG (\nNous n’avons noté aucune différence quant aux taux de fuites ou de fistules pancréatiques, de complications globales ou de mortalité entre les patients soumis à la PG et à la PJ post-pancréatoduodénectomie.', 'title': 'In search of the best reconstructive technique after pancreaticoduodenectomy: pancreaticojejunostomy versus pancreaticogastrostomy.', 'date': '2015-03-24'}, '26135690': {'article_id': '26135690', 'content': 'To assess pancreatic fistula rate and secondary endpoints after pancreatogastrostomy (PG) versus pancreatojejunostomy (PJ) for reconstruction in pancreatoduodenectomy in the setting of a multicenter randomized controlled trial.\nPJ and PG are established methods for reconstruction in pancreatoduodenectomy. Recent prospective trials suggest superiority of the PG regarding perioperative complications.\nA multicenter prospective randomized controlled trial comparing PG with PJ was conducted involving 14 German high-volume academic centers for pancreatic surgery. The primary endpoint was clinically relevant postoperative pancreatic fistula. Secondary endpoints comprised perioperative outcome and pancreatic function and quality of life measured at 6 and 12 months of follow-up.\nFrom May 2011 to December 2012, 440 patients were randomized, and 320 were included in the intention-to-treat analysis. There was no significant difference in the rate of grade B/C fistula after PG versus PJ (20% vs 22%, P = 0.617). The overall incidence of grade B/C fistula was 21%, and the in-hospital mortality was 6%. Multivariate analysis of the primary endpoint disclosed soft pancreatic texture (odds ratio: 2.1, P = 0.016) as the only independent risk factor. Compared with PJ, PG was associated with an increased rate of grade A/B bleeding events, perioperative stroke, less enzyme supplementation at 6 months, and improved results in some quality of life parameters.\nThe rate of grade B/C fistula after PG versus PJ was not different. There were more postoperative bleeding events with PG. Perioperative morbidity and mortality of pancreatoduodenectomy seem to be underestimated, even in the high-volume center setting.', 'title': 'Pancreatogastrostomy Versus Pancreatojejunostomy for RECOnstruction After PANCreatoduodenectomy (RECOPANC, DRKS 00000767): Perioperative and Long-term Results of a Multicenter Randomized Controlled Trial.', 'date': '2015-07-03'}, '23643139': {'article_id': '23643139', 'content': 'Postoperative pancreatic fistula is the leading cause of death and morbidity after pancreaticoduodenectomy. However, the best reconstruction method to reduce occurrence of fistula is debated. We did a multicentre, randomised superiority trial to compare the outcomes of different reconstructive techniques in patients undergoing pancreaticoduodenectomy for pancreatic or periampullary tumours.\nPatients aged 18-85 years with confirmed or suspected neoplasms of the pancreas, distal bile duct, ampulla vateri, duodenum, or periampullary tumours were eligible for inclusion. An internet-based platform was used to randomly assign patients to either pancreaticojejunostomy or pancreaticogastrostomy as reconstruction after pancreaticoduodenectomy, using permuted blocks with six patients per block. Within each centre the randomisation was stratified on the pancreatic duct diameter (≤3 mm vs >3 mm) measured at the time of surgery. The primary endpoint was the occurrence of clinical postoperative pancreatic fistula (grade B or C) as defined by the International Study Group on Pancreatic Fistula. The study was not masked and analyses were done by intention to treat. Patient follow-up was closed 2 months after discharge from the hospital. This study is registered with ClinicalTrials.gov, number NCT00830778.\nBetween June, 2009, and August, 2012, we randomly allocated 167 patients to receive pancreaticojejunostomy and 162 to receive pancreaticogastrostomy. 33 (19.8%) patients in the pancreaticojejunostomy group and 13 (8.0%) in the pancreaticogastrostomy group had clinical postoperative pancreatic fistula (OR 2.86, 95% CI 1.38-6.17; p=0.002). The overall incidence of postoperative complications did not differ significantly between the groups (99 in the pancreaticojejunostomy group vs 100 in the pancreaticogastrostomy group), although more events in the pancreaticojejunostomy group were of grade ≥3a than in the pancreaticogastrostomy group (39 vs 35).\nIn patients undergoing pancreaticoduodenectomy for pancreatic head or periampullary tumours, pancreaticogastrostomy is more efficient than pancreaticojejunostomy in reducing the incidence of postoperative pancreatic fistula.\nFunding Johnson & Johnson Medical Devices, Belgium.', 'title': 'Pancreaticojejunostomy versus pancreaticogastrostomy reconstruction after pancreaticoduodenectomy for pancreatic or periampullary tumours: a multicentre randomised trial.', 'date': '2013-05-07'}, '7574936': {'article_id': '7574936', 'content': 'The authors hypothesized that pancreaticogastrostomy is safer than pancreaticojejunostomy after pancreaticoduodenectomy and less likely to be associated with a postoperative pancreatic fistula.\nPancreatic fistula is a leading cause of morbidity and mortality after pancreaticoduodenectomy, occurring in 10% to 20% of patients. Nonrandomized reports have suggested that pancreaticogastrostomy is less likely than pancreaticojejunostomy to be associated with postoperative complications.\nBetween May 1993 and January 1995, the findings for 145 patients were analyzed in this prospective trial at The Johns Hopkins Hospital. After giving their appropriate preoperative informed consent, patients were randomly assigned to pancreaticogastrostomy or pancreaticojejunostomy after completion of the pancreaticoduodenal resection. All pancreatic anastomoses were performed in two layers without pancreatic duct stents and with closed suction drainage. Pancreatic fistula was defined as drainage of greater than 50 mL of amylase-rich fluid on or after postoperative day 10.\nThe pancreaticogastrostomy (n = 73) and pancreaticojejunostomy (n = 72) groups were comparable with regard to multiple parameters, including demographics, medical history, preoperative laboratory values, and intraoperative factors, such as operative time, blood transfusions, pancreatic texture, length of pancreatic remnant mobilized, and pancreatic duct diameter. The overall incidence of pancreatic fistula after pancreaticoduodenectomy was 11.7% (17/145). The incidence of pancreatic fistula was similar for the pancreaticogastrostomy (12.3%) and pancreaticojejunostomy (11.1%) groups. Pancreatic fistula was associated with a significant prolongation of postoperative hospital stay (36 +/- 5 vs. 15 +/- 1 days) (p < 0.001). Factors significantly increasing the risk of pancreatic fistula by univariate logistic regression analysis included ampullary or duodenal disease, soft pancreatic texture, longer operative time, greater intraoperative red blood cell transfusions, and lower surgical volume (p < 0.05). A multivariate logistic regression analysis revealed the factors most highly associated with pancreatic fistula to be lower surgical volume and ampullary or duodenal disease in the resected specimen.\nPancreatic fistula is a common complication after pancreaticoduodenectomy, with an incidence most strongly associated with surgical volume and underlying disease. These data do not support the hypothesis that pancreaticogastrostomy is safer than pancreaticojejunostomy or is associated with a lower incidence of pancreatic fistula.', 'title': 'A prospective randomized trial of pancreaticogastrostomy versus pancreaticojejunostomy after pancreaticoduodenectomy.', 'date': '1995-10-01'}}",0.666666667,Surgery
10,"Is the breast cancer detection rate in women with dense breasts higher, lower, or the same when comparing screening with a combination of mammography and ultrasonography to screening with mammography alone?",higher,high,no,"['34406400', '29571797', '23116728', '26549432']",36999589,2023,"{'34406400': {'article_id': '34406400', 'content': 'Mammography has limited accuracy in breast cancer screening. Ultrasonography, when used in conjunction with mammography screening, is helpful to detect early-stage and invasive cancers for asymptomatic women with dense and nondense breasts.\nTo evaluate the performance of adjunctive ultrasonography with mammography for breast cancer screening, according to differences in breast density.\nThis study is a secondary analysis of the Japan Strategic Anti-cancer Randomized Trial. Between July 2007 and March 2011, asymptomatic women aged 40 to 49 years were enrolled in Japan. The present study used data from cases enrolled from the screening center in Miyagi prefecture during 2007 to 2020. Participants were randomly assigned in a 1:1 ratio to undergo either mammography with ultrasonography (intervention group) or mammography alone (control group). Data analysis was performed from February to March 2020.\nUltrasonography adjunctive to mammography for breast cancer screening regardless of breast density.\nSensitivity, specificity, recall rates, biopsy rates, and characteristics of screen-detected cancers and interval breast cancers were evaluated between study groups and for each modality according to breast density.\nA total of 76\u202f119 women were enrolled, and data for 19\u202f213 women (mean [SD] age, 44.5 [2.8] years) from the Miyagi prefecture were analyzed; 9705 were randomized to the intervention group and 9508 were randomized to the control group. A total of 11\u202f390 women (59.3%) had heterogeneously or extremely dense breasts. Among the overall group, 130 cancers were found. Sensitivity was significantly higher in the intervention group than the control group (93.2% [95% CI, 87.4%-99.0%] vs 66.7% [95% CI, 54.4%-78.9%]; P\u2009<\u2009.001). Similar trends were observed in women with dense breasts (sensitivity in intervention vs control groups, 93.2% [95% CI, 85.7%-100.0%] vs 70.6% [95% CI, 55.3%-85.9%]; P\u2009<\u2009.001) and nondense breasts (sensitivity in intervention vs control groups, 93.1% [95% CI, 83.9%-102.3%] vs 60.9% [95% CI, 40.9%-80.8%]; P\u2009<\u2009.001). The rate of interval cancers per 1000 screenings was lower in the intervention group compared with the control group (0.5 cancers [95% CI, 0.1-1.0 cancers] vs 2.0 cancers [95% CI, 1.1-2.9 cancers]; P\u2009=\u2009.004). Within the intervention group, the rate of invasive cancers detected by ultrasonography alone was significantly higher than that for mammography alone in both dense (82.4% [95% CI, 56.6%-96.2%] vs 41.7% [95% CI, 15.2%-72.3%]; P\u2009=\u2009.02) and nondense (85.7% [95% CI, 42.1%-99.6%] vs 25.0% [95% CI, 5.5%-57.2%]; P\u2009=\u2009.02) breasts. However, sensitivity of mammography or ultrasonography alone did not exceed 80% across all breast densities in the 2 groups. Compared with the control group, specificity was significantly lower in the intervention group (91.8% [95% CI, 91.2%-92.3%] vs 86.8% [95% CI, 86.2%-87.5%]; P\u2009<\u2009.001). Recall rates (13.8% [95% CI, 13.1%-14.5%] vs 8.6% [95% CI, 8.0%-9.1%]; P\u2009<\u2009.001) and biopsy rates (5.5% [95% CI, 5.1%-6.0%] vs 2.1% [95% CI, 1.8%-2.4%]; P\u2009<\u2009.001) were significantly higher in the intervention group than the control group.\nIn this secondary analysis of a randomized clinical trial, screening mammography alone demonstrated low sensitivity, whereas adjunctive ultrasonography was associated with increased sensitivity. These findings suggest that adjunctive ultrasonography has the potential to improve detection of early-stage and invasive cancers across both dense and nondense breasts. Supplemental ultrasonography should be considered as an appropriate imaging modality for breast cancer screening in asymptomatic women aged 40 to 49 years regardless of breast density.\nNIPH Clinical Trial Identifier: UMIN000000757.', 'title': 'Evaluation of Adjunctive Ultrasonography for Breast Cancer Detection Among Women Aged 40-49 Years With Varying Breast Density Undergoing Screening Mammography: A Secondary Analysis of a Randomized Clinical Trial.', 'date': '2021-08-19'}, '29571797': {'article_id': '29571797', 'content': 'To compare the performance of screening with mammography combined with ultrasound versus mammography alone in women at average risk for breast cancer.\n66,680 women underwent physician-performed ultrasound as an adjunct to screening mammography. Histological results and follow-up at one year were used as reference standard for sensitivity. Main outcome measures were cancer detection rate, sensitivity, recall rate, biopsy rate, and positive predictive value of biopsy for combined screening with mammography plus ultrasound versus mammography alone.\nThe overall sensitivity of mammography only was 61.5% in women with dense breasts and 86.6% in women with non-dense breasts. The sensitivity of mammography plus ultrasound combined was 81.3% in women with dense breasts and 95.0% in women with non-dense breasts. Adjunctive ultrasound increased the recall rate from 10.5 to 16.5 per 1000 women screened, and increased the biopsy rate from 6.3 to 9.3 per 1000 women screened. The positive predictive value of biopsy was 55.5% (95% CI 50.6%-60.3%) for mammography alone and 43.3 (95% CI 39.4%-47.3%) for combined mammography plus ultrasound.\nSupplemental ultrasound improves cancer detection in screening of women at average risk for breast cancer. Recall rates and biopsy rates can be kept within acceptable limits.', 'title': 'Combined screening with mammography and ultrasound in a population-based screening program.', 'date': '2018-03-25'}, '23116728': {'article_id': '23116728', 'content': 'Automated breast ultrasound (ABUS)was performed in 3418 asymptomatic women with mammographically dense breasts. The addition of ABUS to mammography in women with greater than 50% breast density resulted in the detection of 12.3 per 1,000 breast cancers, compared to 4.6 per 1,000 by mammography alone. The mean tumor size was 14.3 mm and overall attributable risk of breast cancer was 19.92 (95% confidence level, 16.75 - 23.61) in our screened population. These preliminary results may justify the cost-benefit of implementing the judicious us of ABUS in conjunction with mammography in the dense breast screening population.', 'title': 'Improved breast cancer detection in asymptomatic women using 3D-automated breast ultrasound in mammographically dense breasts.', 'date': '2012-11-03'}, '26549432': {'article_id': '26549432', 'content': 'To determine which modalities [2D mammography (2D), digital breast tomosynthesis (DBT), whole breast sonography (WBS)] are optimal for screening depending on breast density.\nInstitutional retrospective cohort study of 2013 screening mammograms (16,789), sorted by modalities and density.\nCancer detection is increased by adding WBS to 2D (P=.02) for the overall study population. Recall rate was lowest with 2D+DBT (10.2%, P<.001) and highest with 2D+DBT+WBS (23.6%, P<.001) for the overall study population as well.\nWomen with dense and nondense breasts benefit from reduced recall rate with the addition of DBT; however, this benefit is negated with the addition of WBS.', 'title': '2D mammography, digital breast tomosynthesis, and ultrasound: which should be used for the different breast densities in breast cancer screening?', 'date': '2015-11-10'}}",1.0,Oncology & Hematology
11,"Is the frequency of interval carcinoma occurence higher, lower, or the same when comparing screening with a combination of mammography and ultrasonography to screening with mammography alone?",lower,high,no,"['34406400', '30882843']",36999589,2023,"{'34406400': {'article_id': '34406400', 'content': 'Mammography has limited accuracy in breast cancer screening. Ultrasonography, when used in conjunction with mammography screening, is helpful to detect early-stage and invasive cancers for asymptomatic women with dense and nondense breasts.\nTo evaluate the performance of adjunctive ultrasonography with mammography for breast cancer screening, according to differences in breast density.\nThis study is a secondary analysis of the Japan Strategic Anti-cancer Randomized Trial. Between July 2007 and March 2011, asymptomatic women aged 40 to 49 years were enrolled in Japan. The present study used data from cases enrolled from the screening center in Miyagi prefecture during 2007 to 2020. Participants were randomly assigned in a 1:1 ratio to undergo either mammography with ultrasonography (intervention group) or mammography alone (control group). Data analysis was performed from February to March 2020.\nUltrasonography adjunctive to mammography for breast cancer screening regardless of breast density.\nSensitivity, specificity, recall rates, biopsy rates, and characteristics of screen-detected cancers and interval breast cancers were evaluated between study groups and for each modality according to breast density.\nA total of 76\u202f119 women were enrolled, and data for 19\u202f213 women (mean [SD] age, 44.5 [2.8] years) from the Miyagi prefecture were analyzed; 9705 were randomized to the intervention group and 9508 were randomized to the control group. A total of 11\u202f390 women (59.3%) had heterogeneously or extremely dense breasts. Among the overall group, 130 cancers were found. Sensitivity was significantly higher in the intervention group than the control group (93.2% [95% CI, 87.4%-99.0%] vs 66.7% [95% CI, 54.4%-78.9%]; P\u2009<\u2009.001). Similar trends were observed in women with dense breasts (sensitivity in intervention vs control groups, 93.2% [95% CI, 85.7%-100.0%] vs 70.6% [95% CI, 55.3%-85.9%]; P\u2009<\u2009.001) and nondense breasts (sensitivity in intervention vs control groups, 93.1% [95% CI, 83.9%-102.3%] vs 60.9% [95% CI, 40.9%-80.8%]; P\u2009<\u2009.001). The rate of interval cancers per 1000 screenings was lower in the intervention group compared with the control group (0.5 cancers [95% CI, 0.1-1.0 cancers] vs 2.0 cancers [95% CI, 1.1-2.9 cancers]; P\u2009=\u2009.004). Within the intervention group, the rate of invasive cancers detected by ultrasonography alone was significantly higher than that for mammography alone in both dense (82.4% [95% CI, 56.6%-96.2%] vs 41.7% [95% CI, 15.2%-72.3%]; P\u2009=\u2009.02) and nondense (85.7% [95% CI, 42.1%-99.6%] vs 25.0% [95% CI, 5.5%-57.2%]; P\u2009=\u2009.02) breasts. However, sensitivity of mammography or ultrasonography alone did not exceed 80% across all breast densities in the 2 groups. Compared with the control group, specificity was significantly lower in the intervention group (91.8% [95% CI, 91.2%-92.3%] vs 86.8% [95% CI, 86.2%-87.5%]; P\u2009<\u2009.001). Recall rates (13.8% [95% CI, 13.1%-14.5%] vs 8.6% [95% CI, 8.0%-9.1%]; P\u2009<\u2009.001) and biopsy rates (5.5% [95% CI, 5.1%-6.0%] vs 2.1% [95% CI, 1.8%-2.4%]; P\u2009<\u2009.001) were significantly higher in the intervention group than the control group.\nIn this secondary analysis of a randomized clinical trial, screening mammography alone demonstrated low sensitivity, whereas adjunctive ultrasonography was associated with increased sensitivity. These findings suggest that adjunctive ultrasonography has the potential to improve detection of early-stage and invasive cancers across both dense and nondense breasts. Supplemental ultrasonography should be considered as an appropriate imaging modality for breast cancer screening in asymptomatic women aged 40 to 49 years regardless of breast density.\nNIPH Clinical Trial Identifier: UMIN000000757.', 'title': 'Evaluation of Adjunctive Ultrasonography for Breast Cancer Detection Among Women Aged 40-49 Years With Varying Breast Density Undergoing Screening Mammography: A Secondary Analysis of a Randomized Clinical Trial.', 'date': '2021-08-19'}, '30882843': {'article_id': '30882843', 'content': 'Whole-breast ultrasonography has been advocated to supplement screening mammography to improve outcomes in women with dense breasts.\nTo determine the performance of screening mammography plus screening ultrasonography compared with screening mammography alone in community practice.\nObservational cohort study. Two Breast Cancer Surveillance Consortium registries provided prospectively collected data on screening mammography with vs without same-day breast ultrasonography from January 1, 2000, to December 31, 2013. The dates of analysis were March 2014 to December 2018. A total of 6081 screening mammography plus same-day screening ultrasonography examinations in 3386 women were propensity score matched 1:5 to 30\u202f062 screening mammograms without screening ultrasonography in 15\u202f176 women from a sample of 113\u202f293 mammograms. Exclusion criteria included a personal history of breast cancer and self-reported breast symptoms.\nScreening mammography with vs without screening ultrasonography.\nCancer detection rate and rates of interval cancer, false-positive biopsy recommendation, short-interval follow-up, and positive predictive value of biopsy recommendation were estimated and compared using log binomial regression.\nScreening mammography with vs without ultrasonography examinations was performed more often in women with dense breasts (74.3% [n\u2009=\u20094317 of 5810] vs 35.9% [n\u2009=\u200939\u202f928 of 111\u202f306] in the overall sample), in women who were younger than 50 years (49.7% [n\u2009=\u20093022 of 6081] vs 31.7% [n\u2009=\u200916\u202f897 of 112\u202f462]), and in women with a family history of breast cancer (42.9% [n\u2009=\u20092595 of 6055] vs 15.0% [n\u2009=\u200916\u202f897 of 112\u202f462]). While 21.4% (n\u2009=\u20091154 of 5392) of screening ultrasonography examinations were performed in women with high or very high (≥2.50%) Breast Cancer Surveillance Consortium 5-year risk scores, 53.6% (n\u2009=\u20092889 of 5392) had low or average (<1.67%) risk. Comparing mammography plus ultrasonography with mammography alone, the cancer detection rate was similar at 5.4 vs 5.5 per 1000 screens (adjusted relative risk [RR], 1.14; 95% CI, 0.76-1.68), as were interval cancer rates at 1.5 vs 1.9 per 1000 screens (RR, 0.67; 95% CI, 0.33-1.37). The false-positive biopsy rates were significantly higher at 52.0 vs 22.2 per 1000 screens (RR, 2.23; 95% CI, 1.93-2.58), as was short-interval follow-up at 3.9% vs 1.1% (RR, 3.10; 95% CI, 2.60-3.70). The positive predictive value of biopsy recommendation was significantly lower at 9.5% vs 21.4% (RR, 0.50; 95% CI, 0.35-0.71).\nIn a relatively young population of women at low, intermediate, and high breast cancer risk, these results suggest that the benefits of supplemental ultrasonography screening may not outweigh associated harms.', 'title': 'Performance of Screening Ultrasonography as an Adjunct to Screening Mammography in Women Across the Spectrum of Breast Cancer Risk.', 'date': '2019-03-19'}}",0.5,Oncology & Hematology
12,"Is the breast cancer detection rate higher, lower, or the same when comparing screening with a combination of mammography and ultrasonography to screening with mammography alone?",higher,high,no,"['34406400', '29571797', '26549432', '23465737', '30882843']",36999589,2023,"{'34406400': {'article_id': '34406400', 'content': 'Mammography has limited accuracy in breast cancer screening. Ultrasonography, when used in conjunction with mammography screening, is helpful to detect early-stage and invasive cancers for asymptomatic women with dense and nondense breasts.\nTo evaluate the performance of adjunctive ultrasonography with mammography for breast cancer screening, according to differences in breast density.\nThis study is a secondary analysis of the Japan Strategic Anti-cancer Randomized Trial. Between July 2007 and March 2011, asymptomatic women aged 40 to 49 years were enrolled in Japan. The present study used data from cases enrolled from the screening center in Miyagi prefecture during 2007 to 2020. Participants were randomly assigned in a 1:1 ratio to undergo either mammography with ultrasonography (intervention group) or mammography alone (control group). Data analysis was performed from February to March 2020.\nUltrasonography adjunctive to mammography for breast cancer screening regardless of breast density.\nSensitivity, specificity, recall rates, biopsy rates, and characteristics of screen-detected cancers and interval breast cancers were evaluated between study groups and for each modality according to breast density.\nA total of 76\u202f119 women were enrolled, and data for 19\u202f213 women (mean [SD] age, 44.5 [2.8] years) from the Miyagi prefecture were analyzed; 9705 were randomized to the intervention group and 9508 were randomized to the control group. A total of 11\u202f390 women (59.3%) had heterogeneously or extremely dense breasts. Among the overall group, 130 cancers were found. Sensitivity was significantly higher in the intervention group than the control group (93.2% [95% CI, 87.4%-99.0%] vs 66.7% [95% CI, 54.4%-78.9%]; P\u2009<\u2009.001). Similar trends were observed in women with dense breasts (sensitivity in intervention vs control groups, 93.2% [95% CI, 85.7%-100.0%] vs 70.6% [95% CI, 55.3%-85.9%]; P\u2009<\u2009.001) and nondense breasts (sensitivity in intervention vs control groups, 93.1% [95% CI, 83.9%-102.3%] vs 60.9% [95% CI, 40.9%-80.8%]; P\u2009<\u2009.001). The rate of interval cancers per 1000 screenings was lower in the intervention group compared with the control group (0.5 cancers [95% CI, 0.1-1.0 cancers] vs 2.0 cancers [95% CI, 1.1-2.9 cancers]; P\u2009=\u2009.004). Within the intervention group, the rate of invasive cancers detected by ultrasonography alone was significantly higher than that for mammography alone in both dense (82.4% [95% CI, 56.6%-96.2%] vs 41.7% [95% CI, 15.2%-72.3%]; P\u2009=\u2009.02) and nondense (85.7% [95% CI, 42.1%-99.6%] vs 25.0% [95% CI, 5.5%-57.2%]; P\u2009=\u2009.02) breasts. However, sensitivity of mammography or ultrasonography alone did not exceed 80% across all breast densities in the 2 groups. Compared with the control group, specificity was significantly lower in the intervention group (91.8% [95% CI, 91.2%-92.3%] vs 86.8% [95% CI, 86.2%-87.5%]; P\u2009<\u2009.001). Recall rates (13.8% [95% CI, 13.1%-14.5%] vs 8.6% [95% CI, 8.0%-9.1%]; P\u2009<\u2009.001) and biopsy rates (5.5% [95% CI, 5.1%-6.0%] vs 2.1% [95% CI, 1.8%-2.4%]; P\u2009<\u2009.001) were significantly higher in the intervention group than the control group.\nIn this secondary analysis of a randomized clinical trial, screening mammography alone demonstrated low sensitivity, whereas adjunctive ultrasonography was associated with increased sensitivity. These findings suggest that adjunctive ultrasonography has the potential to improve detection of early-stage and invasive cancers across both dense and nondense breasts. Supplemental ultrasonography should be considered as an appropriate imaging modality for breast cancer screening in asymptomatic women aged 40 to 49 years regardless of breast density.\nNIPH Clinical Trial Identifier: UMIN000000757.', 'title': 'Evaluation of Adjunctive Ultrasonography for Breast Cancer Detection Among Women Aged 40-49 Years With Varying Breast Density Undergoing Screening Mammography: A Secondary Analysis of a Randomized Clinical Trial.', 'date': '2021-08-19'}, '29571797': {'article_id': '29571797', 'content': 'To compare the performance of screening with mammography combined with ultrasound versus mammography alone in women at average risk for breast cancer.\n66,680 women underwent physician-performed ultrasound as an adjunct to screening mammography. Histological results and follow-up at one year were used as reference standard for sensitivity. Main outcome measures were cancer detection rate, sensitivity, recall rate, biopsy rate, and positive predictive value of biopsy for combined screening with mammography plus ultrasound versus mammography alone.\nThe overall sensitivity of mammography only was 61.5% in women with dense breasts and 86.6% in women with non-dense breasts. The sensitivity of mammography plus ultrasound combined was 81.3% in women with dense breasts and 95.0% in women with non-dense breasts. Adjunctive ultrasound increased the recall rate from 10.5 to 16.5 per 1000 women screened, and increased the biopsy rate from 6.3 to 9.3 per 1000 women screened. The positive predictive value of biopsy was 55.5% (95% CI 50.6%-60.3%) for mammography alone and 43.3 (95% CI 39.4%-47.3%) for combined mammography plus ultrasound.\nSupplemental ultrasound improves cancer detection in screening of women at average risk for breast cancer. Recall rates and biopsy rates can be kept within acceptable limits.', 'title': 'Combined screening with mammography and ultrasound in a population-based screening program.', 'date': '2018-03-25'}, '26549432': {'article_id': '26549432', 'content': 'To determine which modalities [2D mammography (2D), digital breast tomosynthesis (DBT), whole breast sonography (WBS)] are optimal for screening depending on breast density.\nInstitutional retrospective cohort study of 2013 screening mammograms (16,789), sorted by modalities and density.\nCancer detection is increased by adding WBS to 2D (P=.02) for the overall study population. Recall rate was lowest with 2D+DBT (10.2%, P<.001) and highest with 2D+DBT+WBS (23.6%, P<.001) for the overall study population as well.\nWomen with dense and nondense breasts benefit from reduced recall rate with the addition of DBT; however, this benefit is negated with the addition of WBS.', 'title': '2D mammography, digital breast tomosynthesis, and ultrasound: which should be used for the different breast densities in breast cancer screening?', 'date': '2015-11-10'}, '23465737': {'article_id': '23465737', 'content': 'To determine whether adding screening ultrasonography to screening mammography can reduce patient recall rates and increase cancer detection rates.\nWe analyzed the results of mammography and ultrasonography breast screenings performed at the Total Health Evaluation Center Tsukuba, Japan, between April 2011 and March 2012. We also reviewed the modalities and results of diagnostic examinations from women with mammographic abnormalities who visited the Tsukuba Medical Center Hospital for further testing.\nOf 11,753 women screened, cancer was diagnosed in 10 (0.22%) of the 4529 participants who underwent mammography alone, 23 (0.37%) of the 6250 participants who underwent ultrasonography alone, and 5 (0.51%) of the 974 participants who underwent mammography and ultrasonography. The recall rate due to mammographic abnormalities was 4.9% for women screened only with mammography and 2.6% for those screened with both modalities. The cancer detection rate was 0.22% for women screened only with mammography (positive predictive value, 4.5%) and 0.31% for those screened with both modalities (positive predictive value, 12.0%). Of the 211 lesions presenting as mammographic abnormalities investigated further, diagnostic ultrasonography found no abnormalities in 63 (29.9%) and benign findings in 69 (33.7%). The rest 36.4% needed mammography, cytological or histological examinations or follow-up in addition to diagnostic ultrasonography.\nIt is possible to reduce the recall rate in screening mammography by combining mammography and ultrasonography for breast screening.', 'title': 'Effect of adding screening ultrasonography to screening mammography on patient recall and cancer detection rates: a retrospective study in Japan.', 'date': '2013-03-08'}, '30882843': {'article_id': '30882843', 'content': 'Whole-breast ultrasonography has been advocated to supplement screening mammography to improve outcomes in women with dense breasts.\nTo determine the performance of screening mammography plus screening ultrasonography compared with screening mammography alone in community practice.\nObservational cohort study. Two Breast Cancer Surveillance Consortium registries provided prospectively collected data on screening mammography with vs without same-day breast ultrasonography from January 1, 2000, to December 31, 2013. The dates of analysis were March 2014 to December 2018. A total of 6081 screening mammography plus same-day screening ultrasonography examinations in 3386 women were propensity score matched 1:5 to 30\u202f062 screening mammograms without screening ultrasonography in 15\u202f176 women from a sample of 113\u202f293 mammograms. Exclusion criteria included a personal history of breast cancer and self-reported breast symptoms.\nScreening mammography with vs without screening ultrasonography.\nCancer detection rate and rates of interval cancer, false-positive biopsy recommendation, short-interval follow-up, and positive predictive value of biopsy recommendation were estimated and compared using log binomial regression.\nScreening mammography with vs without ultrasonography examinations was performed more often in women with dense breasts (74.3% [n\u2009=\u20094317 of 5810] vs 35.9% [n\u2009=\u200939\u202f928 of 111\u202f306] in the overall sample), in women who were younger than 50 years (49.7% [n\u2009=\u20093022 of 6081] vs 31.7% [n\u2009=\u200916\u202f897 of 112\u202f462]), and in women with a family history of breast cancer (42.9% [n\u2009=\u20092595 of 6055] vs 15.0% [n\u2009=\u200916\u202f897 of 112\u202f462]). While 21.4% (n\u2009=\u20091154 of 5392) of screening ultrasonography examinations were performed in women with high or very high (≥2.50%) Breast Cancer Surveillance Consortium 5-year risk scores, 53.6% (n\u2009=\u20092889 of 5392) had low or average (<1.67%) risk. Comparing mammography plus ultrasonography with mammography alone, the cancer detection rate was similar at 5.4 vs 5.5 per 1000 screens (adjusted relative risk [RR], 1.14; 95% CI, 0.76-1.68), as were interval cancer rates at 1.5 vs 1.9 per 1000 screens (RR, 0.67; 95% CI, 0.33-1.37). The false-positive biopsy rates were significantly higher at 52.0 vs 22.2 per 1000 screens (RR, 2.23; 95% CI, 1.93-2.58), as was short-interval follow-up at 3.9% vs 1.1% (RR, 3.10; 95% CI, 2.60-3.70). The positive predictive value of biopsy recommendation was significantly lower at 9.5% vs 21.4% (RR, 0.50; 95% CI, 0.35-0.71).\nIn a relatively young population of women at low, intermediate, and high breast cancer risk, these results suggest that the benefits of supplemental ultrasonography screening may not outweigh associated harms.', 'title': 'Performance of Screening Ultrasonography as an Adjunct to Screening Mammography in Women Across the Spectrum of Breast Cancer Risk.', 'date': '2019-03-19'}}",0.6,Oncology & Hematology
13,"Is food intake higher, lower, or the same when comparing grehlin to placebo?",higher,very low,no,['15181065'],29489032,2018,"{'15181065': {'article_id': '15181065', 'content': 'There is a pressing need for more effective appetite-stimulatory therapies for many patient groups including those with cancer. We have previously demonstrated that the gastric hormone ghrelin potently enhances appetite in healthy volunteers. Here, we performed an acute, randomized, placebo-controlled, cross-over clinical trial to determine whether ghrelin stimulates appetite in cancer patients with anorexia. Seven cancer patients who reported loss of appetite were recruited from oncology clinics at Charing Cross Hospital. The main outcome measures were energy intake from a buffet meal during ghrelin or saline infusion and meal appreciation as assessed by visual analog scale. A marked increase in energy intake (31 +/- 7%; P = 0.005) was observed with ghrelin infusion compared with saline control, and every patient ate more. The meal appreciation score was greater by 28 +/- 8% (P = 0.02) with ghrelin treatment. No side effects were observed. The stimulatory effects of ghrelin on food intake and meal appreciation seen in this preliminary study suggest that ghrelin could be an effective treatment for cancer anorexia and possibly for appetite loss in other patient groups.', 'title': 'Ghrelin increases energy intake in cancer patients with impaired appetite: acute, randomized, placebo-controlled trial.', 'date': '2004-06-08'}}",1.0,Internal Medicine & Subspecialties
14,"Is length gain higher, lower, or the same when comparing moderate protein concentration to low protein concentration?",higher,low,no,"['27801753', '10838460', '8906139']",33215474,2020,"{'27801753': {'article_id': '27801753', 'content': 'The aim of the study was to determine whether higher enteral protein intake leads to improved head growth at 40 weeks postmenstrual age (PMA) in preterm infants <32 weeks or 1500 g.\nRandomized controlled trial in which 120 infants were assigned to either group A with higher enteral protein intake achieved by fortification with higher protein containing fortifier (1 g/100 mL expressed breast milk) or to group B with lower enteral protein intake where fortification was done with standard available protein fortifier (0.4 g /100 mL expressed breast milk).\nThe mean (standard deviation) protein intake was higher in group A as compared to group B; 4.2 (0.47) compared with 3.6 (0.37) g\u200a·\u200akg\u200a·\u200aday, P\u200a<\u200a0.001. At 40 weeks PMA, the mean (standard deviation) weekly occipitofrontal circumference gain was significantly higher in group A as compared to group B; 0.66 (0.16) compared with 0.60 (0.15) cm/week (mean difference 0.064, 95% confidence interval [0.004-0.123], [P\u200a=\u200a0.04]). Weight growth velocity in group A was 11.95 (2.2) g\u200a·\u200akg\u200a·\u200aday as compared to 10.78 (2.6) g\u200a·\u200akg\u200a·\u200aday in group B (mean difference 1.10, 95% confidence interval [0.25-2.07], [P\u200a=\u200a0.01]). No difference was observed in the length between the 2 groups. There was no difference in growth indices and neurodevelopmental outcomes at 12 to 18 months corrected age in the 2 groups.\nFortification of expressed human milk with fortifier containing higher protein results in better head growth and weight gain at 40 weeks PMA in preterm infants <32 weeks or 1500 g without any benefits on long-term growth and neurodevelopment at 12 to 18 months corrected age (CTRI/2014/06/004661).', 'title': 'Effect of Differential Enteral Protein on Growth and Neurodevelopment in Infants <1500 g: A Randomized Controlled Trial.', 'date': '2016-11-02'}, '10838460': {'article_id': '10838460', 'content': ""Human milk fortification has been advocated to enhance premature infants' growth. We, therefore, undertook this study of a new human milk fortifier containing more protein than a reference one.\nOpen, randomized, controlled, multiclinic trial, with weekly growth parameters and safety evaluations in premature infants <1,500 g.\nThe 2 groups did not differ in demographic and baseline characteristics. The adjusted daily milk intake was significantly higher in the infants fed reference human milk fortifier (n = 29; 154.2 +/- 2.1 vs. 144.4 +/- 2.5 ml/kg/day, mean +/- SE; p < 0.05). Both human milk fortifiers produced increases over baseline in weight, length, and head circumference, with greater gains observed in the new human milk fortifier-fed infants for the former two parameters (weight gain 26.8 +/- 1.3 and 20.4 +/- 1.2 g/day, p < 0.05; head circumference 1.0 +/- 0.1 and 0.8 +/- 0.1 cm/week; length 0.9 +/- 0.1 and 0.8 +/- 0.1 cm/week, respectively). Serum chemistries were normal and acceptable for age. Study events were typical for premature infants and similar in both groups.\nThis new human milk fortifier had comparable safety to the reference human milk fortifier and promoted faster weight gain and head circumference growth."", 'title': 'Growth in human milk-Fed very low birth weight infants receiving a new human milk fortifier.', 'date': '2000-06-06'}, '8906139': {'article_id': '8906139', 'content': ""To evaluate the added nutritional value of the two commercially available human breast milk fortifiers: Similac Natural Care (NC) and Enfamil Powder (EP).\nA randomized controlled evaluation in healthy preterm neonates.\nNeonatal Intensive Care Unit, Royal University Hospital, Saskatoon, Saskatchewan, and Neonatal Intensive Care Unit, Jewish General Hospital, Montreal, Quebec, Canada.\nHealthy preterm infants admitted to and cared for in the aforementioned neonatal intensive care units.\nHealthy preterm neonates who were receiving expressed breast milk from their own mothers were supplemented with human milk fortifiers (NC and EP) per manufacturer's recommendations.\nGestational age and birth weight, gender, and race. At entry to and exit from the study, serum concentrations of albumin, protein, calcium, phosphorus, and alkaline phosphatase. The age at which the supplements were added and the number of days the infant remained in the hospital. Daily weight gain, head circumference, length, and height were also measured.\nStudent's t test was used to test the differences between the groups and within the groups at entry to and exit from the study. Fisher's exact test was used to determine differences in race, size, and gestational age in each group. When necessary, a chi 2 test was used to analyze the preponderance of either sex in each group. A Wilcoxon rank test was applied to the true exit date to determine whether the bias was comparable in each group.\nThe mean (+/- standard error) gestational age and birth weight were similar in both groups: 30 +/- 0.3 weeks and 1,314 +/- 40 g, respectively, for NC vs 29.6 +/- 0.35 weeks and 1,262 +/- 45 g, respectively, for EP. At entry to the study, values for the NC group (N = 29) were albumin 31 +/- 1.2 g/L, serum protein 48 +/- 1.4 g/L, calcium 2.4 +/- 0.03 mmol/L, phosphorus 1.85 +/- 0.08 mmol/L, alkaline phosphatase 347 +/- 27 IU/L. The values for the EP group (N = 30) were albumin 32 +/- 0.9 g/L, serum protein 49 +/- 1.4 g/L, calcium 2.4 +/- 0.4 mmol/L, phosphorus 1.9 +/- 0.1 mmol/L, alkaline phosphatase 420 +/- 34 IU/L. At the study exit, the values for the NC group were albumin 30 +/- 0.7 g/L, serum protein 45 +/- 0.9 g/L, calcium 2.4 +/- 0.3 mmol/L, phosphorus 1.96 +/- 0.07 mmol/L, and alkaline phosphatase 371 +/- 23 IU/L. The values for the EP group were albumin 32 +/- 1.0 g/L, serum protein 46.0 +/- 1.4 g/L, calcium 2.5 +/- 0.03 mmol/L, serum phosphorus 2.2 +/- 0.1, and alkaline phosphatase 367 +/- 27 IU/L. No significant differences were observed between groups at entry to and exit from the study. However, in the EP group the alkaline phosphatase decreased significantly (P = .02) from entry to exit and calcium increased significantly during the same period compared with the NC group (P = .003). The mean daily weight gain was 33 +/- 0.7 g for the NC group and 31 +/- 1 g for the EP group. The weekly gain in head circumference and body length were also similar in both groups: approximately 1 cm/week. Both groups tolerated the fortifiers well.\nThese findings suggest that both products provide the additional nutritional support necessary for optimal overall postnatal growth in healthy preterm infants. The differences in calcium and alkaline phosphatase may be due to the differences in vitamin D content in fortifiers 88 IU/100 mL in mixed NC vs 270 IU/100 mL in mixed EP. This observation calls for careful monitoring of calcium and alkaline phosphatase values and possible adjustments of vitamin D intake when fortifiers are used for extended periods."", 'title': 'A randomized, controlled evaluation of two commercially available human breast milk fortifiers in healthy preterm neonates.', 'date': '1996-11-01'}}",0.333333333,Pediatrics & Neonatology
15,"Is head circumference gain higher, lower, or the same when comparing moderate protein concentration to low protein concentration?",higher,very low,no,"['27801753', '10838460', '8906139']",33215474,2020,"{'27801753': {'article_id': '27801753', 'content': 'The aim of the study was to determine whether higher enteral protein intake leads to improved head growth at 40 weeks postmenstrual age (PMA) in preterm infants <32 weeks or 1500 g.\nRandomized controlled trial in which 120 infants were assigned to either group A with higher enteral protein intake achieved by fortification with higher protein containing fortifier (1 g/100 mL expressed breast milk) or to group B with lower enteral protein intake where fortification was done with standard available protein fortifier (0.4 g /100 mL expressed breast milk).\nThe mean (standard deviation) protein intake was higher in group A as compared to group B; 4.2 (0.47) compared with 3.6 (0.37) g\u200a·\u200akg\u200a·\u200aday, P\u200a<\u200a0.001. At 40 weeks PMA, the mean (standard deviation) weekly occipitofrontal circumference gain was significantly higher in group A as compared to group B; 0.66 (0.16) compared with 0.60 (0.15) cm/week (mean difference 0.064, 95% confidence interval [0.004-0.123], [P\u200a=\u200a0.04]). Weight growth velocity in group A was 11.95 (2.2) g\u200a·\u200akg\u200a·\u200aday as compared to 10.78 (2.6) g\u200a·\u200akg\u200a·\u200aday in group B (mean difference 1.10, 95% confidence interval [0.25-2.07], [P\u200a=\u200a0.01]). No difference was observed in the length between the 2 groups. There was no difference in growth indices and neurodevelopmental outcomes at 12 to 18 months corrected age in the 2 groups.\nFortification of expressed human milk with fortifier containing higher protein results in better head growth and weight gain at 40 weeks PMA in preterm infants <32 weeks or 1500 g without any benefits on long-term growth and neurodevelopment at 12 to 18 months corrected age (CTRI/2014/06/004661).', 'title': 'Effect of Differential Enteral Protein on Growth and Neurodevelopment in Infants <1500 g: A Randomized Controlled Trial.', 'date': '2016-11-02'}, '10838460': {'article_id': '10838460', 'content': ""Human milk fortification has been advocated to enhance premature infants' growth. We, therefore, undertook this study of a new human milk fortifier containing more protein than a reference one.\nOpen, randomized, controlled, multiclinic trial, with weekly growth parameters and safety evaluations in premature infants <1,500 g.\nThe 2 groups did not differ in demographic and baseline characteristics. The adjusted daily milk intake was significantly higher in the infants fed reference human milk fortifier (n = 29; 154.2 +/- 2.1 vs. 144.4 +/- 2.5 ml/kg/day, mean +/- SE; p < 0.05). Both human milk fortifiers produced increases over baseline in weight, length, and head circumference, with greater gains observed in the new human milk fortifier-fed infants for the former two parameters (weight gain 26.8 +/- 1.3 and 20.4 +/- 1.2 g/day, p < 0.05; head circumference 1.0 +/- 0.1 and 0.8 +/- 0.1 cm/week; length 0.9 +/- 0.1 and 0.8 +/- 0.1 cm/week, respectively). Serum chemistries were normal and acceptable for age. Study events were typical for premature infants and similar in both groups.\nThis new human milk fortifier had comparable safety to the reference human milk fortifier and promoted faster weight gain and head circumference growth."", 'title': 'Growth in human milk-Fed very low birth weight infants receiving a new human milk fortifier.', 'date': '2000-06-06'}, '8906139': {'article_id': '8906139', 'content': ""To evaluate the added nutritional value of the two commercially available human breast milk fortifiers: Similac Natural Care (NC) and Enfamil Powder (EP).\nA randomized controlled evaluation in healthy preterm neonates.\nNeonatal Intensive Care Unit, Royal University Hospital, Saskatoon, Saskatchewan, and Neonatal Intensive Care Unit, Jewish General Hospital, Montreal, Quebec, Canada.\nHealthy preterm infants admitted to and cared for in the aforementioned neonatal intensive care units.\nHealthy preterm neonates who were receiving expressed breast milk from their own mothers were supplemented with human milk fortifiers (NC and EP) per manufacturer's recommendations.\nGestational age and birth weight, gender, and race. At entry to and exit from the study, serum concentrations of albumin, protein, calcium, phosphorus, and alkaline phosphatase. The age at which the supplements were added and the number of days the infant remained in the hospital. Daily weight gain, head circumference, length, and height were also measured.\nStudent's t test was used to test the differences between the groups and within the groups at entry to and exit from the study. Fisher's exact test was used to determine differences in race, size, and gestational age in each group. When necessary, a chi 2 test was used to analyze the preponderance of either sex in each group. A Wilcoxon rank test was applied to the true exit date to determine whether the bias was comparable in each group.\nThe mean (+/- standard error) gestational age and birth weight were similar in both groups: 30 +/- 0.3 weeks and 1,314 +/- 40 g, respectively, for NC vs 29.6 +/- 0.35 weeks and 1,262 +/- 45 g, respectively, for EP. At entry to the study, values for the NC group (N = 29) were albumin 31 +/- 1.2 g/L, serum protein 48 +/- 1.4 g/L, calcium 2.4 +/- 0.03 mmol/L, phosphorus 1.85 +/- 0.08 mmol/L, alkaline phosphatase 347 +/- 27 IU/L. The values for the EP group (N = 30) were albumin 32 +/- 0.9 g/L, serum protein 49 +/- 1.4 g/L, calcium 2.4 +/- 0.4 mmol/L, phosphorus 1.9 +/- 0.1 mmol/L, alkaline phosphatase 420 +/- 34 IU/L. At the study exit, the values for the NC group were albumin 30 +/- 0.7 g/L, serum protein 45 +/- 0.9 g/L, calcium 2.4 +/- 0.3 mmol/L, phosphorus 1.96 +/- 0.07 mmol/L, and alkaline phosphatase 371 +/- 23 IU/L. The values for the EP group were albumin 32 +/- 1.0 g/L, serum protein 46.0 +/- 1.4 g/L, calcium 2.5 +/- 0.03 mmol/L, serum phosphorus 2.2 +/- 0.1, and alkaline phosphatase 367 +/- 27 IU/L. No significant differences were observed between groups at entry to and exit from the study. However, in the EP group the alkaline phosphatase decreased significantly (P = .02) from entry to exit and calcium increased significantly during the same period compared with the NC group (P = .003). The mean daily weight gain was 33 +/- 0.7 g for the NC group and 31 +/- 1 g for the EP group. The weekly gain in head circumference and body length were also similar in both groups: approximately 1 cm/week. Both groups tolerated the fortifiers well.\nThese findings suggest that both products provide the additional nutritional support necessary for optimal overall postnatal growth in healthy preterm infants. The differences in calcium and alkaline phosphatase may be due to the differences in vitamin D content in fortifiers 88 IU/100 mL in mixed NC vs 270 IU/100 mL in mixed EP. This observation calls for careful monitoring of calcium and alkaline phosphatase values and possible adjustments of vitamin D intake when fortifiers are used for extended periods."", 'title': 'A randomized, controlled evaluation of two commercially available human breast milk fortifiers in healthy preterm neonates.', 'date': '1996-11-01'}}",0.666666667,Pediatrics & Neonatology
16,"Is weight gain higher, lower, or the same when comparing moderate protein concentration to low protein concentration?",higher,very low,no,"['27801753', '10838460']",33215474,2020,"{'27801753': {'article_id': '27801753', 'content': 'The aim of the study was to determine whether higher enteral protein intake leads to improved head growth at 40 weeks postmenstrual age (PMA) in preterm infants <32 weeks or 1500 g.\nRandomized controlled trial in which 120 infants were assigned to either group A with higher enteral protein intake achieved by fortification with higher protein containing fortifier (1 g/100 mL expressed breast milk) or to group B with lower enteral protein intake where fortification was done with standard available protein fortifier (0.4 g /100 mL expressed breast milk).\nThe mean (standard deviation) protein intake was higher in group A as compared to group B; 4.2 (0.47) compared with 3.6 (0.37) g\u200a·\u200akg\u200a·\u200aday, P\u200a<\u200a0.001. At 40 weeks PMA, the mean (standard deviation) weekly occipitofrontal circumference gain was significantly higher in group A as compared to group B; 0.66 (0.16) compared with 0.60 (0.15) cm/week (mean difference 0.064, 95% confidence interval [0.004-0.123], [P\u200a=\u200a0.04]). Weight growth velocity in group A was 11.95 (2.2) g\u200a·\u200akg\u200a·\u200aday as compared to 10.78 (2.6) g\u200a·\u200akg\u200a·\u200aday in group B (mean difference 1.10, 95% confidence interval [0.25-2.07], [P\u200a=\u200a0.01]). No difference was observed in the length between the 2 groups. There was no difference in growth indices and neurodevelopmental outcomes at 12 to 18 months corrected age in the 2 groups.\nFortification of expressed human milk with fortifier containing higher protein results in better head growth and weight gain at 40 weeks PMA in preterm infants <32 weeks or 1500 g without any benefits on long-term growth and neurodevelopment at 12 to 18 months corrected age (CTRI/2014/06/004661).', 'title': 'Effect of Differential Enteral Protein on Growth and Neurodevelopment in Infants <1500 g: A Randomized Controlled Trial.', 'date': '2016-11-02'}, '10838460': {'article_id': '10838460', 'content': ""Human milk fortification has been advocated to enhance premature infants' growth. We, therefore, undertook this study of a new human milk fortifier containing more protein than a reference one.\nOpen, randomized, controlled, multiclinic trial, with weekly growth parameters and safety evaluations in premature infants <1,500 g.\nThe 2 groups did not differ in demographic and baseline characteristics. The adjusted daily milk intake was significantly higher in the infants fed reference human milk fortifier (n = 29; 154.2 +/- 2.1 vs. 144.4 +/- 2.5 ml/kg/day, mean +/- SE; p < 0.05). Both human milk fortifiers produced increases over baseline in weight, length, and head circumference, with greater gains observed in the new human milk fortifier-fed infants for the former two parameters (weight gain 26.8 +/- 1.3 and 20.4 +/- 1.2 g/day, p < 0.05; head circumference 1.0 +/- 0.1 and 0.8 +/- 0.1 cm/week; length 0.9 +/- 0.1 and 0.8 +/- 0.1 cm/week, respectively). Serum chemistries were normal and acceptable for age. Study events were typical for premature infants and similar in both groups.\nThis new human milk fortifier had comparable safety to the reference human milk fortifier and promoted faster weight gain and head circumference growth."", 'title': 'Growth in human milk-Fed very low birth weight infants receiving a new human milk fortifier.', 'date': '2000-06-06'}}",1.0,Pediatrics & Neonatology
17,"Is head circumference gain higher, lower, or the same when comparing high protein concentration low protein concentration?",uncertain effect,very low,no,"['26488118', '22301933', '22987877', '29772833', '28727654']",33215474,2020,"{'26488118': {'article_id': '26488118', 'content': 'This study was a comparison of growth and tolerance in premature infants fed either standard powdered human milk fortifier (HMF) or a newly formulated concentrated liquid that contained extensively hydrolyzed protein.\nThis was an unblinded randomized controlled multicenter noninferiority study on preterm infants receiving human milk (HM) supplemented with 2 randomly assigned HMFs, either concentrated liquid HMF containing extensively hydrolyzed protein (LE-HMF) or a powdered intact protein HMF (PI-HMF) as the control. The study population consisted of preterm infants ≤33 weeks who were enterally fed HM. Infants were studied from the first day of HM fortification until day 29 or hospital discharge, whichever came first.\nA total of 147 preterm infants were enrolled. Noninferiority was observed in weight gain reported in the intent-to-treat (ITT) analysis was 18.2 and 17.5 g · kg(-1) · day(-1) for the LE-HMF and PI-HMF groups, respectively. In an a priori defined subgroup of strict protocol followers (n\u200a=\u200a75), the infants fed LE-HMF achieved greater weight over time than those fed PI-HMF (P\u200a=\u200a0.036). The LE-HMF group achieved greater linear growth over time compared to the PI-HMF (P\u200a=\u200a0.029). The protein intake from fortified HM was significantly higher in the LE-HMF group compared with the PI-HMF group (3.9 vs 3.3 g · kg(-1) · day(-1), P\u200a<\u200a0.0001). Both fortifiers were well tolerated with no significant differences in overall morbidity.\nBoth fortifiers showed excellent weight gain (grams per kilograms per day), tolerance, and low incidence of morbidity outcomes with the infants who were strict protocol followers fed LE-HMF having improved growth during the study. These data point to the safety and suitability of this new concentrated liquid HMF (LE-HMF) in preterm infants. Growth with this fortifier closely matches the recent recommendations for a weight gain of >18 g · kg(-1) · day(-1).', 'title': 'Growth and Tolerance of Preterm Infants Fed a New Extensively Hydrolyzed Liquid Human Milk Fortifier.', 'date': '2015-10-22'}, '22301933': {'article_id': '22301933', 'content': 'Preterm human milk-fed infants often experience suboptimal growth despite the use of human milk fortifier (HMF). The extra protein supplied in fortifiers may be inadequate to meet dietary protein requirements for preterm infants.\nWe assessed the effect of human milk fortified with a higher-protein HMF on growth in preterm infants.\nThis is a randomized controlled trial in 92 preterm infants born at <31 wk gestation who received maternal breast milk that was fortified with HMF containing 1.4 g protein/100 mL (higher-protein group) or 1.0 g protein/100 mL (current practice) until discharge or estimated due date, whichever came first. The HMFs used were isocaloric and differed only in the amount of protein or carbohydrate. Length, weight, and head-circumference gains were assessed over the study duration.\nLength gains did not differ between the higher- and standard-protein groups (mean difference: 0.06 cm/wk; 95% CI: -0.01, 0.12 cm/wk; P = 0.08). Infants in the higher-protein group achieved a greater weight at study end (mean difference: 220 g; 95% CI: 23, 419 g; P = 0.03). Secondary analyses showed a significant reduction in the proportion of infants who were less than the 10th percentile for length at the study end in the higher-protein group (risk difference: 0.186; 95% CI: 0.370, 0.003; P = 0.047).\nA higher protein intake results in less growth faltering in human milk-fed preterm infants. It is possible that a higher-protein fortifier than used in this study is needed. This trial was registered with the Australian New Zealand Clinical Trials Registry (http://www.anzctr.org.au/) as ACTRN12606000525583.', 'title': 'Effect of increasing protein content of human milk fortifier on growth in preterm infants born at <31 wk gestation: a randomized controlled trial.', 'date': '2012-02-04'}, '22987877': {'article_id': '22987877', 'content': 'To evaluate the growth, tolerance, and safety of a new ultraconcentrated liquid human milk fortifier (LHMF) designed to provide optimal nutrients for preterm infants receiving human breast milk in a safe, nonpowder formulation.\nPreterm infants with a body weight ≤ 1250 g fed expressed and/or donor breast milk were randomized to receive a control powder human milk fortifier (HMF) or a new LHMF for 28 days. When added to breast milk, the LHMF provided ∼20% more protein than the control HMF. Weight, length, head circumference, and serum prealbumin, albumin, blood urea nitrogen, electrolytes, and blood gases were measured. The occurrence of sepsis, necrotizing enterocolitis, and serious adverse events were monitored.\nThis multicenter, third party-blinded, randomized controlled, prospective study enrolled 150 infants. Achieved weight and linear growth rate were significantly higher in the LHMF versus control groups (P = .04 and 0.03, respectively). Among infants who adhered closely to the protocol, the LHMF had a significantly higher achieved weight, length, head circumference, and linear growth rate than the control HMF (P = .004, P = .003, P = .04, and P = .01, respectively). There were no differences in measures of feeding tolerance or days to achieve full feeding volumes. Prealbumin, albumin, and blood urea nitrogen were higher in the LHMF group versus the control group (all P < .05). There was no difference in the incidence of confirmed sepsis or necrotizing enterocolitis.\nUse of a new LHMF in preterm infants instead of powder HMF is safe. Benefits of LHMF include improvements in growth and avoidance of the use of powder products in the NICU.', 'title': 'A new liquid human milk fortifier and linear growth in preterm infants.', 'date': '2012-09-19'}, '29772833': {'article_id': '29772833', 'content': ""The aim of this study was to assess the effect of feeding high protein human milk fortifier (HMF) on growth in preterm infants. In this single-centre randomised trial, 60 infants born 28⁻32 weeks' gestation were randomised to receive a higher protein HMF providing 1.8 g protein ("", 'title': 'The Effect of Increasing the Protein Content of Human Milk Fortifier to 1.8 g/100 mL on Growth in Preterm Infants: A Randomised Controlled Trial.', 'date': '2018-05-19'}, '28727654': {'article_id': '28727654', 'content': 'The aim of this study was to assess growth and nutritional biomarkers of preterm infants fed human milk (HM) supplemented with a new powdered HM fortifier (nHMF) or a control HM fortifier (cHMF). The nHMF provides similar energy content, 16% more protein (partially hydrolyzed whey), and higher micronutrient levels than the cHMF, along with medium-chain triglycerides and docosahexaenoic acid.\nIn this controlled, multicenter, double-blind study, a sample of preterm infants ≤32 weeks or ≤1500\u200ag were randomized to receive nHMF (n\u200a=\u200a77) or cHMF (n\u200a=\u200a76) for a minimum of 21 days. Weight gain was evaluated for noninferiority (margin\u200a=\u200a-1\u200ag/day) and superiority (margin\u200a=\u200a0\u200ag/day). Nutritional status and gut inflammation were assessed by blood, urine, and fecal biochemistries. Adverse events were monitored.\nAdjusted mean weight gain (analysis of covariance) was 2.3\u200ag/day greater in nHMF versus cHMF; the lower limit of the 95% CI (0.4\u200ag/day) exceeded both noninferiority (P\u200a<\u200a0.001) and superiority margins (P\u200a=\u200a0.01). Weight gain rate (unadjusted) was 18.3 (nHMF) and 16.8\u200ag\u200a·\u200akg\u200a·\u200aday (cHMF) between study days 1 and 21 (D1-D21). Length and head circumference (HC) gains between D1 and D21 were not different. Adjusted weight-for-age z score at D21 and HC-for-age z score at week 40 corrected age were greater in nHMF versus cHMF (P\u200a=\u200a0.013, P\u200a=\u200a0.003 respectively). nHMF had higher serum blood urea nitrogen, pre-albumin, alkaline phosphatase, and calcium (all within normal ranges; all P\u200a≤\u200a0.019) at D21 versus cHMF. Both HMFs were well tolerated with similar incidence of gastrointestinal adverse events.\nnHMF providing more protein and fat compared to a control fortifier is safe, well-tolerated, and improves the weight gain of preterm infants.', 'title': 'Growth and Nutritional Biomarkers of Preterm Infants Fed a New Powdered Human Milk Fortifier: A Randomized Trial.', 'date': '2017-07-21'}}",0.0,Pediatrics & Neonatology
18,"Is length gain higher, lower, or the same when comparing high protein concentration low protein concentration?",uncertain effect,very low,no,"['26488118', '22301933', '22987877', '29772833', '28727654']",33215474,2020,"{'26488118': {'article_id': '26488118', 'content': 'This study was a comparison of growth and tolerance in premature infants fed either standard powdered human milk fortifier (HMF) or a newly formulated concentrated liquid that contained extensively hydrolyzed protein.\nThis was an unblinded randomized controlled multicenter noninferiority study on preterm infants receiving human milk (HM) supplemented with 2 randomly assigned HMFs, either concentrated liquid HMF containing extensively hydrolyzed protein (LE-HMF) or a powdered intact protein HMF (PI-HMF) as the control. The study population consisted of preterm infants ≤33 weeks who were enterally fed HM. Infants were studied from the first day of HM fortification until day 29 or hospital discharge, whichever came first.\nA total of 147 preterm infants were enrolled. Noninferiority was observed in weight gain reported in the intent-to-treat (ITT) analysis was 18.2 and 17.5 g · kg(-1) · day(-1) for the LE-HMF and PI-HMF groups, respectively. In an a priori defined subgroup of strict protocol followers (n\u200a=\u200a75), the infants fed LE-HMF achieved greater weight over time than those fed PI-HMF (P\u200a=\u200a0.036). The LE-HMF group achieved greater linear growth over time compared to the PI-HMF (P\u200a=\u200a0.029). The protein intake from fortified HM was significantly higher in the LE-HMF group compared with the PI-HMF group (3.9 vs 3.3 g · kg(-1) · day(-1), P\u200a<\u200a0.0001). Both fortifiers were well tolerated with no significant differences in overall morbidity.\nBoth fortifiers showed excellent weight gain (grams per kilograms per day), tolerance, and low incidence of morbidity outcomes with the infants who were strict protocol followers fed LE-HMF having improved growth during the study. These data point to the safety and suitability of this new concentrated liquid HMF (LE-HMF) in preterm infants. Growth with this fortifier closely matches the recent recommendations for a weight gain of >18 g · kg(-1) · day(-1).', 'title': 'Growth and Tolerance of Preterm Infants Fed a New Extensively Hydrolyzed Liquid Human Milk Fortifier.', 'date': '2015-10-22'}, '22301933': {'article_id': '22301933', 'content': 'Preterm human milk-fed infants often experience suboptimal growth despite the use of human milk fortifier (HMF). The extra protein supplied in fortifiers may be inadequate to meet dietary protein requirements for preterm infants.\nWe assessed the effect of human milk fortified with a higher-protein HMF on growth in preterm infants.\nThis is a randomized controlled trial in 92 preterm infants born at <31 wk gestation who received maternal breast milk that was fortified with HMF containing 1.4 g protein/100 mL (higher-protein group) or 1.0 g protein/100 mL (current practice) until discharge or estimated due date, whichever came first. The HMFs used were isocaloric and differed only in the amount of protein or carbohydrate. Length, weight, and head-circumference gains were assessed over the study duration.\nLength gains did not differ between the higher- and standard-protein groups (mean difference: 0.06 cm/wk; 95% CI: -0.01, 0.12 cm/wk; P = 0.08). Infants in the higher-protein group achieved a greater weight at study end (mean difference: 220 g; 95% CI: 23, 419 g; P = 0.03). Secondary analyses showed a significant reduction in the proportion of infants who were less than the 10th percentile for length at the study end in the higher-protein group (risk difference: 0.186; 95% CI: 0.370, 0.003; P = 0.047).\nA higher protein intake results in less growth faltering in human milk-fed preterm infants. It is possible that a higher-protein fortifier than used in this study is needed. This trial was registered with the Australian New Zealand Clinical Trials Registry (http://www.anzctr.org.au/) as ACTRN12606000525583.', 'title': 'Effect of increasing protein content of human milk fortifier on growth in preterm infants born at <31 wk gestation: a randomized controlled trial.', 'date': '2012-02-04'}, '22987877': {'article_id': '22987877', 'content': 'To evaluate the growth, tolerance, and safety of a new ultraconcentrated liquid human milk fortifier (LHMF) designed to provide optimal nutrients for preterm infants receiving human breast milk in a safe, nonpowder formulation.\nPreterm infants with a body weight ≤ 1250 g fed expressed and/or donor breast milk were randomized to receive a control powder human milk fortifier (HMF) or a new LHMF for 28 days. When added to breast milk, the LHMF provided ∼20% more protein than the control HMF. Weight, length, head circumference, and serum prealbumin, albumin, blood urea nitrogen, electrolytes, and blood gases were measured. The occurrence of sepsis, necrotizing enterocolitis, and serious adverse events were monitored.\nThis multicenter, third party-blinded, randomized controlled, prospective study enrolled 150 infants. Achieved weight and linear growth rate were significantly higher in the LHMF versus control groups (P = .04 and 0.03, respectively). Among infants who adhered closely to the protocol, the LHMF had a significantly higher achieved weight, length, head circumference, and linear growth rate than the control HMF (P = .004, P = .003, P = .04, and P = .01, respectively). There were no differences in measures of feeding tolerance or days to achieve full feeding volumes. Prealbumin, albumin, and blood urea nitrogen were higher in the LHMF group versus the control group (all P < .05). There was no difference in the incidence of confirmed sepsis or necrotizing enterocolitis.\nUse of a new LHMF in preterm infants instead of powder HMF is safe. Benefits of LHMF include improvements in growth and avoidance of the use of powder products in the NICU.', 'title': 'A new liquid human milk fortifier and linear growth in preterm infants.', 'date': '2012-09-19'}, '29772833': {'article_id': '29772833', 'content': ""The aim of this study was to assess the effect of feeding high protein human milk fortifier (HMF) on growth in preterm infants. In this single-centre randomised trial, 60 infants born 28⁻32 weeks' gestation were randomised to receive a higher protein HMF providing 1.8 g protein ("", 'title': 'The Effect of Increasing the Protein Content of Human Milk Fortifier to 1.8 g/100 mL on Growth in Preterm Infants: A Randomised Controlled Trial.', 'date': '2018-05-19'}, '28727654': {'article_id': '28727654', 'content': 'The aim of this study was to assess growth and nutritional biomarkers of preterm infants fed human milk (HM) supplemented with a new powdered HM fortifier (nHMF) or a control HM fortifier (cHMF). The nHMF provides similar energy content, 16% more protein (partially hydrolyzed whey), and higher micronutrient levels than the cHMF, along with medium-chain triglycerides and docosahexaenoic acid.\nIn this controlled, multicenter, double-blind study, a sample of preterm infants ≤32 weeks or ≤1500\u200ag were randomized to receive nHMF (n\u200a=\u200a77) or cHMF (n\u200a=\u200a76) for a minimum of 21 days. Weight gain was evaluated for noninferiority (margin\u200a=\u200a-1\u200ag/day) and superiority (margin\u200a=\u200a0\u200ag/day). Nutritional status and gut inflammation were assessed by blood, urine, and fecal biochemistries. Adverse events were monitored.\nAdjusted mean weight gain (analysis of covariance) was 2.3\u200ag/day greater in nHMF versus cHMF; the lower limit of the 95% CI (0.4\u200ag/day) exceeded both noninferiority (P\u200a<\u200a0.001) and superiority margins (P\u200a=\u200a0.01). Weight gain rate (unadjusted) was 18.3 (nHMF) and 16.8\u200ag\u200a·\u200akg\u200a·\u200aday (cHMF) between study days 1 and 21 (D1-D21). Length and head circumference (HC) gains between D1 and D21 were not different. Adjusted weight-for-age z score at D21 and HC-for-age z score at week 40 corrected age were greater in nHMF versus cHMF (P\u200a=\u200a0.013, P\u200a=\u200a0.003 respectively). nHMF had higher serum blood urea nitrogen, pre-albumin, alkaline phosphatase, and calcium (all within normal ranges; all P\u200a≤\u200a0.019) at D21 versus cHMF. Both HMFs were well tolerated with similar incidence of gastrointestinal adverse events.\nnHMF providing more protein and fat compared to a control fortifier is safe, well-tolerated, and improves the weight gain of preterm infants.', 'title': 'Growth and Nutritional Biomarkers of Preterm Infants Fed a New Powdered Human Milk Fortifier: A Randomized Trial.', 'date': '2017-07-21'}}",0.0,Pediatrics & Neonatology
19,"Is mortality higher, lower, or the same when comparing rapid ART to standard initiation?",lower,very low,yes,"['28742880', '27658873', '29509839', '27163694', '29136001', '29112963', '28542080']",31206168,2019,"{'28742880': {'article_id': '28742880', 'content': ""Attrition during the period from HIV testing to antiretroviral therapy (ART) initiation is high worldwide. We assessed whether same-day HIV testing and ART initiation improves retention and virologic suppression.\nWe conducted an unblinded, randomized trial of standard ART initiation versus same-day HIV testing and ART initiation among eligible adults ≥18 years old with World Health Organization Stage 1 or 2 disease and CD4 count ≤500 cells/mm3. The study was conducted among outpatients at the Haitian Group for the Study of Kaposi's Sarcoma and Opportunistic infections (GHESKIO) Clinic in Port-au-Prince, Haiti. Participants were randomly assigned (1:1) to standard ART initiation or same-day HIV testing and ART initiation. The standard group initiated ART 3 weeks after HIV testing, and the same-day group initiated ART on the day of testing. The primary study endpoint was retention in care 12 months after HIV testing with HIV-1 RNA <50 copies/ml. We assessed the impact of treatment arm with a modified intention-to-treat analysis, using multivariable logistic regression controlling for potential confounders. Between August 2013 and October 2015, 762 participants were enrolled; 59 participants transferred to other clinics during the study period, and were excluded as per protocol, leaving 356 in the standard and 347 in the same-day ART groups. In the standard ART group, 156 (44%) participants were retained in care with 12-month HIV-1 RNA <50 copies, and 184 (52%) had <1,000 copies/ml; 20 participants (6%) died. In the same-day ART group, 184 (53%) participants were retained with HIV-1 RNA <50 copies/ml, and 212 (61%) had <1,000 copies/ml; 10 (3%) participants died. The unadjusted risk ratio (RR) of being retained at 12 months with HIV-1 RNA <50 copies/ml was 1.21 (95% CI: 1.04, 1.38; p = 0.015) for the same-day ART group compared to the standard ART group, and the unadjusted RR for being retained with HIV-1 RNA <1,000 copies was 1.18 (95% CI: 1.04, 1.31; p = 0.012). The main limitation of this study is that it was conducted at a single urban clinic, and the generalizability to other settings is uncertain.\nSame-day HIV testing and ART initiation is feasible and beneficial in this setting, as it improves retention in care with virologic suppression among patients with early clinical HIV disease.\nThis study is registered with ClinicalTrials.gov number NCT01900080."", 'title': 'Same-day HIV testing with initiation of antiretroviral therapy versus standard care for persons living with HIV: A randomized unblinded trial.', 'date': '2017-07-26'}, '27658873': {'article_id': '27658873', 'content': ""In Africa, up to 30% of HIV-infected patients who are clinically eligible for antiretroviral therapy (ART) do not start timely treatment. We assessed the effects of an intervention targeting prevalent health systems barriers to ART initiation on timing and completeness of treatment initiation.\nIn this stepped-wedge, non-blinded, cluster-randomised controlled trial, 20 clinics in southwestern Uganda were randomly assigned in groups of five clinics every 6 months to the intervention by a computerised random number generator. This procedure continued until all clinics had crossed over from control (standard of care) to the intervention, which consisted of opinion-leader-led training and coaching of front-line health workers, a point-of-care CD4 cell count testing platform, a revised counselling approach without mandatory multiple pre-initiation sessions, and feedback to the facilities on their ART initiation rates and how they compared with other facilities. Treatment-naive, HIV-infected adults (aged ≥18 years) who were clinically eligible for ART during the study period were included in the study population. The primary outcome was ART initiation 14 days after first clinical eligibility for ART. This study is registered with ClinicalTrials.gov, number NCT01810289.\nBetween April 11, 2013, and Feb 2, 2015, 12\u2008024 eligible patients visited one of the 20 participating clinics. Median CD4 count was 310 cells per μL (IQR 179-424). 3753 of 4747 patients (weighted proportion 80%) in the intervention group had started ART by 2 weeks after eligibility compared with 2585 of 7066 patients (38%) in the control group (risk difference 41·9%, 95% CI 40·1-43·8). Vital status was ascertained in a random sample of 208 patients in the intervention group and 199 patients in the control group. Four deaths (2%) occurred in the intervention group and five (3%) occurred in the control group.\nA multicomponent intervention targeting health-care worker behaviour increased the probability of ART initiation 14 days after eligibility. This intervention consists of widely accessible components and has been tested in a real-world setting, and is therefore well positioned for use at scale.\nNational Institute of Allergy and Infectious Diseases (NIAID) and the President's Emergency Fund for AIDS Relief (PEPFAR)."", 'title': 'Effects of a multicomponent intervention to streamline initiation of antiretroviral therapy in Africa: a stepped-wedge cluster-randomised trial.', 'date': '2016-10-30'}, '29509839': {'article_id': '29509839', 'content': 'Home-based HIV testing is a frequently used strategy to increase awareness of HIV status in sub-Saharan Africa. However, with referral to health facilities, less than half of those who test HIV positive link to care and initiate antiretroviral therapy (ART).\nTo determine whether offering same-day home-based ART to patients with HIV improves linkage to care and viral suppression in a rural, high-prevalence setting in sub-Saharan Africa.\nOpen-label, 2-group, randomized clinical trial (February 22, 2016-September 17, 2017), involving 6 health care facilities in northern Lesotho. During home-based HIV testing in 6655 households from 60 rural villages and 17 urban areas, 278 individuals aged 18 years or older who tested HIV positive and were ART naive from 268 households consented and enrolled. Individuals from the same household were randomized into the same group.\nParticipants were randomly assigned to be offered same-day home-based ART initiation (n\u2009=\u2009138) and subsequent follow-up intervals of 1.5, 3, 6, 9, and 12 months after treatment initiation at the health facility or to receive usual care (n\u2009=\u2009140) with referral to the nearest health facility for preparatory counseling followed by ART initiation and monthly follow-up visits thereafter.\nPrimary end points were rates of linkage to care within 3 months (presenting at the health facility within 90 days after the home visit) and viral suppression at 12 months, defined as a viral load of less than 100 copies/mL from 11 through 14 months after enrollment.\nAmong 278 randomized individuals (median age, 39 years [interquartile range, 28.0-52.0]; 180 women [65.7%]), 274 (98.6%) were included in the analysis (137 in the same-day group and 137 in the usual care group). In the same-day group, 134 (97.8%) indicated readiness to start ART that day and 2 (1.5%) within the next few days and were given a 1-month supply of ART. At 3 months, 68.6% (94) in same-day group vs 43.1% (59) in usual care group had linked to care (absolute difference, 25.6%; 95% CI, 13.8% to 36.3%; P\u2009<\u2009.001). At 12 months, 50.4% (69) in the same-day group vs 34.3% (47) in usual care group achieved viral suppression (absolute difference, 16.0%; 4.4%-27.2%; P\u2009=\u2009.007). Two deaths (1.5%) were reported in the same-day group, none in usual care group.\nAmong adults in rural Lesotho, a setting of high HIV prevalence, offering same-day home-based ART initiation to individuals who tested positive during home-based HIV testing, compared with usual care and standard clinic referral, significantly increased linkage to care at 3 months and HIV viral suppression at 12 months. These findings support the practice of offering same-day ART initiation during home-based HIV testing.\nclinicaltrials.gov Identifier: NCT02692027.', 'title': 'Effect of Offering Same-Day ART vs Usual Health Facility Referral During Home-Based HIV Testing on Linkage to Care and Viral Suppression Among Adults With HIV in Lesotho: The CASCADE Randomized Clinical Trial.', 'date': '2018-03-07'}, '27163694': {'article_id': '27163694', 'content': 'High rates of patient attrition from care between HIV testing and antiretroviral therapy (ART) initiation have been documented in sub-Saharan Africa, contributing to persistently low CD4 cell counts at treatment initiation. One reason for this is that starting ART in many countries is a lengthy and burdensome process, imposing long waits and multiple clinic visits on patients. We estimated the effect on uptake of ART and viral suppression of an accelerated initiation algorithm that allowed treatment-eligible patients to be dispensed their first supply of antiretroviral medications on the day of their first HIV-related clinic visit.\nRapIT (Rapid Initiation of Treatment) was an unblinded randomized controlled trial of single-visit ART initiation in two public sector clinics in South Africa, a primary health clinic (PHC) and a hospital-based HIV clinic. Adult (≥18 y old), non-pregnant patients receiving a positive HIV test or first treatment-eligible CD4 count were randomized to standard or rapid initiation. Patients in the rapid-initiation arm of the study (""rapid arm"") received a point-of-care (POC) CD4 count if needed; those who were ART-eligible received a POC tuberculosis (TB) test if symptomatic, POC blood tests, physical exam, education, counseling, and antiretroviral (ARV) dispensing. Patients in the standard-initiation arm of the study (""standard arm"") followed standard clinic procedures (three to five additional clinic visits over 2-4 wk prior to ARV dispensing). Follow up was by record review only. The primary outcome was viral suppression, defined as initiated, retained in care, and suppressed (≤400 copies/ml) within 10 mo of study enrollment. Secondary outcomes included initiation of ART ≤90 d of study enrollment, retention in care, time to ART initiation, patient-level predictors of primary outcomes, prevalence of TB symptoms, and the feasibility and acceptability of the intervention. A survival analysis was conducted comparing attrition from care after ART initiation between the groups among those who initiated within 90 d. Three hundred and seventy-seven patients were enrolled in the study between May 8, 2013 and August 29, 2014 (median CD4 count 210 cells/mm3). In the rapid arm, 119/187 patients (64%) initiated treatment and were virally suppressed at 10 mo, compared to 96/190 (51%) in the standard arm (relative risk [RR] 1.26 [1.05-1.50]). In the rapid arm 182/187 (97%) initiated ART ≤90 d, compared to 136/190 (72%) in the standard arm (RR 1.36, 95% confidence interval [CI], 1.24-1.49). Among 318 patients who did initiate ART within 90 d, the hazard of attrition within the first 10 mo did not differ between the treatment arms (hazard ratio [HR] 1.06; 95% CI 0.61-1.84). The study was limited by the small number of sites and small sample size, and the generalizability of the results to other settings and to non-research conditions is uncertain.\nOffering single-visit ART initiation to adult patients in South Africa increased uptake of ART by 36% and viral suppression by 26%. This intervention should be considered for adoption in the public sector in Africa.\nClinicalTrials.gov NCT01710397, and South African National Clinical Trials Register DOH-27-0213-4177.', 'title': ""Initiating Antiretroviral Therapy for HIV at a Patient's First Clinic Visit: The RapIT Randomized Controlled Trial."", 'date': '2016-05-11'}, '29136001': {'article_id': '29136001', 'content': 'Concerning gaps in the HIV care continuum compromise individual and population health. We evaluated a combination intervention strategy (CIS) targeting prevalent barriers to timely linkage and sustained retention in HIV care in Mozambique.\nIn this cluster-randomized trial, 10 primary health facilities in the city of Maputo and Inhambane Province were randomly assigned to provide the CIS or the standard of care (SOC). The CIS included point-of-care CD4 testing at the time of diagnosis, accelerated ART initiation, and short message service (SMS) health messages and appointment reminders. A pre-post intervention 2-sample design was nested within the CIS arm to assess the effectiveness of CIS+, an enhanced version of the CIS that additionally included conditional non-cash financial incentives for linkage and retention. The primary outcome was a combined outcome of linkage to care within 1 month and retention at 12 months after diagnosis. From April 22, 2013, to June 30, 2015, we enrolled 2,004 out of 5,327 adults ≥18 years of age diagnosed with HIV in the voluntary counseling and testing clinics of participating health facilities: 744 (37%) in the CIS group, 493 (25%) in the CIS+ group, and 767 (38%) in the SOC group. Fifty-seven percent of the CIS group achieved the primary outcome versus 35% in the SOC group (relative risk [RR]CIS vs SOC = 1.58, 95% CI 1.05-2.39). Eighty-nine percent of the CIS group linked to care on the day of diagnosis versus 16% of the SOC group (RRCIS vs SOC = 9.13, 95% CI 1.65-50.40). There was no significant benefit of adding financial incentives to the CIS in terms of the combined outcome (55% of the CIS+ group achieved the primary outcome, RRCIS+ vs CIS = 0.96, 95% CI 0.81-1.16). Key limitations include the use of existing medical records to assess outcomes, the inability to isolate the effect of each component of the CIS, non-concurrent enrollment of the CIS+ group, and exclusion of many patients newly diagnosed with HIV.\nThe CIS showed promise for making much needed gains in the HIV care continuum in our study, particularly in the critical first step of timely linkage to care following diagnosis.\nClinicalTrials.gov NCT01930084.', 'title': 'A combination intervention strategy to improve linkage to and retention in HIV care following diagnosis in Mozambique: A cluster-randomized study.', 'date': '2017-11-15'}, '29112963': {'article_id': '29112963', 'content': 'Gaps in the HIV care continuum contribute to poor health outcomes and increase HIV transmission. A combination of interventions targeting multiple steps in the continuum is needed to achieve the full beneficial impact of HIV treatment.\nLink4Health, a cluster-randomized controlled trial, evaluated the effectiveness of a combination intervention strategy (CIS) versus the standard of care (SOC) on the primary outcome of linkage to care within 1 month plus retention in care at 12 months after HIV-positive testing. Ten clusters of HIV clinics in Swaziland were randomized 1:1 to CIS versus SOC. The CIS included point-of-care CD4+ testing at the time of an HIV-positive test, accelerated antiretroviral therapy (ART) initiation for treatment-eligible participants, mobile phone appointment reminders, health educational packages, and noncash financial incentives. Secondary outcomes included each component of the primary outcome, mean time to linkage, assessment for ART eligibility, ART initiation and time to ART initiation, viral suppression defined as HIV-1 RNA < 1,000 copies/mL at 12 months after HIV testing among patients on ART ≥6 months, and loss to follow-up and death at 12 months after HIV testing. A total of 2,197 adults aged ≥18 years, newly tested HIV positive, were enrolled from 19 August 2013 to 21 November 2014 (1,096 CIS arm; 1,101 SOC arm) and followed for 12 months. The median participant age was 31 years (IQR 26-39), and 59% were women. In an intention-to-treat analysis, 64% (705/1,096) of participants at the CIS sites achieved the primary outcome versus 43% (477/1,101) at the SOC sites (adjusted relative risk [RR] 1.52, 95% CI 1.19-1.96, p = 0.002). Participants in the CIS arm versus the SOC arm had the following secondary outcomes: linkage to care regardless of retention at 12 months (RR 1.08, 95% CI 0.97-1.21, p = 0.13), mean time to linkage (2.5 days versus 7.5 days, p = 0.189), retention in care at 12 months regardless of time to linkage (RR 1.48, 95% CI 1.18-1.86, p = 0.002), assessment for ART eligibility (RR 1.20, 95% CI 1.07-1.34, p = 0.004), ART initiation (RR 1.16, 95% CI 0.96-1.40, p = 0.12), mean time to ART initiation from time of HIV testing (7 days versus 14 days, p < 0.001), viral suppression among those on ART for ≥6 months (RR 0.97, 95% CI 0.88-1.07, p = 0.55), loss to follow-up at 12 months after HIV testing (RR 0.56, 95% CI 0.40-0.79, p = 0.002), and death (N = 78) within 12 months of HIV testing (RR 0.80, 95% CI 0.46-1.35, p = 0.41). Limitations of this study include a small number of clusters and the inability to evaluate the incremental effectiveness of individual components of the combination strategy.\nA combination strategy inclusive of 5 evidence-based interventions aimed at multiple steps in the HIV care continuum was associated with significant increase in linkage to care plus 12-month retention. This strategy offers promise of enhanced outcomes for HIV-positive patients.\nClinicalTrials.gov NCT01904994.', 'title': 'Effectiveness of a combination strategy for linkage and retention in adult HIV care in Swaziland: The Link4Health cluster randomized trial.', 'date': '2017-11-08'}, '28542080': {'article_id': '28542080', 'content': 'Lack of accessible laboratory infrastructure limits HIV antiretroviral therapy (ART) initiation, monitoring, and retention in many resource-limited settings. Point-of-care testing (POCT) is advocated as a mechanism to overcome these limitations. We executed a pragmatic, prospective, randomized, controlled trial comparing the impact of POCT vs. standard of care (SOC) on treatment initiation and retention in care.\nSelected POC technologies were embedded at 3 primary health clinics in South Africa. Confirmed HIV-positive participants were randomized to either SOC or POC: SOC participants were venesected and specimens referred to the laboratory with patient follow-up as per algorithm (∼3 visits); POC participants had phlebotomy and POCT immediately on-site using Pima CD4 to assess ART eligibility followed by hematology, chemistry, and tuberculosis screening with the goal of receiving same-day adherence counseling and treatment initiation. Participant outcomes measured at recruitment 6 and 12 months after initiation.\nFour hundred thirty-two of 717 treatment eligible participants enrolled between May 2012 and September 2013: 198 (56.7%) SOC; 234 (63.6%) POC. Mean age was 37.4 years; 60.5% were female. Significantly more participants were initiated using POC [adjusted prevalence ratio (aPR) 0.83; 95% confidence interval (CI): 0.74 to 0.93; P < 0.0001], the median time to initiation was 1 day for POC and 26.5 days for SOC. The proportion of patients in care and on ART was similar for both arms at 6 months (47 vs. 50%) (aPR 0.96; 95% CI: 0.79 to 1.16) and 12 months (32 vs. 32%) (aPR 1.05; 95% CI: 0.80 to 1.38), with similar mortality rates. Loss to follow-up at 12 months was higher for POC (36% vs. 51%) (aPR 0.82; 95% CI: 0.65 to 1.04).\nAdoption of POCT accelerated ART initiation but once on treatment, there was unexpectedly higher loss to follow-up on POC and no improvement in outcomes at 12 months over SOC.', 'title': 'Multidisciplinary Point-of-Care Testing in South African Primary Health Care Clinics Accelerates HIV ART Initiation but Does Not Alter Retention in Care.', 'date': '2017-05-26'}}",0.142857143,"Public Health, Epidemiology & Health Systems"
20,"Is retention in care at 12 months higher, lower, or the same when comparing rapid ART to standard initiation?",higher,low,yes,"['28742880', '29509839', '27163694', '29136001', '29112963', '28542080']",31206168,2019,"{'28742880': {'article_id': '28742880', 'content': ""Attrition during the period from HIV testing to antiretroviral therapy (ART) initiation is high worldwide. We assessed whether same-day HIV testing and ART initiation improves retention and virologic suppression.\nWe conducted an unblinded, randomized trial of standard ART initiation versus same-day HIV testing and ART initiation among eligible adults ≥18 years old with World Health Organization Stage 1 or 2 disease and CD4 count ≤500 cells/mm3. The study was conducted among outpatients at the Haitian Group for the Study of Kaposi's Sarcoma and Opportunistic infections (GHESKIO) Clinic in Port-au-Prince, Haiti. Participants were randomly assigned (1:1) to standard ART initiation or same-day HIV testing and ART initiation. The standard group initiated ART 3 weeks after HIV testing, and the same-day group initiated ART on the day of testing. The primary study endpoint was retention in care 12 months after HIV testing with HIV-1 RNA <50 copies/ml. We assessed the impact of treatment arm with a modified intention-to-treat analysis, using multivariable logistic regression controlling for potential confounders. Between August 2013 and October 2015, 762 participants were enrolled; 59 participants transferred to other clinics during the study period, and were excluded as per protocol, leaving 356 in the standard and 347 in the same-day ART groups. In the standard ART group, 156 (44%) participants were retained in care with 12-month HIV-1 RNA <50 copies, and 184 (52%) had <1,000 copies/ml; 20 participants (6%) died. In the same-day ART group, 184 (53%) participants were retained with HIV-1 RNA <50 copies/ml, and 212 (61%) had <1,000 copies/ml; 10 (3%) participants died. The unadjusted risk ratio (RR) of being retained at 12 months with HIV-1 RNA <50 copies/ml was 1.21 (95% CI: 1.04, 1.38; p = 0.015) for the same-day ART group compared to the standard ART group, and the unadjusted RR for being retained with HIV-1 RNA <1,000 copies was 1.18 (95% CI: 1.04, 1.31; p = 0.012). The main limitation of this study is that it was conducted at a single urban clinic, and the generalizability to other settings is uncertain.\nSame-day HIV testing and ART initiation is feasible and beneficial in this setting, as it improves retention in care with virologic suppression among patients with early clinical HIV disease.\nThis study is registered with ClinicalTrials.gov number NCT01900080."", 'title': 'Same-day HIV testing with initiation of antiretroviral therapy versus standard care for persons living with HIV: A randomized unblinded trial.', 'date': '2017-07-26'}, '29509839': {'article_id': '29509839', 'content': 'Home-based HIV testing is a frequently used strategy to increase awareness of HIV status in sub-Saharan Africa. However, with referral to health facilities, less than half of those who test HIV positive link to care and initiate antiretroviral therapy (ART).\nTo determine whether offering same-day home-based ART to patients with HIV improves linkage to care and viral suppression in a rural, high-prevalence setting in sub-Saharan Africa.\nOpen-label, 2-group, randomized clinical trial (February 22, 2016-September 17, 2017), involving 6 health care facilities in northern Lesotho. During home-based HIV testing in 6655 households from 60 rural villages and 17 urban areas, 278 individuals aged 18 years or older who tested HIV positive and were ART naive from 268 households consented and enrolled. Individuals from the same household were randomized into the same group.\nParticipants were randomly assigned to be offered same-day home-based ART initiation (n\u2009=\u2009138) and subsequent follow-up intervals of 1.5, 3, 6, 9, and 12 months after treatment initiation at the health facility or to receive usual care (n\u2009=\u2009140) with referral to the nearest health facility for preparatory counseling followed by ART initiation and monthly follow-up visits thereafter.\nPrimary end points were rates of linkage to care within 3 months (presenting at the health facility within 90 days after the home visit) and viral suppression at 12 months, defined as a viral load of less than 100 copies/mL from 11 through 14 months after enrollment.\nAmong 278 randomized individuals (median age, 39 years [interquartile range, 28.0-52.0]; 180 women [65.7%]), 274 (98.6%) were included in the analysis (137 in the same-day group and 137 in the usual care group). In the same-day group, 134 (97.8%) indicated readiness to start ART that day and 2 (1.5%) within the next few days and were given a 1-month supply of ART. At 3 months, 68.6% (94) in same-day group vs 43.1% (59) in usual care group had linked to care (absolute difference, 25.6%; 95% CI, 13.8% to 36.3%; P\u2009<\u2009.001). At 12 months, 50.4% (69) in the same-day group vs 34.3% (47) in usual care group achieved viral suppression (absolute difference, 16.0%; 4.4%-27.2%; P\u2009=\u2009.007). Two deaths (1.5%) were reported in the same-day group, none in usual care group.\nAmong adults in rural Lesotho, a setting of high HIV prevalence, offering same-day home-based ART initiation to individuals who tested positive during home-based HIV testing, compared with usual care and standard clinic referral, significantly increased linkage to care at 3 months and HIV viral suppression at 12 months. These findings support the practice of offering same-day ART initiation during home-based HIV testing.\nclinicaltrials.gov Identifier: NCT02692027.', 'title': 'Effect of Offering Same-Day ART vs Usual Health Facility Referral During Home-Based HIV Testing on Linkage to Care and Viral Suppression Among Adults With HIV in Lesotho: The CASCADE Randomized Clinical Trial.', 'date': '2018-03-07'}, '27163694': {'article_id': '27163694', 'content': 'High rates of patient attrition from care between HIV testing and antiretroviral therapy (ART) initiation have been documented in sub-Saharan Africa, contributing to persistently low CD4 cell counts at treatment initiation. One reason for this is that starting ART in many countries is a lengthy and burdensome process, imposing long waits and multiple clinic visits on patients. We estimated the effect on uptake of ART and viral suppression of an accelerated initiation algorithm that allowed treatment-eligible patients to be dispensed their first supply of antiretroviral medications on the day of their first HIV-related clinic visit.\nRapIT (Rapid Initiation of Treatment) was an unblinded randomized controlled trial of single-visit ART initiation in two public sector clinics in South Africa, a primary health clinic (PHC) and a hospital-based HIV clinic. Adult (≥18 y old), non-pregnant patients receiving a positive HIV test or first treatment-eligible CD4 count were randomized to standard or rapid initiation. Patients in the rapid-initiation arm of the study (""rapid arm"") received a point-of-care (POC) CD4 count if needed; those who were ART-eligible received a POC tuberculosis (TB) test if symptomatic, POC blood tests, physical exam, education, counseling, and antiretroviral (ARV) dispensing. Patients in the standard-initiation arm of the study (""standard arm"") followed standard clinic procedures (three to five additional clinic visits over 2-4 wk prior to ARV dispensing). Follow up was by record review only. The primary outcome was viral suppression, defined as initiated, retained in care, and suppressed (≤400 copies/ml) within 10 mo of study enrollment. Secondary outcomes included initiation of ART ≤90 d of study enrollment, retention in care, time to ART initiation, patient-level predictors of primary outcomes, prevalence of TB symptoms, and the feasibility and acceptability of the intervention. A survival analysis was conducted comparing attrition from care after ART initiation between the groups among those who initiated within 90 d. Three hundred and seventy-seven patients were enrolled in the study between May 8, 2013 and August 29, 2014 (median CD4 count 210 cells/mm3). In the rapid arm, 119/187 patients (64%) initiated treatment and were virally suppressed at 10 mo, compared to 96/190 (51%) in the standard arm (relative risk [RR] 1.26 [1.05-1.50]). In the rapid arm 182/187 (97%) initiated ART ≤90 d, compared to 136/190 (72%) in the standard arm (RR 1.36, 95% confidence interval [CI], 1.24-1.49). Among 318 patients who did initiate ART within 90 d, the hazard of attrition within the first 10 mo did not differ between the treatment arms (hazard ratio [HR] 1.06; 95% CI 0.61-1.84). The study was limited by the small number of sites and small sample size, and the generalizability of the results to other settings and to non-research conditions is uncertain.\nOffering single-visit ART initiation to adult patients in South Africa increased uptake of ART by 36% and viral suppression by 26%. This intervention should be considered for adoption in the public sector in Africa.\nClinicalTrials.gov NCT01710397, and South African National Clinical Trials Register DOH-27-0213-4177.', 'title': ""Initiating Antiretroviral Therapy for HIV at a Patient's First Clinic Visit: The RapIT Randomized Controlled Trial."", 'date': '2016-05-11'}, '29136001': {'article_id': '29136001', 'content': 'Concerning gaps in the HIV care continuum compromise individual and population health. We evaluated a combination intervention strategy (CIS) targeting prevalent barriers to timely linkage and sustained retention in HIV care in Mozambique.\nIn this cluster-randomized trial, 10 primary health facilities in the city of Maputo and Inhambane Province were randomly assigned to provide the CIS or the standard of care (SOC). The CIS included point-of-care CD4 testing at the time of diagnosis, accelerated ART initiation, and short message service (SMS) health messages and appointment reminders. A pre-post intervention 2-sample design was nested within the CIS arm to assess the effectiveness of CIS+, an enhanced version of the CIS that additionally included conditional non-cash financial incentives for linkage and retention. The primary outcome was a combined outcome of linkage to care within 1 month and retention at 12 months after diagnosis. From April 22, 2013, to June 30, 2015, we enrolled 2,004 out of 5,327 adults ≥18 years of age diagnosed with HIV in the voluntary counseling and testing clinics of participating health facilities: 744 (37%) in the CIS group, 493 (25%) in the CIS+ group, and 767 (38%) in the SOC group. Fifty-seven percent of the CIS group achieved the primary outcome versus 35% in the SOC group (relative risk [RR]CIS vs SOC = 1.58, 95% CI 1.05-2.39). Eighty-nine percent of the CIS group linked to care on the day of diagnosis versus 16% of the SOC group (RRCIS vs SOC = 9.13, 95% CI 1.65-50.40). There was no significant benefit of adding financial incentives to the CIS in terms of the combined outcome (55% of the CIS+ group achieved the primary outcome, RRCIS+ vs CIS = 0.96, 95% CI 0.81-1.16). Key limitations include the use of existing medical records to assess outcomes, the inability to isolate the effect of each component of the CIS, non-concurrent enrollment of the CIS+ group, and exclusion of many patients newly diagnosed with HIV.\nThe CIS showed promise for making much needed gains in the HIV care continuum in our study, particularly in the critical first step of timely linkage to care following diagnosis.\nClinicalTrials.gov NCT01930084.', 'title': 'A combination intervention strategy to improve linkage to and retention in HIV care following diagnosis in Mozambique: A cluster-randomized study.', 'date': '2017-11-15'}, '29112963': {'article_id': '29112963', 'content': 'Gaps in the HIV care continuum contribute to poor health outcomes and increase HIV transmission. A combination of interventions targeting multiple steps in the continuum is needed to achieve the full beneficial impact of HIV treatment.\nLink4Health, a cluster-randomized controlled trial, evaluated the effectiveness of a combination intervention strategy (CIS) versus the standard of care (SOC) on the primary outcome of linkage to care within 1 month plus retention in care at 12 months after HIV-positive testing. Ten clusters of HIV clinics in Swaziland were randomized 1:1 to CIS versus SOC. The CIS included point-of-care CD4+ testing at the time of an HIV-positive test, accelerated antiretroviral therapy (ART) initiation for treatment-eligible participants, mobile phone appointment reminders, health educational packages, and noncash financial incentives. Secondary outcomes included each component of the primary outcome, mean time to linkage, assessment for ART eligibility, ART initiation and time to ART initiation, viral suppression defined as HIV-1 RNA < 1,000 copies/mL at 12 months after HIV testing among patients on ART ≥6 months, and loss to follow-up and death at 12 months after HIV testing. A total of 2,197 adults aged ≥18 years, newly tested HIV positive, were enrolled from 19 August 2013 to 21 November 2014 (1,096 CIS arm; 1,101 SOC arm) and followed for 12 months. The median participant age was 31 years (IQR 26-39), and 59% were women. In an intention-to-treat analysis, 64% (705/1,096) of participants at the CIS sites achieved the primary outcome versus 43% (477/1,101) at the SOC sites (adjusted relative risk [RR] 1.52, 95% CI 1.19-1.96, p = 0.002). Participants in the CIS arm versus the SOC arm had the following secondary outcomes: linkage to care regardless of retention at 12 months (RR 1.08, 95% CI 0.97-1.21, p = 0.13), mean time to linkage (2.5 days versus 7.5 days, p = 0.189), retention in care at 12 months regardless of time to linkage (RR 1.48, 95% CI 1.18-1.86, p = 0.002), assessment for ART eligibility (RR 1.20, 95% CI 1.07-1.34, p = 0.004), ART initiation (RR 1.16, 95% CI 0.96-1.40, p = 0.12), mean time to ART initiation from time of HIV testing (7 days versus 14 days, p < 0.001), viral suppression among those on ART for ≥6 months (RR 0.97, 95% CI 0.88-1.07, p = 0.55), loss to follow-up at 12 months after HIV testing (RR 0.56, 95% CI 0.40-0.79, p = 0.002), and death (N = 78) within 12 months of HIV testing (RR 0.80, 95% CI 0.46-1.35, p = 0.41). Limitations of this study include a small number of clusters and the inability to evaluate the incremental effectiveness of individual components of the combination strategy.\nA combination strategy inclusive of 5 evidence-based interventions aimed at multiple steps in the HIV care continuum was associated with significant increase in linkage to care plus 12-month retention. This strategy offers promise of enhanced outcomes for HIV-positive patients.\nClinicalTrials.gov NCT01904994.', 'title': 'Effectiveness of a combination strategy for linkage and retention in adult HIV care in Swaziland: The Link4Health cluster randomized trial.', 'date': '2017-11-08'}, '28542080': {'article_id': '28542080', 'content': 'Lack of accessible laboratory infrastructure limits HIV antiretroviral therapy (ART) initiation, monitoring, and retention in many resource-limited settings. Point-of-care testing (POCT) is advocated as a mechanism to overcome these limitations. We executed a pragmatic, prospective, randomized, controlled trial comparing the impact of POCT vs. standard of care (SOC) on treatment initiation and retention in care.\nSelected POC technologies were embedded at 3 primary health clinics in South Africa. Confirmed HIV-positive participants were randomized to either SOC or POC: SOC participants were venesected and specimens referred to the laboratory with patient follow-up as per algorithm (∼3 visits); POC participants had phlebotomy and POCT immediately on-site using Pima CD4 to assess ART eligibility followed by hematology, chemistry, and tuberculosis screening with the goal of receiving same-day adherence counseling and treatment initiation. Participant outcomes measured at recruitment 6 and 12 months after initiation.\nFour hundred thirty-two of 717 treatment eligible participants enrolled between May 2012 and September 2013: 198 (56.7%) SOC; 234 (63.6%) POC. Mean age was 37.4 years; 60.5% were female. Significantly more participants were initiated using POC [adjusted prevalence ratio (aPR) 0.83; 95% confidence interval (CI): 0.74 to 0.93; P < 0.0001], the median time to initiation was 1 day for POC and 26.5 days for SOC. The proportion of patients in care and on ART was similar for both arms at 6 months (47 vs. 50%) (aPR 0.96; 95% CI: 0.79 to 1.16) and 12 months (32 vs. 32%) (aPR 1.05; 95% CI: 0.80 to 1.38), with similar mortality rates. Loss to follow-up at 12 months was higher for POC (36% vs. 51%) (aPR 0.82; 95% CI: 0.65 to 1.04).\nAdoption of POCT accelerated ART initiation but once on treatment, there was unexpectedly higher loss to follow-up on POC and no improvement in outcomes at 12 months over SOC.', 'title': 'Multidisciplinary Point-of-Care Testing in South African Primary Health Care Clinics Accelerates HIV ART Initiation but Does Not Alter Retention in Care.', 'date': '2017-05-26'}}",0.666666667,"Public Health, Epidemiology & Health Systems"
21,"Is ART uptake at 12 months higher, lower, or the same when comparing rapid ART to standard initiation?",higher,moderate,yes,"['28742880', '27658873', '29509839', '27163694']",31206168,2019,"{'28742880': {'article_id': '28742880', 'content': ""Attrition during the period from HIV testing to antiretroviral therapy (ART) initiation is high worldwide. We assessed whether same-day HIV testing and ART initiation improves retention and virologic suppression.\nWe conducted an unblinded, randomized trial of standard ART initiation versus same-day HIV testing and ART initiation among eligible adults ≥18 years old with World Health Organization Stage 1 or 2 disease and CD4 count ≤500 cells/mm3. The study was conducted among outpatients at the Haitian Group for the Study of Kaposi's Sarcoma and Opportunistic infections (GHESKIO) Clinic in Port-au-Prince, Haiti. Participants were randomly assigned (1:1) to standard ART initiation or same-day HIV testing and ART initiation. The standard group initiated ART 3 weeks after HIV testing, and the same-day group initiated ART on the day of testing. The primary study endpoint was retention in care 12 months after HIV testing with HIV-1 RNA <50 copies/ml. We assessed the impact of treatment arm with a modified intention-to-treat analysis, using multivariable logistic regression controlling for potential confounders. Between August 2013 and October 2015, 762 participants were enrolled; 59 participants transferred to other clinics during the study period, and were excluded as per protocol, leaving 356 in the standard and 347 in the same-day ART groups. In the standard ART group, 156 (44%) participants were retained in care with 12-month HIV-1 RNA <50 copies, and 184 (52%) had <1,000 copies/ml; 20 participants (6%) died. In the same-day ART group, 184 (53%) participants were retained with HIV-1 RNA <50 copies/ml, and 212 (61%) had <1,000 copies/ml; 10 (3%) participants died. The unadjusted risk ratio (RR) of being retained at 12 months with HIV-1 RNA <50 copies/ml was 1.21 (95% CI: 1.04, 1.38; p = 0.015) for the same-day ART group compared to the standard ART group, and the unadjusted RR for being retained with HIV-1 RNA <1,000 copies was 1.18 (95% CI: 1.04, 1.31; p = 0.012). The main limitation of this study is that it was conducted at a single urban clinic, and the generalizability to other settings is uncertain.\nSame-day HIV testing and ART initiation is feasible and beneficial in this setting, as it improves retention in care with virologic suppression among patients with early clinical HIV disease.\nThis study is registered with ClinicalTrials.gov number NCT01900080."", 'title': 'Same-day HIV testing with initiation of antiretroviral therapy versus standard care for persons living with HIV: A randomized unblinded trial.', 'date': '2017-07-26'}, '27658873': {'article_id': '27658873', 'content': ""In Africa, up to 30% of HIV-infected patients who are clinically eligible for antiretroviral therapy (ART) do not start timely treatment. We assessed the effects of an intervention targeting prevalent health systems barriers to ART initiation on timing and completeness of treatment initiation.\nIn this stepped-wedge, non-blinded, cluster-randomised controlled trial, 20 clinics in southwestern Uganda were randomly assigned in groups of five clinics every 6 months to the intervention by a computerised random number generator. This procedure continued until all clinics had crossed over from control (standard of care) to the intervention, which consisted of opinion-leader-led training and coaching of front-line health workers, a point-of-care CD4 cell count testing platform, a revised counselling approach without mandatory multiple pre-initiation sessions, and feedback to the facilities on their ART initiation rates and how they compared with other facilities. Treatment-naive, HIV-infected adults (aged ≥18 years) who were clinically eligible for ART during the study period were included in the study population. The primary outcome was ART initiation 14 days after first clinical eligibility for ART. This study is registered with ClinicalTrials.gov, number NCT01810289.\nBetween April 11, 2013, and Feb 2, 2015, 12\u2008024 eligible patients visited one of the 20 participating clinics. Median CD4 count was 310 cells per μL (IQR 179-424). 3753 of 4747 patients (weighted proportion 80%) in the intervention group had started ART by 2 weeks after eligibility compared with 2585 of 7066 patients (38%) in the control group (risk difference 41·9%, 95% CI 40·1-43·8). Vital status was ascertained in a random sample of 208 patients in the intervention group and 199 patients in the control group. Four deaths (2%) occurred in the intervention group and five (3%) occurred in the control group.\nA multicomponent intervention targeting health-care worker behaviour increased the probability of ART initiation 14 days after eligibility. This intervention consists of widely accessible components and has been tested in a real-world setting, and is therefore well positioned for use at scale.\nNational Institute of Allergy and Infectious Diseases (NIAID) and the President's Emergency Fund for AIDS Relief (PEPFAR)."", 'title': 'Effects of a multicomponent intervention to streamline initiation of antiretroviral therapy in Africa: a stepped-wedge cluster-randomised trial.', 'date': '2016-10-30'}, '29509839': {'article_id': '29509839', 'content': 'Home-based HIV testing is a frequently used strategy to increase awareness of HIV status in sub-Saharan Africa. However, with referral to health facilities, less than half of those who test HIV positive link to care and initiate antiretroviral therapy (ART).\nTo determine whether offering same-day home-based ART to patients with HIV improves linkage to care and viral suppression in a rural, high-prevalence setting in sub-Saharan Africa.\nOpen-label, 2-group, randomized clinical trial (February 22, 2016-September 17, 2017), involving 6 health care facilities in northern Lesotho. During home-based HIV testing in 6655 households from 60 rural villages and 17 urban areas, 278 individuals aged 18 years or older who tested HIV positive and were ART naive from 268 households consented and enrolled. Individuals from the same household were randomized into the same group.\nParticipants were randomly assigned to be offered same-day home-based ART initiation (n\u2009=\u2009138) and subsequent follow-up intervals of 1.5, 3, 6, 9, and 12 months after treatment initiation at the health facility or to receive usual care (n\u2009=\u2009140) with referral to the nearest health facility for preparatory counseling followed by ART initiation and monthly follow-up visits thereafter.\nPrimary end points were rates of linkage to care within 3 months (presenting at the health facility within 90 days after the home visit) and viral suppression at 12 months, defined as a viral load of less than 100 copies/mL from 11 through 14 months after enrollment.\nAmong 278 randomized individuals (median age, 39 years [interquartile range, 28.0-52.0]; 180 women [65.7%]), 274 (98.6%) were included in the analysis (137 in the same-day group and 137 in the usual care group). In the same-day group, 134 (97.8%) indicated readiness to start ART that day and 2 (1.5%) within the next few days and were given a 1-month supply of ART. At 3 months, 68.6% (94) in same-day group vs 43.1% (59) in usual care group had linked to care (absolute difference, 25.6%; 95% CI, 13.8% to 36.3%; P\u2009<\u2009.001). At 12 months, 50.4% (69) in the same-day group vs 34.3% (47) in usual care group achieved viral suppression (absolute difference, 16.0%; 4.4%-27.2%; P\u2009=\u2009.007). Two deaths (1.5%) were reported in the same-day group, none in usual care group.\nAmong adults in rural Lesotho, a setting of high HIV prevalence, offering same-day home-based ART initiation to individuals who tested positive during home-based HIV testing, compared with usual care and standard clinic referral, significantly increased linkage to care at 3 months and HIV viral suppression at 12 months. These findings support the practice of offering same-day ART initiation during home-based HIV testing.\nclinicaltrials.gov Identifier: NCT02692027.', 'title': 'Effect of Offering Same-Day ART vs Usual Health Facility Referral During Home-Based HIV Testing on Linkage to Care and Viral Suppression Among Adults With HIV in Lesotho: The CASCADE Randomized Clinical Trial.', 'date': '2018-03-07'}, '27163694': {'article_id': '27163694', 'content': 'High rates of patient attrition from care between HIV testing and antiretroviral therapy (ART) initiation have been documented in sub-Saharan Africa, contributing to persistently low CD4 cell counts at treatment initiation. One reason for this is that starting ART in many countries is a lengthy and burdensome process, imposing long waits and multiple clinic visits on patients. We estimated the effect on uptake of ART and viral suppression of an accelerated initiation algorithm that allowed treatment-eligible patients to be dispensed their first supply of antiretroviral medications on the day of their first HIV-related clinic visit.\nRapIT (Rapid Initiation of Treatment) was an unblinded randomized controlled trial of single-visit ART initiation in two public sector clinics in South Africa, a primary health clinic (PHC) and a hospital-based HIV clinic. Adult (≥18 y old), non-pregnant patients receiving a positive HIV test or first treatment-eligible CD4 count were randomized to standard or rapid initiation. Patients in the rapid-initiation arm of the study (""rapid arm"") received a point-of-care (POC) CD4 count if needed; those who were ART-eligible received a POC tuberculosis (TB) test if symptomatic, POC blood tests, physical exam, education, counseling, and antiretroviral (ARV) dispensing. Patients in the standard-initiation arm of the study (""standard arm"") followed standard clinic procedures (three to five additional clinic visits over 2-4 wk prior to ARV dispensing). Follow up was by record review only. The primary outcome was viral suppression, defined as initiated, retained in care, and suppressed (≤400 copies/ml) within 10 mo of study enrollment. Secondary outcomes included initiation of ART ≤90 d of study enrollment, retention in care, time to ART initiation, patient-level predictors of primary outcomes, prevalence of TB symptoms, and the feasibility and acceptability of the intervention. A survival analysis was conducted comparing attrition from care after ART initiation between the groups among those who initiated within 90 d. Three hundred and seventy-seven patients were enrolled in the study between May 8, 2013 and August 29, 2014 (median CD4 count 210 cells/mm3). In the rapid arm, 119/187 patients (64%) initiated treatment and were virally suppressed at 10 mo, compared to 96/190 (51%) in the standard arm (relative risk [RR] 1.26 [1.05-1.50]). In the rapid arm 182/187 (97%) initiated ART ≤90 d, compared to 136/190 (72%) in the standard arm (RR 1.36, 95% confidence interval [CI], 1.24-1.49). Among 318 patients who did initiate ART within 90 d, the hazard of attrition within the first 10 mo did not differ between the treatment arms (hazard ratio [HR] 1.06; 95% CI 0.61-1.84). The study was limited by the small number of sites and small sample size, and the generalizability of the results to other settings and to non-research conditions is uncertain.\nOffering single-visit ART initiation to adult patients in South Africa increased uptake of ART by 36% and viral suppression by 26%. This intervention should be considered for adoption in the public sector in Africa.\nClinicalTrials.gov NCT01710397, and South African National Clinical Trials Register DOH-27-0213-4177.', 'title': ""Initiating Antiretroviral Therapy for HIV at a Patient's First Clinic Visit: The RapIT Randomized Controlled Trial."", 'date': '2016-05-11'}}",0.25,"Public Health, Epidemiology & Health Systems"
22,"Is viral suppression at 12 months higher, lower, or the same when comparing rapid ART to standard initiation?",higher,moderate,yes,"['28742880', '27658873', '29509839', '27163694']",31206168,2019,"{'28742880': {'article_id': '28742880', 'content': ""Attrition during the period from HIV testing to antiretroviral therapy (ART) initiation is high worldwide. We assessed whether same-day HIV testing and ART initiation improves retention and virologic suppression.\nWe conducted an unblinded, randomized trial of standard ART initiation versus same-day HIV testing and ART initiation among eligible adults ≥18 years old with World Health Organization Stage 1 or 2 disease and CD4 count ≤500 cells/mm3. The study was conducted among outpatients at the Haitian Group for the Study of Kaposi's Sarcoma and Opportunistic infections (GHESKIO) Clinic in Port-au-Prince, Haiti. Participants were randomly assigned (1:1) to standard ART initiation or same-day HIV testing and ART initiation. The standard group initiated ART 3 weeks after HIV testing, and the same-day group initiated ART on the day of testing. The primary study endpoint was retention in care 12 months after HIV testing with HIV-1 RNA <50 copies/ml. We assessed the impact of treatment arm with a modified intention-to-treat analysis, using multivariable logistic regression controlling for potential confounders. Between August 2013 and October 2015, 762 participants were enrolled; 59 participants transferred to other clinics during the study period, and were excluded as per protocol, leaving 356 in the standard and 347 in the same-day ART groups. In the standard ART group, 156 (44%) participants were retained in care with 12-month HIV-1 RNA <50 copies, and 184 (52%) had <1,000 copies/ml; 20 participants (6%) died. In the same-day ART group, 184 (53%) participants were retained with HIV-1 RNA <50 copies/ml, and 212 (61%) had <1,000 copies/ml; 10 (3%) participants died. The unadjusted risk ratio (RR) of being retained at 12 months with HIV-1 RNA <50 copies/ml was 1.21 (95% CI: 1.04, 1.38; p = 0.015) for the same-day ART group compared to the standard ART group, and the unadjusted RR for being retained with HIV-1 RNA <1,000 copies was 1.18 (95% CI: 1.04, 1.31; p = 0.012). The main limitation of this study is that it was conducted at a single urban clinic, and the generalizability to other settings is uncertain.\nSame-day HIV testing and ART initiation is feasible and beneficial in this setting, as it improves retention in care with virologic suppression among patients with early clinical HIV disease.\nThis study is registered with ClinicalTrials.gov number NCT01900080."", 'title': 'Same-day HIV testing with initiation of antiretroviral therapy versus standard care for persons living with HIV: A randomized unblinded trial.', 'date': '2017-07-26'}, '27658873': {'article_id': '27658873', 'content': ""In Africa, up to 30% of HIV-infected patients who are clinically eligible for antiretroviral therapy (ART) do not start timely treatment. We assessed the effects of an intervention targeting prevalent health systems barriers to ART initiation on timing and completeness of treatment initiation.\nIn this stepped-wedge, non-blinded, cluster-randomised controlled trial, 20 clinics in southwestern Uganda were randomly assigned in groups of five clinics every 6 months to the intervention by a computerised random number generator. This procedure continued until all clinics had crossed over from control (standard of care) to the intervention, which consisted of opinion-leader-led training and coaching of front-line health workers, a point-of-care CD4 cell count testing platform, a revised counselling approach without mandatory multiple pre-initiation sessions, and feedback to the facilities on their ART initiation rates and how they compared with other facilities. Treatment-naive, HIV-infected adults (aged ≥18 years) who were clinically eligible for ART during the study period were included in the study population. The primary outcome was ART initiation 14 days after first clinical eligibility for ART. This study is registered with ClinicalTrials.gov, number NCT01810289.\nBetween April 11, 2013, and Feb 2, 2015, 12\u2008024 eligible patients visited one of the 20 participating clinics. Median CD4 count was 310 cells per μL (IQR 179-424). 3753 of 4747 patients (weighted proportion 80%) in the intervention group had started ART by 2 weeks after eligibility compared with 2585 of 7066 patients (38%) in the control group (risk difference 41·9%, 95% CI 40·1-43·8). Vital status was ascertained in a random sample of 208 patients in the intervention group and 199 patients in the control group. Four deaths (2%) occurred in the intervention group and five (3%) occurred in the control group.\nA multicomponent intervention targeting health-care worker behaviour increased the probability of ART initiation 14 days after eligibility. This intervention consists of widely accessible components and has been tested in a real-world setting, and is therefore well positioned for use at scale.\nNational Institute of Allergy and Infectious Diseases (NIAID) and the President's Emergency Fund for AIDS Relief (PEPFAR)."", 'title': 'Effects of a multicomponent intervention to streamline initiation of antiretroviral therapy in Africa: a stepped-wedge cluster-randomised trial.', 'date': '2016-10-30'}, '29509839': {'article_id': '29509839', 'content': 'Home-based HIV testing is a frequently used strategy to increase awareness of HIV status in sub-Saharan Africa. However, with referral to health facilities, less than half of those who test HIV positive link to care and initiate antiretroviral therapy (ART).\nTo determine whether offering same-day home-based ART to patients with HIV improves linkage to care and viral suppression in a rural, high-prevalence setting in sub-Saharan Africa.\nOpen-label, 2-group, randomized clinical trial (February 22, 2016-September 17, 2017), involving 6 health care facilities in northern Lesotho. During home-based HIV testing in 6655 households from 60 rural villages and 17 urban areas, 278 individuals aged 18 years or older who tested HIV positive and were ART naive from 268 households consented and enrolled. Individuals from the same household were randomized into the same group.\nParticipants were randomly assigned to be offered same-day home-based ART initiation (n\u2009=\u2009138) and subsequent follow-up intervals of 1.5, 3, 6, 9, and 12 months after treatment initiation at the health facility or to receive usual care (n\u2009=\u2009140) with referral to the nearest health facility for preparatory counseling followed by ART initiation and monthly follow-up visits thereafter.\nPrimary end points were rates of linkage to care within 3 months (presenting at the health facility within 90 days after the home visit) and viral suppression at 12 months, defined as a viral load of less than 100 copies/mL from 11 through 14 months after enrollment.\nAmong 278 randomized individuals (median age, 39 years [interquartile range, 28.0-52.0]; 180 women [65.7%]), 274 (98.6%) were included in the analysis (137 in the same-day group and 137 in the usual care group). In the same-day group, 134 (97.8%) indicated readiness to start ART that day and 2 (1.5%) within the next few days and were given a 1-month supply of ART. At 3 months, 68.6% (94) in same-day group vs 43.1% (59) in usual care group had linked to care (absolute difference, 25.6%; 95% CI, 13.8% to 36.3%; P\u2009<\u2009.001). At 12 months, 50.4% (69) in the same-day group vs 34.3% (47) in usual care group achieved viral suppression (absolute difference, 16.0%; 4.4%-27.2%; P\u2009=\u2009.007). Two deaths (1.5%) were reported in the same-day group, none in usual care group.\nAmong adults in rural Lesotho, a setting of high HIV prevalence, offering same-day home-based ART initiation to individuals who tested positive during home-based HIV testing, compared with usual care and standard clinic referral, significantly increased linkage to care at 3 months and HIV viral suppression at 12 months. These findings support the practice of offering same-day ART initiation during home-based HIV testing.\nclinicaltrials.gov Identifier: NCT02692027.', 'title': 'Effect of Offering Same-Day ART vs Usual Health Facility Referral During Home-Based HIV Testing on Linkage to Care and Viral Suppression Among Adults With HIV in Lesotho: The CASCADE Randomized Clinical Trial.', 'date': '2018-03-07'}, '27163694': {'article_id': '27163694', 'content': 'High rates of patient attrition from care between HIV testing and antiretroviral therapy (ART) initiation have been documented in sub-Saharan Africa, contributing to persistently low CD4 cell counts at treatment initiation. One reason for this is that starting ART in many countries is a lengthy and burdensome process, imposing long waits and multiple clinic visits on patients. We estimated the effect on uptake of ART and viral suppression of an accelerated initiation algorithm that allowed treatment-eligible patients to be dispensed their first supply of antiretroviral medications on the day of their first HIV-related clinic visit.\nRapIT (Rapid Initiation of Treatment) was an unblinded randomized controlled trial of single-visit ART initiation in two public sector clinics in South Africa, a primary health clinic (PHC) and a hospital-based HIV clinic. Adult (≥18 y old), non-pregnant patients receiving a positive HIV test or first treatment-eligible CD4 count were randomized to standard or rapid initiation. Patients in the rapid-initiation arm of the study (""rapid arm"") received a point-of-care (POC) CD4 count if needed; those who were ART-eligible received a POC tuberculosis (TB) test if symptomatic, POC blood tests, physical exam, education, counseling, and antiretroviral (ARV) dispensing. Patients in the standard-initiation arm of the study (""standard arm"") followed standard clinic procedures (three to five additional clinic visits over 2-4 wk prior to ARV dispensing). Follow up was by record review only. The primary outcome was viral suppression, defined as initiated, retained in care, and suppressed (≤400 copies/ml) within 10 mo of study enrollment. Secondary outcomes included initiation of ART ≤90 d of study enrollment, retention in care, time to ART initiation, patient-level predictors of primary outcomes, prevalence of TB symptoms, and the feasibility and acceptability of the intervention. A survival analysis was conducted comparing attrition from care after ART initiation between the groups among those who initiated within 90 d. Three hundred and seventy-seven patients were enrolled in the study between May 8, 2013 and August 29, 2014 (median CD4 count 210 cells/mm3). In the rapid arm, 119/187 patients (64%) initiated treatment and were virally suppressed at 10 mo, compared to 96/190 (51%) in the standard arm (relative risk [RR] 1.26 [1.05-1.50]). In the rapid arm 182/187 (97%) initiated ART ≤90 d, compared to 136/190 (72%) in the standard arm (RR 1.36, 95% confidence interval [CI], 1.24-1.49). Among 318 patients who did initiate ART within 90 d, the hazard of attrition within the first 10 mo did not differ between the treatment arms (hazard ratio [HR] 1.06; 95% CI 0.61-1.84). The study was limited by the small number of sites and small sample size, and the generalizability of the results to other settings and to non-research conditions is uncertain.\nOffering single-visit ART initiation to adult patients in South Africa increased uptake of ART by 36% and viral suppression by 26%. This intervention should be considered for adoption in the public sector in Africa.\nClinicalTrials.gov NCT01710397, and South African National Clinical Trials Register DOH-27-0213-4177.', 'title': ""Initiating Antiretroviral Therapy for HIV at a Patient's First Clinic Visit: The RapIT Randomized Controlled Trial."", 'date': '2016-05-11'}}",0.75,"Public Health, Epidemiology & Health Systems"
23,"Is the rate of hypotension higher, lower, or the same when comparing capnography to standard monitoring?",no difference,moderate,no,['27006732'],28334427,2017,"{'27006732': {'article_id': '27006732', 'content': 'This prospective, randomized trial was undertaken to evaluate the utility of adding end-tidal capnometry (ETC) to pulse oximetry (PO) in patients undergoing procedural sedation and analgesia (PSA) in the emergency department (ED).\nThe patients were randomized to monitoring with or without ETC in addition to the current standard of care. Primary endpoints included respiratory adverse events, with secondary endpoints of level of sedation, hypotension, other PSA-related adverse events and patient satisfaction.\nOf 986 patients, 501 were randomized to usual care and 485 to additional ETC monitoring. In this series, 48% of the patients were female, with a mean age of 46 years. Orthopedic manipulations (71%), cardioversion (12%) and abscess incision and drainage (12%) were the most common procedures, and propofol and fentanyl were the sedative/analgesic combination used for most patients. There was no difference in patients experiencing de-saturation (SaO2<90%) between the two groups; however, patients in the ETC group were more likely to require airway repositioning (12.9% vs. 9.3%, P=0.003). Hypotension (SBP<100 mmHg or <85 mmHg if baseline <100 mmHg) was observed in 16 (3.3%) patients in the ETC group and 7 (1.4%) in the control group (P=0.048).\nThe addition of ETC does not appear to change any clinically significant outcomes. We found an increased incidence of the use of airway repositioning maneuvers and hypotension in cases where ETC was used. We do not believe that ETC should be recommended as a standard of care for the monitoring of patients undergoing PSA.', 'title': 'End-tidal capnometry during emergency department procedural sedation and analgesia: a randomized, controlled study.', 'date': '2016-03-24'}}",0.0,Surgery
24,"Is the risk of HIV transmission at 12 months higher, lower, or the same when comparing triple antiretroviral prophylaxis during pregnancy and breastfeeding to short antiretroviral prophylaxis?",lower,low,no,['21237718'],25280769,2014,"{'21237718': {'article_id': '21237718', 'content': ""Breastfeeding is essential for child health and development in low-resource settings but carries a significant risk of transmission of HIV-1, especially in late stages of maternal disease. We aimed to assess the efficacy and safety of triple antiretroviral compared with zidovudine and single-dose nevirapine prophylaxis in pregnant women infected with HIV.\nPregnant women with WHO stage 1, 2, or 3 HIV-1 infection who had CD4 cell counts of 200-500 cells per μL were enrolled at five study sites in Burkina Faso, Kenya, and South Africa to start study treatment at 28-36 weeks' gestation. Women were randomly assigned (1:1) by a computer generated random sequence to either triple antiretroviral prophylaxis (a combination of 300 mg zidovudine, 150 mg lamivudine, and 400 mg lopinavir plus 100 mg ritonavir twice daily until cessation of breastfeeding to a maximum of 6·5 months post partum) or zidovudine and single-dose nevirapine (300 mg zidovudine twice daily until delivery and a dose of 600 mg zidovudine plus 200 mg nevirapine at the onset of labour and, after a protocol amendment in December, 2006, 1 week post-partum zidovudine 300 mg twice daily and lamivudine 150 mg twice daily). All infants received a 0·6 mL dose of nevirapine at birth and, from December, 2006, 4 mg/kg twice daily of zidovudine for 1 week after birth. Patients and investigators were not masked to treatment. The primary endpoints were HIV-free infant survival at 6 weeks and 12 months; HIV-free survival at 12 months in infants who were ever breastfed; AIDS-free survival in mothers at 18 months; and serious adverse events in mothers and babies. Analysis was by intention to treat. This trial is registered with Current Controlled Trials, ISRCTN71468401.\nFrom June, 2005, to August, 2008, 882 women were enrolled, 824 of whom were randomised and gave birth to 805 singleton or first, liveborn infants. The cumulative rate of HIV transmission at 6 weeks was 3·3% (95% CI 1·9-5·6%) in the triple antiretroviral group compared with 5·0% (3·3-7·7%) in the zidovudine and single-dose nevirapine group, and at 12 months was 5·4% (3·6-8·1%) in the triple antiretroviral group compared with 9·5% (7·0-12·9%) in the zidovudine and single-dose nevirapine group (p=0·029). The cumulative rate of HIV transmission or death at 12 months was 10·2% (95% CI 7·6-13·6%) in the triple antiretroviral group compared with 16·0% (12·7-20·0%) in the zidovudine and single-dose nevirapine group (p=0·017). In infants whose mothers declared they intended to breastfeed, the cumulative rate of HIV transmission at 12 months was 5·6% (95% CI 3·4-8·9%) in the triple antiretroviral group compared with 10·7% (7·6-14·8%) in the zidovudine and single-dose nevirapine group (p=0·02). AIDS-free survival in mothers at 18 months will be reported in a different publication. The incidence of laboratory and clinical serious adverse events in both mothers and their babies was similar between groups.\nTriple antiretroviral prophylaxis during pregnancy and breastfeeding is safe and reduces the risk of HIV transmission to infants. Revised WHO guidelines now recommend antiretroviral prophylaxis (either to the mother or to the baby) during breastfeeding if the mother is not already receiving antiretroviral treatment for her own health.\nAgence nationale de recherches sur le sida et les hépatites virales, Department for International Development, European and Developing Countries Clinical Trials Partnership, Thrasher Research Fund, Belgian Directorate General for International Cooperation, Centers for Disease Control and Prevention, Eunice Kennedy Shriver National Institute of Child Health and Human Development, and UNDP/UNFPA/World Bank/WHO Special Programme of Research, Development and Research Training in Human Reproduction."", 'title': 'Triple antiretroviral compared with zidovudine and single-dose nevirapine prophylaxis during pregnancy and breastfeeding for prevention of mother-to-child transmission of HIV-1 (Kesho Bora study): a randomised controlled trial.', 'date': '2011-01-18'}}",1.0,Obstetrics & Gynecology
25,"Is the risk of infant HIV infection higher, lower, or the same when comparing zidovudine, lamivudine, and lopinavir/ritonavir to zidovudine, lamivudine, and abacavir from 26‐34 weeks gestation through six months of breastfeeding?",no difference,,no,['20554983'],25280769,2014,"{'20554983': {'article_id': '20554983', 'content': ""The most effective highly active antiretroviral therapy (HAART) to prevent mother-to-child transmission of human immunodeficiency virus type 1 (HIV-1) in pregnancy and its efficacy during breast-feeding are unknown.\nWe randomly assigned 560 HIV-1-infected pregnant women (CD4+ count, > or = 200 cells per cubic millimeter) to receive coformulated abacavir, zidovudine, and lamivudine (the nucleoside reverse-transcriptase inhibitor [NRTI] group) or lopinavir-ritonavir plus zidovudine-lamivudine (the protease-inhibitor group) from 26 to 34 weeks' gestation through planned weaning by 6 months post partum. A total of 170 women with CD4+ counts of less than 200 cells per cubic millimeter received nevirapine plus zidovudine-lamivudine (the observational group). Infants received single-dose nevirapine and 4 weeks of zidovudine.\nThe rate of virologic suppression to less than 400 copies per milliliter was high and did not differ significantly among the three groups at delivery (96% in the NRTI group, 93% in the protease-inhibitor group, and 94% in the observational group) or throughout the breast-feeding period (92% in the NRTI group, 93% in the protease-inhibitor group, and 95% in the observational group). By 6 months of age, 8 of 709 live-born infants (1.1%) were infected (95% confidence interval [CI], 0.5 to 2.2): 6 were infected in utero (4 in the NRTI group, 1 in the protease-inhibitor group, and 1 in the observational group), and 2 were infected during the breast-feeding period (in the NRTI group). Treatment-limiting adverse events occurred in 2% of women in the NRTI group, 2% of women in the protease-inhibitor group, and 11% of women in the observational group.\nAll regimens of HAART from pregnancy through 6 months post partum resulted in high rates of virologic suppression, with an overall rate of mother-to-child transmission of 1.1%. (ClinicalTrials.gov number, NCT00270296.)"", 'title': 'Antiretroviral regimens in pregnancy and breast-feeding in Botswana.', 'date': '2010-06-18'}}",1.0,Pediatrics & Neonatology
26,"Is the risk of infant mortality after 12 months higher, lower, or the same when comparing six weeks of infant nevirapine to single dose infant nevirapine?",lower,moderate,no,['21330912'],25280769,2014,"{'21330912': {'article_id': '21330912', 'content': 'We previously reported combined analysis of 6-week and 6-month endpoints of three randomized controlled trials [Six Week Extended Dose Nevirapine (SWEN) trials] that compared extended-dose nevirapine through 6 weeks of age to single-dose nevirapine to prevent HIV transmission via breastfeeding and mortality. We now present endpoints through 12 months of age.\nInfants in Ethiopia, India, and Uganda born to HIV-infected women who chose to breastfeed were randomized to receive single-dose or extended-dose nevirapine.\nHIV transmission, mortality, HIV transmission or death.\nPrimary analysis included 987 and 903 infants in the single-dose and the extended-dose arms, respectively. HIV transmission was 8.9% in the extended-dose group compared to 10.4% in the single-dose group, but the difference was not significant [risk ratio: 0.87, 95% confidence interval (CI): 0.65-1.15]. Cumulative mortality at 12 months was half in the extended-dose group compared to the single-dose group (risk ratio: 0.53, 95% CI: 0.32-0.85). The impact of extended-dose nevirapine was highest in infants of mothers with CD4 cell count more than 350 cells/μl. Risk ratios for death (risk ratio: 0.38, 95% CI: 0.17-0.84) and HIV transmission or death (risk ratio: 0.54, 95% CI: 0.35-0.85) were statistically significant for the CD4 cell counts more than 350 cells/μl category, whereas none of the risk ratios were significant for the CD4 cell counts 200 cells/μl or less and CD4 cell counts 201-350 cells/μl categories.\nFor populations with limited access to HAART, our results provide evidence for the use of extended-dose regimens to prevent infant deaths and increase HIV-free survival in infants of HIV-infected breastfeeding women, particularly for infants of women with CD4 cell counts more than 350 cells/μl.', 'title': 'Twelve-month follow-up of Six Week Extended Dose Nevirapine randomized controlled trials: differential impact of extended-dose nevirapine on mother-to-child transmission and infant death by maternal CD4 cell count.', 'date': '2011-02-19'}}",1.0,Pediatrics & Neonatology
33,"Is the risk of infant HIV infection or death after 12 months higher, lower, or the same when comparing six weeks of infant nevirapine to single dose infant nevirapine?",no difference,moderate,no,['21330912'],25280769,2014,"{'21330912': {'article_id': '21330912', 'content': 'We previously reported combined analysis of 6-week and 6-month endpoints of three randomized controlled trials [Six Week Extended Dose Nevirapine (SWEN) trials] that compared extended-dose nevirapine through 6 weeks of age to single-dose nevirapine to prevent HIV transmission via breastfeeding and mortality. We now present endpoints through 12 months of age.\nInfants in Ethiopia, India, and Uganda born to HIV-infected women who chose to breastfeed were randomized to receive single-dose or extended-dose nevirapine.\nHIV transmission, mortality, HIV transmission or death.\nPrimary analysis included 987 and 903 infants in the single-dose and the extended-dose arms, respectively. HIV transmission was 8.9% in the extended-dose group compared to 10.4% in the single-dose group, but the difference was not significant [risk ratio: 0.87, 95% confidence interval (CI): 0.65-1.15]. Cumulative mortality at 12 months was half in the extended-dose group compared to the single-dose group (risk ratio: 0.53, 95% CI: 0.32-0.85). The impact of extended-dose nevirapine was highest in infants of mothers with CD4 cell count more than 350 cells/μl. Risk ratios for death (risk ratio: 0.38, 95% CI: 0.17-0.84) and HIV transmission or death (risk ratio: 0.54, 95% CI: 0.35-0.85) were statistically significant for the CD4 cell counts more than 350 cells/μl category, whereas none of the risk ratios were significant for the CD4 cell counts 200 cells/μl or less and CD4 cell counts 201-350 cells/μl categories.\nFor populations with limited access to HAART, our results provide evidence for the use of extended-dose regimens to prevent infant deaths and increase HIV-free survival in infants of HIV-infected breastfeeding women, particularly for infants of women with CD4 cell counts more than 350 cells/μl.', 'title': 'Twelve-month follow-up of Six Week Extended Dose Nevirapine randomized controlled trials: differential impact of extended-dose nevirapine on mother-to-child transmission and infant death by maternal CD4 cell count.', 'date': '2011-02-19'}}",0.0,Pediatrics & Neonatology
34,"Is the risk of infant HIV infection after 12 months higher, lower, or the same when comparing six weeks of infant nevirapine to single dose infant nevirapine?",no difference,moderate,no,['21330912'],25280769,2014,"{'21330912': {'article_id': '21330912', 'content': 'We previously reported combined analysis of 6-week and 6-month endpoints of three randomized controlled trials [Six Week Extended Dose Nevirapine (SWEN) trials] that compared extended-dose nevirapine through 6 weeks of age to single-dose nevirapine to prevent HIV transmission via breastfeeding and mortality. We now present endpoints through 12 months of age.\nInfants in Ethiopia, India, and Uganda born to HIV-infected women who chose to breastfeed were randomized to receive single-dose or extended-dose nevirapine.\nHIV transmission, mortality, HIV transmission or death.\nPrimary analysis included 987 and 903 infants in the single-dose and the extended-dose arms, respectively. HIV transmission was 8.9% in the extended-dose group compared to 10.4% in the single-dose group, but the difference was not significant [risk ratio: 0.87, 95% confidence interval (CI): 0.65-1.15]. Cumulative mortality at 12 months was half in the extended-dose group compared to the single-dose group (risk ratio: 0.53, 95% CI: 0.32-0.85). The impact of extended-dose nevirapine was highest in infants of mothers with CD4 cell count more than 350 cells/μl. Risk ratios for death (risk ratio: 0.38, 95% CI: 0.17-0.84) and HIV transmission or death (risk ratio: 0.54, 95% CI: 0.35-0.85) were statistically significant for the CD4 cell counts more than 350 cells/μl category, whereas none of the risk ratios were significant for the CD4 cell counts 200 cells/μl or less and CD4 cell counts 201-350 cells/μl categories.\nFor populations with limited access to HAART, our results provide evidence for the use of extended-dose regimens to prevent infant deaths and increase HIV-free survival in infants of HIV-infected breastfeeding women, particularly for infants of women with CD4 cell counts more than 350 cells/μl.', 'title': 'Twelve-month follow-up of Six Week Extended Dose Nevirapine randomized controlled trials: differential impact of extended-dose nevirapine on mother-to-child transmission and infant death by maternal CD4 cell count.', 'date': '2011-02-19'}}",1.0,Pediatrics & Neonatology
35,"Is the risk of HIV transmission at 24 months higher, lower, or the same when comparing nevirapine up to 14 weeks plus one week zidovudine to only single dose nevirapine plus one week zidovudine?",lower,moderate,no,['21423025'],25280769,2014,"{'21423025': {'article_id': '21423025', 'content': 'This analysis updates and extends efficacy estimates of the PEPI-Malawi trial through age 24 months at study completion in September 2009.\nInfants of breastfeeding HIV-infected women were randomized at birth to the following: (1) single-dose nevirapine (NVP) + 1-week zidovudine (ZDV) (control); (2) control + extended daily NVP (ExtNVP) through 14 weeks; (3) control + extended daily NVP + ZDV (ExtNVP/ZDV) through 14 weeks. We estimated rates of HIV infection, death and HIV infection, or death using Kaplan-Meier analysis.\nThis analysis includes 3126 infants uninfected at birth as follows: 1004 control, 1071 ExtNVP, and 1051 ExtNVP/ZDV. By 9 months, HIV infection rates were 5.0% in ExtNVP, 6.0% in ExtNVP/ZDV, and 11.1% in control (P < 0.001 comparing extended regimens with control). At age 24 months, HIV infection rates had risen to ~11% in the extended arms compared with 15.6% in the controls (P < 0.05). The rates of HIV infection or death were also significantly lower in extended arms. There were no differences in severe adverse events with the exception of higher possibly related events in the ExtNVP/ZDV arm.\nDaily infant antiretroviral prophylaxis reduces postnatal HIV infection by ~70% during the period of prophylaxis. But continued HIV transmission after prophylaxis stops suggests more prolonged infant prophylaxis is needed.', 'title': 'Postexposure prophylaxis of breastfeeding HIV-exposed infants with antiretroviral drugs to age 14 weeks: updated efficacy results of the PEPI-Malawi trial.', 'date': '2011-03-23'}}",1.0,Pediatrics & Neonatology
36,"Is the risk of HIV transmission or death at 24 months higher, lower, or the same when comparing nevirapine up to 14 weeks plus one week zidovudine to only single dose nevirapine plus one week zidovudine?",lower,moderate,no,['21423025'],25280769,2014,"{'21423025': {'article_id': '21423025', 'content': 'This analysis updates and extends efficacy estimates of the PEPI-Malawi trial through age 24 months at study completion in September 2009.\nInfants of breastfeeding HIV-infected women were randomized at birth to the following: (1) single-dose nevirapine (NVP) + 1-week zidovudine (ZDV) (control); (2) control + extended daily NVP (ExtNVP) through 14 weeks; (3) control + extended daily NVP + ZDV (ExtNVP/ZDV) through 14 weeks. We estimated rates of HIV infection, death and HIV infection, or death using Kaplan-Meier analysis.\nThis analysis includes 3126 infants uninfected at birth as follows: 1004 control, 1071 ExtNVP, and 1051 ExtNVP/ZDV. By 9 months, HIV infection rates were 5.0% in ExtNVP, 6.0% in ExtNVP/ZDV, and 11.1% in control (P < 0.001 comparing extended regimens with control). At age 24 months, HIV infection rates had risen to ~11% in the extended arms compared with 15.6% in the controls (P < 0.05). The rates of HIV infection or death were also significantly lower in extended arms. There were no differences in severe adverse events with the exception of higher possibly related events in the ExtNVP/ZDV arm.\nDaily infant antiretroviral prophylaxis reduces postnatal HIV infection by ~70% during the period of prophylaxis. But continued HIV transmission after prophylaxis stops suggests more prolonged infant prophylaxis is needed.', 'title': 'Postexposure prophylaxis of breastfeeding HIV-exposed infants with antiretroviral drugs to age 14 weeks: updated efficacy results of the PEPI-Malawi trial.', 'date': '2011-03-23'}}",1.0,Pediatrics & Neonatology
37,"Is the risk of HIV transmission or death at 24 months higher, lower, or the same when comparing nevirapine plus zidovudine with dual prophylaxis up to 14 weeks to only single dose nevirapine plus one week zidovudine?",lower,moderate,no,['21423025'],25280769,2014,"{'21423025': {'article_id': '21423025', 'content': 'This analysis updates and extends efficacy estimates of the PEPI-Malawi trial through age 24 months at study completion in September 2009.\nInfants of breastfeeding HIV-infected women were randomized at birth to the following: (1) single-dose nevirapine (NVP) + 1-week zidovudine (ZDV) (control); (2) control + extended daily NVP (ExtNVP) through 14 weeks; (3) control + extended daily NVP + ZDV (ExtNVP/ZDV) through 14 weeks. We estimated rates of HIV infection, death and HIV infection, or death using Kaplan-Meier analysis.\nThis analysis includes 3126 infants uninfected at birth as follows: 1004 control, 1071 ExtNVP, and 1051 ExtNVP/ZDV. By 9 months, HIV infection rates were 5.0% in ExtNVP, 6.0% in ExtNVP/ZDV, and 11.1% in control (P < 0.001 comparing extended regimens with control). At age 24 months, HIV infection rates had risen to ~11% in the extended arms compared with 15.6% in the controls (P < 0.05). The rates of HIV infection or death were also significantly lower in extended arms. There were no differences in severe adverse events with the exception of higher possibly related events in the ExtNVP/ZDV arm.\nDaily infant antiretroviral prophylaxis reduces postnatal HIV infection by ~70% during the period of prophylaxis. But continued HIV transmission after prophylaxis stops suggests more prolonged infant prophylaxis is needed.', 'title': 'Postexposure prophylaxis of breastfeeding HIV-exposed infants with antiretroviral drugs to age 14 weeks: updated efficacy results of the PEPI-Malawi trial.', 'date': '2011-03-23'}}",1.0,Pediatrics & Neonatology
38,"Is the risk of HIV transmission at 24 months higher, lower, or the same when comparing nevirapine plus zidovudine with dual prophylaxis up to 14 weeks to only single dose nevirapine plus one week zidovudine?",lower,moderate,no,['21423025'],25280769,2014,"{'21423025': {'article_id': '21423025', 'content': 'This analysis updates and extends efficacy estimates of the PEPI-Malawi trial through age 24 months at study completion in September 2009.\nInfants of breastfeeding HIV-infected women were randomized at birth to the following: (1) single-dose nevirapine (NVP) + 1-week zidovudine (ZDV) (control); (2) control + extended daily NVP (ExtNVP) through 14 weeks; (3) control + extended daily NVP + ZDV (ExtNVP/ZDV) through 14 weeks. We estimated rates of HIV infection, death and HIV infection, or death using Kaplan-Meier analysis.\nThis analysis includes 3126 infants uninfected at birth as follows: 1004 control, 1071 ExtNVP, and 1051 ExtNVP/ZDV. By 9 months, HIV infection rates were 5.0% in ExtNVP, 6.0% in ExtNVP/ZDV, and 11.1% in control (P < 0.001 comparing extended regimens with control). At age 24 months, HIV infection rates had risen to ~11% in the extended arms compared with 15.6% in the controls (P < 0.05). The rates of HIV infection or death were also significantly lower in extended arms. There were no differences in severe adverse events with the exception of higher possibly related events in the ExtNVP/ZDV arm.\nDaily infant antiretroviral prophylaxis reduces postnatal HIV infection by ~70% during the period of prophylaxis. But continued HIV transmission after prophylaxis stops suggests more prolonged infant prophylaxis is needed.', 'title': 'Postexposure prophylaxis of breastfeeding HIV-exposed infants with antiretroviral drugs to age 14 weeks: updated efficacy results of the PEPI-Malawi trial.', 'date': '2011-03-23'}}",1.0,Obstetrics & Gynecology
39,"Is the risk of HIV infection or death higher, lower, or the same when comparing a maternal extended triple‐drug antiretroviral regimen to no extended postnatal antiretroviral regimen?",lower,moderate,no,['20554982'],25280769,2014,"{'20554982': {'article_id': '20554982', 'content': 'We evaluated the efficacy of a maternal triple-drug antiretroviral regimen or infant nevirapine prophylaxis for 28 weeks during breast-feeding to reduce postnatal transmission of human immunodeficiency virus type 1 (HIV-1) in Malawi.\nWe randomly assigned 2369 HIV-1-positive, breast-feeding mothers with a CD4+ lymphocyte count of at least 250 cells per cubic millimeter and their infants to receive a maternal antiretroviral regimen, infant nevirapine, or no extended postnatal antiretroviral regimen (control group). All mothers and infants received perinatal prophylaxis with single-dose nevirapine and 1 week of zidovudine plus lamivudine. We used the Kaplan-Meier method to estimate the cumulative risk of HIV-1 transmission or death by 28 weeks among infants who were HIV-1-negative 2 weeks after birth. Rates were compared with the use of the log-rank test.\nAmong mother-infant pairs, 5.0% of infants were HIV-1-positive at 2 weeks of life. The estimated risk of HIV-1 transmission between 2 and 28 weeks was higher in the control group (5.7%) than in either the maternal-regimen group (2.9%, P=0.009) or the infant-regimen group (1.7%, P<0.001). The estimated risk of infant HIV-1 infection or death between 2 and 28 weeks was 7.0% in the control group, 4.1% in the maternal-regimen group (P=0.02), and 2.6% in the infant-regimen group (P<0.001). The proportion of women with neutropenia was higher among those receiving the antiretroviral regimen (6.2%) than among those in either the nevirapine group (2.6%) or the control group (2.3%). Among infants receiving nevirapine, 1.9% had a hypersensitivity reaction.\nThe use of either a maternal antiretroviral regimen or infant nevirapine for 28 weeks was effective in reducing HIV-1 transmission during breast-feeding. (ClinicalTrials.gov number, NCT00164736.)', 'title': 'Maternal or infant antiretroviral drugs to reduce HIV-1 transmission.', 'date': '2010-06-18'}}",1.0,Obstetrics & Gynecology
40,"Is the risk of HIV infection higher, lower, or the same when comparing a maternal extended triple‐drug antiretroviral regimen to no extended postnatal antiretroviral regimen?",lower,moderate,no,['20554982'],25280769,2014,"{'20554982': {'article_id': '20554982', 'content': 'We evaluated the efficacy of a maternal triple-drug antiretroviral regimen or infant nevirapine prophylaxis for 28 weeks during breast-feeding to reduce postnatal transmission of human immunodeficiency virus type 1 (HIV-1) in Malawi.\nWe randomly assigned 2369 HIV-1-positive, breast-feeding mothers with a CD4+ lymphocyte count of at least 250 cells per cubic millimeter and their infants to receive a maternal antiretroviral regimen, infant nevirapine, or no extended postnatal antiretroviral regimen (control group). All mothers and infants received perinatal prophylaxis with single-dose nevirapine and 1 week of zidovudine plus lamivudine. We used the Kaplan-Meier method to estimate the cumulative risk of HIV-1 transmission or death by 28 weeks among infants who were HIV-1-negative 2 weeks after birth. Rates were compared with the use of the log-rank test.\nAmong mother-infant pairs, 5.0% of infants were HIV-1-positive at 2 weeks of life. The estimated risk of HIV-1 transmission between 2 and 28 weeks was higher in the control group (5.7%) than in either the maternal-regimen group (2.9%, P=0.009) or the infant-regimen group (1.7%, P<0.001). The estimated risk of infant HIV-1 infection or death between 2 and 28 weeks was 7.0% in the control group, 4.1% in the maternal-regimen group (P=0.02), and 2.6% in the infant-regimen group (P<0.001). The proportion of women with neutropenia was higher among those receiving the antiretroviral regimen (6.2%) than among those in either the nevirapine group (2.6%) or the control group (2.3%). Among infants receiving nevirapine, 1.9% had a hypersensitivity reaction.\nThe use of either a maternal antiretroviral regimen or infant nevirapine for 28 weeks was effective in reducing HIV-1 transmission during breast-feeding. (ClinicalTrials.gov number, NCT00164736.)', 'title': 'Maternal or infant antiretroviral drugs to reduce HIV-1 transmission.', 'date': '2010-06-18'}}",1.0,Pediatrics & Neonatology
41,"Is the risk of HIV infection higher, lower, or the same when comparing an extended infant nevirapine regimen to no extended postnatal antiretroviral regimen?",lower,low,no,['20554982'],25280769,2014,"{'20554982': {'article_id': '20554982', 'content': 'We evaluated the efficacy of a maternal triple-drug antiretroviral regimen or infant nevirapine prophylaxis for 28 weeks during breast-feeding to reduce postnatal transmission of human immunodeficiency virus type 1 (HIV-1) in Malawi.\nWe randomly assigned 2369 HIV-1-positive, breast-feeding mothers with a CD4+ lymphocyte count of at least 250 cells per cubic millimeter and their infants to receive a maternal antiretroviral regimen, infant nevirapine, or no extended postnatal antiretroviral regimen (control group). All mothers and infants received perinatal prophylaxis with single-dose nevirapine and 1 week of zidovudine plus lamivudine. We used the Kaplan-Meier method to estimate the cumulative risk of HIV-1 transmission or death by 28 weeks among infants who were HIV-1-negative 2 weeks after birth. Rates were compared with the use of the log-rank test.\nAmong mother-infant pairs, 5.0% of infants were HIV-1-positive at 2 weeks of life. The estimated risk of HIV-1 transmission between 2 and 28 weeks was higher in the control group (5.7%) than in either the maternal-regimen group (2.9%, P=0.009) or the infant-regimen group (1.7%, P<0.001). The estimated risk of infant HIV-1 infection or death between 2 and 28 weeks was 7.0% in the control group, 4.1% in the maternal-regimen group (P=0.02), and 2.6% in the infant-regimen group (P<0.001). The proportion of women with neutropenia was higher among those receiving the antiretroviral regimen (6.2%) than among those in either the nevirapine group (2.6%) or the control group (2.3%). Among infants receiving nevirapine, 1.9% had a hypersensitivity reaction.\nThe use of either a maternal antiretroviral regimen or infant nevirapine for 28 weeks was effective in reducing HIV-1 transmission during breast-feeding. (ClinicalTrials.gov number, NCT00164736.)', 'title': 'Maternal or infant antiretroviral drugs to reduce HIV-1 transmission.', 'date': '2010-06-18'}}",1.0,Pediatrics & Neonatology
42,"Is the risk of HIV infection or death higher, lower, or the same when comparing an extended infant nevirapine regimen to no extended postnatal antiretroviral regimen?",lower,moderate,no,['20554982'],25280769,2014,"{'20554982': {'article_id': '20554982', 'content': 'We evaluated the efficacy of a maternal triple-drug antiretroviral regimen or infant nevirapine prophylaxis for 28 weeks during breast-feeding to reduce postnatal transmission of human immunodeficiency virus type 1 (HIV-1) in Malawi.\nWe randomly assigned 2369 HIV-1-positive, breast-feeding mothers with a CD4+ lymphocyte count of at least 250 cells per cubic millimeter and their infants to receive a maternal antiretroviral regimen, infant nevirapine, or no extended postnatal antiretroviral regimen (control group). All mothers and infants received perinatal prophylaxis with single-dose nevirapine and 1 week of zidovudine plus lamivudine. We used the Kaplan-Meier method to estimate the cumulative risk of HIV-1 transmission or death by 28 weeks among infants who were HIV-1-negative 2 weeks after birth. Rates were compared with the use of the log-rank test.\nAmong mother-infant pairs, 5.0% of infants were HIV-1-positive at 2 weeks of life. The estimated risk of HIV-1 transmission between 2 and 28 weeks was higher in the control group (5.7%) than in either the maternal-regimen group (2.9%, P=0.009) or the infant-regimen group (1.7%, P<0.001). The estimated risk of infant HIV-1 infection or death between 2 and 28 weeks was 7.0% in the control group, 4.1% in the maternal-regimen group (P=0.02), and 2.6% in the infant-regimen group (P<0.001). The proportion of women with neutropenia was higher among those receiving the antiretroviral regimen (6.2%) than among those in either the nevirapine group (2.6%) or the control group (2.3%). Among infants receiving nevirapine, 1.9% had a hypersensitivity reaction.\nThe use of either a maternal antiretroviral regimen or infant nevirapine for 28 weeks was effective in reducing HIV-1 transmission during breast-feeding. (ClinicalTrials.gov number, NCT00164736.)', 'title': 'Maternal or infant antiretroviral drugs to reduce HIV-1 transmission.', 'date': '2010-06-18'}}",1.0,Pediatrics & Neonatology
43,"Is the risk of HIV infection higher, lower, or the same when comparing an extended infant nevirapine regimen to a maternal extended triple‐drug antiretroviral regimen?",no difference,moderate,no,['20554982'],25280769,2014,"{'20554982': {'article_id': '20554982', 'content': 'We evaluated the efficacy of a maternal triple-drug antiretroviral regimen or infant nevirapine prophylaxis for 28 weeks during breast-feeding to reduce postnatal transmission of human immunodeficiency virus type 1 (HIV-1) in Malawi.\nWe randomly assigned 2369 HIV-1-positive, breast-feeding mothers with a CD4+ lymphocyte count of at least 250 cells per cubic millimeter and their infants to receive a maternal antiretroviral regimen, infant nevirapine, or no extended postnatal antiretroviral regimen (control group). All mothers and infants received perinatal prophylaxis with single-dose nevirapine and 1 week of zidovudine plus lamivudine. We used the Kaplan-Meier method to estimate the cumulative risk of HIV-1 transmission or death by 28 weeks among infants who were HIV-1-negative 2 weeks after birth. Rates were compared with the use of the log-rank test.\nAmong mother-infant pairs, 5.0% of infants were HIV-1-positive at 2 weeks of life. The estimated risk of HIV-1 transmission between 2 and 28 weeks was higher in the control group (5.7%) than in either the maternal-regimen group (2.9%, P=0.009) or the infant-regimen group (1.7%, P<0.001). The estimated risk of infant HIV-1 infection or death between 2 and 28 weeks was 7.0% in the control group, 4.1% in the maternal-regimen group (P=0.02), and 2.6% in the infant-regimen group (P<0.001). The proportion of women with neutropenia was higher among those receiving the antiretroviral regimen (6.2%) than among those in either the nevirapine group (2.6%) or the control group (2.3%). Among infants receiving nevirapine, 1.9% had a hypersensitivity reaction.\nThe use of either a maternal antiretroviral regimen or infant nevirapine for 28 weeks was effective in reducing HIV-1 transmission during breast-feeding. (ClinicalTrials.gov number, NCT00164736.)', 'title': 'Maternal or infant antiretroviral drugs to reduce HIV-1 transmission.', 'date': '2010-06-18'}}",0.0,Pediatrics & Neonatology
44,"Is the risk of HIV infection or death higher, lower, or the same when comparing an extended infant nevirapine regimen to a maternal extended triple‐drug antiretroviral regimen?",no difference,moderate,no,['20554982'],25280769,2014,"{'20554982': {'article_id': '20554982', 'content': 'We evaluated the efficacy of a maternal triple-drug antiretroviral regimen or infant nevirapine prophylaxis for 28 weeks during breast-feeding to reduce postnatal transmission of human immunodeficiency virus type 1 (HIV-1) in Malawi.\nWe randomly assigned 2369 HIV-1-positive, breast-feeding mothers with a CD4+ lymphocyte count of at least 250 cells per cubic millimeter and their infants to receive a maternal antiretroviral regimen, infant nevirapine, or no extended postnatal antiretroviral regimen (control group). All mothers and infants received perinatal prophylaxis with single-dose nevirapine and 1 week of zidovudine plus lamivudine. We used the Kaplan-Meier method to estimate the cumulative risk of HIV-1 transmission or death by 28 weeks among infants who were HIV-1-negative 2 weeks after birth. Rates were compared with the use of the log-rank test.\nAmong mother-infant pairs, 5.0% of infants were HIV-1-positive at 2 weeks of life. The estimated risk of HIV-1 transmission between 2 and 28 weeks was higher in the control group (5.7%) than in either the maternal-regimen group (2.9%, P=0.009) or the infant-regimen group (1.7%, P<0.001). The estimated risk of infant HIV-1 infection or death between 2 and 28 weeks was 7.0% in the control group, 4.1% in the maternal-regimen group (P=0.02), and 2.6% in the infant-regimen group (P<0.001). The proportion of women with neutropenia was higher among those receiving the antiretroviral regimen (6.2%) than among those in either the nevirapine group (2.6%) or the control group (2.3%). Among infants receiving nevirapine, 1.9% had a hypersensitivity reaction.\nThe use of either a maternal antiretroviral regimen or infant nevirapine for 28 weeks was effective in reducing HIV-1 transmission during breast-feeding. (ClinicalTrials.gov number, NCT00164736.)', 'title': 'Maternal or infant antiretroviral drugs to reduce HIV-1 transmission.', 'date': '2010-06-18'}}",0.0,Pediatrics & Neonatology
27,"Is fatigue severity higher, lower, or the same when comparing doxepin to placebo?",insufficient data,low,no,['23561946'],26447539,2015,"{'23561946': {'article_id': '23561946', 'content': ""Although a variety of pharmacologic and non-pharmacologic treatments are effective for insomnia in the general population, insomnia in Parkinson's disease differs in important ways and may need different treatments. No studies have conclusively demonstrated effective insomnia treatments in Parkinson's disease.\nWe conducted a three-arm six-week randomized pilot study assessing non-pharmacologic treatment (cognitive behavioural therapy with bright light therapy) or doxepin (10 mg daily), compared to an inactive placebo in Parkinson's patients with insomnia. Sleep outcomes included insomnia scales, clinical global impression, sleep diaries and actigraphy. Secondary outcomes included motor severity, fatigue, depression and quality of life.\n18 patients were randomized, 6 to each group. Compared to placebo, doxepin improved the Insomnia Severity Index (-9 ± 5.4 vs. -2 ± 3.9, p = 0.03), the SCOPA-night score (-5.2 ± 1.5 vs. -2.3 ± 2.8, p = 0.049), the Pittsburgh Sleep Quality Index-sleep disturbances subscale (-0.5 ± 0.5 vs 0.2 ± 0.4, p = 0.02), and both patient and examiner-rated clinical global impression of change (1.7 ± 0.8 vs. 0.5 ± 0.8, p = 0.03 and 1.4 ± 0.5 vs. 0.3 ± 0.5, p = 0.003). On secondary outcomes doxepin reduced the fatigue severity scale (p = 0.02) and improved scores on the Montreal Cognitive Assessment (p = 0.007). Non-pharmacological treatment reduced the Insomnia Severity Index (-7.8 ± 3.8 vs. -2.0 ± 3.9, p = 0.03), and the examiner-reported clinical global impression of change (p = 0.006), but was associated with decline in Parkinson Disease Questionnaire-39. There were no changes in other primary and secondary outcomes, including actigraphy outcomes. Adverse events were comparable in all groups.\nDoxepin and non-pharmacologic treatment substantially improved insomnia in Parkinson's disease. These potential benefits must be replicated in a full confirmatory randomized controlled trial."", 'title': ""Doxepin and cognitive behavioural therapy for insomnia in patients with Parkinson's disease -- a randomized study."", 'date': '2013-04-09'}}",0.0,Psychiatry & Neurology
28,"Is the progression of physical aspects of fatigue higher, lower, or the same when comparing rasagiline to placebo?",lower,high,no,['21482191'],26447539,2015,"{'21482191': {'article_id': '21482191', 'content': ""The ADAGIO study investigated whether rasagiline has disease-modifying effects in Parkinson's disease. Rasagiline 1 mg per day, but not 2 mg per day, was shown to be efficacious in the primary analysis. Here, we report additional secondary and post-hoc analyses of the ADAGIO study.\nADAGIO was a placebo-controlled, double-blind, multicentre, delayed-start study, in which 1176 patients with untreated early Parkinson's disease were randomly assigned to receive rasagiline 1 mg or 2 mg per day for 72 weeks (early-start groups) or placebo for 36 weeks followed by rasagiline 1 mg or 2 mg per day for 36 weeks (delayed-start groups). We assessed the need for additional antiparkinsonian therapy and changes in non-motor experiences of daily living and fatigue scales (prespecified outcomes) and changes in unified Parkinson's disease rating scale (UPDRS) scores and subscores in placebo and active groups (post-hoc outcomes). The ADAGIO study is registered with ClinicalTrials.gov, number NCT00256204.\nThe need for additional antiparkinsonian therapy was reduced with rasagiline 1 mg (25 of 288 [9%] patients) and 2 mg (26 of 293 [9%]) versus placebo (108 of 593 [18%]; odds ratio for 1 mg rasagiline vs placebo 0·41, 95% CI 0·25-0·65, p=0·0002; 2 mg rasagiline vs placebo 0·41, 0·26-0·64, p=0·0001). At week 36, both doses significantly improved UPDRS motor subscores compared with placebo (1 mg rasagiline mean difference -1·88 [SE 0·35]; 2 mg rasagiline -2·18 [0·35]; both p<0·0001) and activities of daily living subscores (ADL; 1 mg rasagiline -0·86 [0·18]; 2 mg rasagiline -0·88 [0·18]; both p<0·0001), and 1 mg rasagiline significantly improved UPDRS mentation subscore (-0·22 [0·08]; p=0·004). At week 72, the only significant difference between early-start and delayed-start groups was for ADL subscore with the 1 mg dose (-0·62 [0·29]; p=0·035). When assessed for the effect on non-motor symptoms at week 36, both doses showed benefits on the Parkinson fatigue scale versus placebo (1 mg rasagiline mean difference -0·14 [SE 0·05], p=0·0032; 2 mg rasagiline -0·19 [0·05], p<0·0001), and the 1 mg dose showed benefits on the scale for non-motor experiences of daily living compared with placebo (mean difference -0·33 [0·17]; p=0·049). The rate of progression of total UPDRS score for patients in the placebo group was 4·3 points [SE 0·3] over 36 weeks, with extrapolation to about 6 units per year. In the placebo group, patients with the lowest quartile of baseline UPDRS scores (≤14; n=160) progressed more slowly than did those with highest scores (>25·5; n=145; mean difference -3·46 [SE 0·77]; p<0·0001).\nThese findings show that rasagiline delayed the need for symptomatic antiparkinsonian drugs and emphasise the contribution of the UPDRS ADL in the response of the rasagiline 1 mg per day early-start versus delayed-start group. The rate of UPDRS deterioration was less than was anticipated from previous studies and correlated with baseline severity. Understanding of the pattern of UPDRS deterioration is essential to assess disease modification.\nTeva Pharmaceutical Industries and H Lundbeck A/S."", 'title': ""A double-blind, delayed-start trial of rasagiline in Parkinson's disease (the ADAGIO study): prespecified and post-hoc analyses of the need for additional therapies, changes in UPDRS scores, and non-motor outcomes."", 'date': '2011-04-13'}}",1.0,Psychiatry & Neurology
29,"Is subjective fatigue higher, lower, or the same when comparing modafinil to placebo?",no difference,,no,"['19620846', '16291885']",26447539,2015,"{'19620846': {'article_id': '19620846', 'content': 'Fatigue is a major nonmotor symptom in Parkinson disease(PD). It is associated with reduced activity and lower quality of life.\nTo determine if modafinil improves subjective fatigue and physical fatigability in PD.\nNineteen PD patients who reported significant fatigue in the Multidimensional Fatigue Inventory (MFI) participated in this 8-week study. Subjects took their regular medications and were randomly assigned to the treatment group (9 subjects, modafinil 100-mg capsule BID) or placebo group (10 subjects). We used the MFI to measure subjective fatigue and used finger tapping and intermittent force generation to evaluate physical fatigability. Subjects also completed the Epworth Sleepiness Scale (ESS) and the Center of Epidemiological Study-Depression Scale.\nThere were no significant differences at baseline and at 1 month in finger tapping and ESS between the modafinil and placebo groups. At 2 months, the modafinil group had a higher tapping frequency (P<0.05), shorter dwell time (P<0.05), and less fatigability in finger tapping and tended to have lower ESS scores (P<0.12) than the placebo group. However, there was no difference between groups over time for any dimension of the MFI .\nThis small study demonstrated that although modafinil may be effective in reducing physical fatigability in PD, it did not improve fatigue symptoms.', 'title': 'Using modafinil to treat fatigue in Parkinson disease: a double-blind, placebo-controlled pilot study.', 'date': '2009-07-22'}, '16291885': {'article_id': '16291885', 'content': ""Excessive daytime somnolence (EDS) commonly complicates Parkinson's disease (PD). The aetiology of EDS is probably multifactorial but is probably exacerbated by dopaminergic medications. Modafinil is a wake-promoting agent approved for use in narcolepsy, but it is often used to treat a variety of somnolent conditions.\nA double blind, placebo controlled parallel design trial was conducted to assess the efficacy of modafinil (200-400 mg/day) for the treatment of EDS in PD. The primary efficacy measure was the Epworth Sleepiness (ES) scale score. Secondary efficacy points included the Unified Parkinson's Disease Rating Scale (UPDRS), the Fatigue Severity Scale, the Hamilton Depression Scale, and the multiple sleep latency test (MSLT).\nOf a total of 40 subjects (29 men, mean (SD) age 64.8 (11.3) years), randomised to modafinil or placebo, 37 completed the study. Modafinil failed to significantly improve ES scores compared with placebo (2.7 v 1.5 points improvement, respectively, p = 0.28). MSLT failed to improve with modafinil relative to placebo (-0.16 v -0.70, respectively, p = 0.14). UPDRS, global impressions, Fatigue Severity Scale, and Hamilton Depression Scale scores were unchanged. Adverse events were minimal.\nModafinil failed to significantly improve EDS in PD compared with placebo. The drug did not alter motor symptoms in PD and was well tolerated."", 'title': ""Modafinil for daytime somnolence in Parkinson's disease: double blind, placebo controlled parallel trial."", 'date': '2005-11-18'}}",1.0,Psychiatry & Neurology
30,"Is fatigue severity higher, lower, or the same when comparing exercise to usual care?",no difference,low,no,"['22257506', '21953509']",26447539,2015,"{'22257506': {'article_id': '22257506', 'content': ""To investigate the feasibility and effectiveness of six weeks of home-based treadmill training in people with mild Parkinson's disease.\nPilot randomized controlled trial of a six-week intervention followed by a further six weeks follow-up.\nHome-based treadmill training with outcome measures taken at a hospital clinic.\nTwenty cognitively intact participants with mild Parkinson's disease and gait disturbance. Two participants from the treadmill training group and one from the control group dropped out.\nThe treadmill training group undertook a semi-supervised home-based programme of treadmill walking for 20-40 minutes, four times a week for six weeks. The control group received usual care.\nThe feasibility of the intervention was assessed by recording exercise adherence and acceptability, exercise intensity, fatigue, muscle soreness and adverse events. The primary outcome measure of efficacy was walking capacity (6-minute walk test distance).\nHome-based treadmill training was feasible, acceptable and safe with participants completing 78% (SD 36) of the prescribed training sessions. The treadmill training group did not improve their walking capacity compared to the control group. The treadmill training group showed a greater improvement than the control group in fatigue at post test (P = 0.04) and in quality of life at six weeks follow-up testing (P = 0.02).\nSemi-supervised home-based treadmill training is a feasible and safe form of exercise for cognitively intact people with mild Parkinson's disease. Further investigation regarding the effectiveness of home-based treadmill training is warranted."", 'title': ""Home-based treadmill training for individuals with Parkinson's disease: a randomized controlled pilot trial."", 'date': '2012-01-20'}, '21953509': {'article_id': '21953509', 'content': ""Fatigue is one of the most disabling non-motor symptoms for people with Parkinson's disease. Exercise may modify fatigue. This study examines prescribed exercise effects on physical activity levels, well-being, and fatigue in Parkinson's disease.\nIn this single-blinded trial, participants were randomly assigned to either a 12 week community exercise program or control group. Primary outcome measures were fatigue (Fatigue Severity Scale) and physical activity.\nThirty-nine people with Parkinson's disease were included: 20 in exercise and 19 in control. Sixty-five percent of the study group were fatigued (n = 24, mean 4.02, SD 1.48). Increased fatigue was associated with lower mobility and activity (P < .05). Individuals participated in a mean of 15 (SD 10) exercise sessions with no significant change in fatigue, mobility, well-being, or physical activity after exercise (P ≥ .05).\nParticipation in weekly exercise did not improve fatigue in people with Parkinson's Disease."", 'title': ""Weekly exercise does not improve fatigue levels in Parkinson's disease."", 'date': '2011-09-29'}}",0.5,Psychiatry & Neurology
31,"Is subjective fatigue higher, lower, or the same when comparing memantine to placebo?",no difference,moderate,no,['21193343'],26447539,2015,"{'21193343': {'article_id': '21193343', 'content': 'To perform an exploratory study evaluating memantine for several common non-motor problems in Parkinson\'s disease.\nWe conducted a single center, double-blind, placebo controlled pilot trial of memantine, titrated to 20 mg/day, in PD subjects. Inclusion criteria were intentionally broad and included both fluctuating and non-fluctuating patients. After baseline assessments, subjects (N = 40) were randomized to drug and placebo groups. They received a battery of traditional and non-motor assessments. After a safety call (2 weeks after baseline) they returned for identical assessments at week 8. An 8-week open label extension was started if desired.\nSubject demographics (age 69.1 ± 7.8; 24 males), were similar in the drug and placebo groups. Four dropped from the study while on drug vs. none on placebo. Two of 36 remaining dropped out over the 8-week open label section. Of the 34 who completed the final open label visit, 24 elected to be prescribed memantine after the study. During the controlled trial, there was no significant change in UPDRS section I or II, Epworth sleepiness scale, fatigue severity scale, Hamilton depression scale, Conner adult inventory, PD Quality of Life-39, or clinical global impressions. UPDRS ""on"" motor scores tended to improved, p = 0.19.\nMemantine was well tolerated in PD; however, specific measures of sleepiness, fatigue, depression, and attention did not significantly improve. The majority of subjects elected to stay on the drug after the open label extension suggesting some unassessed benefit.', 'title': ""Memantine for non-motor features of Parkinson's disease: a double-blind placebo controlled exploratory pilot trial."", 'date': '2011-01-05'}}",1.0,Psychiatry & Neurology
32,"Is subjective fatigue higher, lower, or the same when comparing methylphenidate to placebo?",no difference,moderate,no,['17674415'],26447539,2015,"{'17674415': {'article_id': '17674415', 'content': 'Fatigue is a common nonmotor symptom in idiopathic Parkinson disease (IPD) that can prominently affect everyday function. This study was a randomized, double-blind, placebo-controlled trial evaluating methylphenidate for the treatment of fatigue in patients with IPD maintained on their regular medications. Thirty-six patients were randomized to receive either methylphenidate (10 mg three times per day; n = 17) or placebo (n = 19) for 6 weeks. Primary outcomes were the change from baseline on two separate self-report fatigue questionnaires: the Fatigue Severity Scale (FSS) and the Multidimensional Fatigue Inventory (MFI). Secondary outcomes included the Unified Parkinson Disease Rating Scale (UPDRS) motor score and the five individual domains of the MFI. Fourteen patients in the methylphenidate group and 16 patients in the control group remained on the intervention for the entire study period. In the treatment arm, mean FSS score was reduced by 6.5 points (from a baseline of 43.8) and mean MFI score was reduced by 8.4 points (from a baseline of 51.0). Both these reductions were significant (P < 0.04). Smaller reductions in the placebo group were nonsignificant. Mean UPDRS motor score did not change significantly in either group. Analysis of MFI subscores showed a significant reduction in General Fatigue in the methylphenidate group (P < 0.001). Overall, adverse effects of medication were more frequent in the placebo group. In conclusion, methylphenidate was effective in lowering fatigue scores in patients with IPD following a 6-week treatment period.', 'title': 'Methylphenidate improves fatigue scores in Parkinson disease: a randomized controlled trial.', 'date': '2007-08-04'}}",0.0,Psychiatry & Neurology
45,"Is the coverage of appropriate treatment from an appropriate provider for any iCCM illness higher, lower, or the same when comparing iCCM to usual facility services plus CCM for malaria?",uncertain effect,very low,yes,['26787147'],33565123,2021,"{'26787147': {'article_id': '26787147', 'content': 'We conducted a prospective evaluation of the ""Rapid Scale-Up"" (RSU) program in Burkina Faso, focusing on the integrated community case management (iCCM) component of the program. We used a quasi-experimental design in which nine RSU districts were compared with seven districts without the program. The evaluation included documentation of program implementation, assessments of implementation and quality of care, baseline and endline coverage surveys, and estimation of mortality changes using the Lives Saved Tool. Although the program trained large numbers of community health workers, there were implementation shortcomings related to training, supervision, and drug stockouts. The quality of care provided to sick children was poor, and utilization of community health workers was low. Changes in intervention coverage were comparable in RSU and comparison areas. Estimated under-five mortality declined by 6.2% (from 110 to 103 deaths per 1,000 live births) in the RSU area and 4.2% (from 114 to 109 per 1,000 live births) in the comparison area. The RSU did not result in coverage increases or mortality reductions in Burkina Faso, but we cannot draw conclusions about the effectiveness of the iCCM strategy, given implementation shortcomings. The evaluation results highlight the need for greater attention to implementation of iCCM programs.', 'title': 'Independent Evaluation of the Rapid Scale-Up Program to Reduce Under-Five Mortality in Burkina Faso.', 'date': '2016-01-21'}}",0.0,"Public Health, Epidemiology & Health Systems"
46,"Is the coverage of careseeking to an appropriate provider for any iCCM illness higher, lower, or the same when comparing iCCM to usual facility services plus CCM for malaria?",no difference,low,yes,['23136276'],33565123,2021,"{'23136276': {'article_id': '23136276', 'content': 'We compared use of community medicine distributors (CMDs) and drug use under integrated community case management and home-based management strategies in children 6-59 months of age in eastern Uganda. A cross-sectional study with 1,095 children was nested in a cluster randomized trial with integrated community case management (CMDs treating malaria and pneumonia) as the intervention and home-based management (CMDs treating only malaria) as the control. Care-seeking from CMDs was higher in intervention areas (31%) than in control areas (22%; P = 0.01). Prompt and appropriate treatment of malaria was higher in intervention areas (18%) than in control areas (12%; P = 0.03) and among CMD users (37%) than other health providers (9%). The mean number of drugs among CMD users compared with other health providers was 1.6 versus 2.4 in intervention areas and 1.4 versus 2.3 in control areas. Use of CMDs was low. However, integrated community case management of childhood illnesses increased use of CMDs and rational drug use.', 'title': 'Increased use of community medicine distributors and rational use of drugs in children less than five years of age in Uganda caused by integrated community case management of fever.', 'date': '2013-01-03'}}",0.0,"Public Health, Epidemiology & Health Systems"
47,"Is the incidence of breast hematomas within 90 days of breast surgery higher, lower, or the same when comparing NSAIDs to placebo?",no difference,low,yes,['27935990'],34753201,2021,"{'27935990': {'article_id': '27935990', 'content': 'Persistent pain is a challenging clinical problem after breast cancer treatment. After surgery, inflammatory pain and nociceptive input from nerve injury induce central sensitization which may play a role in the genesis of persistent pain. Using quantitative sensory testing, we tested the hypothesis that adding COX-2 inhibition to standard treatment reduces hyperalgesia after breast cancer surgery. A secondary hypothesis was that patients developing persistent pain would exhibit more postoperative hyperalgesia.\n138 women scheduled for lumpectomy/mastectomy under general anesthesia with paravertebral block were randomized to COX-2 inhibition (2x40mg parecoxib on day of surgery, thereafter 2x200mg celecoxib/day until day five) or placebo. Preoperatively and 1, 5, 15 days and 1, 3, 6, 12 months postoperatively, we determined electric and pressure pain tolerance thresholds in dermatomes C6/T4/L1 and a 100mm VAS score for pain. We calculated the sum of pain tolerance thresholds and analyzed change in these versus preoperatively using mixed models analysis with factor medication. To assess hyperalgesia in persistent pain patients we performed an additional analysis on patients reporting VAS>30 at 12 months.\n48 COX-2 inhibition and 46 placebo patients were analyzed in a modified intention to treat analysis. Contrary to our primary hypothesis, change in the sum of tolerance thresholds in the COX-2 inhibition group was not different versus placebo. COX-2 inhibition had an effect on pain on movement at postoperative day 5 (p<0.01). Consistent with our secondary hypothesis, change in sum of pressure pain tolerance thresholds in 11 patients that developed persistent pain was negative versus patients without pain (p<0.01) from day 5 to 1 year postoperatively.\nPerioperative COX-2 inhibition has limited value in preventing sensitization and persistent pain after breast cancer surgery. Central sensitization may play a role in the genesis of persistent postsurgical pain.', 'title': 'Hyperalgesia and Persistent Pain after Breast Cancer Surgery: A Prospective Randomized Controlled Trial with Perioperative COX-2 Inhibition.', 'date': '2016-12-10'}}",0.0,Surgery
48,"Is pain intensity 24 (± 12) hours following surgery higher, lower, or the same when comparing NSAIDs to placebo?",lower,low,yes,"['31800611', '16146476', '8770304']",34753201,2021,"{'31800611': {'article_id': '31800611', 'content': 'Ketorolac has been associated with a lower risk of recurrence in retrospective studies, especially in patients with positive inflammatory markers. It is still unknown whether a single dose of pre-incisional ketorolac can prolong recurrence-free survival.\nThe KBC trial is a multicenter, placebo-controlled, randomized phase III trial in high-risk breast cancer patients powered for 33% reduction in recurrence rate (from 60 to 40%). Patients received one dose of ketorolac tromethamine or a placebo before surgery. Eligible patients were breast cancer patients, planned for curative surgery, and with a Neutrophil-to-Lymphocyte Ratio≥4, node-positive disease or a triple-negative phenotype. The primary endpoint was Disease-Free Survival (DFS) at two years. Secondary endpoints included safety, pain assessment and overall survival.\nBetween February 2013 and July 2015, 203 patients were assigned to ketorolac (n = 96) or placebo (n = 107). Baseline characteristics were similar between arms. Patients had a mean age of 55.7 (SD14) years. At two years, 83.1% of the patients were alive and disease free in the ketorolac vs. 89.7% in the placebo arm (HR: 1.23; 95%CI: 0.65-2.31) and, respectively, 96.8% vs. 98.1% were alive (HR: 1.09; 95%CI: 0.34-3.51).\nA single administration of 30 mg of ketorolac tromethamine before surgery does not increase disease-free survival in high risk breast cancer patients. Overall survival difference between ketorolac tromethamine group and placebo group was not statistically significant. The study was however underpowered because of lower recurrence rates than initially anticipated. No safety concerns were observed.\nClinicalTrials.gov NCT01806259.', 'title': 'Intraoperative ketorolac in high-risk breast cancer patients. A prospective, randomized, placebo-controlled clinical trial.', 'date': '2019-12-05'}, '16146476': {'article_id': '16146476', 'content': 'Breast cancer treatment with mastectomy and immediate breast reconstruction (IBR) is associated with intense pain in the primary post-operative period. The present prospective, placebo-controlled and double-blind study aimed to evaluate the analgesic efficacy of diclofenac, a non-steroid anti-inflammatory drug (NSAID), in combination with paracetamol and opioids. This was done by 64-h assessment of post-operative pain intensity, opioid consumption, blood loss, nausea and tiredness.\nFifty women selected for mastectomy and IBR with submuscular implants with or without axillary lymph node dissection (ALND) were randomized to receive diclofenac 50 mg x 3 or placebo rectally in addition to oral paracetamol and intravenous opioids delivered using a patient-controlled analgesia (PCA) technique.\nDuring the first 20 h post-surgery, patients who received diclofenac experienced significantly less pain when resting than those who received placebo. When moving, a non-significant estimated difference in pain in favour of diclofenac was also noted. Opioid consumption during the first 6 h post-operatively was 34% less with diclofenac than with placebo. Means (SD) were 16.9 (10.3) mg and 25.6 (10.2) mg, respectively (P = 0.007). After 64 h, the difference was no longer statistically significant. Post-operative bleeding was significantly higher with diclofenac than with placebo (P < 0.01). Nausea and tiredness did not differ between the groups.\nThe addition of NSAID to paracetamol and opioid-PCA reduced opioid consumption and improved pain relief during the first 20 h at rest but was not convincingly effective during mobilization. Post-operative blood loss was higher with diclofenac.', 'title': 'Analgesic efficacy of diclofenac in combination with morphine and paracetamol after mastectomy and immediate breast reconstruction.', 'date': '2005-09-09'}, '8770304': {'article_id': '8770304', 'content': 'Ketorolac is a parenteral nonsteroidal antiinflammatory drug (NSAID). Two features have limited its clinical utility: tendency to elicit kidney failure and inability to produce complete analgesia. Because most NSAIDs are weak acids (pKa 3-5) and become concentrated in acidic tissues, such as injured and inflamed tissues, we hypothesized that local administration may enhance its analgesic efficacy while lowering the potential for systemic complications.\nWe conducted a randomized, placebo-controlled study of 60 group I-II (American Society of Anesthesiology criteria) mastectomy patients, 20 in each group. Near the end of surgery and every 6 h postoperatively, 20 ml of the study solution containing normal saline with or without 30 mg of ketorolac were administered simultaneously either via a Jackson-Pratt drain or intravenously in a double-blind fashion. The quality of pain control, the amount and character of the drain fluid, incidence of nausea and vomiting, length of stay in the postoperative care unit, and amount of morphine used for treatment of break-through pain were recorded.\nIntraoperative administration of ketorolac resulted in better quality of pain control in the immediate postoperative period regardless of route of administration. The incidence of nausea was significantly higher in the placebo group, and drain output in the ketorolac groups did not exceed the output in the placebo group.\nAnalgesic of the locally administered ketorolac is equally effective to the efficacy of ketorolac administered intravenously.', 'title': 'Comparison of analgesic effect of locally and systemically administered ketorolac in mastectomy patients.', 'date': '1996-01-01'}}",0.0,Surgery
49,"Is the incidence of pneumonia higher, lower, or the same when comparing zinc supplementation to placebo?",lower,low,no,"['9651405', '12052800', '16168782', '17593956']",27915460,2016,"{'9651405': {'article_id': '9651405', 'content': 'Increased acute lower respiratory infection incidence, severity, and mortality are associated with malnutrition, and reduced immunological competence may be a mechanism for this association. Because zinc deficiency results in impaired immunocompetence and zinc supplementation improves immune status, we hypothesized that zinc deficiency is associated with increased incidence and severity of acute lower respiratory infection.\nWe evaluated the effect of daily supplementation with 10 mg of elemental zinc on the incidence and prevalence of acute lower respiratory infection in a double-blind, randomized, controlled trial in 609 children (zinc, n = 298; control, n = 311) 6 to 35 months of age. Supplementation and morbidity surveillance were done for 6 months.\nAfter 120 days of supplementation, the percentage of children with plasma zinc concentrations <60 microg/dL decreased from 35.6% to 11.6% in the zinc group, whereas in the control group it increased from 36.8% to 43.6%. Zinc-supplemented children had 0.19 acute lower respiratory infection episodes/child/year compared with 0.35 episodes/child/year in the control children. After correction for correlation of data using generalized estimating equation regression methods, there was a reduction of 45% (95% confidence interval, 10% to 67%) in the incidence of acute lower respiratory infections in zinc-supplemented children.\nA dietary zinc supplement resulted in a significant reduction in respiratory morbidity in preschool children. These findings suggest that interventions to improve zinc intake will improve the health and survival of children in developing countries.', 'title': 'Zinc supplementation reduces the incidence of acute lower respiratory infections in infants and preschool children: a double-blind, controlled trial.', 'date': '1998-07-04'}, '12052800': {'article_id': '12052800', 'content': 'To evaluate the effect of daily zinc supplementation in children on the incidence of acute lower respiratory tract infections and pneumonia.\nDouble masked, randomised placebo controlled trial.\nA slum community in New Delhi, India.\n2482 children aged 6 to 30 months.\nDaily elemental zinc, 10 mg to infants and 20 mg to older children or placebo for four months. Both groups received single massive dose of vitamin A (100 000 IU for infants and 200 000 IU for older children) at enrollment.\nAll households were visited weekly. Any children with cough and lower chest indrawing or respiratory rate 5 breaths per minute less than the World Health Organization criteria for fast breathing were brought to study physicians.\nAt four months the mean plasma zinc concentration was higher in the zinc group (19.8 (SD 10.1) v 9.3 (2.1) micromol/l, P<0.001). The proportion of children who had acute lower respiratory tract infection during follow up was no different in the two groups (absolute risk reduction -0.2%, 95% confidence interval -3.9% to 3.6%). Zinc supplementation resulted in a lower incidence of pneumonia than placebo (absolute risk reduction 2.5%, 95% confidence interval 0.4% to 4.6%). After correction for multiple episodes in the same child by generalised estimating equations analysis the odds ratio was 0.74, 95% confidence interval 0.56 to 0.99.\nZinc supplementation substantially reduced the incidence of pneumonia in children who had received vitamin A.', 'title': 'Effect of routine zinc supplementation on pneumonia in children aged 6 months to 3 years: randomised controlled trial in an urban slum.', 'date': '2002-06-08'}, '16168782': {'article_id': '16168782', 'content': 'Pneumonia and diarrhoea cause much morbidity and mortality in children younger than 5 years. Most deaths occur during infancy and in developing countries. Daily regimens of zinc have been reported to prevent acute lower respiratory tract infection and diarrhoea, and to reduce child mortality. We aimed to examine whether giving zinc weekly could prevent clinical pneumonia and diarrhoea in children younger than 2 years.\n1665 poor, urban children aged 60 days to 12 months were randomly assigned zinc (70 mg) or placebo orally once weekly for 12 months. Children were assessed every week by field research assistants. Our primary outcomes were the rate of pneumonia and diarrhoea. The rates of other respiratory tract infections were the secondary outcomes. Growth, final serum copper, and final haemoglobin were also measured. Analysis was by intention to treat.\n34 children were excluded before random assignment to treatment group because they had tuberculosis. 809 children were assigned zinc, and 812 placebo. After treatment assignment, 103 children in the treatment group and 44 in the control group withdrew. There were significantly fewer incidents of pneumonia in the zinc group than the control group (199 vs 286; relative risk 0.83, 95% CI 0.73-0.95), and a small but significant effect on incidence of diarrhoea (1881 cases vs 2407; 0.94, 0.88-0.99). There were two deaths in the zinc group and 14 in the placebo group (p=0.013). There were no pneumonia-related deaths in the zinc group, but ten in the placebo group (p=0.013). The zinc group had a small gain in height, but not weight at 10 months compared with the placebo group. Serum copper and haemoglobin concentrations were not adversely affected after 10 months of zinc supplementation.\n70 mg of zinc weekly reduces pneumonia and mortality in young children. However, compliance with weekly intake might be problematic outside a research programme.', 'title': 'Effect of weekly zinc supplements on incidence of pneumonia and diarrhoea in children younger than 2 years in an urban, low-income population in Bangladesh: randomised controlled trial.', 'date': '2005-09-20'}, '17593956': {'article_id': '17593956', 'content': 'Prophylactic zinc supplementation has been shown to reduce diarrhea and respiratory illness in children in many developing countries, but its efficacy in children in Africa is uncertain.\nTo determine if zinc, or zinc plus multiple micronutrients, reduces diarrhea and respiratory disease prevalence.\nRandomized, double-blind, controlled trial.\nRural community in South Africa.\nTHREE COHORTS: 32 HIV-infected children; 154 HIV-uninfected children born to HIV-infected mothers; and 187 HIV-uninfected children born to HIV-uninfected mothers.\nChildren received either 1250 IU of vitamin A; vitamin A and 10 mg of zinc; or vitamin A, zinc, vitamins B1, B2, B6, B12, C, D, E, and K and copper, iodine, iron, and niacin starting at 6 months and continuing to 24 months of age. Homes were visited weekly.\nPrimary outcome was percentage of days of diarrhea per child by study arm within each of the three cohorts. Secondary outcomes were prevalence of upper respiratory symptoms and percentage of children who ever had pneumonia by maternal report, or confirmed by the field worker.\nAmong HIV-uninfected children born to HIV-infected mothers, median percentage of days with diarrhea was 2.3% for 49 children allocated to vitamin A; 2.5% in 47 children allocated to receive vitamin A and zinc; and 2.2% for 46 children allocated to multiple micronutrients (P = 0.852). Among HIV-uninfected children born to HIV-uninfected mothers, median percentage of days of diarrhea was 2.4% in 56 children in the vitamin A group; 1.8% in 57 children in the vitamin A and zinc group; and 2.7% in 52 children in the multiple micronutrient group (P = 0.857). Only 32 HIV-infected children were enrolled, and there were no differences between treatment arms in the prevalence of diarrhea. The prevalence of upper respiratory symptoms or incidence of pneumonia did not differ by treatment arms in any of the cohorts.\nWhen compared with vitamin A alone, supplementation with zinc, or with zinc and multiple micronutrients, did not reduce diarrhea and respiratory morbidity in rural South African children.\nClinicalTrials.gov NCT00156832.', 'title': 'Zinc or multiple micronutrient supplementation to reduce diarrhea and respiratory disease in South African children: a randomized controlled trial.', 'date': '2007-06-28'}}",0.5,"Public Health, Epidemiology & Health Systems"
50,"Is overall survival higher, lower, or the same when comparing cisplatin in combination with third‐generation drugs to carboplatin in combination with third‐generation drugs?",no difference,high,no,"['12928123', '11784875', '28643733', '12377641', '12826316', '12837811', '17409843']",31930743,2020,"{'12928123': {'article_id': '12928123', 'content': 'This randomized, multicenter, phase III trial was conducted to compare the tolerability of gemcitabine plus cisplatin (GP) vs. gemcitabine plus carboplatin (GC) in chemonaive patients with stage IIIb and IV non-small cell lung carcinoma (NSCLC). Secondary objectives were to evaluate response, duration of response, time to progressive disease (TTPD), and survival.\nEligible patients were required to have stage IIIb or IV NSCLC, no previous chemotherapy, Karnofsky performance status of at least 70, bidimensionally measurable disease, and age 18-75 years. Randomized patients in both arms were given gemcitabine 1200 mg/m(2) on days 1 and 8, followed on day 1 by cisplatin 80 mg/m(2) (GP) or carboplatin AUC=5 (GC). Treatment cycles were repeated every 21 days for a maximum of six cycles, or until disease progression or unacceptable toxicity occurred.\nEnrolled patients in both arms, 87 in GP and 89 in GC, were well balanced for demographics and disease characteristics. Dose intensity was 93.8 and 92.7% for gemcitabine in GP/GC arms, respectively; 97.7% for cisplatin and 99.9% for carboplatin. Patients with at least one grade 3/4 toxicity excluding nausea, vomiting or alopecia, were 44% in GP arm and 54% in GC arm. The only significantly different toxicities were, nausea and vomiting in GP and thrombocytopenia in GC group. The overall response rates, median TTPD, response duration and survival were, 41/29%, 5.87/4.75 months, 7.48/5.15 months, and 8.75/7.97 months for GP and GC arms, respectively.\nGP and GC are effective and feasible regimens for advanced NSCLC, and are comparable in efficacy and toxicity. GC may offer acceptable option to patients with advanced NSCLC, especially those who are unable to receive cisplatin.', 'title': 'Gemcitabine plus cisplatin vs. gemcitabine plus carboplatin in stage IIIb and IV non-small cell lung cancer: a phase III randomized trial.', 'date': '2003-08-21'}, '11784875': {'article_id': '11784875', 'content': 'We conducted a randomized study to determine whether any of three chemotherapy regimens was superior to cisplatin and paclitaxel in patients with advanced non-small-cell lung cancer.\nA total of 1207 patients with advanced non-small-cell lung cancer were randomly assigned to a reference regimen of cisplatin and paclitaxel or to one of three experimental regimens: cisplatin and gemcitabine, cisplatin and docetaxel, or carboplatin and paclitaxel.\nThe response rate for all 1155 eligible patients was 19 percent, with a median survival of 7.9 months (95 percent confidence interval, 7.3 to 8.5), a 1-year survival rate of 33 percent (95 percent confidence interval, 30 to 36 percent), and a 2-year survival rate of 11 percent (95 percent confidence interval, 8 to 12 percent). The response rate and survival did not differ significantly between patients assigned to receive cisplatin and paclitaxel and those assigned to receive any of the three experimental regimens. Treatment with cisplatin and gemcitabine was associated with a significantly longer time to the progression of disease than was treatment with cisplatin and paclitaxel but was more likely to cause grade 3, 4, or 5 renal toxicity (in 9 percent of patients, vs. 3 percent of those treated with cisplatin plus paclitaxel). Patients with a performance status of 2 had a significantly lower rate of survival than did those with a performance status of 0 or 1.\nNone of four chemotherapy regimens offered a significant advantage over the others in the treatment of advanced non-small-cell lung cancer.', 'title': 'Comparison of four chemotherapy regimens for advanced non-small-cell lung cancer.', 'date': '2002-01-11'}, '28643733': {'article_id': '28643733', 'content': 'The use of cisplatin (Cis) versus carboplatin (Carb) in the treatment of advanced nonsmall cell lung cancer (NSCLC) is controversial. The aim of the study was to compare the safety and efficacy of Cis versus Carb in squamous NSCLC.\nA prospective, randomized, controlled, open-label study was conducted on advanced squamous NSCLC patients who were randomly assigned to receive Cis (40 mg/m 2 [day 1 and day 8]) or Carb (area under the curve = 5 [day 1]) combined with gemcitabine [Gem] (1000 mg/m 2 [day 1 and day 8]) of a 3-week schedule for six cycles. Study objectives were a radiological response after three cycles and six cycles, 1-year progression-free survival (PFS), 1-year overall survival (OS), and quality of life (QOL) assessment using functional assessment of cancer therapy-lung at baseline, after three cycles, and after six cycles.\nStatistical analysis was done using Statistical Package for Social Science version 15. A P < 0.05 was considered statistically significant.\nSeventy-one patients were enrolled (Gem/Cis group [n = 36], Gem/Carb group [n = 35]). Response rates were comparable in both arms. Nonsignificant differences were found regarding 1-year PFS (P = 0.308) and 1-year OS (P = 0.929) between the two groups. Neutropenia was significantly higher in Gem/Carb group, while vomiting and ototoxicity were significantly higher in Gem/Cis group. The effect on QOL was similar in both groups.\nCis and Carb have similar efficacy, tolerability, and effect on QOL and both can be used as a first-line treatment of squamous NSCLC.', 'title': 'A prospective randomized controlled study of cisplatin versus carboplatin-based regimen in advanced squamous nonsmall cell lung cancer.', 'date': '2017-06-24'}, '12377641': {'article_id': '12377641', 'content': 'The combination of paclitaxel with cisplatin or carboplatin has significant activity in non-small-cell lung cancer (NSCLC). This phase III study of chemotherapy-naïve advanced NSCLC patients was designed to assess whether response rate in patients receiving a paclitaxel/carboplatin combination was similar to that in patients receiving a paclitaxel/cisplatin combination. Paclitaxel was given at a dose of 200 mg/m(2) (3-h intravenous infusion) followed by either carboplatin at an AUC of 6 or cisplatin at a dose of 80 mg/m(2), all repeated every 3 weeks. Survival, toxicity and quality of life were also compared.\nPatients were randomised to receive one of the two combinations, stratified according to centre, performance status, disease stage and histology. The primary analyses of response rate and survival were carried out on response-evaluable patients. Survival was also analysed for all randomised patients. Toxicity analyses were carried out on all treated patients.\nA total of 618 patients were randomised. The two treatment arms were well balanced with regard to gender (83% male), age (median 58 years), performance status (83% ECOG 0-1), stage (68% IV, 32% IIIB) and histology (38% squamous cell carcinoma). In the paclitaxel/carboplatin arm, 306 patients received a total of 1311 courses (median four courses, range 1-10 courses) while in the paclitaxel/cisplatin arm, 302 patients received a total of 1321 courses (median four courses, range 1-10 courses). In only 76% of courses, carboplatin was administered as planned at an AUC of 6, while in 96% of courses, cisplatin was given at the planned dose of 80 mg/m(2). The response rate was 25% (70 of 279) in the paclitaxel/carboplatin arm and 28% (80 of 284) in the paclitaxel/cisplatin arm (P = 0.45). Responses were reviewed by an independent radiological committee. For all randomised patients, median survival was 8.5 months in the paclitaxel/carboplatin arm and 9.8 months in the paclitaxel/cisplatin arm [hazard ratio 1.20, 90% confidence interval (CI) 1.03-1.40]; the 1-year survival rates were 33% and 38%, respectively. On the same dataset, a survival update after 22 months of additional follow-up yielded a median survival of 8.2 months in the paclitaxel/carboplatin arm and 9.8 months in the paclitaxel/cisplatin arm (hazard ratio 1.22, 90% CI 1.06-1.40; P = 0.019); the 2-year survival rates were 9% and 15%, respectively. Excluding neutropenia and thrombocytopenia, which were more frequent in the paclitaxel/carboplatin arm, and nausea/vomiting and nephrotoxicity, which were more frequent in the paclitaxel/cisplatin arm, the rate of severe toxicities was generally low and comparable between the two arms. Overall quality of life (EORTC QLQ-C30 and LC-13) was also similar between the two arms.\nThis is the first trial comparing carboplatin and cisplatin in the treatment of advanced NSCLC. Although paclitaxel/carboplatin yielded a similar response rate, the significantly longer median survival obtained with paclitaxel/cisplatin indicates that cisplatin-based chemotherapy should be the first treatment option.', 'title': 'Phase III randomised trial comparing paclitaxel/carboplatin with paclitaxel/cisplatin in patients with advanced non-small-cell lung cancer: a cooperative multinational trial.', 'date': '2002-10-16'}, '12826316': {'article_id': '12826316', 'content': 'We conducted a phase II randomized study to assess the efficacy, with response as the primary endpoint, and the toxicity of gemcitabine/cisplatin (GP) and gemcitabine/carboplatin (GC) in patients with advanced non-small cell lung cancer (NSCLC).\nPatients were randomized to GP (gemcitabine 1200 mg/m(2), days 1 and 8 plus cisplatin 80 mg/m(2) day 2) or GC (gemcitabine 1200 mg/m(2), days 1 and 8 plus carboplatin AUC=5 day 2). Cycles were repeated every 3 weeks.\nSixty-two patients were randomized to GP and 58 to GC. A total of 533 cycles were delivered (264 GP, 269 GC), with a median of four cycles/patient. The objective response rate was 41.9% (95% C.I., 29.6-54.2%) for GP and 31.0% (95% C.I., 18.2-42.8%) for GC (P=0.29). No significant differences between arms were observed in median survival (10.4 months GP, 10.8 months GC) and median time to progression (5.4 months GP, 5.1 months GC). Both regimens were very well tolerated with no statistical differences between arms in grade 3/4 toxicities. When all toxicity grades were combined, emesis, neuropathy and renal toxicity occurred more frequently on the GP arm (P<0.005).\nGC arm did not provide a significant difference in response rate compared with GP arm, with better overall tolerability. Carboplatin could be a valid alternative to cisplatin in the palliative setting.', 'title': 'Randomized, multicenter, phase II study of gemcitabine plus cisplatin versus gemcitabine plus carboplatin in patients with advanced non-small cell lung cancer.', 'date': '2003-06-27'}, '12837811': {'article_id': '12837811', 'content': 'To investigate whether docetaxel plus platinum regimens improve survival and affect quality of life (QoL) in advanced non-small-cell lung cancer (NSCLC) compared with vinorelbine plus cisplatin as first-line chemotherapy.\nPatients (n = 1,218) with stage IIIB to IV NSCLC were randomly assigned to receive docetaxel 75 mg/m2 and cisplatin 75 mg/m2 every 3 weeks (DC); docetaxel 75 mg/m2 and carboplatin area under the curve of 6 mg/mL * min every 3 weeks (DCb); or vinorelbine 25 mg/m2/wk and cisplatin 100 mg/m2 every 4 weeks (VC).\nPatients treated with DC had a median survival of 11.3 v 10.1 months for VC-treated patients (P =.044; hazard ratio, 1.183 [97.2% confidence interval, 0.989 to 1.416]). The 2-year survival rate was 21% for DC-treated patients and 14% for VC-treated patients. Overall response rate was 31.6% for DC-treated patients v 24.5% for VC-treated patients (P =.029). Median survival (9.4 v 9.9 months [for VC]; P =.657; hazard ratio, 1.048 [97.2 confidence interval, 0.877 to 1.253]) and response (23.9%) with DCb were similar to those results for VC. Neutropenia, thrombocytopenia, infection, and febrile neutropenia were similar with all three regimens. Grade 3 to 4 anemia, nausea, and vomiting were more common (P <.01) with VC than with DC or DCb. Patients treated with either docetaxel regimen had consistently improved QoL compared with VC-treated patients, who experienced deterioration in QoL.\nDC resulted in a more favorable overall response and survival rate than VC. Both DC and DCb were better tolerated and provided patients with consistently improved QoL compared with VC. These findings demonstrate that a docetaxel plus platinum combination is an effective treatment option with a favorable therapeutic index for first-line treatment of advanced or metastatic NSCLC.', 'title': 'Randomized, multinational, phase III study of docetaxel plus platinum combinations versus vinorelbine plus cisplatin for advanced non-small-cell lung cancer: the TAX 326 study group.', 'date': '2003-07-03'}, '17409843': {'article_id': '17409843', 'content': 'Paclitaxel plus carboplatin (CAR) or cisplatin (CIS) has shown activity in the treatment of advanced non-small cell lung cancer (NSCLC). Our aim was to determine whether paclitaxel plus platinum is an appropriate regimen for chemo-naïve NSCLC in patients aged 70 years or older. Patients were randomized into paclitaxel plus CAR or paclitaxel plus CIS treatment arms. Treatment consisted of paclitaxel 160 mg/m and carboplatin at AUC = 6 (predicted using measured clearances and the Calvert formula) IV infusion on day 1 every 3 weeks, or paclitaxel 160 mg/m and cisplatin 60 mg/m IV on day 1 every 3 weeks. In total, 81 patients were enrolled from September 2000 to February 2005, including 40 who received CAR treatment and 41 who received CIS treatment. In all, 152 cycles of CAR (median, four cycles per patient) and 172 cycles of CIS (median, four cycles per patient) were given. Each arm had one complete response and 15 partial responses to the treatment, with overall response rates of 40% and 39%, respectively. Myelosuppression was mild in both arms, and there was no statistical difference between the two arms. Alopecia (P < 0.001), peripheral neuropathy (P = 0.017), and fatigue (P < 0.001) were more severe in the CIS treatment arm than in the CAR treatment arm. Median time to disease progression was 6.6 months in the CAR arm and 6.9 months in the CIS arm. Median survival time was 10.3 months in the CAR arm and 10.5 months in the CIS arm. In conclusion, paclitaxel plus CAR or CIS treatment is feasible in elderly patients and has similar activity. However, paclitaxel plus CAR had less non-hematological toxicity than paclitaxel plus CIS.', 'title': 'A Phase II randomized study of paclitaxel plus carboplatin or cisplatin against chemo-naive inoperable non-small cell lung cancer in the elderly.', 'date': '2007-04-06'}}",0.428571429,Oncology & Hematology
51,"Is response rate higher, lower, or the same when comparing cisplatin in combination with third‐generation drugs to carboplatin in combination with third‐generation drugs?",no difference,high,no,"['12928123', '11784875', '28643733', '12377641', '12826316', '12837811', '17409843', '28780466', '21047474', '11745199', '21333222']",31930743,2020,"{'12928123': {'article_id': '12928123', 'content': 'This randomized, multicenter, phase III trial was conducted to compare the tolerability of gemcitabine plus cisplatin (GP) vs. gemcitabine plus carboplatin (GC) in chemonaive patients with stage IIIb and IV non-small cell lung carcinoma (NSCLC). Secondary objectives were to evaluate response, duration of response, time to progressive disease (TTPD), and survival.\nEligible patients were required to have stage IIIb or IV NSCLC, no previous chemotherapy, Karnofsky performance status of at least 70, bidimensionally measurable disease, and age 18-75 years. Randomized patients in both arms were given gemcitabine 1200 mg/m(2) on days 1 and 8, followed on day 1 by cisplatin 80 mg/m(2) (GP) or carboplatin AUC=5 (GC). Treatment cycles were repeated every 21 days for a maximum of six cycles, or until disease progression or unacceptable toxicity occurred.\nEnrolled patients in both arms, 87 in GP and 89 in GC, were well balanced for demographics and disease characteristics. Dose intensity was 93.8 and 92.7% for gemcitabine in GP/GC arms, respectively; 97.7% for cisplatin and 99.9% for carboplatin. Patients with at least one grade 3/4 toxicity excluding nausea, vomiting or alopecia, were 44% in GP arm and 54% in GC arm. The only significantly different toxicities were, nausea and vomiting in GP and thrombocytopenia in GC group. The overall response rates, median TTPD, response duration and survival were, 41/29%, 5.87/4.75 months, 7.48/5.15 months, and 8.75/7.97 months for GP and GC arms, respectively.\nGP and GC are effective and feasible regimens for advanced NSCLC, and are comparable in efficacy and toxicity. GC may offer acceptable option to patients with advanced NSCLC, especially those who are unable to receive cisplatin.', 'title': 'Gemcitabine plus cisplatin vs. gemcitabine plus carboplatin in stage IIIb and IV non-small cell lung cancer: a phase III randomized trial.', 'date': '2003-08-21'}, '11784875': {'article_id': '11784875', 'content': 'We conducted a randomized study to determine whether any of three chemotherapy regimens was superior to cisplatin and paclitaxel in patients with advanced non-small-cell lung cancer.\nA total of 1207 patients with advanced non-small-cell lung cancer were randomly assigned to a reference regimen of cisplatin and paclitaxel or to one of three experimental regimens: cisplatin and gemcitabine, cisplatin and docetaxel, or carboplatin and paclitaxel.\nThe response rate for all 1155 eligible patients was 19 percent, with a median survival of 7.9 months (95 percent confidence interval, 7.3 to 8.5), a 1-year survival rate of 33 percent (95 percent confidence interval, 30 to 36 percent), and a 2-year survival rate of 11 percent (95 percent confidence interval, 8 to 12 percent). The response rate and survival did not differ significantly between patients assigned to receive cisplatin and paclitaxel and those assigned to receive any of the three experimental regimens. Treatment with cisplatin and gemcitabine was associated with a significantly longer time to the progression of disease than was treatment with cisplatin and paclitaxel but was more likely to cause grade 3, 4, or 5 renal toxicity (in 9 percent of patients, vs. 3 percent of those treated with cisplatin plus paclitaxel). Patients with a performance status of 2 had a significantly lower rate of survival than did those with a performance status of 0 or 1.\nNone of four chemotherapy regimens offered a significant advantage over the others in the treatment of advanced non-small-cell lung cancer.', 'title': 'Comparison of four chemotherapy regimens for advanced non-small-cell lung cancer.', 'date': '2002-01-11'}, '28643733': {'article_id': '28643733', 'content': 'The use of cisplatin (Cis) versus carboplatin (Carb) in the treatment of advanced nonsmall cell lung cancer (NSCLC) is controversial. The aim of the study was to compare the safety and efficacy of Cis versus Carb in squamous NSCLC.\nA prospective, randomized, controlled, open-label study was conducted on advanced squamous NSCLC patients who were randomly assigned to receive Cis (40 mg/m 2 [day 1 and day 8]) or Carb (area under the curve = 5 [day 1]) combined with gemcitabine [Gem] (1000 mg/m 2 [day 1 and day 8]) of a 3-week schedule for six cycles. Study objectives were a radiological response after three cycles and six cycles, 1-year progression-free survival (PFS), 1-year overall survival (OS), and quality of life (QOL) assessment using functional assessment of cancer therapy-lung at baseline, after three cycles, and after six cycles.\nStatistical analysis was done using Statistical Package for Social Science version 15. A P < 0.05 was considered statistically significant.\nSeventy-one patients were enrolled (Gem/Cis group [n = 36], Gem/Carb group [n = 35]). Response rates were comparable in both arms. Nonsignificant differences were found regarding 1-year PFS (P = 0.308) and 1-year OS (P = 0.929) between the two groups. Neutropenia was significantly higher in Gem/Carb group, while vomiting and ototoxicity were significantly higher in Gem/Cis group. The effect on QOL was similar in both groups.\nCis and Carb have similar efficacy, tolerability, and effect on QOL and both can be used as a first-line treatment of squamous NSCLC.', 'title': 'A prospective randomized controlled study of cisplatin versus carboplatin-based regimen in advanced squamous nonsmall cell lung cancer.', 'date': '2017-06-24'}, '12377641': {'article_id': '12377641', 'content': 'The combination of paclitaxel with cisplatin or carboplatin has significant activity in non-small-cell lung cancer (NSCLC). This phase III study of chemotherapy-naïve advanced NSCLC patients was designed to assess whether response rate in patients receiving a paclitaxel/carboplatin combination was similar to that in patients receiving a paclitaxel/cisplatin combination. Paclitaxel was given at a dose of 200 mg/m(2) (3-h intravenous infusion) followed by either carboplatin at an AUC of 6 or cisplatin at a dose of 80 mg/m(2), all repeated every 3 weeks. Survival, toxicity and quality of life were also compared.\nPatients were randomised to receive one of the two combinations, stratified according to centre, performance status, disease stage and histology. The primary analyses of response rate and survival were carried out on response-evaluable patients. Survival was also analysed for all randomised patients. Toxicity analyses were carried out on all treated patients.\nA total of 618 patients were randomised. The two treatment arms were well balanced with regard to gender (83% male), age (median 58 years), performance status (83% ECOG 0-1), stage (68% IV, 32% IIIB) and histology (38% squamous cell carcinoma). In the paclitaxel/carboplatin arm, 306 patients received a total of 1311 courses (median four courses, range 1-10 courses) while in the paclitaxel/cisplatin arm, 302 patients received a total of 1321 courses (median four courses, range 1-10 courses). In only 76% of courses, carboplatin was administered as planned at an AUC of 6, while in 96% of courses, cisplatin was given at the planned dose of 80 mg/m(2). The response rate was 25% (70 of 279) in the paclitaxel/carboplatin arm and 28% (80 of 284) in the paclitaxel/cisplatin arm (P = 0.45). Responses were reviewed by an independent radiological committee. For all randomised patients, median survival was 8.5 months in the paclitaxel/carboplatin arm and 9.8 months in the paclitaxel/cisplatin arm [hazard ratio 1.20, 90% confidence interval (CI) 1.03-1.40]; the 1-year survival rates were 33% and 38%, respectively. On the same dataset, a survival update after 22 months of additional follow-up yielded a median survival of 8.2 months in the paclitaxel/carboplatin arm and 9.8 months in the paclitaxel/cisplatin arm (hazard ratio 1.22, 90% CI 1.06-1.40; P = 0.019); the 2-year survival rates were 9% and 15%, respectively. Excluding neutropenia and thrombocytopenia, which were more frequent in the paclitaxel/carboplatin arm, and nausea/vomiting and nephrotoxicity, which were more frequent in the paclitaxel/cisplatin arm, the rate of severe toxicities was generally low and comparable between the two arms. Overall quality of life (EORTC QLQ-C30 and LC-13) was also similar between the two arms.\nThis is the first trial comparing carboplatin and cisplatin in the treatment of advanced NSCLC. Although paclitaxel/carboplatin yielded a similar response rate, the significantly longer median survival obtained with paclitaxel/cisplatin indicates that cisplatin-based chemotherapy should be the first treatment option.', 'title': 'Phase III randomised trial comparing paclitaxel/carboplatin with paclitaxel/cisplatin in patients with advanced non-small-cell lung cancer: a cooperative multinational trial.', 'date': '2002-10-16'}, '12826316': {'article_id': '12826316', 'content': 'We conducted a phase II randomized study to assess the efficacy, with response as the primary endpoint, and the toxicity of gemcitabine/cisplatin (GP) and gemcitabine/carboplatin (GC) in patients with advanced non-small cell lung cancer (NSCLC).\nPatients were randomized to GP (gemcitabine 1200 mg/m(2), days 1 and 8 plus cisplatin 80 mg/m(2) day 2) or GC (gemcitabine 1200 mg/m(2), days 1 and 8 plus carboplatin AUC=5 day 2). Cycles were repeated every 3 weeks.\nSixty-two patients were randomized to GP and 58 to GC. A total of 533 cycles were delivered (264 GP, 269 GC), with a median of four cycles/patient. The objective response rate was 41.9% (95% C.I., 29.6-54.2%) for GP and 31.0% (95% C.I., 18.2-42.8%) for GC (P=0.29). No significant differences between arms were observed in median survival (10.4 months GP, 10.8 months GC) and median time to progression (5.4 months GP, 5.1 months GC). Both regimens were very well tolerated with no statistical differences between arms in grade 3/4 toxicities. When all toxicity grades were combined, emesis, neuropathy and renal toxicity occurred more frequently on the GP arm (P<0.005).\nGC arm did not provide a significant difference in response rate compared with GP arm, with better overall tolerability. Carboplatin could be a valid alternative to cisplatin in the palliative setting.', 'title': 'Randomized, multicenter, phase II study of gemcitabine plus cisplatin versus gemcitabine plus carboplatin in patients with advanced non-small cell lung cancer.', 'date': '2003-06-27'}, '12837811': {'article_id': '12837811', 'content': 'To investigate whether docetaxel plus platinum regimens improve survival and affect quality of life (QoL) in advanced non-small-cell lung cancer (NSCLC) compared with vinorelbine plus cisplatin as first-line chemotherapy.\nPatients (n = 1,218) with stage IIIB to IV NSCLC were randomly assigned to receive docetaxel 75 mg/m2 and cisplatin 75 mg/m2 every 3 weeks (DC); docetaxel 75 mg/m2 and carboplatin area under the curve of 6 mg/mL * min every 3 weeks (DCb); or vinorelbine 25 mg/m2/wk and cisplatin 100 mg/m2 every 4 weeks (VC).\nPatients treated with DC had a median survival of 11.3 v 10.1 months for VC-treated patients (P =.044; hazard ratio, 1.183 [97.2% confidence interval, 0.989 to 1.416]). The 2-year survival rate was 21% for DC-treated patients and 14% for VC-treated patients. Overall response rate was 31.6% for DC-treated patients v 24.5% for VC-treated patients (P =.029). Median survival (9.4 v 9.9 months [for VC]; P =.657; hazard ratio, 1.048 [97.2 confidence interval, 0.877 to 1.253]) and response (23.9%) with DCb were similar to those results for VC. Neutropenia, thrombocytopenia, infection, and febrile neutropenia were similar with all three regimens. Grade 3 to 4 anemia, nausea, and vomiting were more common (P <.01) with VC than with DC or DCb. Patients treated with either docetaxel regimen had consistently improved QoL compared with VC-treated patients, who experienced deterioration in QoL.\nDC resulted in a more favorable overall response and survival rate than VC. Both DC and DCb were better tolerated and provided patients with consistently improved QoL compared with VC. These findings demonstrate that a docetaxel plus platinum combination is an effective treatment option with a favorable therapeutic index for first-line treatment of advanced or metastatic NSCLC.', 'title': 'Randomized, multinational, phase III study of docetaxel plus platinum combinations versus vinorelbine plus cisplatin for advanced non-small-cell lung cancer: the TAX 326 study group.', 'date': '2003-07-03'}, '17409843': {'article_id': '17409843', 'content': 'Paclitaxel plus carboplatin (CAR) or cisplatin (CIS) has shown activity in the treatment of advanced non-small cell lung cancer (NSCLC). Our aim was to determine whether paclitaxel plus platinum is an appropriate regimen for chemo-naïve NSCLC in patients aged 70 years or older. Patients were randomized into paclitaxel plus CAR or paclitaxel plus CIS treatment arms. Treatment consisted of paclitaxel 160 mg/m and carboplatin at AUC = 6 (predicted using measured clearances and the Calvert formula) IV infusion on day 1 every 3 weeks, or paclitaxel 160 mg/m and cisplatin 60 mg/m IV on day 1 every 3 weeks. In total, 81 patients were enrolled from September 2000 to February 2005, including 40 who received CAR treatment and 41 who received CIS treatment. In all, 152 cycles of CAR (median, four cycles per patient) and 172 cycles of CIS (median, four cycles per patient) were given. Each arm had one complete response and 15 partial responses to the treatment, with overall response rates of 40% and 39%, respectively. Myelosuppression was mild in both arms, and there was no statistical difference between the two arms. Alopecia (P < 0.001), peripheral neuropathy (P = 0.017), and fatigue (P < 0.001) were more severe in the CIS treatment arm than in the CAR treatment arm. Median time to disease progression was 6.6 months in the CAR arm and 6.9 months in the CIS arm. Median survival time was 10.3 months in the CAR arm and 10.5 months in the CIS arm. In conclusion, paclitaxel plus CAR or CIS treatment is feasible in elderly patients and has similar activity. However, paclitaxel plus CAR had less non-hematological toxicity than paclitaxel plus CIS.', 'title': 'A Phase II randomized study of paclitaxel plus carboplatin or cisplatin against chemo-naive inoperable non-small cell lung cancer in the elderly.', 'date': '2007-04-06'}, '28780466': {'article_id': '28780466', 'content': 'Platinum-based combination chemotherapy is standard treatment for the majority of patients with advanced non-small-cell lung cancer (NSCLC). The trial investigates the importance of the choice of platinum agent and dose of cisplatin in relation to patient outcomes.\nThe three-arm randomised phase III trial assigned patients with chemo-naïve stage IIIB/IV NSCLC in a 1:1:1 ratio to receive gemcitabine 1250\xa0mg/m\nThe trial recruited 1363 patients. Survival time differed significantly across the three treatment arms (p\xa0=\xa00.046) with GC50 worst with median 8.2 months compared to 9.5 for GC80 and 10.0 for GCb6. HRs (adjusted) for GC50 compared to GC80 was 1.13 (95% confidence interval [CI] 0.99-1.29) and for GC50 compared to GCb6 was 1.23 (95% CI: 1.08-1.41). GCb6 was significantly non-inferior to GC80 (HR\xa0=\xa00.93, upper limit of one-sided 95% CI 1.04). Adjusting for QoL did not change the findings. Best objective response rates were 29% (GC80), 20% (GC50) and 27% (GCb6), p\xa0<\xa00.007. There were more dose reductions and treatment delays in the GCb6 arm and more adverse events (60% with at least one grade 3-4 compared to 43% GC80 and 30% GC50).\nIn combination with gemcitabine, carboplatin at AUC6 is not inferior to cisplatin at 80\xa0mg/m\nNCT00112710.\n2004-003868-30.\nCRUK/04/009.', 'title': 'Carboplatin versus two doses of cisplatin in combination with gemcitabine in the treatment of advanced non-small-cell lung cancer: Results from a British Thoracic Oncology Group randomised phase III trial.', 'date': '2017-08-07'}, '21047474': {'article_id': '21047474', 'content': 'To compare the efficacy and toxicity of paclitaxel-carboplatin (TAX-CBP) and paclitaxel-cisplatin (TAX-DDP) chemotherapy protocols for advanced non-small cell lung cancer.\nOne hundred and twenty-six patients with non-small cell lung cancer were randomized into TAX-DDP and TAX-CBP groups. TAX-CBP group: TAX 175 mg/m² and CBP 350 mg/m², d1 iv; TAX-DDP group: TAX 175 mg/m² and DDP 100 mg/m² d1 iv. The therapy was repeated every 28 days. The response rate was assessed after three treatments.\nTAX-CBP group: response rate (RR) was 36% (22/61), 1-year survival rate was 34.1%. TAX-DDP group: RR was 33.9% (21/62),1-year survival rate was 33.1%. There was no significant difference of RR and 1-year survival rate between TAX-CBP and TAX-DDP group (P>0.05). The median survival time of TAX-CBP group (11.2 months) was significant higher than that of TAX-DDP group (9 months) (P<0.05). The major toxicity associated with paclitaxel included alpecia, myelosuppression, gastrointestinal reaction and myalgia or arthralgia. The thrombocytopenia in TAX-CBP group was more severe than that in TAX-DDP group (P<0.05). The Gastrointestinal and myalgia or arthralgia in TAX-DDP group were more severe than those in TAX-CBP group (P<0.05).\nTAX-CBP and TAX-DDP chemotherapy may be used as first choice protocol in the chemotherapy of non-small cell lung cancer.', 'title': '[A randomized phase II trial of paclitaxel in combination chemotherapy with platinum in the treatment of non-small cell lung cancer].', 'date': '2001-06-20'}, '11745199': {'article_id': '11745199', 'content': 'Eastern Cooperative Oncology Group (ECOG) Study E1594 compared paclitaxel and cisplatin with three newer chemotherapy doublets in the treatment of patients with advanced nonsmall cell lung carcinoma (NSCLC). The accrual of patients with an ECOG performance status (PS) of 2 was discontinued due to a perceived rate of unacceptable toxicity.\nPatients were stratified by PS and randomized to one of the following treatments: 1) paclitaxel (135 mg/m2) over 24 hours with cisplatin (75 mg/m2) on a 21-day cycle; 2) cisplatin (100 mg/m2) with gemcitabine (1 g/m2) on Days 1, 8, and 15 on a 28-day cycle; 3) cisplatin (75 mg/m2) with docetaxel (75 mg/m2) on a 21-day cycle; and 4) paclitaxel (225 mg/m2) over 3 hours with carboplatin (area under the curve, 6). All tests of statistical significance were two-sided.\nSixty-eight patients with an ECOG PS of 2 were enrolled, and 64 patients were evaluable for toxicity and response. Fifty-six percent of 64 evaluable patients were male, and 81% had Stage IV disease. Grade 3-4 hematologic toxicities occurred in > 50% of the patients in each treatment group. Nonhematologic Grade 3-4 toxicities occurred significantly less often in the paclitaxel and carboplatin arm (P = 0.0032). The overall rate of toxicity did not differ significantly from the rate of toxicity in the PS-0 or PS-1 cohorts. There were 5 deaths (7.35%) among 68 patients with a PS of 2 during therapy; however, only 2 of those deaths were attributed to therapy. The overall response rate for the 64 evaluable patients was 14%. The overall median survival of all 68 patients with a PS of 2, as determined by an intent-to-treat analysis, was 4.1 months.\nPatients with advanced NSCLC and a PS of 2 experienced a large number of adverse reactions and overall poor survival. A comparison with patients with a PS of 0-1 suggests that these events and the shorter survival were related to disease process rather than treatment. Alternative strategies need to be explored with therapy specifically tailored for this group of patients.', 'title': 'Outcome of patients with a performance status of 2 in Eastern Cooperative Oncology Group Study E1594: a Phase II trial in patients with metastatic nonsmall cell lung carcinoma .', 'date': '2001-12-18'}, '21333222': {'article_id': '21333222', 'content': 'To observe the efficacy and toxicity of gemcitabine plus carboplatin (GCarb) versus gemcitabine plus cisplatin (GCis) in the treatment of advanced non-small cell lung cancer (NSCLC).\nForty patients with histologically confirmed NSCLC were randomized to enter the study. GCarb group:Gemcitabine 1 000 mg/m² IV on day 1,8; carboplatin AUC 4-6 IV on day 1. GCis group: Gemcitabine 1 000 mg/m² IV on day 1,8; cisplatin 30-40 mg/m² IV on day 1-3.\nThe response rate was 65% and 60% for GCarb group and GCis group respectively (P > 0.5). Toxicities included myelosuppression, digestive reaction, alopecia and rash. Digestive toxicity in GCarb group was less than that in GCis group (P < 0.05).\nBoth GCarb and GCis regimes can be used as first-line protocol in the chemotherapy of non small cell lung cancer.', 'title': '[Comparison of efficacy and toxicity between gemcitabine plus carboplatin and gemcitabine plus cisplatin in the treatment of advanced non-small cell lung cancer].', 'date': '2002-12-20'}}",0.545454545,Oncology & Hematology
52,"Is the rate of 50% or greater reduction in seizure frequency higher, lower, or the same when comparing eslicarbazepine acetate (ESL) of any dose to placebo?",higher,moderate,no,"['20299189', '17319919', '19243424', '19832771', '25528898']",29067682,2017,"{'20299189': {'article_id': '20299189', 'content': 'To investigate the efficacy and safety of once-daily eslicarbazepine acetate (ESL) when used as add-on treatment in adults with > or = 4 partial-onset seizures per 4-week despite treatment with 1 to 3 antiepileptic drugs (AEDs).\nThis double-blind, parallel-group, multicenter study consisted of an 8-week observational baseline period, after which patients were randomized to placebo (n=100) or once-daily ESL 400 mg (n=96), 800 mg (n=101), or 1200 mg (n=98). Patients then entered a 14-week double-blind treatment phase. All patients started on their full maintenance dose except for those in the ESL 1200 mg group who received once-daily ESL 800 mg for 2 weeks before reaching their full maintenance dose.\nSeizure frequency per 4-week (primary endpoint) over the 14-week double-blind treatment period was significantly lower than placebo in the ESL 800 mg and 1200 mg (p<0.001) groups. Responder rate (> or = 50% reduction in seizure frequency) was 13.0% (placebo), 16.7% (400 mg), 40.0% (800 mg, p<0.001), and 37.1% (1200 mg, p<0.001). Median relative reduction in seizure frequency was 0.8% (placebo), 18.7% (400 mg), 32.6% (800 mg, p<0.001), and 32.8% (1200 mg). Discontinuation rates due to adverse events (AEs) were 3.0% (placebo), 12.5% (400 mg), 18.8% (800 mg), and 26.5% (1200 mg). The most common (>5%) AEs in any group were dizziness, somnolence, headache, nausea, diplopia, abnormal coordination, vomiting, blurred vision, and fatigue. The majority of AEs were of mild or moderate severity.\nTreatment with once-daily eslicarbazepine acetate 800 mg and 1200 mg was more effective than placebo and generally well tolerated in patients with partial-onset seizures refractory to treatment with 1 to 3 concomitant AEDs.', 'title': 'Eslicarbazepine acetate as adjunctive therapy in adult patients with partial epilepsy.', 'date': '2010-03-20'}, '17319919': {'article_id': '17319919', 'content': 'To explore the efficacy and safety of eslicarbazepine acetate (BIA 2-093), a new antiepileptic drug, as adjunctive therapy in adult patients with partial epilepsy.\nA multicenter, double-blind, randomized, placebo-controlled study was conducted in 143 refractory patients aged 18-65 years with >or=4 partial-onset seizures/month. The study consisted of a 12-week treatment period followed by a 1-week tapering off. Patients were randomly assigned to one of three groups: treatment with eslicarbazepine acetate once daily (QD, n=50), twice daily (BID, n=46), or placebo (PL, n=47). The daily dose was titrated from 400 mg to 800 mg and to 1,200 mg at 4-week intervals. The proportion of responders (patients with a >or=50% seizure reduction) was the primary end point.\nThe percentage of responders versus baseline showed a statistically significant difference between QD and PL groups (54% vs. 28%; 90% CI =-infinity, -14; p=0.008). The difference between the BID (41%) and PL did not reach statistical significance (90% CI =-infinity, -1; p=0.12). A significantly higher proportion of responders in weeks 5-8 was found in the QD group than in the BID group (58% vs. 33%, respectively, p=0.022). At the end of the 12-week treatment, the number of seizure-free patients in the QD and BID groups was 24%, which was significantly different from the PL group. The incidence of adverse events was similar between the treatment groups and no drug-related serious adverse events occurred.\nEslicarbazepine acetate was efficacious and well tolerated as an adjunctive therapy of refractory epileptic patients.', 'title': 'Eslicarbazepine acetate: a double-blind, add-on, placebo-controlled exploratory trial in adult patients with partial-onset seizures.', 'date': '2007-02-27'}, '19243424': {'article_id': '19243424', 'content': 'To study the efficacy and safety of eslicarbazepine acetate (ESL) as adjunctive therapy for refractory partial seizures in adults with >or=4 partial-onset seizures (simple or complex, with or without secondary generalization) per 4 weeks despite treatment with 1-2 antiepileptic drugs (AEDs).\nThis multicenter, parallel-group study had an 8-week, single-blind, placebo baseline phase, after which patients were randomized to placebo (n = 102) or once-daily ESL 400 mg (n = 100), 800 mg (n = 98), or 1,200 mg (n = 102) in the double-blind treatment phase. ESL starting dose was 400 mg; thereafter, ESL was titrated at weekly 400-mg steps to the full maintenance dose (12 weeks).\nSeizure frequency adjusted per 4 weeks over the maintenance period (primary endpoint) was significantly lower than placebo in the ESL 1,200-mg (p = 0.0003) and 800-mg (p = 0.0028) groups [analysis of covariance (ANCOVA) of log-transformed seizure frequency]. Responder rate was 20% (placebo), 23% (400 mg), 34% (800 mg), and 43% (1,200 mg). Median relative reduction in seizure frequency was 16% (placebo), 26% (400 mg), 36% (800 mg), and 45% (1,200 mg). The most frequent concomitant AEDs were carbamazepine (56-62% of patients), lamotrigine (25-27%), and valproic acid (22-28%). Similar efficacy results were obtained in patients administered ESL with or without carbamazepine as concomitant AED. Discontinuation rates caused by adverse events (AEs) were 3.9% (placebo), 4% (400 mg), 8.2% (800 mg), and 19.6% (1,200 mg). AEs in >10% of any group were dizziness, headache, and diplopia. Most AEs were mild or moderate.\nESL, 800 and 1,200 mg once-daily, was well tolerated and more effective than placebo in patients who were refractory to treatment with one or two concomitant AEDs.', 'title': 'Efficacy and safety of eslicarbazepine acetate as adjunctive treatment in adults with refractory partial-onset seizures: a randomized, double-blind, placebo-controlled, parallel-group phase III study.', 'date': '2009-02-27'}, '19832771': {'article_id': '19832771', 'content': 'To evaluate the efficacy and safety of eslicarbazepine acetate (ESL) as adjunctive therapy in adults with partial-onset seizures.\nDouble-blind, placebo-controlled, parallel-group, multicenter study consisting of an 8-week baseline period, after which patients were randomized to placebo (n = 87) or once-daily ESL 800 mg (n = 85) or 1200 mg (n = 80). Patients received half dose during 2 weeks preceding a 12-week maintenance period.\nSeizure frequency over the maintenance period was significantly (P < 0.05) lower than placebo in both ESL groups. Responder rate was 23% (placebo), 35% (800 mg), and 38% (1200 mg). Median relative reduction in seizure frequency was 17% (placebo), 38% (800 mg), and 42% (1200 mg). The most common adverse events (AEs) (>10%) were dizziness, somnolence, headache, and nausea. The majority of AEs were of mild or moderate severity.\nOnce-daily treatment with ESL 800 and 1200 mg was effective and generally well tolerated.', 'title': 'Efficacy and safety of 800 and 1200 mg eslicarbazepine acetate as adjunctive treatment in adults with refractory partial-onset seizures.', 'date': '2009-10-17'}, '25528898': {'article_id': '25528898', 'content': 'To evaluate the efficacy and safety of adjunctive eslicarbazepine acetate (ESL) in patients with refractory partial-onset seizures.\nThis randomized, placebo-controlled, double-blind, parallel-group, phase III study was conducted at 173 centers in 19 countries, including the United States and Canada. Eligible patients were aged ≥16 years and had uncontrolled partial-onset seizures despite treatment with 1-2 antiepileptic drugs (AEDs). After an 8-week baseline period, patients were randomized to once-daily placebo (n = 226), ESL 800 mg (n = 216), or ESL 1,200 mg (n = 211). Following a 2-week titration period, patients received ESL 800 or 1,200 mg once-daily for 12 weeks. Seizure data were captured and documented using event-entry or daily entry diaries.\nStandardized seizure frequency (SSF) during the maintenance period (primary end point) was reduced with ESL 1,200 mg (p = 0.004), and there was a trend toward improvement with ESL 800 mg (p = 0.06), compared with placebo. When data for titration and maintenance periods were combined, ESL 800 mg (p = 0.001) and 1,200 mg (p < 0.001) both reduced SSF. There were no statistically significant interactions between treatment response and geographical region (p = 0.38) or diary version (p = 0.76). Responder rate (≥50% reduction in SSF) was significantly higher with ESL 1,200 mg (42.6%, p < 0.001) but not ESL 800 mg (30.5%, p = 0.07) than placebo (23.1%). Incidence of treatment-emergent adverse events (TEAEs) and TEAEs leading to discontinuation increased with ESL dose. The most common TEAEs were dizziness, somnolence, nausea, headache, and diplopia.\nAdjunctive ESL 1,200 mg once-daily was more efficacious than placebo in adult patients with refractory partial-onset seizures. The once-daily 800 mg dose showed a marginal effect on SSF, but did not reach statistical significance. Both doses were well tolerated. Efficacy assessment was not affected by diary format used.', 'title': 'Eslicarbazepine acetate as adjunctive therapy in patients with uncontrolled partial-onset seizures: Results of a phase III, double-blind, randomized, placebo-controlled trial.', 'date': '2014-12-23'}}",1.0,Psychiatry & Neurology
53,"Is the rate of study withdrawal due to adverse effects higher, lower, or the same when comparing eslicarbazepine acetate (ESL) of any dose to placebo?",higher,moderate,yes,"['20299189', '17319919', '19243424', '19832771', '25528898']",29067682,2017,"{'20299189': {'article_id': '20299189', 'content': 'To investigate the efficacy and safety of once-daily eslicarbazepine acetate (ESL) when used as add-on treatment in adults with > or = 4 partial-onset seizures per 4-week despite treatment with 1 to 3 antiepileptic drugs (AEDs).\nThis double-blind, parallel-group, multicenter study consisted of an 8-week observational baseline period, after which patients were randomized to placebo (n=100) or once-daily ESL 400 mg (n=96), 800 mg (n=101), or 1200 mg (n=98). Patients then entered a 14-week double-blind treatment phase. All patients started on their full maintenance dose except for those in the ESL 1200 mg group who received once-daily ESL 800 mg for 2 weeks before reaching their full maintenance dose.\nSeizure frequency per 4-week (primary endpoint) over the 14-week double-blind treatment period was significantly lower than placebo in the ESL 800 mg and 1200 mg (p<0.001) groups. Responder rate (> or = 50% reduction in seizure frequency) was 13.0% (placebo), 16.7% (400 mg), 40.0% (800 mg, p<0.001), and 37.1% (1200 mg, p<0.001). Median relative reduction in seizure frequency was 0.8% (placebo), 18.7% (400 mg), 32.6% (800 mg, p<0.001), and 32.8% (1200 mg). Discontinuation rates due to adverse events (AEs) were 3.0% (placebo), 12.5% (400 mg), 18.8% (800 mg), and 26.5% (1200 mg). The most common (>5%) AEs in any group were dizziness, somnolence, headache, nausea, diplopia, abnormal coordination, vomiting, blurred vision, and fatigue. The majority of AEs were of mild or moderate severity.\nTreatment with once-daily eslicarbazepine acetate 800 mg and 1200 mg was more effective than placebo and generally well tolerated in patients with partial-onset seizures refractory to treatment with 1 to 3 concomitant AEDs.', 'title': 'Eslicarbazepine acetate as adjunctive therapy in adult patients with partial epilepsy.', 'date': '2010-03-20'}, '17319919': {'article_id': '17319919', 'content': 'To explore the efficacy and safety of eslicarbazepine acetate (BIA 2-093), a new antiepileptic drug, as adjunctive therapy in adult patients with partial epilepsy.\nA multicenter, double-blind, randomized, placebo-controlled study was conducted in 143 refractory patients aged 18-65 years with >or=4 partial-onset seizures/month. The study consisted of a 12-week treatment period followed by a 1-week tapering off. Patients were randomly assigned to one of three groups: treatment with eslicarbazepine acetate once daily (QD, n=50), twice daily (BID, n=46), or placebo (PL, n=47). The daily dose was titrated from 400 mg to 800 mg and to 1,200 mg at 4-week intervals. The proportion of responders (patients with a >or=50% seizure reduction) was the primary end point.\nThe percentage of responders versus baseline showed a statistically significant difference between QD and PL groups (54% vs. 28%; 90% CI =-infinity, -14; p=0.008). The difference between the BID (41%) and PL did not reach statistical significance (90% CI =-infinity, -1; p=0.12). A significantly higher proportion of responders in weeks 5-8 was found in the QD group than in the BID group (58% vs. 33%, respectively, p=0.022). At the end of the 12-week treatment, the number of seizure-free patients in the QD and BID groups was 24%, which was significantly different from the PL group. The incidence of adverse events was similar between the treatment groups and no drug-related serious adverse events occurred.\nEslicarbazepine acetate was efficacious and well tolerated as an adjunctive therapy of refractory epileptic patients.', 'title': 'Eslicarbazepine acetate: a double-blind, add-on, placebo-controlled exploratory trial in adult patients with partial-onset seizures.', 'date': '2007-02-27'}, '19243424': {'article_id': '19243424', 'content': 'To study the efficacy and safety of eslicarbazepine acetate (ESL) as adjunctive therapy for refractory partial seizures in adults with >or=4 partial-onset seizures (simple or complex, with or without secondary generalization) per 4 weeks despite treatment with 1-2 antiepileptic drugs (AEDs).\nThis multicenter, parallel-group study had an 8-week, single-blind, placebo baseline phase, after which patients were randomized to placebo (n = 102) or once-daily ESL 400 mg (n = 100), 800 mg (n = 98), or 1,200 mg (n = 102) in the double-blind treatment phase. ESL starting dose was 400 mg; thereafter, ESL was titrated at weekly 400-mg steps to the full maintenance dose (12 weeks).\nSeizure frequency adjusted per 4 weeks over the maintenance period (primary endpoint) was significantly lower than placebo in the ESL 1,200-mg (p = 0.0003) and 800-mg (p = 0.0028) groups [analysis of covariance (ANCOVA) of log-transformed seizure frequency]. Responder rate was 20% (placebo), 23% (400 mg), 34% (800 mg), and 43% (1,200 mg). Median relative reduction in seizure frequency was 16% (placebo), 26% (400 mg), 36% (800 mg), and 45% (1,200 mg). The most frequent concomitant AEDs were carbamazepine (56-62% of patients), lamotrigine (25-27%), and valproic acid (22-28%). Similar efficacy results were obtained in patients administered ESL with or without carbamazepine as concomitant AED. Discontinuation rates caused by adverse events (AEs) were 3.9% (placebo), 4% (400 mg), 8.2% (800 mg), and 19.6% (1,200 mg). AEs in >10% of any group were dizziness, headache, and diplopia. Most AEs were mild or moderate.\nESL, 800 and 1,200 mg once-daily, was well tolerated and more effective than placebo in patients who were refractory to treatment with one or two concomitant AEDs.', 'title': 'Efficacy and safety of eslicarbazepine acetate as adjunctive treatment in adults with refractory partial-onset seizures: a randomized, double-blind, placebo-controlled, parallel-group phase III study.', 'date': '2009-02-27'}, '19832771': {'article_id': '19832771', 'content': 'To evaluate the efficacy and safety of eslicarbazepine acetate (ESL) as adjunctive therapy in adults with partial-onset seizures.\nDouble-blind, placebo-controlled, parallel-group, multicenter study consisting of an 8-week baseline period, after which patients were randomized to placebo (n = 87) or once-daily ESL 800 mg (n = 85) or 1200 mg (n = 80). Patients received half dose during 2 weeks preceding a 12-week maintenance period.\nSeizure frequency over the maintenance period was significantly (P < 0.05) lower than placebo in both ESL groups. Responder rate was 23% (placebo), 35% (800 mg), and 38% (1200 mg). Median relative reduction in seizure frequency was 17% (placebo), 38% (800 mg), and 42% (1200 mg). The most common adverse events (AEs) (>10%) were dizziness, somnolence, headache, and nausea. The majority of AEs were of mild or moderate severity.\nOnce-daily treatment with ESL 800 and 1200 mg was effective and generally well tolerated.', 'title': 'Efficacy and safety of 800 and 1200 mg eslicarbazepine acetate as adjunctive treatment in adults with refractory partial-onset seizures.', 'date': '2009-10-17'}, '25528898': {'article_id': '25528898', 'content': 'To evaluate the efficacy and safety of adjunctive eslicarbazepine acetate (ESL) in patients with refractory partial-onset seizures.\nThis randomized, placebo-controlled, double-blind, parallel-group, phase III study was conducted at 173 centers in 19 countries, including the United States and Canada. Eligible patients were aged ≥16 years and had uncontrolled partial-onset seizures despite treatment with 1-2 antiepileptic drugs (AEDs). After an 8-week baseline period, patients were randomized to once-daily placebo (n = 226), ESL 800 mg (n = 216), or ESL 1,200 mg (n = 211). Following a 2-week titration period, patients received ESL 800 or 1,200 mg once-daily for 12 weeks. Seizure data were captured and documented using event-entry or daily entry diaries.\nStandardized seizure frequency (SSF) during the maintenance period (primary end point) was reduced with ESL 1,200 mg (p = 0.004), and there was a trend toward improvement with ESL 800 mg (p = 0.06), compared with placebo. When data for titration and maintenance periods were combined, ESL 800 mg (p = 0.001) and 1,200 mg (p < 0.001) both reduced SSF. There were no statistically significant interactions between treatment response and geographical region (p = 0.38) or diary version (p = 0.76). Responder rate (≥50% reduction in SSF) was significantly higher with ESL 1,200 mg (42.6%, p < 0.001) but not ESL 800 mg (30.5%, p = 0.07) than placebo (23.1%). Incidence of treatment-emergent adverse events (TEAEs) and TEAEs leading to discontinuation increased with ESL dose. The most common TEAEs were dizziness, somnolence, nausea, headache, and diplopia.\nAdjunctive ESL 1,200 mg once-daily was more efficacious than placebo in adult patients with refractory partial-onset seizures. The once-daily 800 mg dose showed a marginal effect on SSF, but did not reach statistical significance. Both doses were well tolerated. Efficacy assessment was not affected by diary format used.', 'title': 'Eslicarbazepine acetate as adjunctive therapy in patients with uncontrolled partial-onset seizures: Results of a phase III, double-blind, randomized, placebo-controlled trial.', 'date': '2014-12-23'}}",0.2,Psychiatry & Neurology
54,"Is the overall cervical dystonia improvement higher, lower, or the same when comparing botulinum toxin type A (BtA) to botulinum toxin type B (BtB)?",higher,low,no,"['16275831', '18098274']",27782297,2016,"{'16275831': {'article_id': '16275831', 'content': 'To directly compare two serotypes of botulinum toxin (BoNTA and BoNTB) in cervical dystonia (CD) using a randomized, double-blind, parallel-arm study design.\nSubjects with CD who had a previous response from BoNTA were randomly assigned to BoNTA or BoNTB and evaluated in a blinded fashion at baseline, 4 weeks, 8 weeks, and 2-week intervals thereafter until loss of 80% of clinical effect or completion of 20 weeks of observation. CD severity was measured with the Toronto Western Spasmodic Torticollis Rating Scale (TWSTRS), and adverse events were assessed by structured interview. Statistical analysis included Wilcoxon rank sum test, log rank tests, and Kaplan-Meier survival curves for duration of effect.\nA total of 139 subjects (BoNTA, n = 74; BoNTB, n = 65) were randomized at 19 study sites. Improvement in TWSTRS score was found at 4 weeks after injection and did not differ between serotypes. Dysphagia and dry mouth were more frequent with BoNTB (dysphagia: BoNTA 19% vs BoNTB 48%, p = 0.0005; dry mouth (BoNTA 41% vs BoNTB 80%, p < 0.0001). In clinical responders, BoNT A had a modestly longer duration of benefit (BoNTA 14 weeks, BoNTB 12.1 weeks, p = 0.033).\nBoth serotypes of botulinum toxin (BoNTA and BoNTB) had equivalent benefit in subjects with cervical dystonia at 4 weeks. BoNTA had fewer adverse events and a marginally longer duration of effect in subjects showing a clinical response.', 'title': 'Comparison of botulinum toxin serotypes A and B for the treatment of cervical dystonia.', 'date': '2005-11-09'}, '18098274': {'article_id': '18098274', 'content': 'The objective of this study was to compare efficacy, safety, and duration of botulinum toxin type A (BoNT-A) and type B (BoNT-B) in toxin-naïve cervical dystonia (CD) subjects. BoNT-naïve CD subjects were randomized to BoNT-A or BoNT-B and evaluated in a double-blind trial at baseline and every 4-weeks following one treatment. The primary measure was the change in Toronto Western Spasmodic Torticollis Rating Scale (TWSTRS) from baseline to week 4 post-injection. Secondary measures included change in TWSTRS-subscale scores, pain, global impressions, and duration of response and safety assessments. The study was designed as a noninferiority trial of BoNT-B to BoNT-A. 111 subjects were randomized (55 BoNT-A; 56 BoNT-B). Improvement in TWSTRS-total scores 4 weeks after BoNT-B was noninferior to BoNT-A (adjusted means 11.0 (SE 1.2) and 8.8 (SE 1.2), respectively; per-protocol-population (PPP)). The median duration of effect of BoNT-A and BoNT-B was not different (13.1 vs. 13.7 weeks, respectively; P-value = 0.833; PPP). There were no significant differences in the occurrence of injection site pain and dysphagia. Mild dry mouth was more frequent with BoNT-B but there were no differences for moderate/severe dry mouth. In this study, both BoNT-A and B were shown to be effective and safe for the treatment of toxin-naive CD subjects.', 'title': 'Botulinum toxin type B vs. type A in toxin-naïve patients with cervical dystonia: Randomized, double-blind, noninferiority trial.', 'date': '2007-12-22'}}",0.0,Psychiatry & Neurology
55,"Is the rate of dysphagia higher, lower, or the same when comparing botulinum toxin type A (BtA) to botulinum toxin type B (BtB)?",no difference,low,no,"['16275831', '18098274', '16157918']",27782297,2016,"{'16275831': {'article_id': '16275831', 'content': 'To directly compare two serotypes of botulinum toxin (BoNTA and BoNTB) in cervical dystonia (CD) using a randomized, double-blind, parallel-arm study design.\nSubjects with CD who had a previous response from BoNTA were randomly assigned to BoNTA or BoNTB and evaluated in a blinded fashion at baseline, 4 weeks, 8 weeks, and 2-week intervals thereafter until loss of 80% of clinical effect or completion of 20 weeks of observation. CD severity was measured with the Toronto Western Spasmodic Torticollis Rating Scale (TWSTRS), and adverse events were assessed by structured interview. Statistical analysis included Wilcoxon rank sum test, log rank tests, and Kaplan-Meier survival curves for duration of effect.\nA total of 139 subjects (BoNTA, n = 74; BoNTB, n = 65) were randomized at 19 study sites. Improvement in TWSTRS score was found at 4 weeks after injection and did not differ between serotypes. Dysphagia and dry mouth were more frequent with BoNTB (dysphagia: BoNTA 19% vs BoNTB 48%, p = 0.0005; dry mouth (BoNTA 41% vs BoNTB 80%, p < 0.0001). In clinical responders, BoNT A had a modestly longer duration of benefit (BoNTA 14 weeks, BoNTB 12.1 weeks, p = 0.033).\nBoth serotypes of botulinum toxin (BoNTA and BoNTB) had equivalent benefit in subjects with cervical dystonia at 4 weeks. BoNTA had fewer adverse events and a marginally longer duration of effect in subjects showing a clinical response.', 'title': 'Comparison of botulinum toxin serotypes A and B for the treatment of cervical dystonia.', 'date': '2005-11-09'}, '18098274': {'article_id': '18098274', 'content': 'The objective of this study was to compare efficacy, safety, and duration of botulinum toxin type A (BoNT-A) and type B (BoNT-B) in toxin-naïve cervical dystonia (CD) subjects. BoNT-naïve CD subjects were randomized to BoNT-A or BoNT-B and evaluated in a double-blind trial at baseline and every 4-weeks following one treatment. The primary measure was the change in Toronto Western Spasmodic Torticollis Rating Scale (TWSTRS) from baseline to week 4 post-injection. Secondary measures included change in TWSTRS-subscale scores, pain, global impressions, and duration of response and safety assessments. The study was designed as a noninferiority trial of BoNT-B to BoNT-A. 111 subjects were randomized (55 BoNT-A; 56 BoNT-B). Improvement in TWSTRS-total scores 4 weeks after BoNT-B was noninferior to BoNT-A (adjusted means 11.0 (SE 1.2) and 8.8 (SE 1.2), respectively; per-protocol-population (PPP)). The median duration of effect of BoNT-A and BoNT-B was not different (13.1 vs. 13.7 weeks, respectively; P-value = 0.833; PPP). There were no significant differences in the occurrence of injection site pain and dysphagia. Mild dry mouth was more frequent with BoNT-B but there were no differences for moderate/severe dry mouth. In this study, both BoNT-A and B were shown to be effective and safe for the treatment of toxin-naive CD subjects.', 'title': 'Botulinum toxin type B vs. type A in toxin-naïve patients with cervical dystonia: Randomized, double-blind, noninferiority trial.', 'date': '2007-12-22'}, '16157918': {'article_id': '16157918', 'content': 'To compare autonomic effects of botulinum toxin (BTX), we randomized patients with cervical dystonia to receive either BTX-A or BTX-B in a double-blind manner. Efficacy and physiologic questionnaire measures of autonomic function were assessed at baseline and 2 weeks after injection. Patients treated with BTX-B had less saliva production (p < 0.01) and greater severity of constipation (p = 0.037) than those treated with BTX-A, but did not differ in other tests of autonomic functions.', 'title': 'Autonomic function after botulinum toxin type A or B: a double-blind, randomized trial.', 'date': '2005-09-15'}}",0.333333333,Psychiatry & Neurology
56,"Is the risk of treatment‐related sore throat/dry mouth higher, lower, or the same when comparing botulinum toxin type A (BtA) to botulinum toxin type B (BtB)?",lower,moderate,no,"['16275831', '18098274']",27782297,2016,"{'16275831': {'article_id': '16275831', 'content': 'To directly compare two serotypes of botulinum toxin (BoNTA and BoNTB) in cervical dystonia (CD) using a randomized, double-blind, parallel-arm study design.\nSubjects with CD who had a previous response from BoNTA were randomly assigned to BoNTA or BoNTB and evaluated in a blinded fashion at baseline, 4 weeks, 8 weeks, and 2-week intervals thereafter until loss of 80% of clinical effect or completion of 20 weeks of observation. CD severity was measured with the Toronto Western Spasmodic Torticollis Rating Scale (TWSTRS), and adverse events were assessed by structured interview. Statistical analysis included Wilcoxon rank sum test, log rank tests, and Kaplan-Meier survival curves for duration of effect.\nA total of 139 subjects (BoNTA, n = 74; BoNTB, n = 65) were randomized at 19 study sites. Improvement in TWSTRS score was found at 4 weeks after injection and did not differ between serotypes. Dysphagia and dry mouth were more frequent with BoNTB (dysphagia: BoNTA 19% vs BoNTB 48%, p = 0.0005; dry mouth (BoNTA 41% vs BoNTB 80%, p < 0.0001). In clinical responders, BoNT A had a modestly longer duration of benefit (BoNTA 14 weeks, BoNTB 12.1 weeks, p = 0.033).\nBoth serotypes of botulinum toxin (BoNTA and BoNTB) had equivalent benefit in subjects with cervical dystonia at 4 weeks. BoNTA had fewer adverse events and a marginally longer duration of effect in subjects showing a clinical response.', 'title': 'Comparison of botulinum toxin serotypes A and B for the treatment of cervical dystonia.', 'date': '2005-11-09'}, '18098274': {'article_id': '18098274', 'content': 'The objective of this study was to compare efficacy, safety, and duration of botulinum toxin type A (BoNT-A) and type B (BoNT-B) in toxin-naïve cervical dystonia (CD) subjects. BoNT-naïve CD subjects were randomized to BoNT-A or BoNT-B and evaluated in a double-blind trial at baseline and every 4-weeks following one treatment. The primary measure was the change in Toronto Western Spasmodic Torticollis Rating Scale (TWSTRS) from baseline to week 4 post-injection. Secondary measures included change in TWSTRS-subscale scores, pain, global impressions, and duration of response and safety assessments. The study was designed as a noninferiority trial of BoNT-B to BoNT-A. 111 subjects were randomized (55 BoNT-A; 56 BoNT-B). Improvement in TWSTRS-total scores 4 weeks after BoNT-B was noninferior to BoNT-A (adjusted means 11.0 (SE 1.2) and 8.8 (SE 1.2), respectively; per-protocol-population (PPP)). The median duration of effect of BoNT-A and BoNT-B was not different (13.1 vs. 13.7 weeks, respectively; P-value = 0.833; PPP). There were no significant differences in the occurrence of injection site pain and dysphagia. Mild dry mouth was more frequent with BoNT-B but there were no differences for moderate/severe dry mouth. In this study, both BoNT-A and B were shown to be effective and safe for the treatment of toxin-naive CD subjects.', 'title': 'Botulinum toxin type B vs. type A in toxin-naïve patients with cervical dystonia: Randomized, double-blind, noninferiority trial.', 'date': '2007-12-22'}}",0.0,Surgery
57,"Is the overall survival rate higher, lower, or the same when comparing percutaneous ethanol injection (PEI) to percutaneous acetic acid injection (PAI)?",no difference,low,no,"['9425919', '16009687']",25620061,2015,"{'9425919': {'article_id': '9425919', 'content': 'To assess whether ultrasound-guided percutaneous acetic acid injection is superior to percutaneous ethanol injection in the treatment of small hepatocellular carcinoma (HCC), 60 patients with one to four HCCs smaller than 3 cm were entered onto a randomized controlled trial. Thirty-one and 29 patients, respectively, were treated by percutaneous acetic acid injection using 50% acetic acid or by percutaneous ethanol injection using absolute ethanol. There were no significant differences in age, sex ratio, Child-Pugh class, size of tumors, or number of tumors between the two groups. When there was no evidence of viable HCC from biopsy, plain and helical dynamic computed tomography, or angiography, the treatment was considered successful and was discontinued. All original tumors were treated successfully by either therapy. However, 8% of 38 tumors treated with percutaneous acetic acid injection and 37% of 35 tumors treated with percutaneous ethanol injection developed a local recurrence (P < .001) during the follow-up periods of 29 +/- 8 months and 23 +/- 10 months, respectively. The 1- and 2-year survival rates were 100% and 92% in percutaneous acetic acid injection and 83% and 63% in percutaneous ethanol injection (P = .0017). A multivariate analysis of prognostic factors revealed that treatment was an independent predictor of survival. The risk ratio of percutaneous acetic acid injection versus percutaneous ethanol injection was 0.120 (range, 0.027-0.528; P = .0050). In conclusion, percutaneous acetic acid injection is superior to percutaneous ethanol injection in the treatment of small HCC.', 'title': 'Prospective randomized controlled trial comparing percutaneous acetic acid injection and percutaneous ethanol injection for small hepatocellular carcinoma.', 'date': '1998-01-13'}, '16009687': {'article_id': '16009687', 'content': 'The aim of this study was to compare the outcomes of radiofrequency thermal ablation (RFTA), percutaneous ethanol injection (PEI), and percutaneous acetic acid injection (PAI) in the treatment of hepatocellular carcinoma (HCC).\nA total of 187 patients with HCCs of 3 cm or less were randomly assigned to RFTA (n = 62), PEI (n = 62), or PAI (n = 63). Tumour recurrence and survival rates were assessed.\nOne, two, and three year local recurrence rates were 10%, 14%, and 14% in the RFTA group, 16%, 34%, and 34% in the PEI group, and 14%, 31%, and 31% in the PAI group (RFTA v PEI, p = 0.012; RFTA v PAI, p = 0.017). One, two, and three year survival rates were 93%, 81%, and 74% in the RFTA group, 88%, 66%, and 51% in the PEI group, and 90%, 67%, and 53% in the PAI group (RFTA v PEI, p = 0.031; RFTA v PAI, p = 0.038). One, two, and three year cancer free survival rates were 74%, 60%, and 43% in the RFTA group, 70%, 41%, and 21% in the PEI group, and 71%, 43%, and 23% in the PAI group (RFTA v PEI, p = 0.038; RFTA v PAI, p = 0.041). Tumour size, tumour differentiation, and treatment methods (RFTA v PEI and PAI) were significant factors for local recurrence, overall survival, and cancer free survival. Major complications occurred in 4.8% of patients (two with haemothorax, one gastric perforation) in the RFTA group and in none in two other groups (RFTA v PEI and PAI, p = 0.035).\nRFTA was superior to PEI and PAI with respect to local recurrence, overall survival, and cancer free survival rates, but RFTA also caused more major complications.', 'title': 'Randomised controlled trial comparing percutaneous radiofrequency thermal ablation, percutaneous ethanol injection, and percutaneous acetic acid injection to treat hepatocellular carcinoma of 3 cm or less.', 'date': '2005-07-13'}}",0.0,Oncology & Hematology
58,"Is the overall survival rate higher, lower, or the same when comparing percutaneous ethanol injection (PEI) to surgery?",no difference,very low,no,['15973099'],25620061,2015,"{'15973099': {'article_id': '15973099', 'content': 'To compare disease recurrence and survival among patients with small hepatocellular carcinoma after surgical resection or percutaneous ethanol injection therapy, 2 treatments that have not been evaluated with a prospective study.\nA total of 76 patients were randomly assigned to 2 groups based on treatment; all had one or 2 tumors with diameter </=3 cm, with hepatitis without cirrhosis or Child class A or B cirrhosis without evident ascites or bleeding tendency.\nFollow-up ranged from 12 to 59 months. Among percutaneous injection patients, 18 had recurrence 1 to 37 months after treatment (true recurrence, 11; original safety margin inadequate, 3; limitation of imaging technology to detect tiny tumors, 4). Three injection therapy patients died of cancer 25, 37, and 57 months after treatment. For the surgical resection group, 15 had recurrence 2 to 54 months after treatment (true recurrence, 12; limitation of imaging, 2; neck metastasis, 1). Five resection patients died of cancer at 11, 20, 23, 26, and 52 months, respectively. By Cox regression model and Kaplan-Meier survival analysis, there is no statistical significance for recurrence and survival between treatment groups. However, tumor size larger than 2 cm and alpha-fetoprotein over 200 ng/mL correlated with higher recurrence rate, and Child class B liver cirrhosis correlated with shorter survival.\nPercutaneous ethanol injection therapy appears to be as safe and effective as resection, and both treatments can be considered first-line options for small hepatocellular carcinoma.', 'title': 'Percutaneous ethanol injection versus surgical resection for the treatment of small hepatocellular carcinoma: a prospective study.', 'date': '2005-06-24'}}",1.0,Surgery
221,"Is the overall survival rate higher, lower, or the same when comparing percutaneous acetic acid injection (PAI) to sham intervention?",insufficient data,,no,"['9425919', '16009687', '15973099']",25620061,2015,"{'9425919': {'article_id': '9425919', 'content': 'To assess whether ultrasound-guided percutaneous acetic acid injection is superior to percutaneous ethanol injection in the treatment of small hepatocellular carcinoma (HCC), 60 patients with one to four HCCs smaller than 3 cm were entered onto a randomized controlled trial. Thirty-one and 29 patients, respectively, were treated by percutaneous acetic acid injection using 50% acetic acid or by percutaneous ethanol injection using absolute ethanol. There were no significant differences in age, sex ratio, Child-Pugh class, size of tumors, or number of tumors between the two groups. When there was no evidence of viable HCC from biopsy, plain and helical dynamic computed tomography, or angiography, the treatment was considered successful and was discontinued. All original tumors were treated successfully by either therapy. However, 8% of 38 tumors treated with percutaneous acetic acid injection and 37% of 35 tumors treated with percutaneous ethanol injection developed a local recurrence (P < .001) during the follow-up periods of 29 +/- 8 months and 23 +/- 10 months, respectively. The 1- and 2-year survival rates were 100% and 92% in percutaneous acetic acid injection and 83% and 63% in percutaneous ethanol injection (P = .0017). A multivariate analysis of prognostic factors revealed that treatment was an independent predictor of survival. The risk ratio of percutaneous acetic acid injection versus percutaneous ethanol injection was 0.120 (range, 0.027-0.528; P = .0050). In conclusion, percutaneous acetic acid injection is superior to percutaneous ethanol injection in the treatment of small HCC.', 'title': 'Prospective randomized controlled trial comparing percutaneous acetic acid injection and percutaneous ethanol injection for small hepatocellular carcinoma.', 'date': '1998-01-13'}, '16009687': {'article_id': '16009687', 'content': 'The aim of this study was to compare the outcomes of radiofrequency thermal ablation (RFTA), percutaneous ethanol injection (PEI), and percutaneous acetic acid injection (PAI) in the treatment of hepatocellular carcinoma (HCC).\nA total of 187 patients with HCCs of 3 cm or less were randomly assigned to RFTA (n = 62), PEI (n = 62), or PAI (n = 63). Tumour recurrence and survival rates were assessed.\nOne, two, and three year local recurrence rates were 10%, 14%, and 14% in the RFTA group, 16%, 34%, and 34% in the PEI group, and 14%, 31%, and 31% in the PAI group (RFTA v PEI, p = 0.012; RFTA v PAI, p = 0.017). One, two, and three year survival rates were 93%, 81%, and 74% in the RFTA group, 88%, 66%, and 51% in the PEI group, and 90%, 67%, and 53% in the PAI group (RFTA v PEI, p = 0.031; RFTA v PAI, p = 0.038). One, two, and three year cancer free survival rates were 74%, 60%, and 43% in the RFTA group, 70%, 41%, and 21% in the PEI group, and 71%, 43%, and 23% in the PAI group (RFTA v PEI, p = 0.038; RFTA v PAI, p = 0.041). Tumour size, tumour differentiation, and treatment methods (RFTA v PEI and PAI) were significant factors for local recurrence, overall survival, and cancer free survival. Major complications occurred in 4.8% of patients (two with haemothorax, one gastric perforation) in the RFTA group and in none in two other groups (RFTA v PEI and PAI, p = 0.035).\nRFTA was superior to PEI and PAI with respect to local recurrence, overall survival, and cancer free survival rates, but RFTA also caused more major complications.', 'title': 'Randomised controlled trial comparing percutaneous radiofrequency thermal ablation, percutaneous ethanol injection, and percutaneous acetic acid injection to treat hepatocellular carcinoma of 3 cm or less.', 'date': '2005-07-13'}, '15973099': {'article_id': '15973099', 'content': 'To compare disease recurrence and survival among patients with small hepatocellular carcinoma after surgical resection or percutaneous ethanol injection therapy, 2 treatments that have not been evaluated with a prospective study.\nA total of 76 patients were randomly assigned to 2 groups based on treatment; all had one or 2 tumors with diameter </=3 cm, with hepatitis without cirrhosis or Child class A or B cirrhosis without evident ascites or bleeding tendency.\nFollow-up ranged from 12 to 59 months. Among percutaneous injection patients, 18 had recurrence 1 to 37 months after treatment (true recurrence, 11; original safety margin inadequate, 3; limitation of imaging technology to detect tiny tumors, 4). Three injection therapy patients died of cancer 25, 37, and 57 months after treatment. For the surgical resection group, 15 had recurrence 2 to 54 months after treatment (true recurrence, 12; limitation of imaging, 2; neck metastasis, 1). Five resection patients died of cancer at 11, 20, 23, 26, and 52 months, respectively. By Cox regression model and Kaplan-Meier survival analysis, there is no statistical significance for recurrence and survival between treatment groups. However, tumor size larger than 2 cm and alpha-fetoprotein over 200 ng/mL correlated with higher recurrence rate, and Child class B liver cirrhosis correlated with shorter survival.\nPercutaneous ethanol injection therapy appears to be as safe and effective as resection, and both treatments can be considered first-line options for small hepatocellular carcinoma.', 'title': 'Percutaneous ethanol injection versus surgical resection for the treatment of small hepatocellular carcinoma: a prospective study.', 'date': '2005-06-24'}}",1.0,Oncology & Hematology
222,"Is the overall survival rate higher, lower, or the same when comparing percutaneous ethanol injection (PEI) to sham intervention?",insufficient data,,no,"['9425919', '16009687', '15973099']",25620061,2015,"{'9425919': {'article_id': '9425919', 'content': 'To assess whether ultrasound-guided percutaneous acetic acid injection is superior to percutaneous ethanol injection in the treatment of small hepatocellular carcinoma (HCC), 60 patients with one to four HCCs smaller than 3 cm were entered onto a randomized controlled trial. Thirty-one and 29 patients, respectively, were treated by percutaneous acetic acid injection using 50% acetic acid or by percutaneous ethanol injection using absolute ethanol. There were no significant differences in age, sex ratio, Child-Pugh class, size of tumors, or number of tumors between the two groups. When there was no evidence of viable HCC from biopsy, plain and helical dynamic computed tomography, or angiography, the treatment was considered successful and was discontinued. All original tumors were treated successfully by either therapy. However, 8% of 38 tumors treated with percutaneous acetic acid injection and 37% of 35 tumors treated with percutaneous ethanol injection developed a local recurrence (P < .001) during the follow-up periods of 29 +/- 8 months and 23 +/- 10 months, respectively. The 1- and 2-year survival rates were 100% and 92% in percutaneous acetic acid injection and 83% and 63% in percutaneous ethanol injection (P = .0017). A multivariate analysis of prognostic factors revealed that treatment was an independent predictor of survival. The risk ratio of percutaneous acetic acid injection versus percutaneous ethanol injection was 0.120 (range, 0.027-0.528; P = .0050). In conclusion, percutaneous acetic acid injection is superior to percutaneous ethanol injection in the treatment of small HCC.', 'title': 'Prospective randomized controlled trial comparing percutaneous acetic acid injection and percutaneous ethanol injection for small hepatocellular carcinoma.', 'date': '1998-01-13'}, '16009687': {'article_id': '16009687', 'content': 'The aim of this study was to compare the outcomes of radiofrequency thermal ablation (RFTA), percutaneous ethanol injection (PEI), and percutaneous acetic acid injection (PAI) in the treatment of hepatocellular carcinoma (HCC).\nA total of 187 patients with HCCs of 3 cm or less were randomly assigned to RFTA (n = 62), PEI (n = 62), or PAI (n = 63). Tumour recurrence and survival rates were assessed.\nOne, two, and three year local recurrence rates were 10%, 14%, and 14% in the RFTA group, 16%, 34%, and 34% in the PEI group, and 14%, 31%, and 31% in the PAI group (RFTA v PEI, p = 0.012; RFTA v PAI, p = 0.017). One, two, and three year survival rates were 93%, 81%, and 74% in the RFTA group, 88%, 66%, and 51% in the PEI group, and 90%, 67%, and 53% in the PAI group (RFTA v PEI, p = 0.031; RFTA v PAI, p = 0.038). One, two, and three year cancer free survival rates were 74%, 60%, and 43% in the RFTA group, 70%, 41%, and 21% in the PEI group, and 71%, 43%, and 23% in the PAI group (RFTA v PEI, p = 0.038; RFTA v PAI, p = 0.041). Tumour size, tumour differentiation, and treatment methods (RFTA v PEI and PAI) were significant factors for local recurrence, overall survival, and cancer free survival. Major complications occurred in 4.8% of patients (two with haemothorax, one gastric perforation) in the RFTA group and in none in two other groups (RFTA v PEI and PAI, p = 0.035).\nRFTA was superior to PEI and PAI with respect to local recurrence, overall survival, and cancer free survival rates, but RFTA also caused more major complications.', 'title': 'Randomised controlled trial comparing percutaneous radiofrequency thermal ablation, percutaneous ethanol injection, and percutaneous acetic acid injection to treat hepatocellular carcinoma of 3 cm or less.', 'date': '2005-07-13'}, '15973099': {'article_id': '15973099', 'content': 'To compare disease recurrence and survival among patients with small hepatocellular carcinoma after surgical resection or percutaneous ethanol injection therapy, 2 treatments that have not been evaluated with a prospective study.\nA total of 76 patients were randomly assigned to 2 groups based on treatment; all had one or 2 tumors with diameter </=3 cm, with hepatitis without cirrhosis or Child class A or B cirrhosis without evident ascites or bleeding tendency.\nFollow-up ranged from 12 to 59 months. Among percutaneous injection patients, 18 had recurrence 1 to 37 months after treatment (true recurrence, 11; original safety margin inadequate, 3; limitation of imaging technology to detect tiny tumors, 4). Three injection therapy patients died of cancer 25, 37, and 57 months after treatment. For the surgical resection group, 15 had recurrence 2 to 54 months after treatment (true recurrence, 12; limitation of imaging, 2; neck metastasis, 1). Five resection patients died of cancer at 11, 20, 23, 26, and 52 months, respectively. By Cox regression model and Kaplan-Meier survival analysis, there is no statistical significance for recurrence and survival between treatment groups. However, tumor size larger than 2 cm and alpha-fetoprotein over 200 ng/mL correlated with higher recurrence rate, and Child class B liver cirrhosis correlated with shorter survival.\nPercutaneous ethanol injection therapy appears to be as safe and effective as resection, and both treatments can be considered first-line options for small hepatocellular carcinoma.', 'title': 'Percutaneous ethanol injection versus surgical resection for the treatment of small hepatocellular carcinoma: a prospective study.', 'date': '2005-06-24'}}",1.0,Oncology & Hematology
223,"Is the overall survival rate higher, lower, or the same when comparing percutaneous acetic acid injection (PAI) to no intervention?",insufficient data,,no,"['9425919', '16009687', '15973099']",25620061,2015,"{'9425919': {'article_id': '9425919', 'content': 'To assess whether ultrasound-guided percutaneous acetic acid injection is superior to percutaneous ethanol injection in the treatment of small hepatocellular carcinoma (HCC), 60 patients with one to four HCCs smaller than 3 cm were entered onto a randomized controlled trial. Thirty-one and 29 patients, respectively, were treated by percutaneous acetic acid injection using 50% acetic acid or by percutaneous ethanol injection using absolute ethanol. There were no significant differences in age, sex ratio, Child-Pugh class, size of tumors, or number of tumors between the two groups. When there was no evidence of viable HCC from biopsy, plain and helical dynamic computed tomography, or angiography, the treatment was considered successful and was discontinued. All original tumors were treated successfully by either therapy. However, 8% of 38 tumors treated with percutaneous acetic acid injection and 37% of 35 tumors treated with percutaneous ethanol injection developed a local recurrence (P < .001) during the follow-up periods of 29 +/- 8 months and 23 +/- 10 months, respectively. The 1- and 2-year survival rates were 100% and 92% in percutaneous acetic acid injection and 83% and 63% in percutaneous ethanol injection (P = .0017). A multivariate analysis of prognostic factors revealed that treatment was an independent predictor of survival. The risk ratio of percutaneous acetic acid injection versus percutaneous ethanol injection was 0.120 (range, 0.027-0.528; P = .0050). In conclusion, percutaneous acetic acid injection is superior to percutaneous ethanol injection in the treatment of small HCC.', 'title': 'Prospective randomized controlled trial comparing percutaneous acetic acid injection and percutaneous ethanol injection for small hepatocellular carcinoma.', 'date': '1998-01-13'}, '16009687': {'article_id': '16009687', 'content': 'The aim of this study was to compare the outcomes of radiofrequency thermal ablation (RFTA), percutaneous ethanol injection (PEI), and percutaneous acetic acid injection (PAI) in the treatment of hepatocellular carcinoma (HCC).\nA total of 187 patients with HCCs of 3 cm or less were randomly assigned to RFTA (n = 62), PEI (n = 62), or PAI (n = 63). Tumour recurrence and survival rates were assessed.\nOne, two, and three year local recurrence rates were 10%, 14%, and 14% in the RFTA group, 16%, 34%, and 34% in the PEI group, and 14%, 31%, and 31% in the PAI group (RFTA v PEI, p = 0.012; RFTA v PAI, p = 0.017). One, two, and three year survival rates were 93%, 81%, and 74% in the RFTA group, 88%, 66%, and 51% in the PEI group, and 90%, 67%, and 53% in the PAI group (RFTA v PEI, p = 0.031; RFTA v PAI, p = 0.038). One, two, and three year cancer free survival rates were 74%, 60%, and 43% in the RFTA group, 70%, 41%, and 21% in the PEI group, and 71%, 43%, and 23% in the PAI group (RFTA v PEI, p = 0.038; RFTA v PAI, p = 0.041). Tumour size, tumour differentiation, and treatment methods (RFTA v PEI and PAI) were significant factors for local recurrence, overall survival, and cancer free survival. Major complications occurred in 4.8% of patients (two with haemothorax, one gastric perforation) in the RFTA group and in none in two other groups (RFTA v PEI and PAI, p = 0.035).\nRFTA was superior to PEI and PAI with respect to local recurrence, overall survival, and cancer free survival rates, but RFTA also caused more major complications.', 'title': 'Randomised controlled trial comparing percutaneous radiofrequency thermal ablation, percutaneous ethanol injection, and percutaneous acetic acid injection to treat hepatocellular carcinoma of 3 cm or less.', 'date': '2005-07-13'}, '15973099': {'article_id': '15973099', 'content': 'To compare disease recurrence and survival among patients with small hepatocellular carcinoma after surgical resection or percutaneous ethanol injection therapy, 2 treatments that have not been evaluated with a prospective study.\nA total of 76 patients were randomly assigned to 2 groups based on treatment; all had one or 2 tumors with diameter </=3 cm, with hepatitis without cirrhosis or Child class A or B cirrhosis without evident ascites or bleeding tendency.\nFollow-up ranged from 12 to 59 months. Among percutaneous injection patients, 18 had recurrence 1 to 37 months after treatment (true recurrence, 11; original safety margin inadequate, 3; limitation of imaging technology to detect tiny tumors, 4). Three injection therapy patients died of cancer 25, 37, and 57 months after treatment. For the surgical resection group, 15 had recurrence 2 to 54 months after treatment (true recurrence, 12; limitation of imaging, 2; neck metastasis, 1). Five resection patients died of cancer at 11, 20, 23, 26, and 52 months, respectively. By Cox regression model and Kaplan-Meier survival analysis, there is no statistical significance for recurrence and survival between treatment groups. However, tumor size larger than 2 cm and alpha-fetoprotein over 200 ng/mL correlated with higher recurrence rate, and Child class B liver cirrhosis correlated with shorter survival.\nPercutaneous ethanol injection therapy appears to be as safe and effective as resection, and both treatments can be considered first-line options for small hepatocellular carcinoma.', 'title': 'Percutaneous ethanol injection versus surgical resection for the treatment of small hepatocellular carcinoma: a prospective study.', 'date': '2005-06-24'}}",1.0,Oncology & Hematology
224,"Is the overall survival rate higher, lower, or the same when comparing percutaneous ethanol injection (PEI) to no intervention?",insufficient data,,no,"['9425919', '16009687', '15973099']",25620061,2015,"{'9425919': {'article_id': '9425919', 'content': 'To assess whether ultrasound-guided percutaneous acetic acid injection is superior to percutaneous ethanol injection in the treatment of small hepatocellular carcinoma (HCC), 60 patients with one to four HCCs smaller than 3 cm were entered onto a randomized controlled trial. Thirty-one and 29 patients, respectively, were treated by percutaneous acetic acid injection using 50% acetic acid or by percutaneous ethanol injection using absolute ethanol. There were no significant differences in age, sex ratio, Child-Pugh class, size of tumors, or number of tumors between the two groups. When there was no evidence of viable HCC from biopsy, plain and helical dynamic computed tomography, or angiography, the treatment was considered successful and was discontinued. All original tumors were treated successfully by either therapy. However, 8% of 38 tumors treated with percutaneous acetic acid injection and 37% of 35 tumors treated with percutaneous ethanol injection developed a local recurrence (P < .001) during the follow-up periods of 29 +/- 8 months and 23 +/- 10 months, respectively. The 1- and 2-year survival rates were 100% and 92% in percutaneous acetic acid injection and 83% and 63% in percutaneous ethanol injection (P = .0017). A multivariate analysis of prognostic factors revealed that treatment was an independent predictor of survival. The risk ratio of percutaneous acetic acid injection versus percutaneous ethanol injection was 0.120 (range, 0.027-0.528; P = .0050). In conclusion, percutaneous acetic acid injection is superior to percutaneous ethanol injection in the treatment of small HCC.', 'title': 'Prospective randomized controlled trial comparing percutaneous acetic acid injection and percutaneous ethanol injection for small hepatocellular carcinoma.', 'date': '1998-01-13'}, '16009687': {'article_id': '16009687', 'content': 'The aim of this study was to compare the outcomes of radiofrequency thermal ablation (RFTA), percutaneous ethanol injection (PEI), and percutaneous acetic acid injection (PAI) in the treatment of hepatocellular carcinoma (HCC).\nA total of 187 patients with HCCs of 3 cm or less were randomly assigned to RFTA (n = 62), PEI (n = 62), or PAI (n = 63). Tumour recurrence and survival rates were assessed.\nOne, two, and three year local recurrence rates were 10%, 14%, and 14% in the RFTA group, 16%, 34%, and 34% in the PEI group, and 14%, 31%, and 31% in the PAI group (RFTA v PEI, p = 0.012; RFTA v PAI, p = 0.017). One, two, and three year survival rates were 93%, 81%, and 74% in the RFTA group, 88%, 66%, and 51% in the PEI group, and 90%, 67%, and 53% in the PAI group (RFTA v PEI, p = 0.031; RFTA v PAI, p = 0.038). One, two, and three year cancer free survival rates were 74%, 60%, and 43% in the RFTA group, 70%, 41%, and 21% in the PEI group, and 71%, 43%, and 23% in the PAI group (RFTA v PEI, p = 0.038; RFTA v PAI, p = 0.041). Tumour size, tumour differentiation, and treatment methods (RFTA v PEI and PAI) were significant factors for local recurrence, overall survival, and cancer free survival. Major complications occurred in 4.8% of patients (two with haemothorax, one gastric perforation) in the RFTA group and in none in two other groups (RFTA v PEI and PAI, p = 0.035).\nRFTA was superior to PEI and PAI with respect to local recurrence, overall survival, and cancer free survival rates, but RFTA also caused more major complications.', 'title': 'Randomised controlled trial comparing percutaneous radiofrequency thermal ablation, percutaneous ethanol injection, and percutaneous acetic acid injection to treat hepatocellular carcinoma of 3 cm or less.', 'date': '2005-07-13'}, '15973099': {'article_id': '15973099', 'content': 'To compare disease recurrence and survival among patients with small hepatocellular carcinoma after surgical resection or percutaneous ethanol injection therapy, 2 treatments that have not been evaluated with a prospective study.\nA total of 76 patients were randomly assigned to 2 groups based on treatment; all had one or 2 tumors with diameter </=3 cm, with hepatitis without cirrhosis or Child class A or B cirrhosis without evident ascites or bleeding tendency.\nFollow-up ranged from 12 to 59 months. Among percutaneous injection patients, 18 had recurrence 1 to 37 months after treatment (true recurrence, 11; original safety margin inadequate, 3; limitation of imaging technology to detect tiny tumors, 4). Three injection therapy patients died of cancer 25, 37, and 57 months after treatment. For the surgical resection group, 15 had recurrence 2 to 54 months after treatment (true recurrence, 12; limitation of imaging, 2; neck metastasis, 1). Five resection patients died of cancer at 11, 20, 23, 26, and 52 months, respectively. By Cox regression model and Kaplan-Meier survival analysis, there is no statistical significance for recurrence and survival between treatment groups. However, tumor size larger than 2 cm and alpha-fetoprotein over 200 ng/mL correlated with higher recurrence rate, and Child class B liver cirrhosis correlated with shorter survival.\nPercutaneous ethanol injection therapy appears to be as safe and effective as resection, and both treatments can be considered first-line options for small hepatocellular carcinoma.', 'title': 'Percutaneous ethanol injection versus surgical resection for the treatment of small hepatocellular carcinoma: a prospective study.', 'date': '2005-06-24'}}",1.0,Oncology & Hematology
59,"Is the risk of seizure recurrence higher, lower, or the same when comparing six months of treatment to 12-24 months treatment?",no difference,low,no,"['16441247', '12134175', '12134176']",31608991,2019,"{'16441247': {'article_id': '16441247', 'content': 'The duration of antiepileptic drug (AED) therapy in cases of solitary cerebral cysticercus granuloma (SCCG) presents a major dilemma and the efficacy of short-term (6 months) vs long-term (2 years) AED therapy has been studied.\nProspective randomized study of short-term vs long-term AED treatment with SCCG has been undertaken. A total of 206 subjects with new onset seizures with SCCG were randomized into two groups: group A (98 patients) were treated for 6 months and group B (108 patients) were treated for 2 years with AED therapy. The patients were evaluated periodically during and at least 18 months after the tapering of drugs.\nPartial seizures with or without secondary generalization has been found to be the commonest manifestation occurring in 80.6% of patients with SCCG. In group A 66.3% and in group B 57.4% patients showed complete resolution of computerized tomographic lesion and rest had punctated residual calcification. Statistically, no significant difference in the recurrence of seizures was found in two groups with disappearance of lesion but the difference between calcified residua and complete resolution subset was significant. In patients having residual calcification, 42.2% in group A and 21.7% in group B had recurrence of seizures and the difference was statistically significant (Z = 1.97, P < 0.05).\nThe study revealed that SCCG with epilepsy is a benign self-limiting disease. A longer duration of therapy is not warranted in patients having total resolution of lesion. Calcified lesion was found to be the most common cause of recurrence of seizures. Higher recurrence rate was observed in short-term therapy in patients having calcified lesions and may require long-term AED treatment.', 'title': 'Outcome of short-term antiepileptic treatment in patients with solitary cerebral cysticercus granuloma.', 'date': '2006-01-31'}, '12134175': {'article_id': '12134175', 'content': 'The duration of anti epileptic drug therapy for single small enhancing CT lesions (SSECTL) presents a major dilemma. We studied the efficacy of short duration (6 months) antiepileptic drug therapy as compared to long duration (2 years) drug therapy. Seventy three patients presenting with seizures and showing SSECTL on cranial CT scans (plain and contrast) were randomized into group A (6 months therapy) and group B (2 years therapy). There were 47 patients in group A and 26 patients in group B. Patients were followed up for one year after withdrawal of anti epileptic drugs. CT Head (plain and contrast) was repeated after 3 months, or earlier in cases of recurrence to rule out reinfection. 53.2% in group A and 53.8% in group B showed complete resolution and were seizure free on one year follow up. Punctate residual calcification was seen in 46.8% in group A and 46.2% in group B. Eight patients (17%) in group A and three (11.5%) in group B had a recurrence. The difference in recurrence of seizure between the two groups was not statistically significant (p<0.77) in the calcified lesion subset. Since none of the patients in total resolution subset showed recurrence, the difference between calcified and total resolution subset was highly significant. The study shows that a short duration (6 months) AED therapy in patients with total resolution of lesion on follow up scan, may be adequate in comparison to those who have calcific speck as a residue. However, a longer duration of therapy in case of calcific group probably does not alter their chances of recurrence.', 'title': 'Acute symptomatic seizures due to single CT lesions: how long to treat with antiepileptic drugs?', 'date': '2002-07-23'}, '12134176': {'article_id': '12134176', 'content': 'The study was conducted in 81 patients of epilepsy with small single enhancing CT (SSECT) lesion in brain to determine the clinical profile and duration of antiepileptic drugs (AEDs) treatment. The patients were randomly divided into group A (41 cases) and group B (40 cases). Group A patients were treated for 6 months and group B for 1 year with AEDS without cysticidal drugs. The most common mode of presentation was simple partial motor seizures with secondary generalization in both the groups. Repeat imaging of brain (CT/MRI) at 6 months showed disappearance of lesion in 82.94% in group A and 87% in group B, while persistence of lesion was present only in 4.87% in group A and 5% in group B. 87.81% patients in group A and 87.17% in group B were seizure free. The recurrence of seizure occurred in 12.19% cases in group A, and 12.82% in group B. 80% of these patients had calcified lesion in both the groups. This study reveals that SSECT lesion with epilpesy is a benign self-limiting disease. It also reveals that 6 months AED treatment is as effective as one year treatment. Patients having calcified lesion or persistence of lesion might require long term AED treatment.', 'title': 'Randomized prospective study of outcome of short term antiepileptic treatment in small single enhancing CT lesion in brain.', 'date': '2002-07-23'}}",0.666666667,Psychiatry & Neurology
60,"Is the risk of seizure recurrence higher, lower, or the same when comparing 6-12 months of treatment to 24 months of treatment?",no difference,low,no,"['16441247', '14604159', '12134175']",31608991,2019,"{'16441247': {'article_id': '16441247', 'content': 'The duration of antiepileptic drug (AED) therapy in cases of solitary cerebral cysticercus granuloma (SCCG) presents a major dilemma and the efficacy of short-term (6 months) vs long-term (2 years) AED therapy has been studied.\nProspective randomized study of short-term vs long-term AED treatment with SCCG has been undertaken. A total of 206 subjects with new onset seizures with SCCG were randomized into two groups: group A (98 patients) were treated for 6 months and group B (108 patients) were treated for 2 years with AED therapy. The patients were evaluated periodically during and at least 18 months after the tapering of drugs.\nPartial seizures with or without secondary generalization has been found to be the commonest manifestation occurring in 80.6% of patients with SCCG. In group A 66.3% and in group B 57.4% patients showed complete resolution of computerized tomographic lesion and rest had punctated residual calcification. Statistically, no significant difference in the recurrence of seizures was found in two groups with disappearance of lesion but the difference between calcified residua and complete resolution subset was significant. In patients having residual calcification, 42.2% in group A and 21.7% in group B had recurrence of seizures and the difference was statistically significant (Z = 1.97, P < 0.05).\nThe study revealed that SCCG with epilepsy is a benign self-limiting disease. A longer duration of therapy is not warranted in patients having total resolution of lesion. Calcified lesion was found to be the most common cause of recurrence of seizures. Higher recurrence rate was observed in short-term therapy in patients having calcified lesions and may require long-term AED treatment.', 'title': 'Outcome of short-term antiepileptic treatment in patients with solitary cerebral cysticercus granuloma.', 'date': '2006-01-31'}, '14604159': {'article_id': '14604159', 'content': 'The duration of anti-epileptic drug (AED) therapy in children with seizures due to single small enhancing CT lesions (SSECTL) is controversial. We sought to determine whether there is any difference in the rate of seizure recurrence after 1 vs. 2 years of AED therapy and to identify the factors predictive of seizure recurrence. A total of 115 consecutive children with seizures and SSECTL were randomly assigned to two groups. Group A received AED(s) for 1 year and Group B for 2 years seizure-free interval. CT scan and EEG were done prior to AED withdrawal and children were followed-up for seizure recurrence for at least 1 year. Association between seizure recurrence and clinical and CT characteristics was analysed. Groups A and B consisted of 55 and 51 children, respectively (nine were lost to follow-up). There were 61 boys and 45 girls; mean age 9.33 years. Most (93 per cent) had focal seizures: 36 per cent complex partial, 22 per cent simple partial, 35 per cent partial with secondary generalization; 21 per cent had status epilepticus. The two groups were comparable in clinical, EEG and CT characteristics. CT scan and EEG prior to AED withdrawal were abnormal in 44 per cent and 33 per cent respectively. Six children, three from each group had seizure recurrence. Significant association was found between seizure recurrence and abnormal CT (persistence/calcification of lesion) and abnormal EEG prior to AED withdrawal (p < 0.01). The relative risk of seizure recurrence in a child with abnormal CT and EEG prior to AED withdrawal was 26.2 (95 per cent confidence interval 3.3-210.2, p = 0.0003). No association was found between seizure recurrence and any of the other variables. There was no difference in seizure recurrence after 1 vs. 2 years of AED therapy. Combination of persistent/calcified CT lesion and abnormal EEG prior to AED withdrawal was the best predictor of seizure recurrence.', 'title': 'One vs. two years of anti-epileptic therapy in children with single small enhancing CT lesions.', 'date': '2003-11-08'}, '12134175': {'article_id': '12134175', 'content': 'The duration of anti epileptic drug therapy for single small enhancing CT lesions (SSECTL) presents a major dilemma. We studied the efficacy of short duration (6 months) antiepileptic drug therapy as compared to long duration (2 years) drug therapy. Seventy three patients presenting with seizures and showing SSECTL on cranial CT scans (plain and contrast) were randomized into group A (6 months therapy) and group B (2 years therapy). There were 47 patients in group A and 26 patients in group B. Patients were followed up for one year after withdrawal of anti epileptic drugs. CT Head (plain and contrast) was repeated after 3 months, or earlier in cases of recurrence to rule out reinfection. 53.2% in group A and 53.8% in group B showed complete resolution and were seizure free on one year follow up. Punctate residual calcification was seen in 46.8% in group A and 46.2% in group B. Eight patients (17%) in group A and three (11.5%) in group B had a recurrence. The difference in recurrence of seizure between the two groups was not statistically significant (p<0.77) in the calcified lesion subset. Since none of the patients in total resolution subset showed recurrence, the difference between calcified and total resolution subset was highly significant. The study shows that a short duration (6 months) AED therapy in patients with total resolution of lesion on follow up scan, may be adequate in comparison to those who have calcific speck as a residue. However, a longer duration of therapy in case of calcific group probably does not alter their chances of recurrence.', 'title': 'Acute symptomatic seizures due to single CT lesions: how long to treat with antiepileptic drugs?', 'date': '2002-07-23'}}",0.666666667,Psychiatry & Neurology
61,"Is the duration of mechanical ventilation higher, lower, or the same when comparing bispectral index (BIS) monitoring to clinical assessment for sedation?",no difference,low,no,"['17444309', '21473824']",29464690,2018,"{'17444309': {'article_id': '17444309', 'content': 'The aim of this prospective randomised controlled trial was to assess the effectiveness of the Bispectral Index (BIS) monitor in supporting clinical sedation management decisions in mechanically ventilated intensive care unit patients. Fifty adult mechanically ventilated surgical and general intensive care unit patients receiving sedative infusions of morphine and midazolam were randomly allocated to receive BIS monitoring (n=25) or standard sedation management (n=25). In the BIS group, sedation was titrated to maintain a BIS value of greater than 70. In the standard management group, sedative needs were titrated based on subjective assessment and clinical signs. There was no statistically significant difference in the amount of sedation administered (morphine P = 0.67 and midazolam P = 0.85). However, there was a statistically significant difference in sedation administration over time. Patients in the BIS group received increasing amounts of sedation over time whilst those in the control group received decreasing amounts of sedation over time. The same inverse relationship existed for both sedative agents (morphine P = 0.005, midazolam P = 0.03). Duration of mechanical ventilation was comparable in the two groups. We conclude that the use of BIS monitoring did not reduce the amount of sedation used, the length of mechanical ventilation time or the length of ICU stay.', 'title': 'The impact of bispectral index monitoring on sedation administration in mechanically ventilated patients.', 'date': '2007-04-21'}, '21473824': {'article_id': '21473824', 'content': 'To compare the value of bispectral index (BIS) monitoring and sedation agitation scale (SAS) in guiding intensive care unit (ICU) sedation therapy for the patients undergoing short term mechanical ventilation.\nOne hundred and five patients aged 18-60 years after operation receiving mechanical ventilation for longer than 12 hours in ICU were enrolled in this study. The patients were randomly divided into two groups: BIS guided group (n=42) and SAS guided group (n=63). All of them received protocolized continuous sedation and analgesia by using fentanyl for analgesia and propofol plus midazolam to sedate intravenously. The effect of sedation was assessed every hour till BIS reaching 50-70 or SAS reaching grade 3-4. Sedatives and analgesics were suspended at 6:00 am on next day after ICU admission , and BIS and the SAS were recorded every hour, sedation time, time to wake up, duration of mechanical ventilation, daily dosage of midazolam and propofol, and the incidence of adverse events including restlessness after suction, endotracheal tube resistance, pain tolerance during sedation, and delirium after extubation were all recorded accordingly.\nDosages of midazolam and propofol were found higher in BIS guided group than the SAS guided group [midazolam (mg×kg(-1) ×h(-1) ): 0.10±0.02 vs. 0.09±0.02, propofol (mg×kg(-1) ×h(-1) ): 0.95±0.23 vs. 0.86±0.20, both P<0.05]. The total time (D t) of patients under sedative control was significantly longer in BIS guided group compared with SAS guided group, and also in first three hours [D 1, D 2, D 3, D t: 75.2% (507) vs. 52.8% (421), D 1: 78.6% (33) vs. 22.2% (14), D 2: 88.1% (37) vs. 20.6% (13), D 3: 81.0% (34) vs. 31.7% (20), all P<0.01]. The time to wake up (minutes) was significantly shorter in BIS guided group compared with SAS guided group [0 (0, 20) vs. 15 (0, 47), P<0.05]. No significant difference in acute physiology and chronic health evaluation II (APACHE II) score (3.57±2.60 vs. 4.19±2.30), duration of mechanical ventilation [hours: 16.5 (14.5, 19.0) vs. 17.0 (15.0, 19.0)], sedation time [hours: 14.0 (12.9, 17.1) vs. 16.0 (13.0, 18.0)] and incidence of adverse events including restlessness after suction (81.0% vs. 79.4%), endotracheal tube resistance (71.4% vs. 74.6%), pain tolerance during sedation (92.8% vs. 93.6%) and delirium after extubation (4.8% vs. 1.6%) was found between BIS guided group and SAS guided group (all P>0.05).\nBIS monitoring is better in sedative control than SAS assessment for ICU patients undergoing short term mechanical ventilation.', 'title': '[A comparison of bispectral index and sedation agitation scale in guiding sedation therapy: a randomized controlled study in patients undergoing short term mechanical ventilation].', 'date': '2011-04-09'}}",1.0,Emergency Medicine & Critical Care
62,"Is the incidence of restlessness after suction higher, lower, or the same when comparing bispectral index (BIS) monitoring to clinical assessment for sedation?",uncertain effect,very low,no,['21473824'],29464690,2018,"{'21473824': {'article_id': '21473824', 'content': 'To compare the value of bispectral index (BIS) monitoring and sedation agitation scale (SAS) in guiding intensive care unit (ICU) sedation therapy for the patients undergoing short term mechanical ventilation.\nOne hundred and five patients aged 18-60 years after operation receiving mechanical ventilation for longer than 12 hours in ICU were enrolled in this study. The patients were randomly divided into two groups: BIS guided group (n=42) and SAS guided group (n=63). All of them received protocolized continuous sedation and analgesia by using fentanyl for analgesia and propofol plus midazolam to sedate intravenously. The effect of sedation was assessed every hour till BIS reaching 50-70 or SAS reaching grade 3-4. Sedatives and analgesics were suspended at 6:00 am on next day after ICU admission , and BIS and the SAS were recorded every hour, sedation time, time to wake up, duration of mechanical ventilation, daily dosage of midazolam and propofol, and the incidence of adverse events including restlessness after suction, endotracheal tube resistance, pain tolerance during sedation, and delirium after extubation were all recorded accordingly.\nDosages of midazolam and propofol were found higher in BIS guided group than the SAS guided group [midazolam (mg×kg(-1) ×h(-1) ): 0.10±0.02 vs. 0.09±0.02, propofol (mg×kg(-1) ×h(-1) ): 0.95±0.23 vs. 0.86±0.20, both P<0.05]. The total time (D t) of patients under sedative control was significantly longer in BIS guided group compared with SAS guided group, and also in first three hours [D 1, D 2, D 3, D t: 75.2% (507) vs. 52.8% (421), D 1: 78.6% (33) vs. 22.2% (14), D 2: 88.1% (37) vs. 20.6% (13), D 3: 81.0% (34) vs. 31.7% (20), all P<0.01]. The time to wake up (minutes) was significantly shorter in BIS guided group compared with SAS guided group [0 (0, 20) vs. 15 (0, 47), P<0.05]. No significant difference in acute physiology and chronic health evaluation II (APACHE II) score (3.57±2.60 vs. 4.19±2.30), duration of mechanical ventilation [hours: 16.5 (14.5, 19.0) vs. 17.0 (15.0, 19.0)], sedation time [hours: 14.0 (12.9, 17.1) vs. 16.0 (13.0, 18.0)] and incidence of adverse events including restlessness after suction (81.0% vs. 79.4%), endotracheal tube resistance (71.4% vs. 74.6%), pain tolerance during sedation (92.8% vs. 93.6%) and delirium after extubation (4.8% vs. 1.6%) was found between BIS guided group and SAS guided group (all P>0.05).\nBIS monitoring is better in sedative control than SAS assessment for ICU patients undergoing short term mechanical ventilation.', 'title': '[A comparison of bispectral index and sedation agitation scale in guiding sedation therapy: a randomized controlled study in patients undergoing short term mechanical ventilation].', 'date': '2011-04-09'}}",0.0,Surgery
63,"Is the incidence of delirium after extubation higher, lower, or the same when comparing bispectral index (BIS) monitoring to clinical assessment for sedation?",uncertain effect,very low,no,['21473824'],29464690,2018,"{'21473824': {'article_id': '21473824', 'content': 'To compare the value of bispectral index (BIS) monitoring and sedation agitation scale (SAS) in guiding intensive care unit (ICU) sedation therapy for the patients undergoing short term mechanical ventilation.\nOne hundred and five patients aged 18-60 years after operation receiving mechanical ventilation for longer than 12 hours in ICU were enrolled in this study. The patients were randomly divided into two groups: BIS guided group (n=42) and SAS guided group (n=63). All of them received protocolized continuous sedation and analgesia by using fentanyl for analgesia and propofol plus midazolam to sedate intravenously. The effect of sedation was assessed every hour till BIS reaching 50-70 or SAS reaching grade 3-4. Sedatives and analgesics were suspended at 6:00 am on next day after ICU admission , and BIS and the SAS were recorded every hour, sedation time, time to wake up, duration of mechanical ventilation, daily dosage of midazolam and propofol, and the incidence of adverse events including restlessness after suction, endotracheal tube resistance, pain tolerance during sedation, and delirium after extubation were all recorded accordingly.\nDosages of midazolam and propofol were found higher in BIS guided group than the SAS guided group [midazolam (mg×kg(-1) ×h(-1) ): 0.10±0.02 vs. 0.09±0.02, propofol (mg×kg(-1) ×h(-1) ): 0.95±0.23 vs. 0.86±0.20, both P<0.05]. The total time (D t) of patients under sedative control was significantly longer in BIS guided group compared with SAS guided group, and also in first three hours [D 1, D 2, D 3, D t: 75.2% (507) vs. 52.8% (421), D 1: 78.6% (33) vs. 22.2% (14), D 2: 88.1% (37) vs. 20.6% (13), D 3: 81.0% (34) vs. 31.7% (20), all P<0.01]. The time to wake up (minutes) was significantly shorter in BIS guided group compared with SAS guided group [0 (0, 20) vs. 15 (0, 47), P<0.05]. No significant difference in acute physiology and chronic health evaluation II (APACHE II) score (3.57±2.60 vs. 4.19±2.30), duration of mechanical ventilation [hours: 16.5 (14.5, 19.0) vs. 17.0 (15.0, 19.0)], sedation time [hours: 14.0 (12.9, 17.1) vs. 16.0 (13.0, 18.0)] and incidence of adverse events including restlessness after suction (81.0% vs. 79.4%), endotracheal tube resistance (71.4% vs. 74.6%), pain tolerance during sedation (92.8% vs. 93.6%) and delirium after extubation (4.8% vs. 1.6%) was found between BIS guided group and SAS guided group (all P>0.05).\nBIS monitoring is better in sedative control than SAS assessment for ICU patients undergoing short term mechanical ventilation.', 'title': '[A comparison of bispectral index and sedation agitation scale in guiding sedation therapy: a randomized controlled study in patients undergoing short term mechanical ventilation].', 'date': '2011-04-09'}}",0.0,Emergency Medicine & Critical Care
64,"Is the incidence of endotracheal tube resistance higher, lower, or the same when comparing bispectral index (BIS) monitoring to clinical assessment for sedation?",uncertain effect,very low,no,['21473824'],29464690,2018,"{'21473824': {'article_id': '21473824', 'content': 'To compare the value of bispectral index (BIS) monitoring and sedation agitation scale (SAS) in guiding intensive care unit (ICU) sedation therapy for the patients undergoing short term mechanical ventilation.\nOne hundred and five patients aged 18-60 years after operation receiving mechanical ventilation for longer than 12 hours in ICU were enrolled in this study. The patients were randomly divided into two groups: BIS guided group (n=42) and SAS guided group (n=63). All of them received protocolized continuous sedation and analgesia by using fentanyl for analgesia and propofol plus midazolam to sedate intravenously. The effect of sedation was assessed every hour till BIS reaching 50-70 or SAS reaching grade 3-4. Sedatives and analgesics were suspended at 6:00 am on next day after ICU admission , and BIS and the SAS were recorded every hour, sedation time, time to wake up, duration of mechanical ventilation, daily dosage of midazolam and propofol, and the incidence of adverse events including restlessness after suction, endotracheal tube resistance, pain tolerance during sedation, and delirium after extubation were all recorded accordingly.\nDosages of midazolam and propofol were found higher in BIS guided group than the SAS guided group [midazolam (mg×kg(-1) ×h(-1) ): 0.10±0.02 vs. 0.09±0.02, propofol (mg×kg(-1) ×h(-1) ): 0.95±0.23 vs. 0.86±0.20, both P<0.05]. The total time (D t) of patients under sedative control was significantly longer in BIS guided group compared with SAS guided group, and also in first three hours [D 1, D 2, D 3, D t: 75.2% (507) vs. 52.8% (421), D 1: 78.6% (33) vs. 22.2% (14), D 2: 88.1% (37) vs. 20.6% (13), D 3: 81.0% (34) vs. 31.7% (20), all P<0.01]. The time to wake up (minutes) was significantly shorter in BIS guided group compared with SAS guided group [0 (0, 20) vs. 15 (0, 47), P<0.05]. No significant difference in acute physiology and chronic health evaluation II (APACHE II) score (3.57±2.60 vs. 4.19±2.30), duration of mechanical ventilation [hours: 16.5 (14.5, 19.0) vs. 17.0 (15.0, 19.0)], sedation time [hours: 14.0 (12.9, 17.1) vs. 16.0 (13.0, 18.0)] and incidence of adverse events including restlessness after suction (81.0% vs. 79.4%), endotracheal tube resistance (71.4% vs. 74.6%), pain tolerance during sedation (92.8% vs. 93.6%) and delirium after extubation (4.8% vs. 1.6%) was found between BIS guided group and SAS guided group (all P>0.05).\nBIS monitoring is better in sedative control than SAS assessment for ICU patients undergoing short term mechanical ventilation.', 'title': '[A comparison of bispectral index and sedation agitation scale in guiding sedation therapy: a randomized controlled study in patients undergoing short term mechanical ventilation].', 'date': '2011-04-09'}}",0.0,Surgery
65,"Is improvement in depression symptoms higher, lower, or the same when comparing SSRIs to placebo?",no difference,very low,no,"['25308771', '17136950']",30566235,2018,"{'25308771': {'article_id': '25308771', 'content': ""There is a lack of randomized controlled trials to assess the effects of pharmacological treatments in patients with stable chronic obstructive pulmonary disease (COPD) complicated with moderate or severe depression.\nTo assess the efficacy of sertraline hydrochloride on improving the quality of life of patients with stable COPD complicated with depression.\nThis randomized controlled trial, conducted from May to November 2013 in the Huai'an Second Hospital, Huai'an, China, enrolled 120 patients with stable COPD who had moderate or severe depression. Patients were randomly assigned to control and interventional groups (n\u2009=\u200960 in each group). In addition to the treatment for COPD, interventional group also received sertraline hydrochloride tablets 50\u2009mg/day for 6\u2009weeks, while the control group received placebo. The primary end point included COPD assessment test (CAT) scores and the secondary endpoint included 6-min walk distance and 17-item Hamilton Depression Rating Scale (HAMD-17) scores. Parameters of spirometry and adverse events were also observed.\nThere was no significant difference in improvements in the parameters of spirometry tests before and after the treatment with sertraline hydrochloride tablets between the placebo and interventional groups (P\u2009>\u20090.05). Patients in the sertraline hydrochloride group showed more changes in the HAMD-17 scores and CAT scores after treatment (P\u2009<\u20090.05) and travelled longer distances in the 6-min walk test than in the placebo group (P\u2009<\u20090.05).\nAntidepressant treatment can improve the quality of life and exercise capacity of patients with depression, and it can also improve depression scores, but not lung function."", 'title': 'Sertraline hydrochloride treatment for patients with stable chronic obstructive pulmonary disease complicated with depression: a randomized controlled trial.', 'date': '2014-10-14'}, '17136950': {'article_id': '17136950', 'content': ""The aim of this study was to determine whether treating concomitant depression improves quality of life and exercise tolerance in COPD patients. Out-patients with moderate to severe, stable COPD completed Hospital Anxiety-Depression (HAD) and General Health questionnaires. A psychiatrist interviewed those with high scores. In a randomised, double-blind fashion, 28 depressed COPD patients took a selective serotonin re-uptake inhibitor, Paroxetine 20 mg daily, or matched placebo for 6 weeks. Subsequently, all patients took un-blinded Paroxetine for 3 months. From these questionnaires, 35% of 135 patients had significant depression, but this was confirmed by psychiatric interview in only 21%. Throughout the study, there were no changes in laboratory lung function nor in home peak flow. Six weeks' treatment produced no significant differences between placebo and treatment group in either depression, quality of life scores or 6-minute walking distances, although overall improvements in depression, correlated with increases in walking distance. Three months of un-blinded treatment, significantly improved depression scores (self-complete HAD, Beck's Depression and psychiatrist-completed Montgomery-Asberg scores), walking distances (369 to 427 m, p = 0.0003) and St. George's Respiratory Questionnaire Total Scores (65 to 58, p = 0.033). Although self-complete questionnaires over-diagnose depression, the condition is nevertheless common in patients with moderately severe COPD. Six weeks of antidepressants is insufficient to improve either depression, quality of life or exercise tolerance. However, our study suggests that a longer course of treatment may be effective and that improvements in depression are associated with improvements in exercise tolerance. A larger, double blind study with a longer treatment period is indicated."", 'title': 'Effect of treating depression on quality-of-life and exercise tolerance in severe COPD.', 'date': '2006-12-02'}}",0.5,Psychiatry & Neurology
66,"Is the risk of major postoperative bleeding requiring intervention higher, lower, or the same when comparing tranexamic acid (TXA) to placebo?",lower,moderate,no,"['2648144', '8229393']",29963686,2018,"{'2648144': {'article_id': '2648144', 'content': 'We carried out a placebo-controlled, double-blind, randomized study of the hemostatic effect of tranexamic acid mouthwash after oral surgery in 39 patients receiving anticoagulant agents because of the presence of cardiac valvular stenosis, a prosthetic cardiac valve, or a vascular prosthesis. Surgery was performed with no change in the level of anticoagulant therapy, and treatment with the anticoagulant agent was continued after surgery. Before it was sutured, the operative field was irrigated in 19 patients with 10 ml of a 4.8 percent aqueous solution of tranexamic acid (an inhibitor of fibrinolysis) and in 20 patients with a placebo solution. For seven days thereafter, patients were instructed to rinse their mouths with 10 ml of the assigned solution for two minutes four times a day. There were no significant differences between the two treatment groups in base-line variables, including the level of anticoagulation at the time of surgery. Eight patients in the placebo group had a total of 10 postoperative bleeding episodes, whereas only 1 patient in the tranexamic acid group had a bleeding episode (P = 0.01). There were no systemic side effects. We conclude that local antifibrinolytic therapy is effective in preventing bleeding after oral surgery in patients who are being treated with anticoagulants.', 'title': 'Hemostatic effect of tranexamic acid mouthwash in anticoagulant-treated patients undergoing oral surgery.', 'date': '1989-03-30'}, '8229393': {'article_id': '8229393', 'content': 'The hemostatic effect of tranexamic acid solution (4.8%) used as a mouthwash was compared with a placebo solution in 93 patients on continuous, unchanged, oral anticoagulant treatment undergoing oral surgery. The investigation was a randomized, double-blind, placebo-controlled, multicenter study. Before suturing, the surgically treated region was irrigated with 10 mL of tranexamic acid (46 patients) or placebo (47 patients) solution. The patients then performed mouthwash with 10 mL of the solution for 2 minutes four times daily for 7 days. The treatment groups were comparable regarding age, smoking habits, and surgery. Laboratory variables measuring vitamin K-dependent coagulation factors were within therapeutic ranges (international normalized ratio 4.00 to 2.10). One of the clinics used a different method for these measurements and therefore the levels of coagulation factor X in plasma obtained for the three clinics were compared. No significant difference in the range at which surgery was performed was found between clinics. In the placebo group, 10 patients developed bleeding requiring treatment, while none of the patients treated with tranexamic acid solution had bleeding. This difference was statistically significant (P < .01). The treatment with mouthwash was well tolerated. It was concluded that patients on oral anticoagulants can undergo oral surgery within the therapeutic range without reducing the dosage of anticoagulants, provided that local antifibrinolytic treatment with tranexamic acid solution is instituted.', 'title': 'Prevention of postsurgical bleeding in oral surgery using tranexamic acid without dose modification of oral anticoagulants.', 'date': '1993-11-01'}}",1.0,Surgery
67,"Is the risk of developing microbiologically confirmed tuberculosis higher, lower, or the same when comparing MVA85A added to BCG to BCG alone?",no difference,moderate,yes,"['25726088', '23391465']",31038197,2019,"{'25726088': {'article_id': '25726088', 'content': 'HIV-1 infection is associated with increased risk of tuberculosis and a safe and effective vaccine would assist control measures. We assessed the safety, immunogenicity, and efficacy of a candidate tuberculosis vaccine, modified vaccinia virus Ankara expressing antigen 85A (MVA85A), in adults infected with HIV-1.\nWe did a randomised, double-blind, placebo-controlled, phase 2 trial of MVA85A in adults infected with HIV-1, at two clinical sites, in Cape Town, South Africa and Dakar, Senegal. Eligible participants were aged 18-50 years, had no evidence of active tuberculosis, and had baseline CD4 counts greater than 350 cells per μL if they had never received antiretroviral therapy or greater than 300 cells per μL (and with undetectable viral load before randomisation) if they were receiving antiretroviral therapy; participants with latent tuberculosis infection were eligible if they had completed at least 5 months of isoniazid preventive therapy, unless they had completed treatment for tuberculosis disease within 3 years before randomisation. Participants were randomly assigned (1:1) in blocks of four by randomly generated sequence to receive two intradermal injections of either MVA85A or placebo. Randomisation was stratified by antiretroviral therapy status and study site. Participants, nurses, investigators, and laboratory staff were masked to group allocation. The second (booster) injection of MVA85A or placebo was given 6-12 months after the first vaccination. The primary study outcome was safety in all vaccinated participants (the safety analysis population). Safety was assessed throughout the trial as defined in the protocol. Secondary outcomes were immunogenicity and vaccine efficacy against Mycobacterium tuberculosis infection and disease, assessed in the per-protocol population. Immunogenicity was assessed in a subset of participants at day 7 and day 28 after the first and second vaccination, and M tuberculosis infection and disease were assessed at the end of the study. The trial is registered with ClinicalTrials.gov, number NCT01151189.\nBetween Aug 4, 2011, and April 24, 2013, 650 participants were enrolled and randomly assigned; 649 were included in the safety analysis (324 in the MVA85A group and 325 in the placebo group) and 645 in the per-protocol analysis (320 and 325). 513 (71%) participants had CD4 counts greater than 300 cells per μL and were receiving antiretroviral therapy; 136 (21%) had CD4 counts above 350 cells per μL and had never received antiretroviral therapy. 277 (43%) had received isoniazid prophylaxis before enrolment. Solicited adverse events were more frequent in participants who received MVA85A (288 [89%]) than in those given placebo (235 [72%]). 34 serious adverse events were reported, 17 (5%) in each group. MVA85A induced a significant increase in antigen 85A-specific T-cell response, which peaked 7 days after both vaccinations and was primarily monofunctional. The number of participants with negative QuantiFERON-TB Gold In-Tube findings at baseline who converted to positive by the end of the study was 38 (20%) of 186 in the MVA85A group and 40 (23%) of 173 in the placebo group, for a vaccine efficacy of 11·7% (95% CI -41·3 to 44·9). In the per-protocol population, six (2%) cases of tuberculosis disease occurred in the MVA85A group and nine (3%) occurred in the placebo group, for a vaccine efficacy of 32·8% (95% CI -111·5 to 80·3).\nMVA85A was well tolerated and immunogenic in adults infected with HIV-1. However, we detected no efficacy against M tuberculosis infection or disease, although the study was underpowered to detect an effect against disease. Potential reasons for the absence of detectable efficacy in this trial include insufficient induction of a vaccine-induced immune response or the wrong type of vaccine-induced immune response, or both.\nEuropean & Developing Countries Clinical Trials Partnership (IP.2007.32080.002), Aeras, Bill & Melinda Gates Foundation, Wellcome Trust, and Oxford-Emergent Tuberculosis Consortium.', 'title': 'Safety, immunogenicity, and efficacy of the candidate tuberculosis vaccine MVA85A in healthy adults infected with HIV-1: a randomised, placebo-controlled, phase 2 trial.', 'date': '2015-03-03'}, '23391465': {'article_id': '23391465', 'content': 'The aim of this clinical caries detection study was to compare the outcome of quantitative light-induced fluorescence (QLF) and meticulous visual inspection (VI) in detecting non-cavitated caries lesions on occlusal surfaces in young adolescents. It was hypothesized that the respective diagnostic performances of meticulous VI and QLF are similar.\nThe subjects were 34 fifteen-year-old students. Five-hundred-and-seventeen cleaned occlusal surfaces were air-dried and examined using VI. Fluorescence images were captured with QLF equipment and custom software was used to display, store and analyze the images. The area of the lesion (area; mm2), fluorescence loss (DeltaF;%) and DeltaQ (Area*DeltaF; mm2*%) were determined at a QLF threshold of -5%. The presence/absence of non-cavitated lesions was independently recorded with both methods.\n78.8% of all untreated surfaces were classified as sound or as having a non-cavitated lesion with both methods uniformly (VI+QLF). On 7.1% of all surfaces a lesion was detected by VI only and on 14.1% by QLF only. All parameters (Area, DeltaF, DeltaQ) differed significantly between lesions registered with both methods (VI+QLF) and lesions recorded with QLF only.\nIt was concluded that our hypothesis cannot be confirmed. The study shows that QLF detects (1) more non-cavitated occlusal lesions and (2) smaller lesions compared to VI. However, taking into consideration time-consuming image capturing and analysis, QLF is not really practical for use in the dental office.', 'title': 'In vivo detection of non-cavitated caries lesions on occlusal surfaces by visual inspection and quantitative light-induced fluorescence.', 'date': '2007-05-22'}}",0.5,"Public Health, Epidemiology & Health Systems"
68,"Is the risk of developing latent tuberculosis higher, lower, or the same when comparing MVA85A added to BCG to BCG alone? ",no difference,moderate,yes,"['25726088', '23391465', '29028973']",31038197,2019,"{'25726088': {'article_id': '25726088', 'content': 'HIV-1 infection is associated with increased risk of tuberculosis and a safe and effective vaccine would assist control measures. We assessed the safety, immunogenicity, and efficacy of a candidate tuberculosis vaccine, modified vaccinia virus Ankara expressing antigen 85A (MVA85A), in adults infected with HIV-1.\nWe did a randomised, double-blind, placebo-controlled, phase 2 trial of MVA85A in adults infected with HIV-1, at two clinical sites, in Cape Town, South Africa and Dakar, Senegal. Eligible participants were aged 18-50 years, had no evidence of active tuberculosis, and had baseline CD4 counts greater than 350 cells per μL if they had never received antiretroviral therapy or greater than 300 cells per μL (and with undetectable viral load before randomisation) if they were receiving antiretroviral therapy; participants with latent tuberculosis infection were eligible if they had completed at least 5 months of isoniazid preventive therapy, unless they had completed treatment for tuberculosis disease within 3 years before randomisation. Participants were randomly assigned (1:1) in blocks of four by randomly generated sequence to receive two intradermal injections of either MVA85A or placebo. Randomisation was stratified by antiretroviral therapy status and study site. Participants, nurses, investigators, and laboratory staff were masked to group allocation. The second (booster) injection of MVA85A or placebo was given 6-12 months after the first vaccination. The primary study outcome was safety in all vaccinated participants (the safety analysis population). Safety was assessed throughout the trial as defined in the protocol. Secondary outcomes were immunogenicity and vaccine efficacy against Mycobacterium tuberculosis infection and disease, assessed in the per-protocol population. Immunogenicity was assessed in a subset of participants at day 7 and day 28 after the first and second vaccination, and M tuberculosis infection and disease were assessed at the end of the study. The trial is registered with ClinicalTrials.gov, number NCT01151189.\nBetween Aug 4, 2011, and April 24, 2013, 650 participants were enrolled and randomly assigned; 649 were included in the safety analysis (324 in the MVA85A group and 325 in the placebo group) and 645 in the per-protocol analysis (320 and 325). 513 (71%) participants had CD4 counts greater than 300 cells per μL and were receiving antiretroviral therapy; 136 (21%) had CD4 counts above 350 cells per μL and had never received antiretroviral therapy. 277 (43%) had received isoniazid prophylaxis before enrolment. Solicited adverse events were more frequent in participants who received MVA85A (288 [89%]) than in those given placebo (235 [72%]). 34 serious adverse events were reported, 17 (5%) in each group. MVA85A induced a significant increase in antigen 85A-specific T-cell response, which peaked 7 days after both vaccinations and was primarily monofunctional. The number of participants with negative QuantiFERON-TB Gold In-Tube findings at baseline who converted to positive by the end of the study was 38 (20%) of 186 in the MVA85A group and 40 (23%) of 173 in the placebo group, for a vaccine efficacy of 11·7% (95% CI -41·3 to 44·9). In the per-protocol population, six (2%) cases of tuberculosis disease occurred in the MVA85A group and nine (3%) occurred in the placebo group, for a vaccine efficacy of 32·8% (95% CI -111·5 to 80·3).\nMVA85A was well tolerated and immunogenic in adults infected with HIV-1. However, we detected no efficacy against M tuberculosis infection or disease, although the study was underpowered to detect an effect against disease. Potential reasons for the absence of detectable efficacy in this trial include insufficient induction of a vaccine-induced immune response or the wrong type of vaccine-induced immune response, or both.\nEuropean & Developing Countries Clinical Trials Partnership (IP.2007.32080.002), Aeras, Bill & Melinda Gates Foundation, Wellcome Trust, and Oxford-Emergent Tuberculosis Consortium.', 'title': 'Safety, immunogenicity, and efficacy of the candidate tuberculosis vaccine MVA85A in healthy adults infected with HIV-1: a randomised, placebo-controlled, phase 2 trial.', 'date': '2015-03-03'}, '23391465': {'article_id': '23391465', 'content': 'The aim of this clinical caries detection study was to compare the outcome of quantitative light-induced fluorescence (QLF) and meticulous visual inspection (VI) in detecting non-cavitated caries lesions on occlusal surfaces in young adolescents. It was hypothesized that the respective diagnostic performances of meticulous VI and QLF are similar.\nThe subjects were 34 fifteen-year-old students. Five-hundred-and-seventeen cleaned occlusal surfaces were air-dried and examined using VI. Fluorescence images were captured with QLF equipment and custom software was used to display, store and analyze the images. The area of the lesion (area; mm2), fluorescence loss (DeltaF;%) and DeltaQ (Area*DeltaF; mm2*%) were determined at a QLF threshold of -5%. The presence/absence of non-cavitated lesions was independently recorded with both methods.\n78.8% of all untreated surfaces were classified as sound or as having a non-cavitated lesion with both methods uniformly (VI+QLF). On 7.1% of all surfaces a lesion was detected by VI only and on 14.1% by QLF only. All parameters (Area, DeltaF, DeltaQ) differed significantly between lesions registered with both methods (VI+QLF) and lesions recorded with QLF only.\nIt was concluded that our hypothesis cannot be confirmed. The study shows that QLF detects (1) more non-cavitated occlusal lesions and (2) smaller lesions compared to VI. However, taking into consideration time-consuming image capturing and analysis, QLF is not really practical for use in the dental office.', 'title': 'In vivo detection of non-cavitated caries lesions on occlusal surfaces by visual inspection and quantitative light-induced fluorescence.', 'date': '2007-05-22'}, '29028973': {'article_id': '29028973', 'content': 'Vaccination of human immunodeficiency virus (HIV)-infected infants with bacille Calmette-Guérin (BCG) is contraindicated. HIV-exposed newborns need a new tuberculosis vaccination strategy that protects against tuberculosis early in life and avoids the potential risk of BCG disease until after HIV infection has been excluded.\nThis double-blind, randomized, controlled trial compared newborn MVA85A prime vaccination (1 × 108 PFU) vs Candin® control, followed by selective, deferred BCG vaccination at age 8 weeks for HIV-uninfected infants and 12 months follow-up for safety and immunogenicity.\nA total of 248 HIV-exposed infants were enrolled. More frequent mild-moderate reactogenicity events were seen after newborn MVA85A vaccination. However, no significant difference was observed in the rate of severe or serious adverse events, HIV acquisition (n = 1 per arm), or incident tuberculosis disease (n = 5 MVA85A; n = 3 control) compared to the control arm. MVA85A vaccination induced modest but significantly higher Ag85A-specific interferon gamma (IFNγ)+ CD4+ T cells compared to control at weeks 4 and 8 (P < .0001). BCG did not further boost this response in MVA85A vaccinees. The BCG-induced Ag85A-specific IFNγ+ CD4+ T-cell response at weeks 16 and 52 was of similar magnitude in the control arm compared to the MVA85A arm at all time points. Proliferative capacity, functional profiles, and memory phenotype of BCG-specific CD4 responses were similar across study arms.\nMVA85A prime vaccination of HIV-exposed newborns was safe and induced an early modest antigen-specific immune response that did not interfere with, or enhance, immunogenicity of subsequent BCG vaccination. New protein-subunit and viral-vectored tuberculosis vaccine candidates should be tested in HIV-exposed newborns.\nNCT01650389.', 'title': 'Safety and Immunogenicity of Newborn MVA85A Vaccination and Selective, Delayed Bacille Calmette-Guerin for Infants of Human Immunodeficiency Virus-Infected Mothers: A Phase 2 Randomized, Controlled Trial.', 'date': '2017-10-14'}}",0.333333333,"Public Health, Epidemiology & Health Systems"
69,"Is the risk of life‐threatening serious adverse effects higher, lower, or the same when comparing MVA85A added to BCG to BCG alone?",no difference,high,yes,"['25726088', '23391465', '29028973']",31038197,2019,"{'25726088': {'article_id': '25726088', 'content': 'HIV-1 infection is associated with increased risk of tuberculosis and a safe and effective vaccine would assist control measures. We assessed the safety, immunogenicity, and efficacy of a candidate tuberculosis vaccine, modified vaccinia virus Ankara expressing antigen 85A (MVA85A), in adults infected with HIV-1.\nWe did a randomised, double-blind, placebo-controlled, phase 2 trial of MVA85A in adults infected with HIV-1, at two clinical sites, in Cape Town, South Africa and Dakar, Senegal. Eligible participants were aged 18-50 years, had no evidence of active tuberculosis, and had baseline CD4 counts greater than 350 cells per μL if they had never received antiretroviral therapy or greater than 300 cells per μL (and with undetectable viral load before randomisation) if they were receiving antiretroviral therapy; participants with latent tuberculosis infection were eligible if they had completed at least 5 months of isoniazid preventive therapy, unless they had completed treatment for tuberculosis disease within 3 years before randomisation. Participants were randomly assigned (1:1) in blocks of four by randomly generated sequence to receive two intradermal injections of either MVA85A or placebo. Randomisation was stratified by antiretroviral therapy status and study site. Participants, nurses, investigators, and laboratory staff were masked to group allocation. The second (booster) injection of MVA85A or placebo was given 6-12 months after the first vaccination. The primary study outcome was safety in all vaccinated participants (the safety analysis population). Safety was assessed throughout the trial as defined in the protocol. Secondary outcomes were immunogenicity and vaccine efficacy against Mycobacterium tuberculosis infection and disease, assessed in the per-protocol population. Immunogenicity was assessed in a subset of participants at day 7 and day 28 after the first and second vaccination, and M tuberculosis infection and disease were assessed at the end of the study. The trial is registered with ClinicalTrials.gov, number NCT01151189.\nBetween Aug 4, 2011, and April 24, 2013, 650 participants were enrolled and randomly assigned; 649 were included in the safety analysis (324 in the MVA85A group and 325 in the placebo group) and 645 in the per-protocol analysis (320 and 325). 513 (71%) participants had CD4 counts greater than 300 cells per μL and were receiving antiretroviral therapy; 136 (21%) had CD4 counts above 350 cells per μL and had never received antiretroviral therapy. 277 (43%) had received isoniazid prophylaxis before enrolment. Solicited adverse events were more frequent in participants who received MVA85A (288 [89%]) than in those given placebo (235 [72%]). 34 serious adverse events were reported, 17 (5%) in each group. MVA85A induced a significant increase in antigen 85A-specific T-cell response, which peaked 7 days after both vaccinations and was primarily monofunctional. The number of participants with negative QuantiFERON-TB Gold In-Tube findings at baseline who converted to positive by the end of the study was 38 (20%) of 186 in the MVA85A group and 40 (23%) of 173 in the placebo group, for a vaccine efficacy of 11·7% (95% CI -41·3 to 44·9). In the per-protocol population, six (2%) cases of tuberculosis disease occurred in the MVA85A group and nine (3%) occurred in the placebo group, for a vaccine efficacy of 32·8% (95% CI -111·5 to 80·3).\nMVA85A was well tolerated and immunogenic in adults infected with HIV-1. However, we detected no efficacy against M tuberculosis infection or disease, although the study was underpowered to detect an effect against disease. Potential reasons for the absence of detectable efficacy in this trial include insufficient induction of a vaccine-induced immune response or the wrong type of vaccine-induced immune response, or both.\nEuropean & Developing Countries Clinical Trials Partnership (IP.2007.32080.002), Aeras, Bill & Melinda Gates Foundation, Wellcome Trust, and Oxford-Emergent Tuberculosis Consortium.', 'title': 'Safety, immunogenicity, and efficacy of the candidate tuberculosis vaccine MVA85A in healthy adults infected with HIV-1: a randomised, placebo-controlled, phase 2 trial.', 'date': '2015-03-03'}, '23391465': {'article_id': '23391465', 'content': 'The aim of this clinical caries detection study was to compare the outcome of quantitative light-induced fluorescence (QLF) and meticulous visual inspection (VI) in detecting non-cavitated caries lesions on occlusal surfaces in young adolescents. It was hypothesized that the respective diagnostic performances of meticulous VI and QLF are similar.\nThe subjects were 34 fifteen-year-old students. Five-hundred-and-seventeen cleaned occlusal surfaces were air-dried and examined using VI. Fluorescence images were captured with QLF equipment and custom software was used to display, store and analyze the images. The area of the lesion (area; mm2), fluorescence loss (DeltaF;%) and DeltaQ (Area*DeltaF; mm2*%) were determined at a QLF threshold of -5%. The presence/absence of non-cavitated lesions was independently recorded with both methods.\n78.8% of all untreated surfaces were classified as sound or as having a non-cavitated lesion with both methods uniformly (VI+QLF). On 7.1% of all surfaces a lesion was detected by VI only and on 14.1% by QLF only. All parameters (Area, DeltaF, DeltaQ) differed significantly between lesions registered with both methods (VI+QLF) and lesions recorded with QLF only.\nIt was concluded that our hypothesis cannot be confirmed. The study shows that QLF detects (1) more non-cavitated occlusal lesions and (2) smaller lesions compared to VI. However, taking into consideration time-consuming image capturing and analysis, QLF is not really practical for use in the dental office.', 'title': 'In vivo detection of non-cavitated caries lesions on occlusal surfaces by visual inspection and quantitative light-induced fluorescence.', 'date': '2007-05-22'}, '29028973': {'article_id': '29028973', 'content': 'Vaccination of human immunodeficiency virus (HIV)-infected infants with bacille Calmette-Guérin (BCG) is contraindicated. HIV-exposed newborns need a new tuberculosis vaccination strategy that protects against tuberculosis early in life and avoids the potential risk of BCG disease until after HIV infection has been excluded.\nThis double-blind, randomized, controlled trial compared newborn MVA85A prime vaccination (1 × 108 PFU) vs Candin® control, followed by selective, deferred BCG vaccination at age 8 weeks for HIV-uninfected infants and 12 months follow-up for safety and immunogenicity.\nA total of 248 HIV-exposed infants were enrolled. More frequent mild-moderate reactogenicity events were seen after newborn MVA85A vaccination. However, no significant difference was observed in the rate of severe or serious adverse events, HIV acquisition (n = 1 per arm), or incident tuberculosis disease (n = 5 MVA85A; n = 3 control) compared to the control arm. MVA85A vaccination induced modest but significantly higher Ag85A-specific interferon gamma (IFNγ)+ CD4+ T cells compared to control at weeks 4 and 8 (P < .0001). BCG did not further boost this response in MVA85A vaccinees. The BCG-induced Ag85A-specific IFNγ+ CD4+ T-cell response at weeks 16 and 52 was of similar magnitude in the control arm compared to the MVA85A arm at all time points. Proliferative capacity, functional profiles, and memory phenotype of BCG-specific CD4 responses were similar across study arms.\nMVA85A prime vaccination of HIV-exposed newborns was safe and induced an early modest antigen-specific immune response that did not interfere with, or enhance, immunogenicity of subsequent BCG vaccination. New protein-subunit and viral-vectored tuberculosis vaccine candidates should be tested in HIV-exposed newborns.\nNCT01650389.', 'title': 'Safety and Immunogenicity of Newborn MVA85A Vaccination and Selective, Delayed Bacille Calmette-Guerin for Infants of Human Immunodeficiency Virus-Infected Mothers: A Phase 2 Randomized, Controlled Trial.', 'date': '2017-10-14'}}",0.666666667,"Public Health, Epidemiology & Health Systems"
70,"Is the risk of starting on tuberculosis treatment higher, lower, or the same when comparing MVA85A added to BCG to BCG alone?",no difference,moderate,yes,"['25726088', '23391465', '29028973']",31038197,2019,"{'25726088': {'article_id': '25726088', 'content': 'HIV-1 infection is associated with increased risk of tuberculosis and a safe and effective vaccine would assist control measures. We assessed the safety, immunogenicity, and efficacy of a candidate tuberculosis vaccine, modified vaccinia virus Ankara expressing antigen 85A (MVA85A), in adults infected with HIV-1.\nWe did a randomised, double-blind, placebo-controlled, phase 2 trial of MVA85A in adults infected with HIV-1, at two clinical sites, in Cape Town, South Africa and Dakar, Senegal. Eligible participants were aged 18-50 years, had no evidence of active tuberculosis, and had baseline CD4 counts greater than 350 cells per μL if they had never received antiretroviral therapy or greater than 300 cells per μL (and with undetectable viral load before randomisation) if they were receiving antiretroviral therapy; participants with latent tuberculosis infection were eligible if they had completed at least 5 months of isoniazid preventive therapy, unless they had completed treatment for tuberculosis disease within 3 years before randomisation. Participants were randomly assigned (1:1) in blocks of four by randomly generated sequence to receive two intradermal injections of either MVA85A or placebo. Randomisation was stratified by antiretroviral therapy status and study site. Participants, nurses, investigators, and laboratory staff were masked to group allocation. The second (booster) injection of MVA85A or placebo was given 6-12 months after the first vaccination. The primary study outcome was safety in all vaccinated participants (the safety analysis population). Safety was assessed throughout the trial as defined in the protocol. Secondary outcomes were immunogenicity and vaccine efficacy against Mycobacterium tuberculosis infection and disease, assessed in the per-protocol population. Immunogenicity was assessed in a subset of participants at day 7 and day 28 after the first and second vaccination, and M tuberculosis infection and disease were assessed at the end of the study. The trial is registered with ClinicalTrials.gov, number NCT01151189.\nBetween Aug 4, 2011, and April 24, 2013, 650 participants were enrolled and randomly assigned; 649 were included in the safety analysis (324 in the MVA85A group and 325 in the placebo group) and 645 in the per-protocol analysis (320 and 325). 513 (71%) participants had CD4 counts greater than 300 cells per μL and were receiving antiretroviral therapy; 136 (21%) had CD4 counts above 350 cells per μL and had never received antiretroviral therapy. 277 (43%) had received isoniazid prophylaxis before enrolment. Solicited adverse events were more frequent in participants who received MVA85A (288 [89%]) than in those given placebo (235 [72%]). 34 serious adverse events were reported, 17 (5%) in each group. MVA85A induced a significant increase in antigen 85A-specific T-cell response, which peaked 7 days after both vaccinations and was primarily monofunctional. The number of participants with negative QuantiFERON-TB Gold In-Tube findings at baseline who converted to positive by the end of the study was 38 (20%) of 186 in the MVA85A group and 40 (23%) of 173 in the placebo group, for a vaccine efficacy of 11·7% (95% CI -41·3 to 44·9). In the per-protocol population, six (2%) cases of tuberculosis disease occurred in the MVA85A group and nine (3%) occurred in the placebo group, for a vaccine efficacy of 32·8% (95% CI -111·5 to 80·3).\nMVA85A was well tolerated and immunogenic in adults infected with HIV-1. However, we detected no efficacy against M tuberculosis infection or disease, although the study was underpowered to detect an effect against disease. Potential reasons for the absence of detectable efficacy in this trial include insufficient induction of a vaccine-induced immune response or the wrong type of vaccine-induced immune response, or both.\nEuropean & Developing Countries Clinical Trials Partnership (IP.2007.32080.002), Aeras, Bill & Melinda Gates Foundation, Wellcome Trust, and Oxford-Emergent Tuberculosis Consortium.', 'title': 'Safety, immunogenicity, and efficacy of the candidate tuberculosis vaccine MVA85A in healthy adults infected with HIV-1: a randomised, placebo-controlled, phase 2 trial.', 'date': '2015-03-03'}, '23391465': {'article_id': '23391465', 'content': 'The aim of this clinical caries detection study was to compare the outcome of quantitative light-induced fluorescence (QLF) and meticulous visual inspection (VI) in detecting non-cavitated caries lesions on occlusal surfaces in young adolescents. It was hypothesized that the respective diagnostic performances of meticulous VI and QLF are similar.\nThe subjects were 34 fifteen-year-old students. Five-hundred-and-seventeen cleaned occlusal surfaces were air-dried and examined using VI. Fluorescence images were captured with QLF equipment and custom software was used to display, store and analyze the images. The area of the lesion (area; mm2), fluorescence loss (DeltaF;%) and DeltaQ (Area*DeltaF; mm2*%) were determined at a QLF threshold of -5%. The presence/absence of non-cavitated lesions was independently recorded with both methods.\n78.8% of all untreated surfaces were classified as sound or as having a non-cavitated lesion with both methods uniformly (VI+QLF). On 7.1% of all surfaces a lesion was detected by VI only and on 14.1% by QLF only. All parameters (Area, DeltaF, DeltaQ) differed significantly between lesions registered with both methods (VI+QLF) and lesions recorded with QLF only.\nIt was concluded that our hypothesis cannot be confirmed. The study shows that QLF detects (1) more non-cavitated occlusal lesions and (2) smaller lesions compared to VI. However, taking into consideration time-consuming image capturing and analysis, QLF is not really practical for use in the dental office.', 'title': 'In vivo detection of non-cavitated caries lesions on occlusal surfaces by visual inspection and quantitative light-induced fluorescence.', 'date': '2007-05-22'}, '29028973': {'article_id': '29028973', 'content': 'Vaccination of human immunodeficiency virus (HIV)-infected infants with bacille Calmette-Guérin (BCG) is contraindicated. HIV-exposed newborns need a new tuberculosis vaccination strategy that protects against tuberculosis early in life and avoids the potential risk of BCG disease until after HIV infection has been excluded.\nThis double-blind, randomized, controlled trial compared newborn MVA85A prime vaccination (1 × 108 PFU) vs Candin® control, followed by selective, deferred BCG vaccination at age 8 weeks for HIV-uninfected infants and 12 months follow-up for safety and immunogenicity.\nA total of 248 HIV-exposed infants were enrolled. More frequent mild-moderate reactogenicity events were seen after newborn MVA85A vaccination. However, no significant difference was observed in the rate of severe or serious adverse events, HIV acquisition (n = 1 per arm), or incident tuberculosis disease (n = 5 MVA85A; n = 3 control) compared to the control arm. MVA85A vaccination induced modest but significantly higher Ag85A-specific interferon gamma (IFNγ)+ CD4+ T cells compared to control at weeks 4 and 8 (P < .0001). BCG did not further boost this response in MVA85A vaccinees. The BCG-induced Ag85A-specific IFNγ+ CD4+ T-cell response at weeks 16 and 52 was of similar magnitude in the control arm compared to the MVA85A arm at all time points. Proliferative capacity, functional profiles, and memory phenotype of BCG-specific CD4 responses were similar across study arms.\nMVA85A prime vaccination of HIV-exposed newborns was safe and induced an early modest antigen-specific immune response that did not interfere with, or enhance, immunogenicity of subsequent BCG vaccination. New protein-subunit and viral-vectored tuberculosis vaccine candidates should be tested in HIV-exposed newborns.\nNCT01650389.', 'title': 'Safety and Immunogenicity of Newborn MVA85A Vaccination and Selective, Delayed Bacille Calmette-Guerin for Infants of Human Immunodeficiency Virus-Infected Mothers: A Phase 2 Randomized, Controlled Trial.', 'date': '2017-10-14'}}",0.0,"Public Health, Epidemiology & Health Systems"
71,"Is improvement in health status up to 6 months (as measured by improvement on both CCQ and SGRQ) higher, lower, or the same when comparing smart technology for self‐management to face‐to‐face/digital and/or written support for self‐management?",higher,low,yes,"['27502583', '26089656', '24293120']",28535331,2017,"{'27502583': {'article_id': '27502583', 'content': ""Regular physical activity (PA) is recommended for persons with chronic obstructive pulmonary disease (COPD). Interventions that promote PA and sustain long-term adherence to PA are needed.\nWe examined the effects of an Internet-mediated, pedometer-based walking intervention, called Taking Healthy Steps, at 12 months.\nVeterans with COPD (N=239) were randomized in a 2:1 ratio to the intervention or wait-list control. During the first 4 months, participants in the intervention group were instructed to wear the pedometer every day, upload daily step counts at least once a week, and were provided access to a website with four key components: individualized goal setting, iterative feedback, educational and motivational content, and an online community forum. The subsequent 8-month maintenance phase was the same except that participants no longer received new educational content. Participants randomized to the wait-list control group were instructed to wear the pedometer, but they did not receive step-count goals or instructions to increase PA. The primary outcome was health-related quality of life (HRQL) assessed by the St George's Respiratory Questionnaire Total Score (SGRQ-TS); the secondary outcome was daily step count. Linear mixed-effect models assessed the effect of intervention over time. One participant was excluded from the analysis because he was an outlier. Within the intervention group, we assessed pedometer adherence and website engagement by examining percent of days with valid step-count data, number of log-ins to the website each month, use of the online community forum, and responses to a structured survey.\nParticipants were 93.7% male (223/238) with a mean age of 67 (SD 9) years. At 12 months, there were no significant between-group differences in SGRQ-TS or daily step count. Between-group difference in daily step count was maximal and statistically significant at month 4 (P<.001), but approached zero in months 8-12. Within the intervention group, mean 76.7% (SD 29.5) of 366 days had valid step-count data, which decreased over the months of study (P<.001). Mean number of log-ins to the website each month also significantly decreased over the months of study (P<.001). The online community forum was used at least once during the study by 83.8% (129/154) of participants. Responses to questions assessing participants' goal commitment and intervention engagement were not significantly different at 12 months compared to 4 months.\nAn Internet-mediated, pedometer-based PA intervention, although efficacious at 4 months, does not maintain improvements in HRQL and daily step counts at 12 months. Waning pedometer adherence and website engagement by the intervention group were observed. Future efforts should focus on improving features of PA interventions to promote long-term behavior change and sustain engagement in PA.\nClinicaltrials.gov NCT01102777; https://clinicaltrials.gov/ct2/show/NCT01102777 (Archived by WebCite at http://www.webcitation.org/6iyNP9KUC)."", 'title': 'Long-Term Effects of an Internet-Mediated Pedometer-Based Walking Program for Chronic Obstructive Pulmonary Disease: Randomized Controlled Trial.', 'date': '2016-08-10'}, '26089656': {'article_id': '26089656', 'content': 'COPD is a leading cause of morbidity and mortality. Self-management interventions are considered important in order to limit the progression of the disease. Computer-tailored interventions could be an effective tool to facilitate self-management.\nThis randomized controlled trial tested the effectiveness of a web-based, computer-tailored COPD self-management intervention on physical activity and smoking behavior. Participants were recruited from an online panel and through primary care practices. Those at risk for or diagnosed with COPD, between 40 and 70 years of age, proficient in Dutch, with access to the Internet, and with basic computer skills (n=1,325), were randomly assigned to either the intervention group (n=662) or control group (n=663). The intervention group received the web-based self-management application, while the control group received no intervention. Participants were not blinded to group assignment. After 6 months, the effect of the intervention was assessed for the primary outcomes, smoking cessation and physical activity, by self-reported 7-day point prevalence abstinence and the International Physical Activity Questionnaire - Short Form.\nOf the 1,325 participants, 1,071 (80.8%) completed the 6-month follow-up questionnaire. No significant treatment effect was found on either outcome. The application however, was used by only 36% of the participants in the experimental group.\nA possible explanation for the nonsignificant effect on the primary outcomes, smoking cessation and physical activity, could be the low exposure to the application as engagement with the program has been shown to be crucial for the effectiveness of computer-tailored interventions. (Netherlands Trial Registry number: NTR3421.).', 'title': 'A randomized controlled trial evaluating the effectiveness of a web-based, computer-tailored self-management intervention for people with or at risk for COPD.', 'date': '2015-06-20'}, '24293120': {'article_id': '24293120', 'content': 'First, to investigate the effects of a telerehabilitation intervention on health status and activity level of patients with Chronic Obstructive Pulmonary Disease (COPD), compared to usual care. Second, to investigate how patients comply with the intervention and whether compliance is related to treatment outcomes.\na randomized controlled pilot trial\nThirty-four patients diagnosed with COPD.\nThe telerehabilitation application consists of an activity coach (3D-accelerometer with smartphone) for ambulant activity registration and real-time feedback, complemented by a web portal with a symptom diary for self-treatment of exacerbations. The intervention group used the application for 4 weeks. The control group received usual care.\nActivity level measured by a pedometer (in steps/day), health status by the Clinical COPD Questionnaire at baseline and after intervention. Compliance was expressed as the time the activity coach was worn.\nFourteen intervention and 16 control patients completed the study. Activity level (steps/day) was not significantly affected by the intervention over time. There was a non-significant difference in improvement in health status between the intervention (-0.34±0.55) and control group (0.02±0.57, p=0.10). Health status significantly improved within the intervention group (p=0.05). The activity coach was used more than prescribed (108%) and compliance was related to the increase in activity level for the first two feedback weeks (r=0.62, p=0.03).\nThis pilot study shows the potential of the telerehabilitation intervention: compliance with the activity coach was high, which directly related to an improvement in activity levels.', 'title': 'A telerehabilitation intervention for patients with Chronic Obstructive Pulmonary Disease: a randomized controlled pilot trial.', 'date': '2013-12-03'}}",0.0,Family Medicine & Preventive Care
72,"Is the risk of adverse events higher, lower, or the same when comparing smart technology for self‐management to face‐to‐face/digital and/or written support for self‐management?",higher,,yes,['27502583'],28535331,2017,"{'27502583': {'article_id': '27502583', 'content': ""Regular physical activity (PA) is recommended for persons with chronic obstructive pulmonary disease (COPD). Interventions that promote PA and sustain long-term adherence to PA are needed.\nWe examined the effects of an Internet-mediated, pedometer-based walking intervention, called Taking Healthy Steps, at 12 months.\nVeterans with COPD (N=239) were randomized in a 2:1 ratio to the intervention or wait-list control. During the first 4 months, participants in the intervention group were instructed to wear the pedometer every day, upload daily step counts at least once a week, and were provided access to a website with four key components: individualized goal setting, iterative feedback, educational and motivational content, and an online community forum. The subsequent 8-month maintenance phase was the same except that participants no longer received new educational content. Participants randomized to the wait-list control group were instructed to wear the pedometer, but they did not receive step-count goals or instructions to increase PA. The primary outcome was health-related quality of life (HRQL) assessed by the St George's Respiratory Questionnaire Total Score (SGRQ-TS); the secondary outcome was daily step count. Linear mixed-effect models assessed the effect of intervention over time. One participant was excluded from the analysis because he was an outlier. Within the intervention group, we assessed pedometer adherence and website engagement by examining percent of days with valid step-count data, number of log-ins to the website each month, use of the online community forum, and responses to a structured survey.\nParticipants were 93.7% male (223/238) with a mean age of 67 (SD 9) years. At 12 months, there were no significant between-group differences in SGRQ-TS or daily step count. Between-group difference in daily step count was maximal and statistically significant at month 4 (P<.001), but approached zero in months 8-12. Within the intervention group, mean 76.7% (SD 29.5) of 366 days had valid step-count data, which decreased over the months of study (P<.001). Mean number of log-ins to the website each month also significantly decreased over the months of study (P<.001). The online community forum was used at least once during the study by 83.8% (129/154) of participants. Responses to questions assessing participants' goal commitment and intervention engagement were not significantly different at 12 months compared to 4 months.\nAn Internet-mediated, pedometer-based PA intervention, although efficacious at 4 months, does not maintain improvements in HRQL and daily step counts at 12 months. Waning pedometer adherence and website engagement by the intervention group were observed. Future efforts should focus on improving features of PA interventions to promote long-term behavior change and sustain engagement in PA.\nClinicaltrials.gov NCT01102777; https://clinicaltrials.gov/ct2/show/NCT01102777 (Archived by WebCite at http://www.webcitation.org/6iyNP9KUC)."", 'title': 'Long-Term Effects of an Internet-Mediated Pedometer-Based Walking Program for Chronic Obstructive Pulmonary Disease: Randomized Controlled Trial.', 'date': '2016-08-10'}}",0.0,Family Medicine & Preventive Care
73,"Is activity level up to 6 months higher, lower, or the same when comparing smart technology for self‐management to face‐to‐face/digital and/or written support for self‐management?",higher,low,no,"['25811395', '27502583']",28535331,2017,"{'25811395': {'article_id': '25811395', 'content': ""Low levels of physical activity (PA) are associated with poor outcomes in people with COPD. Interventions to increase PA could improve outcomes.\nWe tested the efficacy of a novel Internet-mediated, pedometer-based exercise intervention. Veterans with COPD (N = 239) were randomized in a 2:1 ratio to the (1) intervention group (Omron HJ-720 ITC pedometer and Internet-mediated program) or (2) wait-list control group (pedometer). The primary outcome was health-related quality of life (HRQL), assessed by the St. George's Respiratory Questionnaire (SGRQ), at 4 months. We examined the SGRQ total score (SGRQ-TS) and three domain scores: Symptoms, Activities, and Impact. The secondary outcome was daily step counts. Linear regression models assessed the effect of intervention on outcomes.\nParticipants had a mean age of 67 ± 9 years, and 94% were men. There was no significant between-group difference in mean 4-month SGRQ-TS (2.3 units, P = .14). Nevertheless, a significantly greater proportion of intervention participants than control subjects had at least a 4-unit improvement in SGRQ-TS, the minimum clinically important difference (53% vs 39%, respectively, P = .05). For domain scores, the intervention group had a lower (reflecting better HRQL) mean than the control group by 4.6 units for Symptoms (P = .046) and by 3.3 units for Impact (P = .049). There was no significant difference in Activities score between the two groups. Compared with the control subjects, intervention participants walked 779 more steps per day at 4 months (P = .005).\nAn Internet-mediated, pedometer-based walking program can improve domains of HRQL and daily step counts at 4 months in people with COPD.\nClinical Trials.gov; No.: NCT01102777; URL: www.clinicaltrials.gov."", 'title': 'An Internet-Mediated Pedometer-Based Program Improves Health-Related Quality-of-Life Domains and Daily Step Counts in COPD: A Randomized Controlled Trial.', 'date': '2015-03-27'}, '27502583': {'article_id': '27502583', 'content': ""Regular physical activity (PA) is recommended for persons with chronic obstructive pulmonary disease (COPD). Interventions that promote PA and sustain long-term adherence to PA are needed.\nWe examined the effects of an Internet-mediated, pedometer-based walking intervention, called Taking Healthy Steps, at 12 months.\nVeterans with COPD (N=239) were randomized in a 2:1 ratio to the intervention or wait-list control. During the first 4 months, participants in the intervention group were instructed to wear the pedometer every day, upload daily step counts at least once a week, and were provided access to a website with four key components: individualized goal setting, iterative feedback, educational and motivational content, and an online community forum. The subsequent 8-month maintenance phase was the same except that participants no longer received new educational content. Participants randomized to the wait-list control group were instructed to wear the pedometer, but they did not receive step-count goals or instructions to increase PA. The primary outcome was health-related quality of life (HRQL) assessed by the St George's Respiratory Questionnaire Total Score (SGRQ-TS); the secondary outcome was daily step count. Linear mixed-effect models assessed the effect of intervention over time. One participant was excluded from the analysis because he was an outlier. Within the intervention group, we assessed pedometer adherence and website engagement by examining percent of days with valid step-count data, number of log-ins to the website each month, use of the online community forum, and responses to a structured survey.\nParticipants were 93.7% male (223/238) with a mean age of 67 (SD 9) years. At 12 months, there were no significant between-group differences in SGRQ-TS or daily step count. Between-group difference in daily step count was maximal and statistically significant at month 4 (P<.001), but approached zero in months 8-12. Within the intervention group, mean 76.7% (SD 29.5) of 366 days had valid step-count data, which decreased over the months of study (P<.001). Mean number of log-ins to the website each month also significantly decreased over the months of study (P<.001). The online community forum was used at least once during the study by 83.8% (129/154) of participants. Responses to questions assessing participants' goal commitment and intervention engagement were not significantly different at 12 months compared to 4 months.\nAn Internet-mediated, pedometer-based PA intervention, although efficacious at 4 months, does not maintain improvements in HRQL and daily step counts at 12 months. Waning pedometer adherence and website engagement by the intervention group were observed. Future efforts should focus on improving features of PA interventions to promote long-term behavior change and sustain engagement in PA.\nClinicaltrials.gov NCT01102777; https://clinicaltrials.gov/ct2/show/NCT01102777 (Archived by WebCite at http://www.webcitation.org/6iyNP9KUC)."", 'title': 'Long-Term Effects of an Internet-Mediated Pedometer-Based Walking Program for Chronic Obstructive Pulmonary Disease: Randomized Controlled Trial.', 'date': '2016-08-10'}}",0.0,Family Medicine & Preventive Care
74,"Is activity level at 12 months higher, lower, or the same when comparing smart technology for self‐management to face‐to‐face/digital and/or written support for self‐management?",no difference,,no,['25811395'],28535331,2017,"{'25811395': {'article_id': '25811395', 'content': ""Low levels of physical activity (PA) are associated with poor outcomes in people with COPD. Interventions to increase PA could improve outcomes.\nWe tested the efficacy of a novel Internet-mediated, pedometer-based exercise intervention. Veterans with COPD (N = 239) were randomized in a 2:1 ratio to the (1) intervention group (Omron HJ-720 ITC pedometer and Internet-mediated program) or (2) wait-list control group (pedometer). The primary outcome was health-related quality of life (HRQL), assessed by the St. George's Respiratory Questionnaire (SGRQ), at 4 months. We examined the SGRQ total score (SGRQ-TS) and three domain scores: Symptoms, Activities, and Impact. The secondary outcome was daily step counts. Linear regression models assessed the effect of intervention on outcomes.\nParticipants had a mean age of 67 ± 9 years, and 94% were men. There was no significant between-group difference in mean 4-month SGRQ-TS (2.3 units, P = .14). Nevertheless, a significantly greater proportion of intervention participants than control subjects had at least a 4-unit improvement in SGRQ-TS, the minimum clinically important difference (53% vs 39%, respectively, P = .05). For domain scores, the intervention group had a lower (reflecting better HRQL) mean than the control group by 4.6 units for Symptoms (P = .046) and by 3.3 units for Impact (P = .049). There was no significant difference in Activities score between the two groups. Compared with the control subjects, intervention participants walked 779 more steps per day at 4 months (P = .005).\nAn Internet-mediated, pedometer-based walking program can improve domains of HRQL and daily step counts at 4 months in people with COPD.\nClinical Trials.gov; No.: NCT01102777; URL: www.clinicaltrials.gov."", 'title': 'An Internet-Mediated Pedometer-Based Program Improves Health-Related Quality-of-Life Domains and Daily Step Counts in COPD: A Randomized Controlled Trial.', 'date': '2015-03-27'}}",0.0,"Public Health, Epidemiology & Health Systems"
75,"Is the risk of hospital admission by 12 months higher, lower, or the same when comparing smart technology for self‐management to face‐to‐face/digital and/or written support for self‐management?",no difference,low,yes,['27502583'],28535331,2017,"{'27502583': {'article_id': '27502583', 'content': ""Regular physical activity (PA) is recommended for persons with chronic obstructive pulmonary disease (COPD). Interventions that promote PA and sustain long-term adherence to PA are needed.\nWe examined the effects of an Internet-mediated, pedometer-based walking intervention, called Taking Healthy Steps, at 12 months.\nVeterans with COPD (N=239) were randomized in a 2:1 ratio to the intervention or wait-list control. During the first 4 months, participants in the intervention group were instructed to wear the pedometer every day, upload daily step counts at least once a week, and were provided access to a website with four key components: individualized goal setting, iterative feedback, educational and motivational content, and an online community forum. The subsequent 8-month maintenance phase was the same except that participants no longer received new educational content. Participants randomized to the wait-list control group were instructed to wear the pedometer, but they did not receive step-count goals or instructions to increase PA. The primary outcome was health-related quality of life (HRQL) assessed by the St George's Respiratory Questionnaire Total Score (SGRQ-TS); the secondary outcome was daily step count. Linear mixed-effect models assessed the effect of intervention over time. One participant was excluded from the analysis because he was an outlier. Within the intervention group, we assessed pedometer adherence and website engagement by examining percent of days with valid step-count data, number of log-ins to the website each month, use of the online community forum, and responses to a structured survey.\nParticipants were 93.7% male (223/238) with a mean age of 67 (SD 9) years. At 12 months, there were no significant between-group differences in SGRQ-TS or daily step count. Between-group difference in daily step count was maximal and statistically significant at month 4 (P<.001), but approached zero in months 8-12. Within the intervention group, mean 76.7% (SD 29.5) of 366 days had valid step-count data, which decreased over the months of study (P<.001). Mean number of log-ins to the website each month also significantly decreased over the months of study (P<.001). The online community forum was used at least once during the study by 83.8% (129/154) of participants. Responses to questions assessing participants' goal commitment and intervention engagement were not significantly different at 12 months compared to 4 months.\nAn Internet-mediated, pedometer-based PA intervention, although efficacious at 4 months, does not maintain improvements in HRQL and daily step counts at 12 months. Waning pedometer adherence and website engagement by the intervention group were observed. Future efforts should focus on improving features of PA interventions to promote long-term behavior change and sustain engagement in PA.\nClinicaltrials.gov NCT01102777; https://clinicaltrials.gov/ct2/show/NCT01102777 (Archived by WebCite at http://www.webcitation.org/6iyNP9KUC)."", 'title': 'Long-Term Effects of an Internet-Mediated Pedometer-Based Walking Program for Chronic Obstructive Pulmonary Disease: Randomized Controlled Trial.', 'date': '2016-08-10'}}",0.0,"Public Health, Epidemiology & Health Systems"
76,"Is smoking cessation higher, lower, or the same when comparing smart technology for self‐management to face‐to‐face/digital and/or written support for self‐management?",no difference,moderate,no,['26089656'],28535331,2017,"{'26089656': {'article_id': '26089656', 'content': 'COPD is a leading cause of morbidity and mortality. Self-management interventions are considered important in order to limit the progression of the disease. Computer-tailored interventions could be an effective tool to facilitate self-management.\nThis randomized controlled trial tested the effectiveness of a web-based, computer-tailored COPD self-management intervention on physical activity and smoking behavior. Participants were recruited from an online panel and through primary care practices. Those at risk for or diagnosed with COPD, between 40 and 70 years of age, proficient in Dutch, with access to the Internet, and with basic computer skills (n=1,325), were randomly assigned to either the intervention group (n=662) or control group (n=663). The intervention group received the web-based self-management application, while the control group received no intervention. Participants were not blinded to group assignment. After 6 months, the effect of the intervention was assessed for the primary outcomes, smoking cessation and physical activity, by self-reported 7-day point prevalence abstinence and the International Physical Activity Questionnaire - Short Form.\nOf the 1,325 participants, 1,071 (80.8%) completed the 6-month follow-up questionnaire. No significant treatment effect was found on either outcome. The application however, was used by only 36% of the participants in the experimental group.\nA possible explanation for the nonsignificant effect on the primary outcomes, smoking cessation and physical activity, could be the low exposure to the application as engagement with the program has been shown to be crucial for the effectiveness of computer-tailored interventions. (Netherlands Trial Registry number: NTR3421.).', 'title': 'A randomized controlled trial evaluating the effectiveness of a web-based, computer-tailored self-management intervention for people with or at risk for COPD.', 'date': '2015-06-20'}}",0.0,Family Medicine & Preventive Care
77,"Is malaria parasite prevalence higher, lower, or the same when comparing non‐pyrethroid‐like indoor residual spraying (IRS) plus insecticide‐treated nets (ITNs) to insecticide‐treated nets (ITNs) alone?",lower,low,yes,"['29229808', '22682536', '24736370', '29655496', '22682536']",31120132,2019,"{'29229808': {'article_id': '29229808', 'content': 'Insecticide-based interventions have contributed to ∼78% of the reduction in the malaria burden in sub-Saharan Africa since 2000. Insecticide resistance in malaria vectors could presage a catastrophic rebound in disease incidence and mortality. A major impediment to the implementation of insecticide resistance management strategies is that evidence of the impact of resistance on malaria disease burden is limited. A cluster randomized trial was conducted in Sudan with pyrethroid-resistant and carbamate-susceptible malaria vectors. Clusters were randomly allocated to receive either long-lasting insecticidal nets (LLINs) alone or LLINs in combination with indoor residual spraying (IRS) with a pyrethroid (deltamethrin) insecticide in the first year and a carbamate (bendiocarb) insecticide in the two subsequent years. Malaria incidence was monitored for 3 y through active case detection in cohorts of children aged 1 to <10 y. When deltamethrin was used for IRS, incidence rates in the LLIN + IRS arm and the LLIN-only arm were similar, with the IRS providing no additional protection [incidence rate ratio (IRR) = 1.0 (95% confidence interval [CI]: 0.36-3.0; ', 'title': 'Impact of insecticide resistance in ', 'date': '2017-12-13'}, '22682536': {'article_id': '22682536', 'content': ""Malaria control efforts and elimination in Africa are being challenged by the development of resistance of parasites to antimalarial drugs and vectors to insecticides. We investigated whether the combination of long-lasting insecticidal mosquito nets (LLINs) with indoor residual spraying (IRS) or carbamate-treated plastic sheeting (CTPS) conferred enhanced protection against malaria and better management of pyrethroid-resistance in vectors than did LLINs alone.\nWe did a cluster randomised controlled trial in 28 villages in southern Benin, west Africa. Inclusion criteria of the villages were moderate level of pyrethroid resistance in malaria vectors and minimum distance between villages of 2 km. We assessed four malaria vector control interventions: LLIN targeted coverage to pregnant women and children younger than 6 years (TLLIN, reference group), LLIN universal coverage of all sleeping units (ULLIN), TLLIN plus full coverage of carbamate-IRS applied every 8 months (TLLIN+IRS), and ULLIN plus full coverage of CTPS lined up to the upper part of the household walls (ULLIN+CTPS). The interventions were allocated to villages by a block randomisation on the basis of preliminary surveys and children of each village were randomly selected to participate with computer-generated numbers. The primary endpoint was the incidence density rate of Plasmodium falciparum clinical malaria in children younger than 6 years as was analysed by Poisson regression taking into account the effect of age and the sampling design with a generalised estimating equation approach. Clinical and parasitological information were obtained by active case detection of malaria episodes during 12 periods of 6 consecutive days scheduled at six weekly intervals and by cross-sectional surveys of asymptomatic plasmodial infections. Children or study investigators were not masked to study group. This study is registered with Current Controlled Trials, number ISRCTN07404145.\nOf 58 villages assessed, 28 were randomly assigned to intervention groups. 413-429 children were followed up in each intervention group for 18 months. The clinical incidence density of malaria was not reduced in the children from the ULLIN group (incidence density rate 0·95, 95% CI 0·67-1·36, p=0·79), nor in those from the TLLIN+IRS group (1·32, 0·90-1·93, p=0·15) or from the ULLIN+CTPS group (1·05, 0·75-1·48, p=0·77) compared with the reference group (TLLIN). The same trend was observed with the prevalence and parasite density of asymptomatic infections (non significant regression coefficients).\nNo significant benefit for reducing malaria morbidity, infection, and transmission was reported when combining LLIN+IRS or LLIN+CTPS compared with a background of LLIN coverage. These findings are important for national malaria control programmes and should help the design of more cost-effective strategies for malaria control and elimination.\nMinistère Français des Affaires Etrangères et Européennes (FSP project 2006-22), Institut de Recherche pour le Développement, President's Malaria Initiative (PMI) of US Governement."", 'title': 'Combination of malaria vector control interventions in pyrethroid resistance area in Benin: a cluster randomised controlled trial.', 'date': '2012-06-12'}, '24736370': {'article_id': '24736370', 'content': ""Insecticide-treated nets (ITNs) and indoor residual spraying (IRS) of houses provide effective malaria transmission control. There is conflicting evidence about whether it is more beneficial to provide both interventions in combination. A cluster randomised controlled trial was conducted to investigate whether the combination provides added protection compared to ITNs alone.\nIn northwest Tanzania, 50 clusters (village areas) were randomly allocated to ITNs only or ITNs and IRS. Dwellings in the ITN+IRS arm were sprayed with two rounds of bendiocarb in 2012. Plasmodium falciparum prevalence rate (PfPR) in children 0.5-14 y old (primary outcome) and anaemia in children <5 y old (secondary outcome) were compared between study arms using three cross-sectional household surveys in 2012. Entomological inoculation rate (secondary outcome) was compared between study arms. IRS coverage was approximately 90%. ITN use ranged from 36% to 50%. In intention-to-treat analysis, mean PfPR was 13% in the ITN+IRS arm and 26% in the ITN only arm, odds ratio\u200a=\u200a0.43 (95% CI 0.19-0.97, n\u200a=\u200a13,146). The strongest effect was observed in the peak transmission season, 6 mo after the first IRS. Subgroup analysis showed that ITN users were additionally protected if their houses were sprayed. Mean monthly entomological inoculation rate was non-significantly lower in the ITN+IRS arm than in the ITN only arm, rate ratio\u200a=\u200a0.17 (95% CI 0.03-1.08).\nThis is the first randomised trial to our knowledge that reports significant added protection from combining IRS and ITNs compared to ITNs alone. The effect is likely to be attributable to IRS providing added protection to ITN users as well as compensating for inadequate ITN use. Policy makers should consider deploying IRS in combination with ITNs to control transmission if local ITN strategies on their own are insufficiently effective. Given the uncertain generalisability of these findings, it would be prudent for malaria control programmes to evaluate the cost-effectiveness of deploying the combination.\nwww.ClinicalTrials.gov NCT01697852 Please see later in the article for the Editors' Summary."", 'title': 'Indoor residual spraying in combination with insecticide-treated nets compared to insecticide-treated nets alone for protection against malaria: a cluster randomised trial in Tanzania.', 'date': '2014-04-17'}, '29655496': {'article_id': '29655496', 'content': 'Progress in malaria control is under threat by wide-scale insecticide resistance in malaria vectors. Two recent vector control products have been developed: a long-lasting insecticidal net that incorporates a synergist piperonyl butoxide (PBO) and a long-lasting indoor residual spraying formulation of the insecticide pirimiphos-methyl. We evaluated the effectiveness of PBO long-lasting insecticidal nets versus standard long-lasting insecticidal nets as single interventions and in combination with the indoor residual spraying of pirimiphos-methyl.\nWe did a four-group cluster randomised controlled trial using a two-by-two factorial design of 48 clusters derived from 40 villages in Muleba (Kagera, Tanzania). We randomly assigned these clusters using restricted randomisation to four groups: standard long-lasting insecticidal nets, PBO long-lasting insecticidal nets, standard long-lasting insecticidal nets plus indoor residual spraying, or PBO long-lasting insecticidal nets plus indoor residual spraying. Both standard and PBO nets were distributed in 2015. Indoor residual spraying was applied only once in 2015. We masked the inhabitants of each cluster to the type of nets received, as well as field staff who took blood samples. Neither the investigators nor the participants were masked to indoor residual spraying. The primary outcome was the prevalence of malaria infection in children aged 6 months to 14 years assessed by cross-sectional surveys at 4, 9, 16, and 21 months after intervention. The endpoint for assessment of indoor residual spraying was 9 months and PBO long-lasting insecticidal nets was 21 months. This trial is registered with ClinicalTrials.gov, number NCT02288637.\n7184 (68·0%) of 10\u2008560 households were selected for post-intervention survey, and 15\u2008469 (89·0%) of 17\u2008377 eligible children from the four surveys were included in the intention-to-treat analysis. Of the 878 households visited in the two indoor residual spraying groups, 827 (94%) had been sprayed. Reported use of long-lasting insecticidal nets, across all groups, was 15\u2008341 (77·3%) of 19\u2008852 residents after 1 year, decreasing to 12\u2008503 (59·2%) of 21\u2008105 in the second year. Malaria infection prevalence after 9 months was lower in the two groups that received PBO long-lasting insecticidal nets than in the two groups that received standard long-lasting insecticidal nets (531 [29%] of 1852 children vs 767 [42%] of 1809; odds ratio [OR] 0·37, 95% CI 0·21-0·65; p=0·0011). At the same timepoint, malaria prevalence in the two groups that received indoor residual spraying was lower than in groups that did not receive indoor residual spraying (508 [28%] of 1846 children vs 790 [44%] of 1815; OR 0·33, 95% CI 0·19-0·55; p<0·0001) and there was evidence of an interaction between PBO long-lasting insecticidal nets and indoor residual spraying (OR 2·43, 95% CI 1·19-4·97; p=0·0158), indicating redundancy when combined. The PBO long-lasting insecticidal net effect was sustained after 21 months with a lower malaria prevalence than the standard long-lasting insecticidal net (865 [45%] of 1930 children vs 1255 [62%] of 2034; OR 0·40, 0·20-0·81; p=0·0122).\nThe PBO long-lasting insecticidal net and non-pyrethroid indoor residual spraying interventions showed improved control of malaria transmission compared with standard long-lasting insecticidal nets where pyrethroid resistance is prevalent and either intervention could be deployed to good effect. As a result, WHO has since recommended to increase coverage of PBO long-lasting insecticidal nets. Combining indoor residual spraying with pirimiphos-methyl and PBO long-lasting insecticidal nets provided no additional benefit compared with PBO long-lasting insecticidal nets alone or standard long-lasting insecticidal nets plus indoor residual spraying.\nUK Department for International Development, Medical Research Council, and Wellcome Trust.', 'title': 'Effectiveness of a long-lasting piperonyl butoxide-treated insecticidal net and indoor residual spray interventions, separately and together, against malaria transmitted by pyrethroid-resistant mosquitoes: a cluster, randomised controlled, two-by-two factorial design trial.', 'date': '2018-04-16'}}",0.5,"Public Health, Epidemiology & Health Systems"
78,"Is malaria incidence higher, lower, or the same when comparing non‐pyrethroid‐like indoor residual spraying (IRS) plus insecticide‐treated nets (ITNs) to insecticide‐treated nets (ITNs) alone?",uncertain effect,very low,no,"['29229808', '22682536']",31120132,2019,"{'29229808': {'article_id': '29229808', 'content': 'Insecticide-based interventions have contributed to ∼78% of the reduction in the malaria burden in sub-Saharan Africa since 2000. Insecticide resistance in malaria vectors could presage a catastrophic rebound in disease incidence and mortality. A major impediment to the implementation of insecticide resistance management strategies is that evidence of the impact of resistance on malaria disease burden is limited. A cluster randomized trial was conducted in Sudan with pyrethroid-resistant and carbamate-susceptible malaria vectors. Clusters were randomly allocated to receive either long-lasting insecticidal nets (LLINs) alone or LLINs in combination with indoor residual spraying (IRS) with a pyrethroid (deltamethrin) insecticide in the first year and a carbamate (bendiocarb) insecticide in the two subsequent years. Malaria incidence was monitored for 3 y through active case detection in cohorts of children aged 1 to <10 y. When deltamethrin was used for IRS, incidence rates in the LLIN + IRS arm and the LLIN-only arm were similar, with the IRS providing no additional protection [incidence rate ratio (IRR) = 1.0 (95% confidence interval [CI]: 0.36-3.0; ', 'title': 'Impact of insecticide resistance in ', 'date': '2017-12-13'}, '22682536': {'article_id': '22682536', 'content': ""Malaria control efforts and elimination in Africa are being challenged by the development of resistance of parasites to antimalarial drugs and vectors to insecticides. We investigated whether the combination of long-lasting insecticidal mosquito nets (LLINs) with indoor residual spraying (IRS) or carbamate-treated plastic sheeting (CTPS) conferred enhanced protection against malaria and better management of pyrethroid-resistance in vectors than did LLINs alone.\nWe did a cluster randomised controlled trial in 28 villages in southern Benin, west Africa. Inclusion criteria of the villages were moderate level of pyrethroid resistance in malaria vectors and minimum distance between villages of 2 km. We assessed four malaria vector control interventions: LLIN targeted coverage to pregnant women and children younger than 6 years (TLLIN, reference group), LLIN universal coverage of all sleeping units (ULLIN), TLLIN plus full coverage of carbamate-IRS applied every 8 months (TLLIN+IRS), and ULLIN plus full coverage of CTPS lined up to the upper part of the household walls (ULLIN+CTPS). The interventions were allocated to villages by a block randomisation on the basis of preliminary surveys and children of each village were randomly selected to participate with computer-generated numbers. The primary endpoint was the incidence density rate of Plasmodium falciparum clinical malaria in children younger than 6 years as was analysed by Poisson regression taking into account the effect of age and the sampling design with a generalised estimating equation approach. Clinical and parasitological information were obtained by active case detection of malaria episodes during 12 periods of 6 consecutive days scheduled at six weekly intervals and by cross-sectional surveys of asymptomatic plasmodial infections. Children or study investigators were not masked to study group. This study is registered with Current Controlled Trials, number ISRCTN07404145.\nOf 58 villages assessed, 28 were randomly assigned to intervention groups. 413-429 children were followed up in each intervention group for 18 months. The clinical incidence density of malaria was not reduced in the children from the ULLIN group (incidence density rate 0·95, 95% CI 0·67-1·36, p=0·79), nor in those from the TLLIN+IRS group (1·32, 0·90-1·93, p=0·15) or from the ULLIN+CTPS group (1·05, 0·75-1·48, p=0·77) compared with the reference group (TLLIN). The same trend was observed with the prevalence and parasite density of asymptomatic infections (non significant regression coefficients).\nNo significant benefit for reducing malaria morbidity, infection, and transmission was reported when combining LLIN+IRS or LLIN+CTPS compared with a background of LLIN coverage. These findings are important for national malaria control programmes and should help the design of more cost-effective strategies for malaria control and elimination.\nMinistère Français des Affaires Etrangères et Européennes (FSP project 2006-22), Institut de Recherche pour le Développement, President's Malaria Initiative (PMI) of US Governement."", 'title': 'Combination of malaria vector control interventions in pyrethroid resistance area in Benin: a cluster randomised controlled trial.', 'date': '2012-06-12'}}",0.0,"Public Health, Epidemiology & Health Systems"
79,"Is anaemia prevalence higher, lower, or the same when comparing non‐pyrethroid‐like indoor residual spraying (IRS) plus insecticide‐treated nets (ITNs) to insecticide‐treated nets (ITNs) alone?",lower,low,yes,"['24736370', '29655496']",31120132,2019,"{'24736370': {'article_id': '24736370', 'content': ""Insecticide-treated nets (ITNs) and indoor residual spraying (IRS) of houses provide effective malaria transmission control. There is conflicting evidence about whether it is more beneficial to provide both interventions in combination. A cluster randomised controlled trial was conducted to investigate whether the combination provides added protection compared to ITNs alone.\nIn northwest Tanzania, 50 clusters (village areas) were randomly allocated to ITNs only or ITNs and IRS. Dwellings in the ITN+IRS arm were sprayed with two rounds of bendiocarb in 2012. Plasmodium falciparum prevalence rate (PfPR) in children 0.5-14 y old (primary outcome) and anaemia in children <5 y old (secondary outcome) were compared between study arms using three cross-sectional household surveys in 2012. Entomological inoculation rate (secondary outcome) was compared between study arms. IRS coverage was approximately 90%. ITN use ranged from 36% to 50%. In intention-to-treat analysis, mean PfPR was 13% in the ITN+IRS arm and 26% in the ITN only arm, odds ratio\u200a=\u200a0.43 (95% CI 0.19-0.97, n\u200a=\u200a13,146). The strongest effect was observed in the peak transmission season, 6 mo after the first IRS. Subgroup analysis showed that ITN users were additionally protected if their houses were sprayed. Mean monthly entomological inoculation rate was non-significantly lower in the ITN+IRS arm than in the ITN only arm, rate ratio\u200a=\u200a0.17 (95% CI 0.03-1.08).\nThis is the first randomised trial to our knowledge that reports significant added protection from combining IRS and ITNs compared to ITNs alone. The effect is likely to be attributable to IRS providing added protection to ITN users as well as compensating for inadequate ITN use. Policy makers should consider deploying IRS in combination with ITNs to control transmission if local ITN strategies on their own are insufficiently effective. Given the uncertain generalisability of these findings, it would be prudent for malaria control programmes to evaluate the cost-effectiveness of deploying the combination.\nwww.ClinicalTrials.gov NCT01697852 Please see later in the article for the Editors' Summary."", 'title': 'Indoor residual spraying in combination with insecticide-treated nets compared to insecticide-treated nets alone for protection against malaria: a cluster randomised trial in Tanzania.', 'date': '2014-04-17'}, '29655496': {'article_id': '29655496', 'content': 'Progress in malaria control is under threat by wide-scale insecticide resistance in malaria vectors. Two recent vector control products have been developed: a long-lasting insecticidal net that incorporates a synergist piperonyl butoxide (PBO) and a long-lasting indoor residual spraying formulation of the insecticide pirimiphos-methyl. We evaluated the effectiveness of PBO long-lasting insecticidal nets versus standard long-lasting insecticidal nets as single interventions and in combination with the indoor residual spraying of pirimiphos-methyl.\nWe did a four-group cluster randomised controlled trial using a two-by-two factorial design of 48 clusters derived from 40 villages in Muleba (Kagera, Tanzania). We randomly assigned these clusters using restricted randomisation to four groups: standard long-lasting insecticidal nets, PBO long-lasting insecticidal nets, standard long-lasting insecticidal nets plus indoor residual spraying, or PBO long-lasting insecticidal nets plus indoor residual spraying. Both standard and PBO nets were distributed in 2015. Indoor residual spraying was applied only once in 2015. We masked the inhabitants of each cluster to the type of nets received, as well as field staff who took blood samples. Neither the investigators nor the participants were masked to indoor residual spraying. The primary outcome was the prevalence of malaria infection in children aged 6 months to 14 years assessed by cross-sectional surveys at 4, 9, 16, and 21 months after intervention. The endpoint for assessment of indoor residual spraying was 9 months and PBO long-lasting insecticidal nets was 21 months. This trial is registered with ClinicalTrials.gov, number NCT02288637.\n7184 (68·0%) of 10\u2008560 households were selected for post-intervention survey, and 15\u2008469 (89·0%) of 17\u2008377 eligible children from the four surveys were included in the intention-to-treat analysis. Of the 878 households visited in the two indoor residual spraying groups, 827 (94%) had been sprayed. Reported use of long-lasting insecticidal nets, across all groups, was 15\u2008341 (77·3%) of 19\u2008852 residents after 1 year, decreasing to 12\u2008503 (59·2%) of 21\u2008105 in the second year. Malaria infection prevalence after 9 months was lower in the two groups that received PBO long-lasting insecticidal nets than in the two groups that received standard long-lasting insecticidal nets (531 [29%] of 1852 children vs 767 [42%] of 1809; odds ratio [OR] 0·37, 95% CI 0·21-0·65; p=0·0011). At the same timepoint, malaria prevalence in the two groups that received indoor residual spraying was lower than in groups that did not receive indoor residual spraying (508 [28%] of 1846 children vs 790 [44%] of 1815; OR 0·33, 95% CI 0·19-0·55; p<0·0001) and there was evidence of an interaction between PBO long-lasting insecticidal nets and indoor residual spraying (OR 2·43, 95% CI 1·19-4·97; p=0·0158), indicating redundancy when combined. The PBO long-lasting insecticidal net effect was sustained after 21 months with a lower malaria prevalence than the standard long-lasting insecticidal net (865 [45%] of 1930 children vs 1255 [62%] of 2034; OR 0·40, 0·20-0·81; p=0·0122).\nThe PBO long-lasting insecticidal net and non-pyrethroid indoor residual spraying interventions showed improved control of malaria transmission compared with standard long-lasting insecticidal nets where pyrethroid resistance is prevalent and either intervention could be deployed to good effect. As a result, WHO has since recommended to increase coverage of PBO long-lasting insecticidal nets. Combining indoor residual spraying with pirimiphos-methyl and PBO long-lasting insecticidal nets provided no additional benefit compared with PBO long-lasting insecticidal nets alone or standard long-lasting insecticidal nets plus indoor residual spraying.\nUK Department for International Development, Medical Research Council, and Wellcome Trust.', 'title': 'Effectiveness of a long-lasting piperonyl butoxide-treated insecticidal net and indoor residual spray interventions, separately and together, against malaria transmitted by pyrethroid-resistant mosquitoes: a cluster, randomised controlled, two-by-two factorial design trial.', 'date': '2018-04-16'}}",0.0,"Public Health, Epidemiology & Health Systems"
80,"Is malaria incidence higher, lower, or the same when comparing pyrethroid‐like indoor residual spraying (IRS) plus insecticide‐treated nets (ITNs) to insecticide‐treated nets (ITNs) alone?",no difference,moderate,no,"['25498847', '29229808']",31120132,2019,"{'25498847': {'article_id': '25498847', 'content': 'Although many malaria control programmes in sub-Saharan Africa use indoor residual spraying with long-lasting insecticidal nets (LLINs), the two studies assessing the benefit of the combination of these two interventions gave conflicting results. We aimed to assess whether the addition of indoor residual spraying to LLINs provided a significantly different level of protection against clinical malaria in children or against house entry by vector mosquitoes.\nIn this two-arm cluster, randomised, controlled efficacy trial we randomly allocated clusters of Gambian villages using a computerised algorithm to LLINs alone (n=35) or indoor residual spraying with dichlorodiphenyltrichloroethane plus LLINs (n=35). In each cluster, 65-213 children, aged 6 months to 14 years, were surveyed at the start of the 2010 transmission season and followed in 2010 and 2011 by passive case detection for clinical malaria. Exposure to parasite transmission was assessed by collection of vector mosquitoes with both light and exit traps indoors. Primary endpoints were the incidence of clinical malaria assessed by passive case detection and number of Anopheles gambiae sensu lato mosquitoes collected per light trap per night. Intervention teams had no role in data collection and the data collection teams were not informed of the spray status of villages. The trial is registered at the ISRCTN registry, number ISRCTN01738840.\nLLIN coverage in 2011 was 3510 (93%) of 3777 children in the indoor residual spraying plus LLIN group and 3622 (95.5%) of 3791 in the LLIN group. In 2010, 7845 children were enrolled, 7829 completed passive case detection, and 7697 (98%) had complete clinical and covariate data. In 2011, 7009 children remained in the study, 648 more were enrolled, 7657 completed passive case detection, and 7545 (98.5%) had complete data. Indoor residual spraying coverage per cluster was more than 80% for both years in the indoor residual spraying plus LLIN group. Incidence of clinical malaria was 0.047 per child-month at risk in the LLIN group and 0.044 per child-month at risk in the indoor residual spraying plus LLIN group in 2010, and 0.032 per child-month at risk in the LLIN group and 0.034 per child-month at risk in the indoor residual spraying plus LLIN group in 2011. The incident rate ratio was 1.08 (95% CI 0.80-1.46) controlling for confounders and cluster by mixed-effect negative binomial regression on all malaria attacks for both years. No significant difference was recorded in the density of vector mosquitoes caught in light traps in houses over the two transmission seasons; the mean number of A gambiae sensu lato mosquitoes per trap per night was 6.7 (4.0-10.1) in the LLIN group and 4.5 (2.4-7.4) in the indoor residual spraying plus LLIN group (p=0.281 in the random-effects linear regression model).\nWe identified no significant difference in clinical malaria or vector density between study groups. In this area with high LLIN coverage, moderate seasonal transmission, and susceptible vectors, indoor residual spraying did not provide additional benefit.\nUK Medical Research Council.', 'title': 'Efficacy of indoor residual spraying with dichlorodiphenyltrichloroethane against malaria in Gambian communities with high usage of long-lasting insecticidal mosquito nets: a cluster-randomised controlled trial.', 'date': '2014-12-17'}, '29229808': {'article_id': '29229808', 'content': 'Insecticide-based interventions have contributed to ∼78% of the reduction in the malaria burden in sub-Saharan Africa since 2000. Insecticide resistance in malaria vectors could presage a catastrophic rebound in disease incidence and mortality. A major impediment to the implementation of insecticide resistance management strategies is that evidence of the impact of resistance on malaria disease burden is limited. A cluster randomized trial was conducted in Sudan with pyrethroid-resistant and carbamate-susceptible malaria vectors. Clusters were randomly allocated to receive either long-lasting insecticidal nets (LLINs) alone or LLINs in combination with indoor residual spraying (IRS) with a pyrethroid (deltamethrin) insecticide in the first year and a carbamate (bendiocarb) insecticide in the two subsequent years. Malaria incidence was monitored for 3 y through active case detection in cohorts of children aged 1 to <10 y. When deltamethrin was used for IRS, incidence rates in the LLIN + IRS arm and the LLIN-only arm were similar, with the IRS providing no additional protection [incidence rate ratio (IRR) = 1.0 (95% confidence interval [CI]: 0.36-3.0; ', 'title': 'Impact of insecticide resistance in ', 'date': '2017-12-13'}}",1.0,"Public Health, Epidemiology & Health Systems"
81,"Is malaria parasite prevalence higher, lower, or the same when comparing pyrethroid‐like indoor residual spraying (IRS) plus insecticide‐treated nets (ITNs) to insecticide‐treated nets (ITNs) alone?",no difference,moderate,no,"['25498847', '29229808', '21565149']",31120132,2019,"{'25498847': {'article_id': '25498847', 'content': 'Although many malaria control programmes in sub-Saharan Africa use indoor residual spraying with long-lasting insecticidal nets (LLINs), the two studies assessing the benefit of the combination of these two interventions gave conflicting results. We aimed to assess whether the addition of indoor residual spraying to LLINs provided a significantly different level of protection against clinical malaria in children or against house entry by vector mosquitoes.\nIn this two-arm cluster, randomised, controlled efficacy trial we randomly allocated clusters of Gambian villages using a computerised algorithm to LLINs alone (n=35) or indoor residual spraying with dichlorodiphenyltrichloroethane plus LLINs (n=35). In each cluster, 65-213 children, aged 6 months to 14 years, were surveyed at the start of the 2010 transmission season and followed in 2010 and 2011 by passive case detection for clinical malaria. Exposure to parasite transmission was assessed by collection of vector mosquitoes with both light and exit traps indoors. Primary endpoints were the incidence of clinical malaria assessed by passive case detection and number of Anopheles gambiae sensu lato mosquitoes collected per light trap per night. Intervention teams had no role in data collection and the data collection teams were not informed of the spray status of villages. The trial is registered at the ISRCTN registry, number ISRCTN01738840.\nLLIN coverage in 2011 was 3510 (93%) of 3777 children in the indoor residual spraying plus LLIN group and 3622 (95.5%) of 3791 in the LLIN group. In 2010, 7845 children were enrolled, 7829 completed passive case detection, and 7697 (98%) had complete clinical and covariate data. In 2011, 7009 children remained in the study, 648 more were enrolled, 7657 completed passive case detection, and 7545 (98.5%) had complete data. Indoor residual spraying coverage per cluster was more than 80% for both years in the indoor residual spraying plus LLIN group. Incidence of clinical malaria was 0.047 per child-month at risk in the LLIN group and 0.044 per child-month at risk in the indoor residual spraying plus LLIN group in 2010, and 0.032 per child-month at risk in the LLIN group and 0.034 per child-month at risk in the indoor residual spraying plus LLIN group in 2011. The incident rate ratio was 1.08 (95% CI 0.80-1.46) controlling for confounders and cluster by mixed-effect negative binomial regression on all malaria attacks for both years. No significant difference was recorded in the density of vector mosquitoes caught in light traps in houses over the two transmission seasons; the mean number of A gambiae sensu lato mosquitoes per trap per night was 6.7 (4.0-10.1) in the LLIN group and 4.5 (2.4-7.4) in the indoor residual spraying plus LLIN group (p=0.281 in the random-effects linear regression model).\nWe identified no significant difference in clinical malaria or vector density between study groups. In this area with high LLIN coverage, moderate seasonal transmission, and susceptible vectors, indoor residual spraying did not provide additional benefit.\nUK Medical Research Council.', 'title': 'Efficacy of indoor residual spraying with dichlorodiphenyltrichloroethane against malaria in Gambian communities with high usage of long-lasting insecticidal mosquito nets: a cluster-randomised controlled trial.', 'date': '2014-12-17'}, '29229808': {'article_id': '29229808', 'content': 'Insecticide-based interventions have contributed to ∼78% of the reduction in the malaria burden in sub-Saharan Africa since 2000. Insecticide resistance in malaria vectors could presage a catastrophic rebound in disease incidence and mortality. A major impediment to the implementation of insecticide resistance management strategies is that evidence of the impact of resistance on malaria disease burden is limited. A cluster randomized trial was conducted in Sudan with pyrethroid-resistant and carbamate-susceptible malaria vectors. Clusters were randomly allocated to receive either long-lasting insecticidal nets (LLINs) alone or LLINs in combination with indoor residual spraying (IRS) with a pyrethroid (deltamethrin) insecticide in the first year and a carbamate (bendiocarb) insecticide in the two subsequent years. Malaria incidence was monitored for 3 y through active case detection in cohorts of children aged 1 to <10 y. When deltamethrin was used for IRS, incidence rates in the LLIN + IRS arm and the LLIN-only arm were similar, with the IRS providing no additional protection [incidence rate ratio (IRR) = 1.0 (95% confidence interval [CI]: 0.36-3.0; ', 'title': 'Impact of insecticide resistance in ', 'date': '2017-12-13'}, '21565149': {'article_id': '21565149', 'content': 'This paper examines the relationship between indoor residual spray (IRS) and malaria parasite infection in Gash Barka Zone, Eritrea, an area with near universal coverage of insecticide treated bednets (ITN) and already low malaria parasite prevalence. A community randomized control trial was conducted in 2009. Malaria parasite infection prevalence was 0.5% [95% confidence interval (CI): 0.37-0.78%], with no significant difference detected between treatment and control areas. ITN possession remains high, with over 70% of households reporting ITN ownership [95% CI: 68.4-72.9]. ITN use among individuals within ITN-owning households was just under half [46.7% (95% CI: 45.4-48.0)]. Slight differences in ITN possession and use were detected between treatment and control areas. There was no significant difference in malaria parasite infection prevalence among individuals in households with ≥1 ITN compared to those in households without ITNs, nor among individuals reporting ITN use. Among individuals in ITN-owning households, sleeping under an ITN offered no statistically significant protection from malaria parasite infection. Community participation in environmental and larval habitat management activities was low: 17.9% (95% CI: 16.0-19.7). It is likely that IRS, larval habitat management and ITN distribution alone may be insufficient to interrupt transmission without corresponding high ITN use, sustained IRS application in areas where infections are clustered, and promptly seeking laboratory diagnosis and treatment of all fevers. Eritrea is ready for elimination, irrespective of inconclusive impact evaluation results.', 'title': 'Evaluating indoor residual spray for reducing malaria infection prevalence in Eritrea: results from a community randomized control trial.', 'date': '2011-05-14'}}",0.666666667,"Public Health, Epidemiology & Health Systems"
82,"Is the rate of major complications higher, lower, or the same when comparing restrictive fluid therapy (RFT) to goal‐directed fluid therapy (GDFT)?",uncertain effect,very low,yes,"['26471495', '22710266', '25595308', '25342408', '23132508']",31829446,2019,"{'26471495': {'article_id': '26471495', 'content': 'The use of goal directed fluid protocols in intermediate risk patients undergoing hip or knee replacement was studied in few trials using invasive monitoring. For this reason we have implemented two different fluid management protocols, both based on a novel totally non-invasive arterial pressure monitoring device and compared them to the standard (no-protocol) treatment applied before the transition in our academic institution.\nThree treatment groups were compared in this prospective study: the observational (CONTROL, N = 40) group before adoption of fluid protocols and two randomized groups after the transition to protocol fluid management with the use of the continuous non-invasive blood pressure monitoring (CNAP®) device. In the PRESSURE group (N = 40) standard variables were used for restrictive fluid therapy. Goal directed fluid therapy using pulse pressure variation was used in the GDFT arm (N = 40). The influence on the rate of postoperative complications, on the hospital length of stay and other parameters was assessed.\nBoth protocols were associated with decreased fluid administration and maintained hemodynamic stability. Reduced rate of postoperative infection and organ complications (22 (55 %) vs. 33 (83 %) patients; p = 0.016; relative risk 0.67 (0.49-0.91)) was observed in the GDFT group compared to CONTROL. Lower number of patients receiving transfusion (4 (10 %) in GDFT vs. 17 (43 %) in CONTROL; p = 0.005) might contribute to this observation. No significant differences were observed in other end-points.\nIn our study, the use of the fluid protocol based on pulse pressure variation assessed using continuous non-invasive arterial pressure measurement seems to be associated with a reduction in postoperative complications and transfusion needs as compared to standard no-protocol treatment.\nACTRN12612001014842.', 'title': 'Fluid management guided by a continuous non-invasive arterial pressure device is associated with decreased postoperative morbidity after total knee and hip replacement.', 'date': '2015-10-17'}, '22710266': {'article_id': '22710266', 'content': 'We aimed to investigate whether fluid therapy with a goal of near-maximal stroke volume (SV) guided by oesophageal Doppler (ED) monitoring result in a better outcome than that with a goal of maintaining bodyweight (BW) and zero fluid balance in patients undergoing colorectal surgery.\nIn a double-blinded clinical multicentre trial, 150 patients undergoing elective colorectal surgery were randomized to receive fluid therapy after either the goal of near-maximal SV guided by ED (Doppler, D group) or the goal of zero balance and normal BW (Zero balance, Z group). Stratification for laparoscopic and open surgery was performed. The postoperative fluid therapy was similar in the two groups. The primary endpoint was postoperative complications defined and divided into subgroups by protocol. Analysis was performed by intention-to-treat. The follow-up was 30 days. The trial had 85% power to show a difference between the groups.\nThe number of patients undergoing laparoscopic or open surgery and the patient characteristics were similar between the groups. No significant differences between the groups were found for overall, major, minor, cardiopulmonary, or tissue-healing complications (P-values: 0.79; 0.62; 0.97; 0.48; and 0.48, respectively). One patient died in each group. No significant difference was found for the length of hospital stay [median (range) Z: 5.00 (1-61) vs D: 5.00 (2-41); P=0.206].\nGoal-directed fluid therapy to near-maximal SV guided by ED adds no extra value to the fluid therapy using zero balance and normal BW in patients undergoing elective colorectal surgery.', 'title': 'Which goal for fluid therapy during colorectal surgery is followed by the best outcome: near-maximal stroke volume or zero fluid balance?', 'date': '2012-06-20'}, '25595308': {'article_id': '25595308', 'content': 'The use of adequate fluid therapy during cytoreductive surgery (CRS) and hyperthermic intraperitoneal chemotherapy (HIPEC) remains controversial. The aim of the study was to assess whether the use of fluid therapy protocol combined with goal-directed therapy (GDT) is associated with a significant change in morbidity, length of hospital stay, and mortality compared to standard fluid therapy. Patients American Society of Anesthesiologists (ASA) II-III undergoing CRS and HIPEC were randomized into two groups. The GDT group (N\u2009=\u200938) received fluid therapy according to a protocol guided by monitored hemodynamic parameters. The control group (N\u2009=\u200942) received standard fluid therapy. We evaluated incidence of major complications, total length of hospital stay, total amount of fluids administered, and mortality rate. The incidence of major abdominal complications was 10.5% in GDT group and 38.1% in the control group (P\u2009=\u20090.005). The median duration of hospitalization was 19 days in GDT group and 29 days in the control group (P\u2009<\u20090.0001). The mortality rate was zero in GDT group vs. 9.5% in the control group (P\u2009=\u20090.12). GDT group received a significantly (P\u2009<\u20090.0001) lower amount of fluid (5812\u2009±\u20091244 ml) than the control group (8269\u2009±\u20091452 ml), with a significantly (P\u2009<\u20090.0001) lower volume of crystalloids (3884\u2009±\u20091003 vs. 68,528\u2009±\u20091413 ml). In CRS and HIPEC, the use of a GDT improves outcome in terms of incidence of major abdominal and systemic postoperative complications and length of hospital stay, compared to standard fluid therapy protocol.', 'title': 'A randomized trial of goal directed vs. standard fluid therapy in cytoreductive surgery with hyperthermic intraperitoneal chemotherapy.', 'date': '2015-01-18'}, '25342408': {'article_id': '25342408', 'content': 'There is continued controversy regarding the benefits of goal-directed fluid therapy, with earlier studies showing marked improvement in morbidity and length-of-stay that have not been replicated more recently. The aim of this study was to compare patient outcomes in elective colorectal surgery patients having goal-directed versus restrictive fluid therapy. Inclusion criteria included suitability for an Enhanced Recovery After Surgery care pathway and patients with an American Society of Anesthesiologists Physical Status score of 1 to 3. Patients were intraoperatively randomised to either restrictive or Doppler-guided goal-directed fluid therapy. The primary outcome was length-of-stay; secondary outcomes included complication rate, change in haemodynamic variables and fluid volumes. Compared to restrictive therapy, goal-directed therapy resulted in a greater volume of intraoperative fluid, 2115 (interquartile range 1350 to 2560) ml versus 1500 (1200 to 2000) ml, P=0.008, and was associated with an increase in Doppler-derived stroke volume index from beginning to end of surgery, 43.7 (16.3) to 54.2 (21.1) ml/m(2), P <0.001, in the latter group. Length-of-stay was similar, 6.5 (5 to 9) versus 6 (4 to 9) days, P=0.421. The number of patients with any complication (minor or major) was similar; 0% (30) versus 52% (26), P=0.42, or major complications, 1 (2%) versus 4 (8%), P=0.36, respectively. The increased perioperative fluid volumes and increased stroke volumes at the end of surgery in patients receiving goal-directed therapy did not translate to a significant difference in length-of-stay and we did not observe a difference in the number of patients experiencing minor or major complications.', 'title': 'A randomised controlled trial of fluid restriction compared to oesophageal Doppler-guided goal-directed fluid therapy in elective major colorectal surgery within an Enhanced Recovery After Surgery program.', 'date': '2014-10-25'}, '23132508': {'article_id': '23132508', 'content': ""Goal-directed fluid therapy (GDFT) has been compared with liberal fluid administration in non-optimized perioperative settings. It is not known whether GDFT is of value within an enhanced recovery protocol incorporating fluid restriction. This study evaluated GDFT under these circumstances in patients undergoing elective colectomy.\nPatients undergoing elective laparoscopic or open colectomy within an established enhanced recovery protocol (including fluid restriction) were randomized to GDFT or no GDFT. Bowel preparation was permitted for left colonic operations at the surgeon's discretion. Exclusion criteria included rectal tumours and stoma formation. The primary outcome was a patient-reported surgical recovery score (SRS). Secondary endpoints included clinical outcomes and physiological measures of recovery.\nEighty-five patients were randomized, and there were 37 patients in each group for analysis. Nine patients in the GDFT and four in the fluid restriction group received oral bowel preparation for either anterior resection (12) or subtotal colectomy (1). Patients in the GDFT group received more colloid during surgery (mean 591 versus 297 ml; P = 0·012) and had superior cardiac indices (mean corrected flow time 374 versus 355 ms; P = 0·018). However, no differences were observed between the GDFT and fluid restriction groups with regard to surgical recovery (mean SRS after 7 days 47 versus 46 respectively; P = 0·853), other secondary outcomes (mean aldosterone/renin ratio 9 versus 8; P = 0·898), total postoperative fluid (median 3750 versus 2400 ml; P = 0·604), length of hospital stay (median 6 versus 5 days; P = 0·570) or number of patients with complications (26 versus 27; P = 1·000).\nGDFT did not provide clinical benefit in patients undergoing elective colectomy within a protocol incorporating fluid restriction.\nNCT00911391 (http://www.clinicaltrials.gov)."", 'title': 'Randomized clinical trial of goal-directed fluid therapy within an enhanced recovery protocol for elective colectomy.', 'date': '2012-11-08'}}",0.0,Surgery
83,"Is the length of hospital stay higher, lower, or the same when comparing restrictive fluid therapy (RFT) to goal‐directed fluid therapy (GDFT)?",no difference,very low,yes,"['23070341', '26471495', '22710266', '25342408', '23132508']",31829446,2019,"{'23070341': {'article_id': '23070341', 'content': ""The optimal strategy for fluid management during gastrointestinal surgery remains unclear. Minimizing the variation in arterial pulse pressure, which is induced by mechanical ventilation, is a potential strategy to improve postoperative outcomes. We tested this hypothesis in a prospective, randomized study with lactated Ringer's solution and 6% hydroxyethyl starch solution.\nA total of 60 patients who were undergoing gastrointestinal surgery were randomized into a restrictive lactated Ringer's group (n = 20), a goal-directed lactated Ringer's group (n = 20) and a goal-directed hydroxyethyl starch group (n = 20). The goal-directed fluid treatment was guided by pulse pressure variation, which was recorded during surgery using a simple manual method with a Datex Ohmeda S/5 Monitor and minimized to 11% or less by volume loading with either lactated Ringer's solution or 6% hydroxyethyl starch solution (130/0.4). The postoperative flatus time, the length of hospital stay and the incidence of complications were recorded as endpoints.\nThe goal-directed lactated Ringer's group received the greatest amount of total operative fluid compared with the two other groups. The flatus time and the length of hospital stay in the goal-directed hydroxyethyl starch group were shorter than those in the goal-directed lactated Ringer's group and the restrictive lactated Ringer's group. No significant differences were found in the postoperative complications among the three groups.\nMonitoring and minimizing pulse pressure variation by 6% hydroxyethyl starch solution (130/0.4) loading during gastrointestinal surgery improves postoperative outcomes and decreases the discharge time of patients who are graded American Society of Anesthesiologists physical status I/II."", 'title': 'Intraoperative fluid management in open gastrointestinal surgery: goal-directed versus restrictive.', 'date': '2012-10-17'}, '26471495': {'article_id': '26471495', 'content': 'The use of goal directed fluid protocols in intermediate risk patients undergoing hip or knee replacement was studied in few trials using invasive monitoring. For this reason we have implemented two different fluid management protocols, both based on a novel totally non-invasive arterial pressure monitoring device and compared them to the standard (no-protocol) treatment applied before the transition in our academic institution.\nThree treatment groups were compared in this prospective study: the observational (CONTROL, N = 40) group before adoption of fluid protocols and two randomized groups after the transition to protocol fluid management with the use of the continuous non-invasive blood pressure monitoring (CNAP®) device. In the PRESSURE group (N = 40) standard variables were used for restrictive fluid therapy. Goal directed fluid therapy using pulse pressure variation was used in the GDFT arm (N = 40). The influence on the rate of postoperative complications, on the hospital length of stay and other parameters was assessed.\nBoth protocols were associated with decreased fluid administration and maintained hemodynamic stability. Reduced rate of postoperative infection and organ complications (22 (55 %) vs. 33 (83 %) patients; p = 0.016; relative risk 0.67 (0.49-0.91)) was observed in the GDFT group compared to CONTROL. Lower number of patients receiving transfusion (4 (10 %) in GDFT vs. 17 (43 %) in CONTROL; p = 0.005) might contribute to this observation. No significant differences were observed in other end-points.\nIn our study, the use of the fluid protocol based on pulse pressure variation assessed using continuous non-invasive arterial pressure measurement seems to be associated with a reduction in postoperative complications and transfusion needs as compared to standard no-protocol treatment.\nACTRN12612001014842.', 'title': 'Fluid management guided by a continuous non-invasive arterial pressure device is associated with decreased postoperative morbidity after total knee and hip replacement.', 'date': '2015-10-17'}, '22710266': {'article_id': '22710266', 'content': 'We aimed to investigate whether fluid therapy with a goal of near-maximal stroke volume (SV) guided by oesophageal Doppler (ED) monitoring result in a better outcome than that with a goal of maintaining bodyweight (BW) and zero fluid balance in patients undergoing colorectal surgery.\nIn a double-blinded clinical multicentre trial, 150 patients undergoing elective colorectal surgery were randomized to receive fluid therapy after either the goal of near-maximal SV guided by ED (Doppler, D group) or the goal of zero balance and normal BW (Zero balance, Z group). Stratification for laparoscopic and open surgery was performed. The postoperative fluid therapy was similar in the two groups. The primary endpoint was postoperative complications defined and divided into subgroups by protocol. Analysis was performed by intention-to-treat. The follow-up was 30 days. The trial had 85% power to show a difference between the groups.\nThe number of patients undergoing laparoscopic or open surgery and the patient characteristics were similar between the groups. No significant differences between the groups were found for overall, major, minor, cardiopulmonary, or tissue-healing complications (P-values: 0.79; 0.62; 0.97; 0.48; and 0.48, respectively). One patient died in each group. No significant difference was found for the length of hospital stay [median (range) Z: 5.00 (1-61) vs D: 5.00 (2-41); P=0.206].\nGoal-directed fluid therapy to near-maximal SV guided by ED adds no extra value to the fluid therapy using zero balance and normal BW in patients undergoing elective colorectal surgery.', 'title': 'Which goal for fluid therapy during colorectal surgery is followed by the best outcome: near-maximal stroke volume or zero fluid balance?', 'date': '2012-06-20'}, '25342408': {'article_id': '25342408', 'content': 'There is continued controversy regarding the benefits of goal-directed fluid therapy, with earlier studies showing marked improvement in morbidity and length-of-stay that have not been replicated more recently. The aim of this study was to compare patient outcomes in elective colorectal surgery patients having goal-directed versus restrictive fluid therapy. Inclusion criteria included suitability for an Enhanced Recovery After Surgery care pathway and patients with an American Society of Anesthesiologists Physical Status score of 1 to 3. Patients were intraoperatively randomised to either restrictive or Doppler-guided goal-directed fluid therapy. The primary outcome was length-of-stay; secondary outcomes included complication rate, change in haemodynamic variables and fluid volumes. Compared to restrictive therapy, goal-directed therapy resulted in a greater volume of intraoperative fluid, 2115 (interquartile range 1350 to 2560) ml versus 1500 (1200 to 2000) ml, P=0.008, and was associated with an increase in Doppler-derived stroke volume index from beginning to end of surgery, 43.7 (16.3) to 54.2 (21.1) ml/m(2), P <0.001, in the latter group. Length-of-stay was similar, 6.5 (5 to 9) versus 6 (4 to 9) days, P=0.421. The number of patients with any complication (minor or major) was similar; 0% (30) versus 52% (26), P=0.42, or major complications, 1 (2%) versus 4 (8%), P=0.36, respectively. The increased perioperative fluid volumes and increased stroke volumes at the end of surgery in patients receiving goal-directed therapy did not translate to a significant difference in length-of-stay and we did not observe a difference in the number of patients experiencing minor or major complications.', 'title': 'A randomised controlled trial of fluid restriction compared to oesophageal Doppler-guided goal-directed fluid therapy in elective major colorectal surgery within an Enhanced Recovery After Surgery program.', 'date': '2014-10-25'}, '23132508': {'article_id': '23132508', 'content': ""Goal-directed fluid therapy (GDFT) has been compared with liberal fluid administration in non-optimized perioperative settings. It is not known whether GDFT is of value within an enhanced recovery protocol incorporating fluid restriction. This study evaluated GDFT under these circumstances in patients undergoing elective colectomy.\nPatients undergoing elective laparoscopic or open colectomy within an established enhanced recovery protocol (including fluid restriction) were randomized to GDFT or no GDFT. Bowel preparation was permitted for left colonic operations at the surgeon's discretion. Exclusion criteria included rectal tumours and stoma formation. The primary outcome was a patient-reported surgical recovery score (SRS). Secondary endpoints included clinical outcomes and physiological measures of recovery.\nEighty-five patients were randomized, and there were 37 patients in each group for analysis. Nine patients in the GDFT and four in the fluid restriction group received oral bowel preparation for either anterior resection (12) or subtotal colectomy (1). Patients in the GDFT group received more colloid during surgery (mean 591 versus 297 ml; P = 0·012) and had superior cardiac indices (mean corrected flow time 374 versus 355 ms; P = 0·018). However, no differences were observed between the GDFT and fluid restriction groups with regard to surgical recovery (mean SRS after 7 days 47 versus 46 respectively; P = 0·853), other secondary outcomes (mean aldosterone/renin ratio 9 versus 8; P = 0·898), total postoperative fluid (median 3750 versus 2400 ml; P = 0·604), length of hospital stay (median 6 versus 5 days; P = 0·570) or number of patients with complications (26 versus 27; P = 1·000).\nGDFT did not provide clinical benefit in patients undergoing elective colectomy within a protocol incorporating fluid restriction.\nNCT00911391 (http://www.clinicaltrials.gov)."", 'title': 'Randomized clinical trial of goal-directed fluid therapy within an enhanced recovery protocol for elective colectomy.', 'date': '2012-11-08'}}",0.8,Surgery
84,"Is survival to hospital discharge higher, lower, or the same when comparing biphasic waveform defibrillation to monophasic waveform defibrillation?",no difference,,no,"['11719116', '17060379', '12867305', '15992986']",26904970,2016,"{'11719116': {'article_id': '11719116', 'content': 'Advances in early defibrillation access, key to the ""Chain of Survival"", will depend on innovations in defibrillation waveforms, because of their impact on device size and weight. This study compared standard monophasic waveform automatic external defibrillators (AEDs) to an innovative biphasic waveform AED.\nImpedance-compensated biphasic truncated exponential (ICBTE) and either monophasic truncated exponential (MTE) or monophasic damped sine (MDS) AEDs were prospectively, randomly assigned by date in four emergency medical services. The study design compared ICBTE with MTE and MDS combined. This subset analysis distinguishes between the two classes of monophasic waveform, MTE and MDS, and compares their performance to each other and to the biphasic waveform, contingent on significant overall effects (ICBTE vs. MTE vs. MDS). Primary endpoint: Defibrillation efficacy with < or =3 shocks. Secondary endpoints: shock efficacy with < or =1 shock, < or =2 shocks, and survival to hospital admission and discharge. Observations included return of spontaneous circulation (ROSC), refibrillation, and time to first shock and to first successful shock.\nOf 338 out-of-hospital cardiac arrests, 115 had a cardiac aetiology, presented with ventricular fibrillation, and were shocked by an AED. Defibrillation efficacy for the first ""stack"" of up to 3 shocks, for up to 2 shocks and for the first shock alone was superior for the ICBTE waveform than for either the MTE or the MDS waveform, while there was no difference between the efficacy of MTE and MDS. Time from the beginning of analysis by the AED to the first shock and to the first successful shock was also superior for the ICBTE devices compared to either the MTE or the MDS devices, while again there was no difference between the MTE and MDS devices. More ICBTE patients achieved ROSC pre-hospital than did MTE patients. While the rates of ROSC were identical for MTE and MDS patients, the difference between ICBTE and MDS was not significant. Rates of refibrillation and survival to hospital admission and discharge did not differ among the three populations.\nICBTE was superior to MTE and MDS in defibrillation efficacy and speed and to MTE in ROSC. MTE and MDS did not differ in efficacy. There were no differences among the waveforms in refibrillation or survival.', 'title': 'Optimal Response to Cardiac Arrest study: defibrillation waveform effects.', 'date': '2001-11-24'}, '17060379': {'article_id': '17060379', 'content': 'Although biphasic, as compared with monophasic, waveform defibrillation for cardiac arrest is increasing in use and popularity, whether it is truly a more lifesaving waveform is unproven.\nConsecutive adults with nontraumatic out-of-hospital ventricular fibrillation cardiac arrest were randomly allocated to defibrillation according to the waveform from automated external defibrillators administered by prehospital medical providers. The primary event of interest was admission alive to the hospital. Secondary events included return of rhythm and circulation, survival, and neurological outcome. Providers were blinded to automated defibrillator waveform. Of 168 randomized patients, 80 (48%) and 68 (40%) consistently received only monophasic or biphasic waveform shocks, respectively, throughout resuscitation. The prevalence of ventricular fibrillation, asystole, or organized rhythms at 5, 10, or 20 seconds after each shock did not differ significantly between treatment groups. The proportion of patients admitted alive to the hospital was relatively high: 73% in monophasic and 76% in biphasic treatment groups (P=0.58). Several favorable trends were consistently associated with receipt of biphasic waveform shock, none of which reached statistical significance. Notably, 27 of 80 monophasic shock recipients (34%), compared with 28 of 68 biphasic shock recipients (41%), survived (P=0.35). Neurological outcome was similar in both treatment groups (P=0.4). Earlier administration of shock did not significantly alter the performance of one waveform relative to the other, nor did shock waveform predict any clinical outcome after multivariate adjustment.\nNo statistically significant differences in outcome could be ascribed to use of one waveform over another when out-of-hospital ventricular fibrillation was treated.', 'title': 'Transthoracic incremental monophasic versus biphasic defibrillation by emergency responders (TIMBER): a randomized comparison of monophasic with biphasic waveform ascending energy defibrillation for the resuscitation of out-of-hospital cardiac arrest due to ventricular fibrillation.', 'date': '2006-10-25'}, '12867305': {'article_id': '12867305', 'content': 'Evidence suggests that biphasic waveforms are more effective than monophasic waveforms for defibrillation in out-of-hospital cardiac arrest (OHCA), yet their performance has only been compared in un-blinded studies.\nWe compared the success of biphasic truncated exponential (BTE) and monophasic damped sine (MDS) shocks for defibrillation in OHCA in a prospective, randomised, double blind clinical trial. First responders were equipped with MDS and BTE automated external defibrillators (AEDs) in a random fashion. Patients in ventricular fibrillation (VF) received BTE or MDS first shocks of 200 J. The ECG was recorded for subsequent analysis continuously. The success of the first shock as a primary endpoint was removal of VF and required a return of an organized rhythm for at least two QRS complexes, with an interval of <5 s, within 1 min after the first shock. The secondary endpoint was termination of VF at 5 s. VF was the initial recorded rhythm in 120 patients in OHCA, 51 patients received BTE and 69 received MDS shocks. The success rate of 200 J first shocks was significantly higher for BTE than for MDS shocks, 35/51 (69%) and 31/69 (45%), P=0.01. In a logistic regression model the odds ratio of success for a BTE shock was 4.01 (95% CI 1.01-10.0), adjusted for baseline cardiopulmonary resuscitation, VF-amplitude and time between collapse and first shock. No difference was found with respect to the secondary endpoint, termination of VF at 5 s (RR 1.07 95% CI: 0.99-1.11) and with respect to survival to hospital discharge (RR 0.73 95% CI: 0.31-1.70).\nBTE-waveform AEDs provide significantly higher rates of successful defibrillation with return of an organized rhythm in OHCA than MDS waveform AEDs.', 'title': 'A prospective, randomised and blinded comparison of first shock success of monophasic and biphasic waveforms in out-of-hospital cardiac arrest.', 'date': '2003-07-18'}, '15992986': {'article_id': '15992986', 'content': 'Although biphasic defibrillation waveforms appear to be superior to monophasic waveforms in terminating VF, their relative benefits in out-of-hospital resuscitation are incompletely understood. Prior comparisons of defibrillation waveform efficacy in out-of-hospital cardiac arrest (OHCA) are confined to patients presenting in a shockable rhythm and resuscitated by first responder (basic life support). This effectiveness study compared monophasic and biphasic defibrillation waveform for conversion of ventricular arrhythmias in all OHCA treated with advance life support (ALS).\nThis prospective randomized controlled trial compared the rectilinear biphasic (RLB) waveform with the monophasic damped sine (MDS) waveform, using step-up energy levels. The study enrolled OHCA patients requiring at least one shock delivered by ALS providers, regardless of initial presenting rhythm. Shock success was defined as conversion at 5s to organized rhythm after one to three escalating shocks. We report efficacy results for the cohort of patients treated by ALS paramedics who presented with an initially shockable rhythm who had not received a shock from a first responder (MDS: n=83; RLB: n=86). Shock success within the first three ascending energy shocks for RLB (120, 150, 200J) was superior to MDS (200, 300, 360J) for patients initially presenting in a shockable rhythm (52% versus 34%, p=0.01). First shock conversion was 23% and12%, for RLB and MDS, respectively (p=0.07). There were no significant differences in return of spontaneous circulation (47% versus 47%), survival to 24h (31% versus 27%), and survival to discharge (9% versus 7%). Mean 24h survival rates of bystander witnessed events showed differences between waveforms in the early circulatory phase at 4-10 min post event (mean (S.D.) RLB 0.45 (0.07) versus MDS 0.31 (0.06), p=0.0002) and demonstrated decline as time to first shock increased to 20 min.\nShock success to an organized rhythm comparing step-up protocol for energy settings demonstrated the RLB waveform was superior to MDS in ALS treatment of OHCA. Survival rates for both waveforms are consistent with current theories on the circulatory and metabolic phases of out-of-hospital cardiac arrest.', 'title': 'Out-of-hospital cardiac arrest rectilinear biphasic to monophasic damped sine defibrillation waveforms with advanced life support intervention trial (ORBIT).', 'date': '2005-07-05'}}",1.0,Emergency Medicine & Critical Care
85,"Is survival to hospital admission higher, lower, or the same when comparing biphasic waveform defibrillation to monophasic waveform defibrillation?",no difference,,no,"['11719116', '17060379']",26904970,2016,"{'11719116': {'article_id': '11719116', 'content': 'Advances in early defibrillation access, key to the ""Chain of Survival"", will depend on innovations in defibrillation waveforms, because of their impact on device size and weight. This study compared standard monophasic waveform automatic external defibrillators (AEDs) to an innovative biphasic waveform AED.\nImpedance-compensated biphasic truncated exponential (ICBTE) and either monophasic truncated exponential (MTE) or monophasic damped sine (MDS) AEDs were prospectively, randomly assigned by date in four emergency medical services. The study design compared ICBTE with MTE and MDS combined. This subset analysis distinguishes between the two classes of monophasic waveform, MTE and MDS, and compares their performance to each other and to the biphasic waveform, contingent on significant overall effects (ICBTE vs. MTE vs. MDS). Primary endpoint: Defibrillation efficacy with < or =3 shocks. Secondary endpoints: shock efficacy with < or =1 shock, < or =2 shocks, and survival to hospital admission and discharge. Observations included return of spontaneous circulation (ROSC), refibrillation, and time to first shock and to first successful shock.\nOf 338 out-of-hospital cardiac arrests, 115 had a cardiac aetiology, presented with ventricular fibrillation, and were shocked by an AED. Defibrillation efficacy for the first ""stack"" of up to 3 shocks, for up to 2 shocks and for the first shock alone was superior for the ICBTE waveform than for either the MTE or the MDS waveform, while there was no difference between the efficacy of MTE and MDS. Time from the beginning of analysis by the AED to the first shock and to the first successful shock was also superior for the ICBTE devices compared to either the MTE or the MDS devices, while again there was no difference between the MTE and MDS devices. More ICBTE patients achieved ROSC pre-hospital than did MTE patients. While the rates of ROSC were identical for MTE and MDS patients, the difference between ICBTE and MDS was not significant. Rates of refibrillation and survival to hospital admission and discharge did not differ among the three populations.\nICBTE was superior to MTE and MDS in defibrillation efficacy and speed and to MTE in ROSC. MTE and MDS did not differ in efficacy. There were no differences among the waveforms in refibrillation or survival.', 'title': 'Optimal Response to Cardiac Arrest study: defibrillation waveform effects.', 'date': '2001-11-24'}, '17060379': {'article_id': '17060379', 'content': 'Although biphasic, as compared with monophasic, waveform defibrillation for cardiac arrest is increasing in use and popularity, whether it is truly a more lifesaving waveform is unproven.\nConsecutive adults with nontraumatic out-of-hospital ventricular fibrillation cardiac arrest were randomly allocated to defibrillation according to the waveform from automated external defibrillators administered by prehospital medical providers. The primary event of interest was admission alive to the hospital. Secondary events included return of rhythm and circulation, survival, and neurological outcome. Providers were blinded to automated defibrillator waveform. Of 168 randomized patients, 80 (48%) and 68 (40%) consistently received only monophasic or biphasic waveform shocks, respectively, throughout resuscitation. The prevalence of ventricular fibrillation, asystole, or organized rhythms at 5, 10, or 20 seconds after each shock did not differ significantly between treatment groups. The proportion of patients admitted alive to the hospital was relatively high: 73% in monophasic and 76% in biphasic treatment groups (P=0.58). Several favorable trends were consistently associated with receipt of biphasic waveform shock, none of which reached statistical significance. Notably, 27 of 80 monophasic shock recipients (34%), compared with 28 of 68 biphasic shock recipients (41%), survived (P=0.35). Neurological outcome was similar in both treatment groups (P=0.4). Earlier administration of shock did not significantly alter the performance of one waveform relative to the other, nor did shock waveform predict any clinical outcome after multivariate adjustment.\nNo statistically significant differences in outcome could be ascribed to use of one waveform over another when out-of-hospital ventricular fibrillation was treated.', 'title': 'Transthoracic incremental monophasic versus biphasic defibrillation by emergency responders (TIMBER): a randomized comparison of monophasic with biphasic waveform ascending energy defibrillation for the resuscitation of out-of-hospital cardiac arrest due to ventricular fibrillation.', 'date': '2006-10-25'}}",1.0,Emergency Medicine & Critical Care
86,"Is the incidence of surgical site infection (SSI) for patients undergoing breast cancer surgery without reconstruction higher, lower, or the same when comparing prophylactic antibiotics administered preoperatively to no antibiotic or placebo?",lower,moderate,no,"['11063500', '23001082', '16989011', '7480237', '9776150', '23052809', '10873356', '19673597', '2403655', '2403697']",31557310,2019,"{'11063500': {'article_id': '11063500', 'content': 'Based on the observation that administration of clarithromycin led to an attenuation of the inflammatory response induced by surgical trauma in a guinea pig model, we investigated the potential beneficial effects of clarithromycin on the local and systemic inflammatory response in patients undergoing mastectomy in an open-label prospective study. During a 16-month period, 54 patients who underwent mastectomy were randomly divided into two groups. In one group, the patients received oral clarithromycin at a dose of 500 mg twice a day, from the day before to 3 days after mastectomy. There was no significant difference in the incidence of antibiotic prophylaxis-related toxicities or postoperative infections between the patients who received clarithromycin and those who did not. Clarithromycin treatment was significantly associated with an attenuation of febrile response, tachycardia, tachypnea, and an increase in monocyte counts (P, <0.0001, <0.01, <0.05, and <0.01, respectively). Clarithromycin also reduced the intensity and duration of postoperative pain (P, <0.05 and <0.005, respectively) and increased the range of motion of the involved shoulder (P < 0.05 for abduction and flexion). We conclude that clarithromycin effectively modulates the acute inflammatory response associated with mastectomy and produces a better clinical outcome.', 'title': 'Clarithromycin attenuates mastectomy-induced acute inflammatory response.', 'date': '2000-11-04'}, '23001082': {'article_id': '23001082', 'content': 'To assess the impact of prophylactic antibiotics on the prevention of surgical site infection (SSI) and the cost-effectiveness of this prophylaxis for breast cancer surgery in overweight or obese women.\nSSI is higher than expected after breast surgery. Obesity was found to be one of the risk factors.\nThe trial was designed as a phase IV randomized, controlled, parallel-group efficacy trial. It was conducted at a tertiary university hospital. Overweight or obese women with clinically early-stage breast cancer who had been assigned to undergo surgery were eligible. Patients were randomly allocated to either a prophylaxis or a control group by using a computer-generated list. The prophylaxis group received 1 g ampicillin-sulbactam intravenously at anesthesia. The control group received no intervention. Patients and observers were blinded to the assignments. The primary outcome was the comparison of SSI incidences of the 2 groups. Patients were monitored for 30 days.\nA total of 369 patients were included in final analysis, out of which 187 were allocated for prophylaxis and 182 were randomly assigned to the control group. Analysis was done according to the intention-to-treat principle. Prophylaxis significantly reduced the SSI rate (4.8%) in the prophylaxis group when compared with that in the control group [13.7%; relative risk (RR) 0.35; 95% CI: 0.17-0.73]. No adverse reaction was observed. The mean SSI-related cost (20.26 USD) was found to be significantly higher in the control group when compared with that (8.48 USD) in the prophylaxis group.\nAntibiotic prophylaxis significantly decreased SSI incidence after elective surgery and was shown to be cost-effective in obese breast cancer patients. ClinicalTrials.gov Identifier: NCT00356148.', 'title': 'Efficacy of prophylactic antibiotic administration for breast cancer surgery in overweight or obese patients: a randomized controlled trial.', 'date': '2012-09-25'}, '16989011': {'article_id': '16989011', 'content': 'The aim of this randomized clinical trial was to determine whether a single intravenous dose of 2 g flucloxacillin could prevent wound infection after primary non-reconstructive breast surgery.\nThe study included 618 patients undergoing local excision (n = 490), mastectomy (n = 107) or microdochectomy (n = 21). Patients were randomized to receive either a single dose of flucloxacillin immediately after the induction of anaesthesia or no intervention. Wound morbidity was monitored by an independent research nurse for 42 days after surgery.\nThe incidence of wound infection was similar in the two groups: 10 of 311 (3.2 percent) in the flucloxacillin group and 14 of 307 (4.6 percent) in the control group (chi(2) = 0.75, P = 0.387; relative risk 0.71, 95 percent confidence interval 0.32 to 1.53). The groups also had similar wound scores and rates of moderate or severe cellulitis. Wound infection presented a median of 16 days after surgery.\nThe administration of a single dose of flucloxacillin failed to reduce the rate of wound infection after non-reconstructive breast surgery.', 'title': 'Randomized clinical trial of single-dose antibiotic prophylaxis for non-reconstructive breast surgery.', 'date': '2006-09-22'}, '7480237': {'article_id': '7480237', 'content': 'Over a 9-month period from September of 1991 to May of 1992, 339 patients were included in a randomized, double-blind, placebo-controlled study using azithromycin as the prophylactic agent to determine whether it effects a clinically meaningful reduction in postoperative surgical infections in plastic surgery. Azithromycin was given as prophylaxis in 171 patients and placebo in 168 patients. The study medication was a single oral dose taken at 8 P.M. the day before surgery. The patients were followed up for a minimum of 4 weeks after surgery. The patients who received wound infection prophylaxis had 5.1 percent infections compared with 20.5 percent in the placebo group (p = 0.00009). Eighty percent of all wound infections were first seen after discharge, explaining why plastic surgeons might overlook their infectious complications. There was a significant reduction in postoperative complications (p = 0.04) and in the additional use of antibiotics postoperatively (p = 0.007) in the prophylaxis group. Subgroup analysis showed a significant reduction in surgical infections in breast surgery (p < 0.05) and reconstructive surgery with flaps (p < 0.05). No effect of the prophylactic regime was demonstrated in patients undergoing secondary surgery for cleft lip and palate disease.', 'title': 'A prospective, double-blind, placebo-controlled trial of a single dose of azithromycin on postoperative wound infections in plastic surgery.', 'date': '1995-11-01'}, '9776150': {'article_id': '9776150', 'content': 'Antibiotic prophylaxis is controversial in patients undergoing axillary lymph node dissection (ALND). We determined whether preoperative antibiotics decreased incidence or treatment cost of infectious complications following ALND.\nTwo hundred patients entered this prospective, randomized, double-blind trial. Patients received either placebo or cefonicid preoperatively. Loco-regional signs of infection were monitored for 4 weeks postoperatively.\nThere was a trend toward fewer infections in the prophylactic group (placebo 13% versus cefonicid 6%; P = 0.080). Cefonicid significantly decreased severe infections requiring hospitalization (placebo 8% versus cefonicid 1%; P = 0.033). Cefonicid also decreased the treatment cost of infection per patient ($49.80 versus $364.87).\nWe demonstrated a trend toward fewer overall infections and significantly fewer severe infections in patients given prophylactic antibiotics, which translated into a decrease in the cost of treatment for infectious complications. These findings support antibiotic prophylaxis for patients undergoing ALND.', 'title': 'Prospective, randomized, double-blind study of prophylactic antibiotics in axillary lymph node dissection.', 'date': '1998-10-17'}, '23052809': {'article_id': '23052809', 'content': 'The effectiveness of antibiotic prophylaxis for prevention of surgical site infection (SSI) following specific types of breast cancer surgery remains uncertain. This study assessed the effectiveness of prophylaxis in modified radical mastectomy (MRM).\nWomen undergoing MRM for breast cancer were recruited. Women were excluded who had diabetes mellitus, severe malnutrition or known allergy to cephalosporins; were receiving corticosteroid therapy or were treated with antibiotics within one week prior to surgery; were scheduled for simultaneous breast reconstruction or bilateral oophorectomy; had existing local infection. Participants were randomized to receive either intravenous cefazolin 1 g or placebo within 30 min prior to skin incision. Standard skin preparation and operative technique for MRM were carried out. Wounds were assessed for SSI and other complications weekly for 30 days.\nA total of 254 women were recruited. Age, clinical stage, prior chemotherapy, and operative time were similar for antibiotic and placebo groups. The overall incidence of SSI was 14.2 %. There were no significant differences in the infection rate over the 30-day follow-up period between the placebo and antibiotic groups (15 % vs 13.4 %; p = 0.719) or at each week. The majority of SSI were either cellulitis or superficial infection for both groups. There were no significant differences between groups in treatments required for SSI, incidence of hematoma or seroma.\nThe findings of this study, alone and when meta-analyzed with data from studies in similar surgical populations, do not support the use of antibiotic prophylaxis in MRM.', 'title': 'A randomized, double-blinded placebo-controlled clinical trial of the routine use of preoperative antibiotic prophylaxis in modified radical mastectomy.', 'date': '2012-10-12'}, '10873356': {'article_id': '10873356', 'content': 'Antibiotic prophylaxis has been used to good effect in the prevention of post-operative wound infections in patients undergoing gastrointestinal operations. We have assessed the use of a single dose of intravenous antibiotic (Augmentin 1.2 g), given with induction of anaesthesia as prophylaxis, against post-operative wound infection in women undergoing clean, elective breast surgery. Three hundred and thirty-four patients were recruited. Of the 164 receiving antibiotic prophylaxis 29 (17.7%) had wound infections compared with 32 (18.8%) in the placebo group (P=0.79). There were no significant differences in any other post-operative infective complications. Antibiotic prophylaxis is probably not required in clean, elective breast surgery.', 'title': 'Antibiotic prophylaxis for post-operative wound infection in clean elective breast surgery.', 'date': '2000-06-30'}, '19673597': {'article_id': '19673597', 'content': 'Preoperative core needle biopsies may increase the risk of surgical site infection (SSI) in breast cancer surgery. The purpose of this randomized trial was to determine whether a prophylactic antibiotic would prevent SSI under these conditions.\nImaging-guided multiple core needle biopsies were performed one to two weeks prior to surgery to obtain confirmation of the presence of breast cancer. Then the patients were randomized to receive either a single intravenous dose of 1.0 g of dicloxacillin (n = 144) or placebo infusion of saline (n = 148) 30 min prior to operation. After breast surgery, incisional morbidity was monitored for 30 days. The number of SSIs was compared with that in 672 patients treated before the implementation of core needle biopsies.\nThe patient characteristics and risk factors for SSI were similar in the antibiotic prophylaxis and placebo groups. The incidence of SSI was 7.2% (21/292) in the prospective trial compared with 6.8% (46/672) in the retrospective cohort (p = 0.890). The incidence of postoperative SSIs was 5.6% (8/144) in the dicloxacillin group and 8.8% (13/148) in the placebo group (p = 0.371). For the first two weeks, there was a non-significant trend to fewer SSIs in the antibiotic group (n = 1) than the placebo group (n = 4). Body mass index, smoking, or previous illness did not affect the likelihood of SSI.\nCore needle biopsy did not increase the incidence of SSI. Antibiotic prophylaxis did not prevent SSI, probably because so few infections occurred.', 'title': 'Does preoperative core needle biopsy increase surgical site infections in breast cancer surgery? Randomized study of antibiotic prophylaxis.', 'date': '2009-08-14'}, '2403655': {'article_id': '2403655', 'content': 'We assessed the efficacy of perioperative antibiotic prophylaxis for surgery in a randomized, double-blind trial of 1218 patients undergoing herniorrhaphy or surgery involving the breast, including excision of a breast mass, mastectomy, reduction mammoplasty, and axillary-node dissection. The prophylactic regimen was a single dose of cefonicid (1 g intravenously) administered approximately half an hour before surgery. The patients were followed up for four to six weeks after surgery. Blinding was maintained until the last patient completed the follow-up and all diagnoses of infection had been made. The patients who received prophylaxis had 48 percent fewer probable or definite infections than those who did not (Mantel-Haenszel risk ratio, 0.52; 95 percent confidence interval, 0.32 to 0.84; P = 0.01). For patients undergoing a procedure involving the breast, infection occurred in 6.6 percent of the cefonicid recipients (20 of 303) and 12.2 percent of the placebo recipients (37 of 303); for those undergoing herniorrhaphy, infection occurred in 2.3 percent of the cefonicid recipients (7 of 301) and 4.2 percent of the placebo recipients (13 of 311). There were comparable reductions in the numbers of definite wound infections (Mantel-Haenszel risk ratio, 0.49), wounds that drained pus (risk ratio, 0.43), Staphylococcus aureus wound isolates (risk ratio, 0.49), and urinary tract infections (risk ratio, 0.40). There were also comparable reductions in the need for postoperative antibiotic therapy, non-routine visits to a physician for problems involving wound healing, incision and drainage procedures, and readmission because of problems with wound healing. We conclude that perioperative antibiotic prophylaxis with cefonicid is useful for herniorrhaphy and certain types of breast surgery.', 'title': 'Perioperative antibiotic prophylaxis for herniorrhaphy and breast surgery.', 'date': '1990-01-18'}, '2403697': {'article_id': '2403697', 'content': 'The ability of perioperative cefazolin to reduce the incidence of postoperative wound infection in patients undergoing ablative surgical treatment for carcinoma of the breast was tested in this prospective, randomized, double-blinded study. From May 1983 until December 1985, 118 women were divided into two groups at random. Group 1 consisted of 59 patients and received cefazolin and group 2 was made up of 59 patients who received a placebo. The groups were similar with respect to age, operative procedure, operative time and time to discharge after operation. Three infections occurred among those in group 1 and five among those in group 2 (p = 0.72). The time to onset of infection was delayed in the patients in group 1 versus those in group 2 (17.7 days versus 9.6 days, p = 0.04). Six of eight infections occurred in patients in whom an interval between biopsy and definitive surgical treatment was present. Prophylactic antibiotics in mammary operations did not reduce postoperative wound infections in this study.', 'title': 'A prospective, randomized double-blind study of the use of antibiotics at the time of mastectomy.', 'date': '1990-01-01'}}",0.1,Surgery
87,"Is the rate of hospital readmission higher, lower, or the same when comparing prophylactic antibiotics administered preoperatively to placebo?",uncertain effect,low,no,"['9776150', '2403655']",31557310,2019,"{'9776150': {'article_id': '9776150', 'content': 'Antibiotic prophylaxis is controversial in patients undergoing axillary lymph node dissection (ALND). We determined whether preoperative antibiotics decreased incidence or treatment cost of infectious complications following ALND.\nTwo hundred patients entered this prospective, randomized, double-blind trial. Patients received either placebo or cefonicid preoperatively. Loco-regional signs of infection were monitored for 4 weeks postoperatively.\nThere was a trend toward fewer infections in the prophylactic group (placebo 13% versus cefonicid 6%; P = 0.080). Cefonicid significantly decreased severe infections requiring hospitalization (placebo 8% versus cefonicid 1%; P = 0.033). Cefonicid also decreased the treatment cost of infection per patient ($49.80 versus $364.87).\nWe demonstrated a trend toward fewer overall infections and significantly fewer severe infections in patients given prophylactic antibiotics, which translated into a decrease in the cost of treatment for infectious complications. These findings support antibiotic prophylaxis for patients undergoing ALND.', 'title': 'Prospective, randomized, double-blind study of prophylactic antibiotics in axillary lymph node dissection.', 'date': '1998-10-17'}, '2403655': {'article_id': '2403655', 'content': 'We assessed the efficacy of perioperative antibiotic prophylaxis for surgery in a randomized, double-blind trial of 1218 patients undergoing herniorrhaphy or surgery involving the breast, including excision of a breast mass, mastectomy, reduction mammoplasty, and axillary-node dissection. The prophylactic regimen was a single dose of cefonicid (1 g intravenously) administered approximately half an hour before surgery. The patients were followed up for four to six weeks after surgery. Blinding was maintained until the last patient completed the follow-up and all diagnoses of infection had been made. The patients who received prophylaxis had 48 percent fewer probable or definite infections than those who did not (Mantel-Haenszel risk ratio, 0.52; 95 percent confidence interval, 0.32 to 0.84; P = 0.01). For patients undergoing a procedure involving the breast, infection occurred in 6.6 percent of the cefonicid recipients (20 of 303) and 12.2 percent of the placebo recipients (37 of 303); for those undergoing herniorrhaphy, infection occurred in 2.3 percent of the cefonicid recipients (7 of 301) and 4.2 percent of the placebo recipients (13 of 311). There were comparable reductions in the numbers of definite wound infections (Mantel-Haenszel risk ratio, 0.49), wounds that drained pus (risk ratio, 0.43), Staphylococcus aureus wound isolates (risk ratio, 0.49), and urinary tract infections (risk ratio, 0.40). There were also comparable reductions in the need for postoperative antibiotic therapy, non-routine visits to a physician for problems involving wound healing, incision and drainage procedures, and readmission because of problems with wound healing. We conclude that perioperative antibiotic prophylaxis with cefonicid is useful for herniorrhaphy and certain types of breast surgery.', 'title': 'Perioperative antibiotic prophylaxis for herniorrhaphy and breast surgery.', 'date': '1990-01-18'}}",0.0,Surgery
88,"Is the incidence of surgical site infection (SSI) for patients undergoing breast cancer surgery without reconstruction higher, lower, or the same when comparing prophylactic antibiotics administered preoperatively to no antibiotic?",lower,moderate,no,"['11063500', '23001082', '16989011']",31557310,2019,"{'11063500': {'article_id': '11063500', 'content': 'Based on the observation that administration of clarithromycin led to an attenuation of the inflammatory response induced by surgical trauma in a guinea pig model, we investigated the potential beneficial effects of clarithromycin on the local and systemic inflammatory response in patients undergoing mastectomy in an open-label prospective study. During a 16-month period, 54 patients who underwent mastectomy were randomly divided into two groups. In one group, the patients received oral clarithromycin at a dose of 500 mg twice a day, from the day before to 3 days after mastectomy. There was no significant difference in the incidence of antibiotic prophylaxis-related toxicities or postoperative infections between the patients who received clarithromycin and those who did not. Clarithromycin treatment was significantly associated with an attenuation of febrile response, tachycardia, tachypnea, and an increase in monocyte counts (P, <0.0001, <0.01, <0.05, and <0.01, respectively). Clarithromycin also reduced the intensity and duration of postoperative pain (P, <0.05 and <0.005, respectively) and increased the range of motion of the involved shoulder (P < 0.05 for abduction and flexion). We conclude that clarithromycin effectively modulates the acute inflammatory response associated with mastectomy and produces a better clinical outcome.', 'title': 'Clarithromycin attenuates mastectomy-induced acute inflammatory response.', 'date': '2000-11-04'}, '23001082': {'article_id': '23001082', 'content': 'To assess the impact of prophylactic antibiotics on the prevention of surgical site infection (SSI) and the cost-effectiveness of this prophylaxis for breast cancer surgery in overweight or obese women.\nSSI is higher than expected after breast surgery. Obesity was found to be one of the risk factors.\nThe trial was designed as a phase IV randomized, controlled, parallel-group efficacy trial. It was conducted at a tertiary university hospital. Overweight or obese women with clinically early-stage breast cancer who had been assigned to undergo surgery were eligible. Patients were randomly allocated to either a prophylaxis or a control group by using a computer-generated list. The prophylaxis group received 1 g ampicillin-sulbactam intravenously at anesthesia. The control group received no intervention. Patients and observers were blinded to the assignments. The primary outcome was the comparison of SSI incidences of the 2 groups. Patients were monitored for 30 days.\nA total of 369 patients were included in final analysis, out of which 187 were allocated for prophylaxis and 182 were randomly assigned to the control group. Analysis was done according to the intention-to-treat principle. Prophylaxis significantly reduced the SSI rate (4.8%) in the prophylaxis group when compared with that in the control group [13.7%; relative risk (RR) 0.35; 95% CI: 0.17-0.73]. No adverse reaction was observed. The mean SSI-related cost (20.26 USD) was found to be significantly higher in the control group when compared with that (8.48 USD) in the prophylaxis group.\nAntibiotic prophylaxis significantly decreased SSI incidence after elective surgery and was shown to be cost-effective in obese breast cancer patients. ClinicalTrials.gov Identifier: NCT00356148.', 'title': 'Efficacy of prophylactic antibiotic administration for breast cancer surgery in overweight or obese patients: a randomized controlled trial.', 'date': '2012-09-25'}, '16989011': {'article_id': '16989011', 'content': 'The aim of this randomized clinical trial was to determine whether a single intravenous dose of 2 g flucloxacillin could prevent wound infection after primary non-reconstructive breast surgery.\nThe study included 618 patients undergoing local excision (n = 490), mastectomy (n = 107) or microdochectomy (n = 21). Patients were randomized to receive either a single dose of flucloxacillin immediately after the induction of anaesthesia or no intervention. Wound morbidity was monitored by an independent research nurse for 42 days after surgery.\nThe incidence of wound infection was similar in the two groups: 10 of 311 (3.2 percent) in the flucloxacillin group and 14 of 307 (4.6 percent) in the control group (chi(2) = 0.75, P = 0.387; relative risk 0.71, 95 percent confidence interval 0.32 to 1.53). The groups also had similar wound scores and rates of moderate or severe cellulitis. Wound infection presented a median of 16 days after surgery.\nThe administration of a single dose of flucloxacillin failed to reduce the rate of wound infection after non-reconstructive breast surgery.', 'title': 'Randomized clinical trial of single-dose antibiotic prophylaxis for non-reconstructive breast surgery.', 'date': '2006-09-22'}}",0.0,Surgery
89,"Is the incidence of surgical site infection (SSI) for patients undergoing breast cancer surgery without reconstruction higher, lower, or the same when comparing prophylactic antibiotics administered preoperatively to placebo?",lower,moderate,no,"['7480237', '9776150', '23052809', '10873356', '19673597', '2403655', '2403697']",31557310,2019,"{'7480237': {'article_id': '7480237', 'content': 'Over a 9-month period from September of 1991 to May of 1992, 339 patients were included in a randomized, double-blind, placebo-controlled study using azithromycin as the prophylactic agent to determine whether it effects a clinically meaningful reduction in postoperative surgical infections in plastic surgery. Azithromycin was given as prophylaxis in 171 patients and placebo in 168 patients. The study medication was a single oral dose taken at 8 P.M. the day before surgery. The patients were followed up for a minimum of 4 weeks after surgery. The patients who received wound infection prophylaxis had 5.1 percent infections compared with 20.5 percent in the placebo group (p = 0.00009). Eighty percent of all wound infections were first seen after discharge, explaining why plastic surgeons might overlook their infectious complications. There was a significant reduction in postoperative complications (p = 0.04) and in the additional use of antibiotics postoperatively (p = 0.007) in the prophylaxis group. Subgroup analysis showed a significant reduction in surgical infections in breast surgery (p < 0.05) and reconstructive surgery with flaps (p < 0.05). No effect of the prophylactic regime was demonstrated in patients undergoing secondary surgery for cleft lip and palate disease.', 'title': 'A prospective, double-blind, placebo-controlled trial of a single dose of azithromycin on postoperative wound infections in plastic surgery.', 'date': '1995-11-01'}, '9776150': {'article_id': '9776150', 'content': 'Antibiotic prophylaxis is controversial in patients undergoing axillary lymph node dissection (ALND). We determined whether preoperative antibiotics decreased incidence or treatment cost of infectious complications following ALND.\nTwo hundred patients entered this prospective, randomized, double-blind trial. Patients received either placebo or cefonicid preoperatively. Loco-regional signs of infection were monitored for 4 weeks postoperatively.\nThere was a trend toward fewer infections in the prophylactic group (placebo 13% versus cefonicid 6%; P = 0.080). Cefonicid significantly decreased severe infections requiring hospitalization (placebo 8% versus cefonicid 1%; P = 0.033). Cefonicid also decreased the treatment cost of infection per patient ($49.80 versus $364.87).\nWe demonstrated a trend toward fewer overall infections and significantly fewer severe infections in patients given prophylactic antibiotics, which translated into a decrease in the cost of treatment for infectious complications. These findings support antibiotic prophylaxis for patients undergoing ALND.', 'title': 'Prospective, randomized, double-blind study of prophylactic antibiotics in axillary lymph node dissection.', 'date': '1998-10-17'}, '23052809': {'article_id': '23052809', 'content': 'The effectiveness of antibiotic prophylaxis for prevention of surgical site infection (SSI) following specific types of breast cancer surgery remains uncertain. This study assessed the effectiveness of prophylaxis in modified radical mastectomy (MRM).\nWomen undergoing MRM for breast cancer were recruited. Women were excluded who had diabetes mellitus, severe malnutrition or known allergy to cephalosporins; were receiving corticosteroid therapy or were treated with antibiotics within one week prior to surgery; were scheduled for simultaneous breast reconstruction or bilateral oophorectomy; had existing local infection. Participants were randomized to receive either intravenous cefazolin 1 g or placebo within 30 min prior to skin incision. Standard skin preparation and operative technique for MRM were carried out. Wounds were assessed for SSI and other complications weekly for 30 days.\nA total of 254 women were recruited. Age, clinical stage, prior chemotherapy, and operative time were similar for antibiotic and placebo groups. The overall incidence of SSI was 14.2 %. There were no significant differences in the infection rate over the 30-day follow-up period between the placebo and antibiotic groups (15 % vs 13.4 %; p = 0.719) or at each week. The majority of SSI were either cellulitis or superficial infection for both groups. There were no significant differences between groups in treatments required for SSI, incidence of hematoma or seroma.\nThe findings of this study, alone and when meta-analyzed with data from studies in similar surgical populations, do not support the use of antibiotic prophylaxis in MRM.', 'title': 'A randomized, double-blinded placebo-controlled clinical trial of the routine use of preoperative antibiotic prophylaxis in modified radical mastectomy.', 'date': '2012-10-12'}, '10873356': {'article_id': '10873356', 'content': 'Antibiotic prophylaxis has been used to good effect in the prevention of post-operative wound infections in patients undergoing gastrointestinal operations. We have assessed the use of a single dose of intravenous antibiotic (Augmentin 1.2 g), given with induction of anaesthesia as prophylaxis, against post-operative wound infection in women undergoing clean, elective breast surgery. Three hundred and thirty-four patients were recruited. Of the 164 receiving antibiotic prophylaxis 29 (17.7%) had wound infections compared with 32 (18.8%) in the placebo group (P=0.79). There were no significant differences in any other post-operative infective complications. Antibiotic prophylaxis is probably not required in clean, elective breast surgery.', 'title': 'Antibiotic prophylaxis for post-operative wound infection in clean elective breast surgery.', 'date': '2000-06-30'}, '19673597': {'article_id': '19673597', 'content': 'Preoperative core needle biopsies may increase the risk of surgical site infection (SSI) in breast cancer surgery. The purpose of this randomized trial was to determine whether a prophylactic antibiotic would prevent SSI under these conditions.\nImaging-guided multiple core needle biopsies were performed one to two weeks prior to surgery to obtain confirmation of the presence of breast cancer. Then the patients were randomized to receive either a single intravenous dose of 1.0 g of dicloxacillin (n = 144) or placebo infusion of saline (n = 148) 30 min prior to operation. After breast surgery, incisional morbidity was monitored for 30 days. The number of SSIs was compared with that in 672 patients treated before the implementation of core needle biopsies.\nThe patient characteristics and risk factors for SSI were similar in the antibiotic prophylaxis and placebo groups. The incidence of SSI was 7.2% (21/292) in the prospective trial compared with 6.8% (46/672) in the retrospective cohort (p = 0.890). The incidence of postoperative SSIs was 5.6% (8/144) in the dicloxacillin group and 8.8% (13/148) in the placebo group (p = 0.371). For the first two weeks, there was a non-significant trend to fewer SSIs in the antibiotic group (n = 1) than the placebo group (n = 4). Body mass index, smoking, or previous illness did not affect the likelihood of SSI.\nCore needle biopsy did not increase the incidence of SSI. Antibiotic prophylaxis did not prevent SSI, probably because so few infections occurred.', 'title': 'Does preoperative core needle biopsy increase surgical site infections in breast cancer surgery? Randomized study of antibiotic prophylaxis.', 'date': '2009-08-14'}, '2403655': {'article_id': '2403655', 'content': 'We assessed the efficacy of perioperative antibiotic prophylaxis for surgery in a randomized, double-blind trial of 1218 patients undergoing herniorrhaphy or surgery involving the breast, including excision of a breast mass, mastectomy, reduction mammoplasty, and axillary-node dissection. The prophylactic regimen was a single dose of cefonicid (1 g intravenously) administered approximately half an hour before surgery. The patients were followed up for four to six weeks after surgery. Blinding was maintained until the last patient completed the follow-up and all diagnoses of infection had been made. The patients who received prophylaxis had 48 percent fewer probable or definite infections than those who did not (Mantel-Haenszel risk ratio, 0.52; 95 percent confidence interval, 0.32 to 0.84; P = 0.01). For patients undergoing a procedure involving the breast, infection occurred in 6.6 percent of the cefonicid recipients (20 of 303) and 12.2 percent of the placebo recipients (37 of 303); for those undergoing herniorrhaphy, infection occurred in 2.3 percent of the cefonicid recipients (7 of 301) and 4.2 percent of the placebo recipients (13 of 311). There were comparable reductions in the numbers of definite wound infections (Mantel-Haenszel risk ratio, 0.49), wounds that drained pus (risk ratio, 0.43), Staphylococcus aureus wound isolates (risk ratio, 0.49), and urinary tract infections (risk ratio, 0.40). There were also comparable reductions in the need for postoperative antibiotic therapy, non-routine visits to a physician for problems involving wound healing, incision and drainage procedures, and readmission because of problems with wound healing. We conclude that perioperative antibiotic prophylaxis with cefonicid is useful for herniorrhaphy and certain types of breast surgery.', 'title': 'Perioperative antibiotic prophylaxis for herniorrhaphy and breast surgery.', 'date': '1990-01-18'}, '2403697': {'article_id': '2403697', 'content': 'The ability of perioperative cefazolin to reduce the incidence of postoperative wound infection in patients undergoing ablative surgical treatment for carcinoma of the breast was tested in this prospective, randomized, double-blinded study. From May 1983 until December 1985, 118 women were divided into two groups at random. Group 1 consisted of 59 patients and received cefazolin and group 2 was made up of 59 patients who received a placebo. The groups were similar with respect to age, operative procedure, operative time and time to discharge after operation. Three infections occurred among those in group 1 and five among those in group 2 (p = 0.72). The time to onset of infection was delayed in the patients in group 1 versus those in group 2 (17.7 days versus 9.6 days, p = 0.04). Six of eight infections occurred in patients in whom an interval between biopsy and definitive surgical treatment was present. Prophylactic antibiotics in mammary operations did not reduce postoperative wound infections in this study.', 'title': 'A prospective, randomized double-blind study of the use of antibiotics at the time of mastectomy.', 'date': '1990-01-01'}}",0.142857143,Surgery
90,"Is the cost of care higher, lower, or the same when comparing prophylactic antibiotics administered preoperatively to no antibiotic or placebo?",uncertain effect,low,no,"['23001082', '9776150']",31557310,2019,"{'23001082': {'article_id': '23001082', 'content': 'To assess the impact of prophylactic antibiotics on the prevention of surgical site infection (SSI) and the cost-effectiveness of this prophylaxis for breast cancer surgery in overweight or obese women.\nSSI is higher than expected after breast surgery. Obesity was found to be one of the risk factors.\nThe trial was designed as a phase IV randomized, controlled, parallel-group efficacy trial. It was conducted at a tertiary university hospital. Overweight or obese women with clinically early-stage breast cancer who had been assigned to undergo surgery were eligible. Patients were randomly allocated to either a prophylaxis or a control group by using a computer-generated list. The prophylaxis group received 1 g ampicillin-sulbactam intravenously at anesthesia. The control group received no intervention. Patients and observers were blinded to the assignments. The primary outcome was the comparison of SSI incidences of the 2 groups. Patients were monitored for 30 days.\nA total of 369 patients were included in final analysis, out of which 187 were allocated for prophylaxis and 182 were randomly assigned to the control group. Analysis was done according to the intention-to-treat principle. Prophylaxis significantly reduced the SSI rate (4.8%) in the prophylaxis group when compared with that in the control group [13.7%; relative risk (RR) 0.35; 95% CI: 0.17-0.73]. No adverse reaction was observed. The mean SSI-related cost (20.26 USD) was found to be significantly higher in the control group when compared with that (8.48 USD) in the prophylaxis group.\nAntibiotic prophylaxis significantly decreased SSI incidence after elective surgery and was shown to be cost-effective in obese breast cancer patients. ClinicalTrials.gov Identifier: NCT00356148.', 'title': 'Efficacy of prophylactic antibiotic administration for breast cancer surgery in overweight or obese patients: a randomized controlled trial.', 'date': '2012-09-25'}, '9776150': {'article_id': '9776150', 'content': 'Antibiotic prophylaxis is controversial in patients undergoing axillary lymph node dissection (ALND). We determined whether preoperative antibiotics decreased incidence or treatment cost of infectious complications following ALND.\nTwo hundred patients entered this prospective, randomized, double-blind trial. Patients received either placebo or cefonicid preoperatively. Loco-regional signs of infection were monitored for 4 weeks postoperatively.\nThere was a trend toward fewer infections in the prophylactic group (placebo 13% versus cefonicid 6%; P = 0.080). Cefonicid significantly decreased severe infections requiring hospitalization (placebo 8% versus cefonicid 1%; P = 0.033). Cefonicid also decreased the treatment cost of infection per patient ($49.80 versus $364.87).\nWe demonstrated a trend toward fewer overall infections and significantly fewer severe infections in patients given prophylactic antibiotics, which translated into a decrease in the cost of treatment for infectious complications. These findings support antibiotic prophylaxis for patients undergoing ALND.', 'title': 'Prospective, randomized, double-blind study of prophylactic antibiotics in axillary lymph node dissection.', 'date': '1998-10-17'}}",0.0,Surgery
91,"Is the likelihood of detoxification at six‐month follow‐up higher, lower, or the same when comparing dihydrocodeine (DHC) to buprenorphine?",no difference,low,no,"['19196468', '17210079']",32068247,2020,"{'19196468': {'article_id': '19196468', 'content': 'Many opiate users entering British prisons require prescribed medication to help them achieve abstinence. This commonly takes the form of a detoxification regime. Previously, a range of detoxification agents have been prescribed without a clear evidence base to recommend a drug of choice. There are few trials and very few in the prison setting. This study compares dihydrocodeine with buprenorphine.\nOpen label, pragmatic, randomised controlled trial in a large remand prison in the North of England. Ninety adult male prisoners requesting an opiate detoxification were randomised to receive either daily sublingual buprenorphine or daily oral dihydrocodeine, given in the context of routine care. All participants gave written, informed consent. Reducing regimens were within a standard regimen of not more than 20 days and were at the discretion of the prescribing doctor. Primary outcome was abstinence from illicit opiates as indicated by a urine test at five days post detoxification. Secondary outcomes were collected during the detoxification period and then at one, three and six months post detoxification. Analysis was undertaken using relative risk tests for categorical data and unpaired t-tests for continuous data.\n64% of those approached took part in the study. 63 men (70%) gave a urine sample at five days post detoxification. At the completion of detoxification, by intention to treat analysis, a higher proportion of people allocated to buprenorphine provided a urine sample negative for opiates (abstinent) compared with those who received dihydrocodeine (57% vs 35%, RR 1.61 CI 1.02-2.56). At the 1, 3 and 6 month follow-up points, there were no significant differences for urine samples negative for opiates between the two groups. Follow up rates were low for those participants who had subsequently been released into the community.\nThese findings would suggest that dihydrocodeine should not be routinely used for detoxification from opiates in the prison setting. The high relapse rate amongst those achieving abstinence would suggest the need for an increased emphasis upon opiate maintenance programmes in the prison setting.\nCurrent Controlled Trials ISRCTN07752728.', 'title': 'The Leeds Evaluation of Efficacy of Detoxification Study (LEEDS) prisons project: a randomised controlled trial comparing dihydrocodeine and buprenorphine for opiate detoxification.', 'date': '2009-02-07'}, '17210079': {'article_id': '17210079', 'content': 'Many drug users present to primary care requesting detoxification from illicit opiates. There are a number of detoxification agents but no recommended drug of choice. The purpose of this study is to compare buprenorphine with dihydrocodeine for detoxification from illicit opiates in primary care.\nOpen label randomised controlled trial in NHS Primary Care (General Practices), Leeds, UK. Sixty consenting adults using illicit opiates received either daily sublingual buprenorphine or daily oral dihydrocodeine. Reducing regimens for both interventions were at the discretion of prescribing doctor within a standard regimen of not more than 15 days. Primary outcome was abstinence from illicit opiates at final prescription as indicated by a urine sample. Secondary outcomes during detoxification period and at three and six months post detoxification were recorded.\nOnly 23% completed the prescribed course of detoxification medication and gave a urine sample on collection of their final prescription. Risk of non-completion of detoxification was reduced if allocated buprenorphine (68% vs 88%, RR 0.58 CI 0.35-0.96, p = 0.065). A higher proportion of people allocated to buprenorphine provided a clean urine sample compared with those who received dihydrocodeine (21% vs 3%, RR 2.06 CI 1.33-3.21, p = 0.028). People allocated to buprenorphine had fewer visits to professional carers during detoxification and more were abstinent at three months (10 vs 4, RR 1.55 CI 0.96-2.52) and six months post detoxification (7 vs 3, RR 1.45 CI 0.84-2.49).\nInformative randomised trials evaluating routine care within the primary care setting are possible amongst drug using populations. This small study generates unique data on commonly used treatment regimens.', 'title': 'Buprenorphine versus dihydrocodeine for opiate detoxification in primary care: a randomised controlled trial.', 'date': '2007-01-11'}}",1.0,Psychiatry & Neurology
92,"Is the likelihood of treatment retention higher, lower, or the same when comparing dihydrocodeine (DHC) to buprenorphine?",no difference,low,yes,"['19196468', '17210079']",32068247,2020,"{'19196468': {'article_id': '19196468', 'content': 'Many opiate users entering British prisons require prescribed medication to help them achieve abstinence. This commonly takes the form of a detoxification regime. Previously, a range of detoxification agents have been prescribed without a clear evidence base to recommend a drug of choice. There are few trials and very few in the prison setting. This study compares dihydrocodeine with buprenorphine.\nOpen label, pragmatic, randomised controlled trial in a large remand prison in the North of England. Ninety adult male prisoners requesting an opiate detoxification were randomised to receive either daily sublingual buprenorphine or daily oral dihydrocodeine, given in the context of routine care. All participants gave written, informed consent. Reducing regimens were within a standard regimen of not more than 20 days and were at the discretion of the prescribing doctor. Primary outcome was abstinence from illicit opiates as indicated by a urine test at five days post detoxification. Secondary outcomes were collected during the detoxification period and then at one, three and six months post detoxification. Analysis was undertaken using relative risk tests for categorical data and unpaired t-tests for continuous data.\n64% of those approached took part in the study. 63 men (70%) gave a urine sample at five days post detoxification. At the completion of detoxification, by intention to treat analysis, a higher proportion of people allocated to buprenorphine provided a urine sample negative for opiates (abstinent) compared with those who received dihydrocodeine (57% vs 35%, RR 1.61 CI 1.02-2.56). At the 1, 3 and 6 month follow-up points, there were no significant differences for urine samples negative for opiates between the two groups. Follow up rates were low for those participants who had subsequently been released into the community.\nThese findings would suggest that dihydrocodeine should not be routinely used for detoxification from opiates in the prison setting. The high relapse rate amongst those achieving abstinence would suggest the need for an increased emphasis upon opiate maintenance programmes in the prison setting.\nCurrent Controlled Trials ISRCTN07752728.', 'title': 'The Leeds Evaluation of Efficacy of Detoxification Study (LEEDS) prisons project: a randomised controlled trial comparing dihydrocodeine and buprenorphine for opiate detoxification.', 'date': '2009-02-07'}, '17210079': {'article_id': '17210079', 'content': 'Many drug users present to primary care requesting detoxification from illicit opiates. There are a number of detoxification agents but no recommended drug of choice. The purpose of this study is to compare buprenorphine with dihydrocodeine for detoxification from illicit opiates in primary care.\nOpen label randomised controlled trial in NHS Primary Care (General Practices), Leeds, UK. Sixty consenting adults using illicit opiates received either daily sublingual buprenorphine or daily oral dihydrocodeine. Reducing regimens for both interventions were at the discretion of prescribing doctor within a standard regimen of not more than 15 days. Primary outcome was abstinence from illicit opiates at final prescription as indicated by a urine sample. Secondary outcomes during detoxification period and at three and six months post detoxification were recorded.\nOnly 23% completed the prescribed course of detoxification medication and gave a urine sample on collection of their final prescription. Risk of non-completion of detoxification was reduced if allocated buprenorphine (68% vs 88%, RR 0.58 CI 0.35-0.96, p = 0.065). A higher proportion of people allocated to buprenorphine provided a clean urine sample compared with those who received dihydrocodeine (21% vs 3%, RR 2.06 CI 1.33-3.21, p = 0.028). People allocated to buprenorphine had fewer visits to professional carers during detoxification and more were abstinent at three months (10 vs 4, RR 1.55 CI 0.96-2.52) and six months post detoxification (7 vs 3, RR 1.45 CI 0.84-2.49).\nInformative randomised trials evaluating routine care within the primary care setting are possible amongst drug using populations. This small study generates unique data on commonly used treatment regimens.', 'title': 'Buprenorphine versus dihydrocodeine for opiate detoxification in primary care: a randomised controlled trial.', 'date': '2007-01-11'}}",0.0,Psychiatry & Neurology
93,"Is the time to postvoid residual volume of urine ≤ 50 mL higher, lower, or the same when comparing nerve‐sparing radical hysterectomy to standard radical hysterectomy?",lower,low,no,"['22209773', '25872890']",30746689,2019,"{'22209773': {'article_id': '22209773', 'content': 'This study evaluated histopathology and clinical outcome of autonomic nerve trauma and vessels removal within the cardinal ligament (CL) during nerve-sparing radical hysterectomy (NSRH) compared with radical hysterectomy (RH).\n25 women with FIGO stage Ib1-IIa cervical cancer underwent RH (n=13) or NSRH (n=12). Removed CLs lengths were measured. Biopsies were collected from the proximal, middle and distal segment of CLs and fixed. Different markers were used for immunohistochemisty analysis: tyrosine hydroxylase for sympathetic nerves; vasoactive intestinal polypeptide for parasympathetic nerves; CD34 for blood vessels; and D2-40 for lymphatic vessels. The volume density (Vv), a parameter of biological stereology, was used to quantitatively measure CL components, while post-operative functions, such as defecation, micturition and two-year disease free survival in RH and NSRH groups were compared.\nThe nerves mainly existed in the middle and distal segments of CLs. The Vv was greater in RH compared with NSRH for both sympathetic and parasympathetic nerve markers (P<0.05), while the Vv of blood and lymphatic vessels were same in the two groups. Average time to achieve residual urine≤50ml and first defecation were shorter in NSRH than in RH (P<0.05).\nLess autonomic nerves within CL are transected in NSRH than in RH, while blood/lymphatic vessels are efficiently removed in both treatments. Compared to RH, NSRH decreases iatrogenic injury, which leads to reduced post-operative co-morbidities, with ensure the same radicality.', 'title': 'Classical and nerve-sparing radical hysterectomy: an evaluation of the nerve trauma in cardinal ligament.', 'date': '2012-01-03'}, '25872890': {'article_id': '25872890', 'content': 'A prospective, randomized controlled trial was conducted to evaluate the efficacy of nerve-sparing radical hysterectomy (NSRH) in preserving bladder function and its oncologic safety in the treatment of cervical cancer.\nFrom March 2003 to November 2005, 92 patients with cervical cancer stage IA2 to IIA were randomly assigned for surgical treatment with conventional radical hysterectomy (CRH) or NSRH, and 86 patients finally included in the analysis. Adequacy of nerve sparing, radicality, bladder function, and oncologic safety were assessed by quantifying the nerve fibers in the paracervix, measuring the extent of paracervix and harvested lymph nodes (LNs), urodynamic study (UDS) with International Prostate Symptom Score (IPSS), and 10-year disease-free survival (DFS), respectively.\nThere were no differences in clinicopathologic characteristics between two groups. The median number of nerve fiber was 12 (range, 6 to 21) and 30 (range, 17 to 45) in the NSRH and CRH, respectively (p<0.001). The extent of resected paracervix and number of LNs were not different between the two groups. Volume of residual urine and bladder compliance were significantly deteriorated at 12 months after CRH. On the contrary, all parameters of UDS were recovered no later than 3 months after NSRH. Evaluation of the IPSS showed that the frequency of long-term urinary symptom was higher in CRH than in the NSRH group. The median duration before the postvoid residual urine volume became less than 50 mL was 11 days (range, 7 to 26 days) in NSRH group and was 18 days (range, 10 to 85 days) in CRH group (p<0.001). No significant difference was observed in the 10-year DFS between two groups.\nNSRH appears to be effective in preserving bladder function without sacrificing oncologic safety.', 'title': 'Efficacy and oncologic safety of nerve-sparing radical hysterectomy for cervical cancer: a randomized controlled trial.', 'date': '2015-04-16'}}",1.0,Surgery
94,"Is the likelihood of disease-free survival higher, lower, or the same when comparing nerve‐sparing radical hysterectomy to standard radical hysterectomy?",no difference,very low,no,['25872890'],30746689,2019,"{'25872890': {'article_id': '25872890', 'content': 'A prospective, randomized controlled trial was conducted to evaluate the efficacy of nerve-sparing radical hysterectomy (NSRH) in preserving bladder function and its oncologic safety in the treatment of cervical cancer.\nFrom March 2003 to November 2005, 92 patients with cervical cancer stage IA2 to IIA were randomly assigned for surgical treatment with conventional radical hysterectomy (CRH) or NSRH, and 86 patients finally included in the analysis. Adequacy of nerve sparing, radicality, bladder function, and oncologic safety were assessed by quantifying the nerve fibers in the paracervix, measuring the extent of paracervix and harvested lymph nodes (LNs), urodynamic study (UDS) with International Prostate Symptom Score (IPSS), and 10-year disease-free survival (DFS), respectively.\nThere were no differences in clinicopathologic characteristics between two groups. The median number of nerve fiber was 12 (range, 6 to 21) and 30 (range, 17 to 45) in the NSRH and CRH, respectively (p<0.001). The extent of resected paracervix and number of LNs were not different between the two groups. Volume of residual urine and bladder compliance were significantly deteriorated at 12 months after CRH. On the contrary, all parameters of UDS were recovered no later than 3 months after NSRH. Evaluation of the IPSS showed that the frequency of long-term urinary symptom was higher in CRH than in the NSRH group. The median duration before the postvoid residual urine volume became less than 50 mL was 11 days (range, 7 to 26 days) in NSRH group and was 18 days (range, 10 to 85 days) in CRH group (p<0.001). No significant difference was observed in the 10-year DFS between two groups.\nNSRH appears to be effective in preserving bladder function without sacrificing oncologic safety.', 'title': 'Efficacy and oncologic safety of nerve-sparing radical hysterectomy for cervical cancer: a randomized controlled trial.', 'date': '2015-04-16'}}",1.0,Surgery
95,"Is functional capacity higher, lower, or the same when comparing prehabilitation to no prehabilitation before colorectal cancer surgery?",higher,very low,no,"['29327644', '25076007']",37162250,2023,"{'29327644': {'article_id': '29327644', 'content': 'Prehabilitation has been previously shown to be more effective in enhancing postoperative functional capacity than rehabilitation alone. The purpose of this study was to determine whether a weekly supervised exercise session could provide further benefit to our current prehabilition program, when comparing to standard post-surgical rehabilitation.\nA parallel-arm single-blind randomized control trial was conducted in patients scheduled for non-metastatic colorectal cancer resection. Patients were assigned to either a once weekly supervised prehabilitation (PREHAB+, n\u2009=\u200941) or standard rehabilitation (REHAB, n\u2009=\u200939) program. Both multimodal programs were home-based program and consisted of moderate intensity aerobic and resistance exercise, nutrition counseling with daily whey protein supplementation and anxiety-reduction strategies. Perioperative care was standardized for both groups as per enhanced recovery after surgery (ERAS\nBoth groups were comparable for baseline walking capacity (PREHAB+: 448\xa0m [IQR 375-525] vs. REHAB: 461\xa0m [419-556], p=.775) and included a similar proportion of patients who improved walking capacity (>20\xa0m) during the preoperative period (PREHAB+: 54% vs. REHAB: 38%, p\u2009=\u2009.222). After surgery, changes in 6MWD were also similar in both groups. In PREHAB+, however, there was a significant association between physical activity energy expenditure and 6MWD (p\u2009<\u2009.01). Previously inactive patients were more likely to improve functional capacity due to PREHAB+ (OR 7.07 [95% CI 1.10-45.51]).\nThe addition of a weekly supervised exercise session to our current prehabilitation program did not further enhance postoperative walking capacity when compared to standard REHAB care. Sedentary patients, however, seemed more likely to benefit from PREHAB+. An association was found between energy spent in physical activity and 6MWD. This information is important to consider when designing cost-effective prehabilitation programs.', 'title': 'Evaluation of supervised multimodal prehabilitation programme in cancer patients undergoing colorectal resection: a randomized control trial.', 'date': '2018-01-13'}, '25076007': {'article_id': '25076007', 'content': 'The preoperative period (prehabilitation) may represent a more appropriate time than the postoperative period to implement an intervention. The impact of prehabilitation on recovery of function al exercise capacity was thus studied in patients undergoing colorectal resection for cancer.\nA parallel-arm single-blind superiority randomized controlled trial was conducted. Seventy-seven patients were randomized to receive either prehabilitation (n = 38) or rehabilitation (n = 39). Both groups received a home-based intervention of moderate aerobic and resistance exercises, nutritional counseling with protein supplementation, and relaxation exercises initiated either 4 weeks before surgery (prehabilitation) or immediately after surgery (rehabilitation), and continued for 8 weeks after surgery. Patients were managed with an enhanced recovery pathway. Primary outcome was functional exercise capacity measured using the validated 6-min walk test.\nMedian duration of prehabilitation was 24.5 days. While awaiting surgery, functional walking capacity increased (≥ 20 m) in a higher proportion of the prehabilitation group compared with the rehabilitation group (53 vs. 15%, adjusted P = 0.006). Complication rates and duration of hospital stay were similar. The difference between baseline and 8-week 6-min walking test was significantly higher in the prehabilitation compared with the rehabilitation group (+23.7 m [SD, 54.8] vs. -21.8 m [SD, 80.7]; mean difference 45.4 m [95% CI, 13.9 to 77.0]). A higher proportion of the prehabilitation group were also recovered to or above baseline exercise capacity at 8 weeks compared with the rehabilitation group (84 vs. 62%, adjusted P = 0.049).\nMeaningful changes in postoperative functional exercise capacity can be achieved with a prehabilitation program.', 'title': 'Prehabilitation versus rehabilitation: a randomized control trial in patients undergoing colorectal resection for cancer.', 'date': '2014-07-31'}}",0.0,Surgery
96,"Is the risk of complications higher, lower, or the same when comparing prehabilitation to no prehabilitation before colorectal cancer surgery?",no difference,low,no,"['31968063', '25076007']",37162250,2023,"{'31968063': {'article_id': '31968063', 'content': 'Research supports use of prehabilitation to optimize physical status before and after colorectal cancer resection, but its effect on postoperative complications remains unclear. Frail patients are a target for prehabilitation interventions owing to increased risk for poor postoperative outcomes.\nTo assess the extent to which a prehabilitation program affects 30-day postoperative complications in frail patients undergoing colorectal cancer resection compared with postoperative rehabilitation.\nThis single-blind, parallel-arm, superiority randomized clinical trial recruited patients undergoing colorectal cancer resection from September 7, 2015, through June 19, 2019. Patients were followed up for 4 weeks before surgery and 4 weeks after surgery at 2 university-affiliated tertiary hospitals. A total of 418 patients 65 years or older were assessed for eligibility. Of these, 298 patients were excluded (not frail [n\u2009=\u2009290], unable to exercise [n\u2009=\u20093], and planned neoadjuvant treatment [n\u2009=\u20095]), and 120 frail patients (Fried Frailty Index,≥2) were randomized. Ten patients were excluded after randomization because they refused surgery (n\u2009=\u20093), died before surgery (n\u2009=\u20093), had no cancer (n\u2009=\u20091), had surgery without bowel resection (n\u2009=\u20091), or were switched to palliative care (n\u2009=\u20092). Hence, 110 patients were included in the intention-to-treat analysis (55 in the prehabilitation [Prehab] and 55 in the rehabilitation [Rehab] groups). Data were analyzed from July 25 through August 21, 2019.\nMultimodal program involving exercise, nutritional, and psychological interventions initiated before (Prehab group) or after (Rehab group) surgery. All patients were treated within a standardized enhanced recovery pathway.\nThe primary outcome included the Comprehensive Complications Index measured at 30 days after surgery. Secondary outcomes were 30-day overall and severe complications, primary and total length of hospital stay, 30-day emergency department visits and hospital readmissions, recovery of walking capacity, and patient-reported outcome measures.\nOf 110 patients randomized, mean (SD) age was 78 (7) years; 52 (47.3%) were men and 58 (52.7%) were women; 31 (28.2%) had rectal cancer; and 87 (79.1%) underwent minimally invasive surgery. There was no between-group difference in the primary outcome measure, 30-day Comprehensive Complications Index (adjusted mean difference, -3.2; 95% CI, -11.8 to 5.3; P\u2009=\u2009.45). Secondary outcome measures were also not different between groups.\nIn frail patients undergoing colorectal cancer resection (predominantly minimally invasive) within an enhanced recovery pathway, a multimodal prehabilitation program did not affect postoperative outcomes. Alternative strategies should be considered to optimize treatment of frail patients preoperatively.\nClinicalTrials.gov identifier: NCT02502760.', 'title': 'Effect of Multimodal Prehabilitation vs Postoperative Rehabilitation on 30-Day Postoperative Complications for Frail Patients Undergoing Resection of Colorectal Cancer: A Randomized Clinical Trial.', 'date': '2020-01-23'}, '25076007': {'article_id': '25076007', 'content': 'The preoperative period (prehabilitation) may represent a more appropriate time than the postoperative period to implement an intervention. The impact of prehabilitation on recovery of function al exercise capacity was thus studied in patients undergoing colorectal resection for cancer.\nA parallel-arm single-blind superiority randomized controlled trial was conducted. Seventy-seven patients were randomized to receive either prehabilitation (n = 38) or rehabilitation (n = 39). Both groups received a home-based intervention of moderate aerobic and resistance exercises, nutritional counseling with protein supplementation, and relaxation exercises initiated either 4 weeks before surgery (prehabilitation) or immediately after surgery (rehabilitation), and continued for 8 weeks after surgery. Patients were managed with an enhanced recovery pathway. Primary outcome was functional exercise capacity measured using the validated 6-min walk test.\nMedian duration of prehabilitation was 24.5 days. While awaiting surgery, functional walking capacity increased (≥ 20 m) in a higher proportion of the prehabilitation group compared with the rehabilitation group (53 vs. 15%, adjusted P = 0.006). Complication rates and duration of hospital stay were similar. The difference between baseline and 8-week 6-min walking test was significantly higher in the prehabilitation compared with the rehabilitation group (+23.7 m [SD, 54.8] vs. -21.8 m [SD, 80.7]; mean difference 45.4 m [95% CI, 13.9 to 77.0]). A higher proportion of the prehabilitation group were also recovered to or above baseline exercise capacity at 8 weeks compared with the rehabilitation group (84 vs. 62%, adjusted P = 0.049).\nMeaningful changes in postoperative functional exercise capacity can be achieved with a prehabilitation program.', 'title': 'Prehabilitation versus rehabilitation: a randomized control trial in patients undergoing colorectal resection for cancer.', 'date': '2014-07-31'}}",0.5,Surgery
97,"Is moderate to severe anaemia prevalence higher, lower, or the same when comparing house modifications to no modifications?",lower,high,yes,"['33838737', '19732949', '33640067']",36200610,2022,"{'33838737': {'article_id': '33838737', 'content': 'In malaria-endemic areas, residents of modern houses have less malaria than those living in traditional houses. We aimed to assess whether children in The Gambia received an incremental benefit from improved housing, where current best practice of insecticide-treated nets, indoor residual spraying, seasonal malaria chemoprevention in children younger than 5 years, and prompt treatment against clinical malaria was in place.\nIn this randomised controlled study, 800 households with traditional thatched-roofed houses were randomly selected from 91 villages in the Upper River Region of The Gambia. Within each village, equal numbers of houses were randomly allocated to the control and intervention groups using a sampling frame. Houses in the intervention group were modified with metal roofs and screened doors and windows, whereas houses in the control group received no modifications. In each group, clinical malaria in children aged 6 months to 13 years was monitored by active case detection over 2 years (2016-17). We did monthly collections from indoor light traps to estimate vector densities. Primary endpoints were the incidence of clinical malaria in study children with more than 50% of observations each year and household vector density. The trial is registered at ISRCTN02622179.\nIn June, 2016, 785 houses had one child each recruited into the study (398 in unmodified houses and 402 in modified houses). 26 children in unmodified houses and 28 children in modified houses did not have at least 50% of visits in a year and so were excluded from analysis. 38 children in unmodified houses were recruited after study commencement, as were 21 children in modified houses, meaning 410 children in unmodified houses and 395 in modified houses were included in the parasitological analyses. At the end of the study, 659 (94%) of 702 children were reported to have slept under an insecticide-treated net; 662 (88%) of 755 children lived in houses that received indoor residual spraying; and 151 (90%) of 168 children younger than 5 years had seasonal malaria chemoprevention. Incidence of clinical malaria was 0·12 episodes per child-year in children in the unmodified houses and 0·20 episodes per child-year in the modified houses (unadjusted incidence rate ratio [RR] 1·68 [95% CI 1·11-2·55], p=0·014). Household vector density was 3·30 Anopheles gambiae per house per night in the unmodified houses compared with 3·60 in modified houses (unadjusted RR 1·28 [0·87-1·89], p=0·21).\nImproved housing did not provide protection against clinical malaria in this area of low seasonal transmission with high coverage of insecticide-treated nets, indoor residual spraying, and seasonal malaria chemoprevention.\nGlobal Health Trials funded by Medical Research Council, UK Department for International Development, and Wellcome Trust.', 'title': 'Improved housing versus usual practice for additional protection against clinical malaria in The Gambia (RooPfs): a household-randomised controlled trial.', 'date': '2021-04-12'}, '19732949': {'article_id': '19732949', 'content': 'House screening should protect people against malaria. We assessed whether two types of house screening--full screening of windows, doors, and closing eaves, or installation of screened ceilings--could reduce house entry of malaria vectors and frequency of anaemia in children in an area of seasonal malaria transmission.\nDuring 2006 and 2007, 500 occupied houses in and near Farafenni town in The Gambia, an area with low use of insecticide-treated bednets, were randomly assigned to receive full screening, screened ceilings, or no screening (control). Randomisation was done by computer-generated list, in permuted blocks of five houses in the ratio 2:2:1. Screening was not treated with insecticide. Exposure to mosquitoes indoors was assessed by fortnightly light trap collections during the transmission season. Primary endpoints included the number of female Anopheles gambiae sensu lato mosquitoes collected per trap per night. Secondary endpoints included frequency of anaemia (haemoglobin concentration <80 g/L) and parasitaemia at the end of the transmission season in children (aged 6 months to 10 years) who were living in the study houses. Analysis was by modified intention to treat (ITT), including all randomised houses for which there were some outcome data and all children from those houses who were sampled for haemoglobin and parasitaemia. This study is registered as an International Standard Randomised Controlled Trial, number ISRCTN51184253.\n462 houses were included in the modified ITT analysis (full screening, n=188; screened ceilings, n=178; control, n=96). The mean number of A gambiae caught in houses without screening was 37.5 per trap per night (95% CI 31.6-43.3), compared with 15.2 (12.9-17.4) in houses with full screening (ratio of means 0.41, 95% CI 0.31-0.54; p<0.0001) and 19.1 (16.1-22.1) in houses with screened ceilings (ratio 0.53, 0.40-0.70; p<0.0001). 755 children completed the study, of whom 731 had complete clinical and covariate data and were used in the analysis of clinical outcomes. 30 (19%) of 158 children from control houses had anaemia, compared with 38 (12%) of 309 from houses with full screening (adjusted odds ratio [OR] 0.53, 95% CI 0.29-0.97; p=0.04), and 31 (12%) of 264 from houses with screened ceilings (OR 0.51, 0.27-0.96; p=0.04). Frequency of parasitaemia did not differ between intervention and control groups.\nHouse screening substantially reduced the number of mosquitoes inside houses and could contribute to prevention of anaemia in children.\nMedical Research Council.', 'title': 'Effect of two different house screening interventions on exposure to malaria vectors and on anaemia in children in The Gambia: a randomised controlled trial.', 'date': '2009-09-08'}, '33640067': {'article_id': '33640067', 'content': ""New vector control tools are required to sustain the fight against malaria. Lethal house lures, which target mosquitoes as they attempt to enter houses to blood feed, are one approach. Here we evaluated lethal house lures consisting of In2Care (Wageningen, Netherlands) Eave Tubes, which provide point-source insecticide treatments against host-seeking mosquitoes, in combination with house screening, which aims to reduce mosquito entry.\nWe did a two-arm, cluster-randomised controlled trial with 40 village-level clusters in central Côte d'Ivoire between Sept 26, 2016, and April 10, 2019. All households received new insecticide-treated nets at universal coverage (one bednet per two people). Suitable households within the clusters assigned to the treatment group were offered screening plus Eave Tubes, with Eave Tubes treated using a 10% wettable powder formulation of the pyrethroid β-cyfluthrin. Because of the nature of the intervention, treatment could not be masked for households and field teams, but all analyses were blinded. The primary endpoint was clinical malaria incidence recorded by active case detection over 2 years in cohorts of children aged 6 months to 10 years. This trial is registered with ISRCTN, ISRCTN18145556.\n3022 houses received screening plus Eave Tubes, with an average coverage of 70% across the intervention clusters. 1300 eligible children were recruited for active case detection in the control group and 1260 in the intervention group. During the 2-year follow-up period, malaria case incidence was 2·29 per child-year (95% CI 1·97-2·61) in the control group and 1·43 per child-year (1·21-1·65) in the intervention group (hazard ratio 0·62, 95% CI 0·51-0·76; p<0·0001). Cost-effectiveness simulations suggested that screening plus Eave Tubes has a 74·0% chance of representing a cost-effective intervention, compared with existing healthcare activities in Côte d'Ivoire, and is similarly cost-effective to other core vector control interventions across sub-Saharan Africa. No serious adverse events associated with the intervention were reported during follow-up.\nScreening plus Eave Tubes can provide protection against malaria in addition to the effects of insecticide-treated nets, offering potential for a new, cost-effective strategy to supplement existing vector control tools. Additional trials are needed to confirm these initial results and further optimise Eave Tubes and the lethal house lure concept to facilitate adoption.\nThe Bill & Melinda Gates Foundation."", 'title': ""Impact and cost-effectiveness of a lethal house lure against malaria transmission in central Côte d'Ivoire: a two-arm, cluster-randomised controlled trial."", 'date': '2021-03-01'}}",0.333333333,"Public Health, Epidemiology & Health Systems"
98,"Is malaria parasite prevalence higher, lower, or the same when comparing housing modifications to no modifications?",lower,moderate,yes,"['19732949', '34022912', '35437129', '32950061', '33838737']",36200610,2022,"{'19732949': {'article_id': '19732949', 'content': 'House screening should protect people against malaria. We assessed whether two types of house screening--full screening of windows, doors, and closing eaves, or installation of screened ceilings--could reduce house entry of malaria vectors and frequency of anaemia in children in an area of seasonal malaria transmission.\nDuring 2006 and 2007, 500 occupied houses in and near Farafenni town in The Gambia, an area with low use of insecticide-treated bednets, were randomly assigned to receive full screening, screened ceilings, or no screening (control). Randomisation was done by computer-generated list, in permuted blocks of five houses in the ratio 2:2:1. Screening was not treated with insecticide. Exposure to mosquitoes indoors was assessed by fortnightly light trap collections during the transmission season. Primary endpoints included the number of female Anopheles gambiae sensu lato mosquitoes collected per trap per night. Secondary endpoints included frequency of anaemia (haemoglobin concentration <80 g/L) and parasitaemia at the end of the transmission season in children (aged 6 months to 10 years) who were living in the study houses. Analysis was by modified intention to treat (ITT), including all randomised houses for which there were some outcome data and all children from those houses who were sampled for haemoglobin and parasitaemia. This study is registered as an International Standard Randomised Controlled Trial, number ISRCTN51184253.\n462 houses were included in the modified ITT analysis (full screening, n=188; screened ceilings, n=178; control, n=96). The mean number of A gambiae caught in houses without screening was 37.5 per trap per night (95% CI 31.6-43.3), compared with 15.2 (12.9-17.4) in houses with full screening (ratio of means 0.41, 95% CI 0.31-0.54; p<0.0001) and 19.1 (16.1-22.1) in houses with screened ceilings (ratio 0.53, 0.40-0.70; p<0.0001). 755 children completed the study, of whom 731 had complete clinical and covariate data and were used in the analysis of clinical outcomes. 30 (19%) of 158 children from control houses had anaemia, compared with 38 (12%) of 309 from houses with full screening (adjusted odds ratio [OR] 0.53, 95% CI 0.29-0.97; p=0.04), and 31 (12%) of 264 from houses with screened ceilings (OR 0.51, 0.27-0.96; p=0.04). Frequency of parasitaemia did not differ between intervention and control groups.\nHouse screening substantially reduced the number of mosquitoes inside houses and could contribute to prevention of anaemia in children.\nMedical Research Council.', 'title': 'Effect of two different house screening interventions on exposure to malaria vectors and on anaemia in children in The Gambia: a randomised controlled trial.', 'date': '2009-09-08'}, '34022912': {'article_id': '34022912', 'content': 'Current standard interventions are not universally sufficient for malaria elimination. The effects of community-based house improvement (HI) and larval source management (LSM) as supplementary interventions to the Malawi National Malaria Control Programme (NMCP) interventions were assessed in the context of an intensive community engagement programme.\nThe study was a two-by-two factorial, cluster-randomized controlled trial in Malawi. Village clusters were randomly assigned to four arms: a control arm; HI; LSM; and HI\u2009+\u2009LSM. Malawi NMCP interventions and community engagement were used in all arms. Household-level, cross-sectional surveys were conducted on a rolling, 2-monthly basis to measure parasitological and entomological outcomes over 3\xa0years, beginning with one baseline year. The primary outcome was the entomological inoculation rate (EIR). Secondary outcomes included mosquito density, Plasmodium falciparum prevalence, and haemoglobin levels. All outcomes were assessed based on intention to treat, and comparisons between trial arms were conducted at both cluster and household level.\nEighteen clusters derived from 53 villages with 4558 households and 20,013 people were randomly assigned to the four trial arms. The mean nightly EIR fell from 0.010 infectious bites per person (95% CI 0.006-0.015) in the baseline year to 0.001 (0.000, 0.003) in the last year of the trial. Over the full trial period, the EIR did not differ between the four trial arms (p\u2009=\u20090.33). Similar results were observed for the other outcomes: mosquito density and P. falciparum prevalence decreased over 3\xa0years of sampling, while haemoglobin levels increased; and there were minimal differences between the trial arms during the trial period.\nIn the context of high insecticide-treated bed net use, neither community-based HI, LSM, nor HI\u2009+\u2009LSM contributed to further reductions in malaria transmission or prevalence beyond the reductions observed over two years across all four trial arms. This was the first trial, as far as the authors are aware, to test the potential complementary impact of LSM and/or HI beyond levels achieved by standard interventions. The unexpectedly low EIR values following intervention implementation indicated a promising reduction in malaria transmission for the area, but also limited the usefulness of this outcome for measuring differences in malaria transmission among the trial arms. Trial registration PACTR, PACTR201604001501493, Registered 3 March 2016, https://pactr.samrc.ac.za/ .', 'title': 'The effect of community-driven larval source management and house improvement on malaria transmission when added to the standard malaria control strategies in Malawi: a cluster-randomized controlled trial.', 'date': '2021-05-24'}, '35437129': {'article_id': '35437129', 'content': 'Increases in bed net coverage and antimalarial treatment have reduced the risk of malaria in sub-Saharan Africa. However, the pace of reduction has slowed, and new tools are needed to reverse this trend. We evaluated houses screened with insecticide-treated ceiling nets using a cluster randomized-controlled trial in western Kenya. The primary endpoints were \n[Figure: see text]', 'title': 'Effectiveness of screened ceilings over the current best practice in reducing malaria prevalence in western Kenya: a cluster randomised controlled trial.', 'date': '2022-04-20'}, '32950061': {'article_id': '32950061', 'content': 'Mosquito-proofing of houses using wire mesh screens is gaining greater recognition as a practical intervention for reducing exposure to malaria transmitting mosquitoes. Screening potentially protects all persons sleeping inside the house against transmission of mosquito-borne diseases indoors. The study assessed the effectiveness of house eaves screening in reducing indoor vector densities and malaria prevalence in Nyabondo, western Kenya.\n160 houses were selected for the study, with half of them randomly chosen for eaves screening with fibre-glass coated wire mesh (experimental group) and the other half left without screening (control group). Randomization was carried out by use of computer-generated list in permuted blocks of ten houses and 16 village blocks, with half of them allocated treatment in a ratio of 1:1. Cross-sectional baseline entomological and parasitological data were collected before eave screening. After baseline data collection, series of sampling of indoor adult mosquitoes were conducted once a month in each village using CDC light traps. Three cross-sectional malaria parasitological surveys were conducted at three month intervals after installation of the screens. The primary outcome measures were indoor Anopheles mosquito density and malaria parasite prevalence.\nA total of 15,286 mosquitoes were collected over the two year period using CDC light traps in 160 houses distributed over 16 study villages (mean mosquitoes\u2009=\u20094.35, SD\u2009=\u200911.48). Of all mosquitoes collected, 2,872 (18.8%) were anophelines (2,869 Anopheles gambiae sensu lato, 1 Anopheles funestus and 2 other Anopheles spp). Overall, among An. gambiae collected, 92.6% were non-blood fed, 3.57% were blood fed and the remaining 0.47% were composed of gravid and half gravid females. More indoor adult mosquitoes were collected in the control than experimental arms of the study. Results from cross-sectional parasitological surveys showed that screened houses recorded relatively low malaria parasite prevalence rates compared to the control houses. Overall, malaria prevalence was 5.6% (95% CI: 4.2-7.5) n\u2009=\u20091,918, with baseline prevalence rate of 6.1% (95% CI: 3.9-9.4), n\u2009=\u2009481 and 3\nThe study demonstrated that house eave screening has potential to reduce indoor vector densities and malaria prevalence in high transmission areas.', 'title': 'Evaluating effectiveness of screening house eaves as a potential intervention for reducing indoor vector densities and malaria prevalence in Nyabondo, western Kenya.', 'date': '2020-09-21'}, '33838737': {'article_id': '33838737', 'content': 'In malaria-endemic areas, residents of modern houses have less malaria than those living in traditional houses. We aimed to assess whether children in The Gambia received an incremental benefit from improved housing, where current best practice of insecticide-treated nets, indoor residual spraying, seasonal malaria chemoprevention in children younger than 5 years, and prompt treatment against clinical malaria was in place.\nIn this randomised controlled study, 800 households with traditional thatched-roofed houses were randomly selected from 91 villages in the Upper River Region of The Gambia. Within each village, equal numbers of houses were randomly allocated to the control and intervention groups using a sampling frame. Houses in the intervention group were modified with metal roofs and screened doors and windows, whereas houses in the control group received no modifications. In each group, clinical malaria in children aged 6 months to 13 years was monitored by active case detection over 2 years (2016-17). We did monthly collections from indoor light traps to estimate vector densities. Primary endpoints were the incidence of clinical malaria in study children with more than 50% of observations each year and household vector density. The trial is registered at ISRCTN02622179.\nIn June, 2016, 785 houses had one child each recruited into the study (398 in unmodified houses and 402 in modified houses). 26 children in unmodified houses and 28 children in modified houses did not have at least 50% of visits in a year and so were excluded from analysis. 38 children in unmodified houses were recruited after study commencement, as were 21 children in modified houses, meaning 410 children in unmodified houses and 395 in modified houses were included in the parasitological analyses. At the end of the study, 659 (94%) of 702 children were reported to have slept under an insecticide-treated net; 662 (88%) of 755 children lived in houses that received indoor residual spraying; and 151 (90%) of 168 children younger than 5 years had seasonal malaria chemoprevention. Incidence of clinical malaria was 0·12 episodes per child-year in children in the unmodified houses and 0·20 episodes per child-year in the modified houses (unadjusted incidence rate ratio [RR] 1·68 [95% CI 1·11-2·55], p=0·014). Household vector density was 3·30 Anopheles gambiae per house per night in the unmodified houses compared with 3·60 in modified houses (unadjusted RR 1·28 [0·87-1·89], p=0·21).\nImproved housing did not provide protection against clinical malaria in this area of low seasonal transmission with high coverage of insecticide-treated nets, indoor residual spraying, and seasonal malaria chemoprevention.\nGlobal Health Trials funded by Medical Research Council, UK Department for International Development, and Wellcome Trust.', 'title': 'Improved housing versus usual practice for additional protection against clinical malaria in The Gambia (RooPfs): a household-randomised controlled trial.', 'date': '2021-04-12'}}",0.4,"Public Health, Epidemiology & Health Systems"
99,"Is clinical malaria incidence higher, lower, or the same when comparing housing modifications to no modifications?",uncertain effect,very low,no,"['29452110', '33838737', '33640067']",36200610,2022,"{'29452110': {'article_id': '29452110', 'content': 'House is the major site for malaria infection where most human-vector contact takes place. Hence, improving housing might reduce the risk of malaria infection by limiting house entry of vectors. This study aimed to explore the impact of screening doors and windows with wire meshes on density and entomological inoculation rate (EIR) of malaria vector, and malaria incidence, and assess the acceptability, durability, and cost of the intervention. The susceptibility status of malaria vector was also assessed. A two-arm randomized trial was done in Arba Minch Town, southwest Ethiopia. 92 houses were randomly included in the trial. The baseline entomological and malaria prevalence data were collected. The mosquito sampling was done twice per household per month by Centers for Diseases Control and Prevention (CDC) light traps for six months. The baseline prevalence of malaria was assessed by testing 396 (83% of the 447 study participants) household members in all the eligible houses. The 92 houses were then randomized into control and intervention groups using mosquito and malaria prevalence baseline data to make the two groups comparable except the intervention. Then, we put wire-mesh on doors and windows of 46 houses. Post-screening mosquito collection was done in each household twice per month for three months. Each household member was visited twice per month for six months to assess malaria episodes. The frequency of damage to different structure of screening was measured twice. In-depth interview was conducted with 24 purposely selected household heads from intervention group. Speciation of Anopheles mosquito was done by morphological key, and the circum-sporozoite proteins (CSPs) analysis was done using enzyme-linked immunosorbent assay. A generalized estimating equation with a negative binomial distribution was used to assess the impact of the intervention on the indoor density of vectors. Clinical malaria case data were analyzed using Poisson regression with generalized linear model. Screening doors and windows reduced the indoor density of An. arabiensis by 48% (mean ratio of intervention to control\u202f=\u202f0.85/1.65; 0.52) (P\u202f=\u202f.001). Plasmodium falciparum CSP rate was 1.6% (3/190) in the intervention houses, while it was 2.7% (10/372) in the control houses. The protective efficacy of screening intervention from CSP positive An. arabiensis was 41% (mean ratio of intervention to control\u202f=\u202f1.6/2.7; 0.59), but was not statistically significant (P\u202f=\u202f.6). The EIR of An. arabiensis was 1.91 in the intervention group, whereas it was 6.45 in the control group. 477 participants were followed for clinical malaria (50.1% from intervention and 49.9% from the control group). Of 49 RDT positive cases, 45 were confirmed to be positive with microscopy. 80% (n\u202f=\u202f36) cases were due to P. falciparum and the rest 20% (n\u202f=\u202f9) were due to P. vivax. The incidence of P. falciparum in the intervention group was lower (IRR: 0.39, 95% CI: 0.2-0.80; P\u202f=\u202f.01) than in the control group. Using incidence of P. falciparum infection, the protective efficacy of intervention was 61% (95% CI: 18-83; P\u202f=\u202f.007). 97.9% of screened windows and 63.8% of screened doors were intact after eleven months of installation. Malaria mosquito was resistance (mortality rate of 75%) to the insecticide used for bed nets treatment. Almost all participants of intervention arm were willing to continue using screened doors and windows. Screening doors and windows reduced the indoor exposure to malaria vectors. The intervention is effective, durable and well-accepted. Hence, the existing interventions can be supplemented with house screening intervention for further reduction and ultimately elimination of malaria by reducing insecticide pressure on malaria vectors. However, further research could be considered in broad setting on different housing improvement and in the way how to scale-up for wider community.', 'title': 'Exploring the impact of house screening intervention on entomological indices and incidence of malaria in Arba Minch town, southwest Ethiopia: A randomized control trial.', 'date': '2018-02-17'}, '33838737': {'article_id': '33838737', 'content': 'In malaria-endemic areas, residents of modern houses have less malaria than those living in traditional houses. We aimed to assess whether children in The Gambia received an incremental benefit from improved housing, where current best practice of insecticide-treated nets, indoor residual spraying, seasonal malaria chemoprevention in children younger than 5 years, and prompt treatment against clinical malaria was in place.\nIn this randomised controlled study, 800 households with traditional thatched-roofed houses were randomly selected from 91 villages in the Upper River Region of The Gambia. Within each village, equal numbers of houses were randomly allocated to the control and intervention groups using a sampling frame. Houses in the intervention group were modified with metal roofs and screened doors and windows, whereas houses in the control group received no modifications. In each group, clinical malaria in children aged 6 months to 13 years was monitored by active case detection over 2 years (2016-17). We did monthly collections from indoor light traps to estimate vector densities. Primary endpoints were the incidence of clinical malaria in study children with more than 50% of observations each year and household vector density. The trial is registered at ISRCTN02622179.\nIn June, 2016, 785 houses had one child each recruited into the study (398 in unmodified houses and 402 in modified houses). 26 children in unmodified houses and 28 children in modified houses did not have at least 50% of visits in a year and so were excluded from analysis. 38 children in unmodified houses were recruited after study commencement, as were 21 children in modified houses, meaning 410 children in unmodified houses and 395 in modified houses were included in the parasitological analyses. At the end of the study, 659 (94%) of 702 children were reported to have slept under an insecticide-treated net; 662 (88%) of 755 children lived in houses that received indoor residual spraying; and 151 (90%) of 168 children younger than 5 years had seasonal malaria chemoprevention. Incidence of clinical malaria was 0·12 episodes per child-year in children in the unmodified houses and 0·20 episodes per child-year in the modified houses (unadjusted incidence rate ratio [RR] 1·68 [95% CI 1·11-2·55], p=0·014). Household vector density was 3·30 Anopheles gambiae per house per night in the unmodified houses compared with 3·60 in modified houses (unadjusted RR 1·28 [0·87-1·89], p=0·21).\nImproved housing did not provide protection against clinical malaria in this area of low seasonal transmission with high coverage of insecticide-treated nets, indoor residual spraying, and seasonal malaria chemoprevention.\nGlobal Health Trials funded by Medical Research Council, UK Department for International Development, and Wellcome Trust.', 'title': 'Improved housing versus usual practice for additional protection against clinical malaria in The Gambia (RooPfs): a household-randomised controlled trial.', 'date': '2021-04-12'}, '33640067': {'article_id': '33640067', 'content': ""New vector control tools are required to sustain the fight against malaria. Lethal house lures, which target mosquitoes as they attempt to enter houses to blood feed, are one approach. Here we evaluated lethal house lures consisting of In2Care (Wageningen, Netherlands) Eave Tubes, which provide point-source insecticide treatments against host-seeking mosquitoes, in combination with house screening, which aims to reduce mosquito entry.\nWe did a two-arm, cluster-randomised controlled trial with 40 village-level clusters in central Côte d'Ivoire between Sept 26, 2016, and April 10, 2019. All households received new insecticide-treated nets at universal coverage (one bednet per two people). Suitable households within the clusters assigned to the treatment group were offered screening plus Eave Tubes, with Eave Tubes treated using a 10% wettable powder formulation of the pyrethroid β-cyfluthrin. Because of the nature of the intervention, treatment could not be masked for households and field teams, but all analyses were blinded. The primary endpoint was clinical malaria incidence recorded by active case detection over 2 years in cohorts of children aged 6 months to 10 years. This trial is registered with ISRCTN, ISRCTN18145556.\n3022 houses received screening plus Eave Tubes, with an average coverage of 70% across the intervention clusters. 1300 eligible children were recruited for active case detection in the control group and 1260 in the intervention group. During the 2-year follow-up period, malaria case incidence was 2·29 per child-year (95% CI 1·97-2·61) in the control group and 1·43 per child-year (1·21-1·65) in the intervention group (hazard ratio 0·62, 95% CI 0·51-0·76; p<0·0001). Cost-effectiveness simulations suggested that screening plus Eave Tubes has a 74·0% chance of representing a cost-effective intervention, compared with existing healthcare activities in Côte d'Ivoire, and is similarly cost-effective to other core vector control interventions across sub-Saharan Africa. No serious adverse events associated with the intervention were reported during follow-up.\nScreening plus Eave Tubes can provide protection against malaria in addition to the effects of insecticide-treated nets, offering potential for a new, cost-effective strategy to supplement existing vector control tools. Additional trials are needed to confirm these initial results and further optimise Eave Tubes and the lethal house lure concept to facilitate adoption.\nThe Bill & Melinda Gates Foundation."", 'title': ""Impact and cost-effectiveness of a lethal house lure against malaria transmission in central Côte d'Ivoire: a two-arm, cluster-randomised controlled trial."", 'date': '2021-03-01'}}",0.0,"Public Health, Epidemiology & Health Systems"
100,"Is indoor adult mosquito density higher, lower, or the same when comparing housing modifications to no modifications?",lower,low,yes,"['19732949', '34022912', '33640067', '33838737']",36200610,2022,"{'19732949': {'article_id': '19732949', 'content': 'House screening should protect people against malaria. We assessed whether two types of house screening--full screening of windows, doors, and closing eaves, or installation of screened ceilings--could reduce house entry of malaria vectors and frequency of anaemia in children in an area of seasonal malaria transmission.\nDuring 2006 and 2007, 500 occupied houses in and near Farafenni town in The Gambia, an area with low use of insecticide-treated bednets, were randomly assigned to receive full screening, screened ceilings, or no screening (control). Randomisation was done by computer-generated list, in permuted blocks of five houses in the ratio 2:2:1. Screening was not treated with insecticide. Exposure to mosquitoes indoors was assessed by fortnightly light trap collections during the transmission season. Primary endpoints included the number of female Anopheles gambiae sensu lato mosquitoes collected per trap per night. Secondary endpoints included frequency of anaemia (haemoglobin concentration <80 g/L) and parasitaemia at the end of the transmission season in children (aged 6 months to 10 years) who were living in the study houses. Analysis was by modified intention to treat (ITT), including all randomised houses for which there were some outcome data and all children from those houses who were sampled for haemoglobin and parasitaemia. This study is registered as an International Standard Randomised Controlled Trial, number ISRCTN51184253.\n462 houses were included in the modified ITT analysis (full screening, n=188; screened ceilings, n=178; control, n=96). The mean number of A gambiae caught in houses without screening was 37.5 per trap per night (95% CI 31.6-43.3), compared with 15.2 (12.9-17.4) in houses with full screening (ratio of means 0.41, 95% CI 0.31-0.54; p<0.0001) and 19.1 (16.1-22.1) in houses with screened ceilings (ratio 0.53, 0.40-0.70; p<0.0001). 755 children completed the study, of whom 731 had complete clinical and covariate data and were used in the analysis of clinical outcomes. 30 (19%) of 158 children from control houses had anaemia, compared with 38 (12%) of 309 from houses with full screening (adjusted odds ratio [OR] 0.53, 95% CI 0.29-0.97; p=0.04), and 31 (12%) of 264 from houses with screened ceilings (OR 0.51, 0.27-0.96; p=0.04). Frequency of parasitaemia did not differ between intervention and control groups.\nHouse screening substantially reduced the number of mosquitoes inside houses and could contribute to prevention of anaemia in children.\nMedical Research Council.', 'title': 'Effect of two different house screening interventions on exposure to malaria vectors and on anaemia in children in The Gambia: a randomised controlled trial.', 'date': '2009-09-08'}, '34022912': {'article_id': '34022912', 'content': 'Current standard interventions are not universally sufficient for malaria elimination. The effects of community-based house improvement (HI) and larval source management (LSM) as supplementary interventions to the Malawi National Malaria Control Programme (NMCP) interventions were assessed in the context of an intensive community engagement programme.\nThe study was a two-by-two factorial, cluster-randomized controlled trial in Malawi. Village clusters were randomly assigned to four arms: a control arm; HI; LSM; and HI\u2009+\u2009LSM. Malawi NMCP interventions and community engagement were used in all arms. Household-level, cross-sectional surveys were conducted on a rolling, 2-monthly basis to measure parasitological and entomological outcomes over 3\xa0years, beginning with one baseline year. The primary outcome was the entomological inoculation rate (EIR). Secondary outcomes included mosquito density, Plasmodium falciparum prevalence, and haemoglobin levels. All outcomes were assessed based on intention to treat, and comparisons between trial arms were conducted at both cluster and household level.\nEighteen clusters derived from 53 villages with 4558 households and 20,013 people were randomly assigned to the four trial arms. The mean nightly EIR fell from 0.010 infectious bites per person (95% CI 0.006-0.015) in the baseline year to 0.001 (0.000, 0.003) in the last year of the trial. Over the full trial period, the EIR did not differ between the four trial arms (p\u2009=\u20090.33). Similar results were observed for the other outcomes: mosquito density and P. falciparum prevalence decreased over 3\xa0years of sampling, while haemoglobin levels increased; and there were minimal differences between the trial arms during the trial period.\nIn the context of high insecticide-treated bed net use, neither community-based HI, LSM, nor HI\u2009+\u2009LSM contributed to further reductions in malaria transmission or prevalence beyond the reductions observed over two years across all four trial arms. This was the first trial, as far as the authors are aware, to test the potential complementary impact of LSM and/or HI beyond levels achieved by standard interventions. The unexpectedly low EIR values following intervention implementation indicated a promising reduction in malaria transmission for the area, but also limited the usefulness of this outcome for measuring differences in malaria transmission among the trial arms. Trial registration PACTR, PACTR201604001501493, Registered 3 March 2016, https://pactr.samrc.ac.za/ .', 'title': 'The effect of community-driven larval source management and house improvement on malaria transmission when added to the standard malaria control strategies in Malawi: a cluster-randomized controlled trial.', 'date': '2021-05-24'}, '33640067': {'article_id': '33640067', 'content': ""New vector control tools are required to sustain the fight against malaria. Lethal house lures, which target mosquitoes as they attempt to enter houses to blood feed, are one approach. Here we evaluated lethal house lures consisting of In2Care (Wageningen, Netherlands) Eave Tubes, which provide point-source insecticide treatments against host-seeking mosquitoes, in combination with house screening, which aims to reduce mosquito entry.\nWe did a two-arm, cluster-randomised controlled trial with 40 village-level clusters in central Côte d'Ivoire between Sept 26, 2016, and April 10, 2019. All households received new insecticide-treated nets at universal coverage (one bednet per two people). Suitable households within the clusters assigned to the treatment group were offered screening plus Eave Tubes, with Eave Tubes treated using a 10% wettable powder formulation of the pyrethroid β-cyfluthrin. Because of the nature of the intervention, treatment could not be masked for households and field teams, but all analyses were blinded. The primary endpoint was clinical malaria incidence recorded by active case detection over 2 years in cohorts of children aged 6 months to 10 years. This trial is registered with ISRCTN, ISRCTN18145556.\n3022 houses received screening plus Eave Tubes, with an average coverage of 70% across the intervention clusters. 1300 eligible children were recruited for active case detection in the control group and 1260 in the intervention group. During the 2-year follow-up period, malaria case incidence was 2·29 per child-year (95% CI 1·97-2·61) in the control group and 1·43 per child-year (1·21-1·65) in the intervention group (hazard ratio 0·62, 95% CI 0·51-0·76; p<0·0001). Cost-effectiveness simulations suggested that screening plus Eave Tubes has a 74·0% chance of representing a cost-effective intervention, compared with existing healthcare activities in Côte d'Ivoire, and is similarly cost-effective to other core vector control interventions across sub-Saharan Africa. No serious adverse events associated with the intervention were reported during follow-up.\nScreening plus Eave Tubes can provide protection against malaria in addition to the effects of insecticide-treated nets, offering potential for a new, cost-effective strategy to supplement existing vector control tools. Additional trials are needed to confirm these initial results and further optimise Eave Tubes and the lethal house lure concept to facilitate adoption.\nThe Bill & Melinda Gates Foundation."", 'title': ""Impact and cost-effectiveness of a lethal house lure against malaria transmission in central Côte d'Ivoire: a two-arm, cluster-randomised controlled trial."", 'date': '2021-03-01'}, '33838737': {'article_id': '33838737', 'content': 'In malaria-endemic areas, residents of modern houses have less malaria than those living in traditional houses. We aimed to assess whether children in The Gambia received an incremental benefit from improved housing, where current best practice of insecticide-treated nets, indoor residual spraying, seasonal malaria chemoprevention in children younger than 5 years, and prompt treatment against clinical malaria was in place.\nIn this randomised controlled study, 800 households with traditional thatched-roofed houses were randomly selected from 91 villages in the Upper River Region of The Gambia. Within each village, equal numbers of houses were randomly allocated to the control and intervention groups using a sampling frame. Houses in the intervention group were modified with metal roofs and screened doors and windows, whereas houses in the control group received no modifications. In each group, clinical malaria in children aged 6 months to 13 years was monitored by active case detection over 2 years (2016-17). We did monthly collections from indoor light traps to estimate vector densities. Primary endpoints were the incidence of clinical malaria in study children with more than 50% of observations each year and household vector density. The trial is registered at ISRCTN02622179.\nIn June, 2016, 785 houses had one child each recruited into the study (398 in unmodified houses and 402 in modified houses). 26 children in unmodified houses and 28 children in modified houses did not have at least 50% of visits in a year and so were excluded from analysis. 38 children in unmodified houses were recruited after study commencement, as were 21 children in modified houses, meaning 410 children in unmodified houses and 395 in modified houses were included in the parasitological analyses. At the end of the study, 659 (94%) of 702 children were reported to have slept under an insecticide-treated net; 662 (88%) of 755 children lived in houses that received indoor residual spraying; and 151 (90%) of 168 children younger than 5 years had seasonal malaria chemoprevention. Incidence of clinical malaria was 0·12 episodes per child-year in children in the unmodified houses and 0·20 episodes per child-year in the modified houses (unadjusted incidence rate ratio [RR] 1·68 [95% CI 1·11-2·55], p=0·014). Household vector density was 3·30 Anopheles gambiae per house per night in the unmodified houses compared with 3·60 in modified houses (unadjusted RR 1·28 [0·87-1·89], p=0·21).\nImproved housing did not provide protection against clinical malaria in this area of low seasonal transmission with high coverage of insecticide-treated nets, indoor residual spraying, and seasonal malaria chemoprevention.\nGlobal Health Trials funded by Medical Research Council, UK Department for International Development, and Wellcome Trust.', 'title': 'Improved housing versus usual practice for additional protection against clinical malaria in The Gambia (RooPfs): a household-randomised controlled trial.', 'date': '2021-04-12'}}",0.5,"Public Health, Epidemiology & Health Systems"
101,"Is global cognitive function level at the end of intervention higher, lower, or the same when comparing computerised cognitive training (CCT) of at least 12 weeks to active control interventions?",higher,low,no,"['21311196', '29261218']",32104914,2020,"{'21311196': {'article_id': '21311196', 'content': 'Many studies have suggested that cognitive training can result in cognitive gains in healthy older adults. We investigated whether personalized computerized cognitive training provides greater benefits than those obtained by playing conventional computer games.\nThis was a randomized double-blind interventional study. Self-referred healthy older adults (n = 155, 68 ± 7 years old) were assigned to either a personalized, computerized cognitive training or to a computer games group. Cognitive performance was assessed at baseline and after 3 months by a neuropsychological assessment battery. Differences in cognitive performance scores between and within groups were evaluated using mixed effects models in 2 approaches: adherence only (AO; n = 121) and intention to treat (ITT; n = 155).\nBoth groups improved in cognitive performance. The improvement in the personalized cognitive training group was significant (p < 0.03, AO and ITT approaches) in all 8 cognitive domains. However, in the computer games group it was significant (p < 0.05) in only 4 (AO) or 6 domains (ITT). In the AO analysis, personalized cognitive training was significantly more effective than playing games in improving visuospatial working memory (p = 0.0001), visuospatial learning (p = 0.0012) and focused attention (p = 0.0019).\nPersonalized, computerized cognitive training appears to be more effective than computer games in improving cognitive performance in healthy older adults. Further studies are needed to evaluate the ecological validity of these findings.', 'title': 'Computer-based, personalized cognitive training versus classical computer games: a randomized double-blind prospective trial of cognitive stimulation.', 'date': '2011-02-12'}, '29261218': {'article_id': '29261218', 'content': 'Home-based computerised cognitive training (CCT) is ineffective at enhancing global cognition, a key marker of cognitive ageing.\nTo test the effectiveness of supervised, group-based, multidomain CCT on global cognition in older adults and to characterise the dose-response relationship during and after training.\nA randomised, double-blind, longitudinal, active-controlled trial.\nCommunity-based training centre in Sydney, Australia Participants: Eighty nondemented community-dwelling older adults (mean age = 72.1, 68.8% females) with multiple dementia risk factors but no major neuropsychiatric or sensory disorder. Of the 80 participants admitted to the study, 65 completed post-training assessment and 55 were followed up one year after training cessation.\nThirty-six group-based sessions over three months of either CCT targeting memory, speed, attention, language and reasoning tasks, or active control training comprising audiovisual educational exercises.\nPrimary outcome was change from baseline in global cognition as defined by a composite score of memory, speed and executive function. Secondary outcome was 15-month change in Bayer Activities of Daily Living from baseline to one year post-training.\nIntention-to-treat analyses revealed significant effects on global cognition in the cognitive training group compared to active control after three weeks of training (ES = 0.33, P=.039) that increased after 3 months of training (ES = 0.49, P=.003) and persisted three months after training cessation (ES = 0.30, P=0.023). Significant and durable improvements were also noted in memory and processing speed. Dose-response characteristics differed among cognitive domains. Training effects waned gradually but residual gains were noted twelve months post-training. No significant effects on activities of daily living were noted and there were no adverse effects.\nIn older adults with multiple dementia risk factors, group-based CCT is a safe and effective intervention for enhancing overall cognition, memory and processing speed. Dose-response relationships vary for each cognitive domain, vital information for clinical and community implementation and further trial design.', 'title': 'The Timecourse of Global Cognitive Gains from Supervised Computer-Assisted Cognitive Training: A Randomised, Active-Controlled Trial in Elderly with Multiple Dementia Risk Factors.', 'date': '2014-01-01'}}",0.5,Psychiatry & Neurology
102,"Is speed of processing at the end of intervention higher, lower, or the same when comparing computerised cognitive training (CCT) of at least 12 weeks to active control interventions?",uncertain effect,very low,yes,"['23531885', '27698558']",32104914,2020,"{'23531885': {'article_id': '23531885', 'content': 'Cognitive training and aerobic training are known to improve cognitive functions. To examine the separate and combined effects of such training on cognitive performance, four groups of healthy older adults embarked on a 4 months cognitive and/or mild aerobic training. A first group [n = 33, mean age = 80 (66-90)] engaged in cognitive training, a second [n = 29, mean age = 81 (65-89)] in mild aerobic training, a third [n = 29, mean age = 79 (70-93)] in the combination of both, and a fourth [n = 31, mean age = 79 (71-92)] control group engaged in book-reading activity. The outcome was a well-validated multi-domain computerized cognitive evaluation for older adults. The results indicate that, when compared to older adults who did not engage in cognitive training (the mild aerobic and control groups) older adults who engaged in cognitive training (separate or combined training groups) showed significant improvement in cognitive performance on Hand-Eye Coordination, Global Visual Memory (GVM; working memory and long-term memory), Speed of Information Processing, Visual Scanning, and Naming. Indeed, individuals who did not engage in cognitive training showed no such improvements. Those results suggest that cognitive training is effective in improving cognitive performance and that it (and not mild aerobic training) is driving the improvement in the combined condition. Results are discussed in terms of the special circumstances of aerobic and cognitive training for older adults who are above 80 years of age.', 'title': 'Does combined cognitive training and physical activity training enhance cognitive abilities more than either alone? A four-condition randomized controlled trial among healthy older adults.', 'date': '2013-03-28'}, '27698558': {'article_id': '27698558', 'content': 'Physical exercise and cognitive training have been shown to enhance cognition among older adults. However, few studies have looked at the potential synergetic effects of combining physical and cognitive training in a single study. Prior trials on combined training have led to interesting yet equivocal results. The aim of this study was to examine the effects of combined physical and cognitive interventions on physical fitness and neuropsychological performance in healthy older adults.\nSeventy-six participants were randomly assigned to one of four training combinations using a 2×2 factorial design. The physical intervention was a mixed aerobic and resistance training program, and the cognitive intervention was a dual-task (DT) training program. Stretching and toning exercises and computer lessons were used as active control conditions. Physical and cognitive measures were collected pre- and postintervention.\nAll groups showed equivalent improvements in measures of functional mobility. The aerobic-strength condition led to larger effect size in lower body strength, independently of cognitive training. All groups showed improved speed of processing and inhibition abilities, but only participants who took part in the DT training, independently of physical training, showed increased task-switching abilities. The level of functional mobility after intervention was significantly associated with task-switching abilities.\nCombined training did not yield synergetic effects. However, DT training did lead to transfer effects on executive performance in neuropsychological tests. Both aerobic-resistance training and stretching-toning exercises can improve functional mobility in older adults.', 'title': 'Effects of combined physical and cognitive training on fitness and neuropsychological outcomes in healthy older adults.', 'date': '2016-10-05'}}",0.0,Psychiatry & Neurology
103,"Is episodic memory at the end of intervention higher, lower, or the same when comparing computerised cognitive training (CCT) of at least 12 weeks to no intervention?",higher,low,no,['20418350'],32104914,2020,"{'20418350': {'article_id': '20418350', 'content': 'Several reports suggest beneficial impacts of either physical or mental activity on cognitive function in old age. However, the differential effects of complex mental and physical activities on cognitive performance in humans remain to be clarified.\nThis randomized controlled trial evaluates a cognitive and a physical standardized 6-month activity intervention (3 x 1.5 h/wk) conducted in Berlin (Germany). Two hundred fifty nine healthy women aged 70-93 years were randomized to a computer course (n = 92), an exercise course (n = 91), or a control group (n = 76), of whom 230 completed the 6-month assessment. Group differences in change over a period of 6 months in episodic memory (story recall, possible range, 0-21; word recall, possible range, 0-16), executive control (working memory, ie, time quotient of Trail Making Tests B/A), and verbal fluency were evaluated by analyses of covariance (intention to treat) adjusting for baseline, fluid intelligence, and educational level.\nIn contrast to the control group, both the exercise group, DeltaM (SD) = 2.09 (2.66), p < .001, and the computer group, DeltaM (SD) =1.89 (2.88), p < .001, showed improved delayed story recall. They maintained performance in delayed word recall and working memory (time measure) as opposed to the control group that showed a decline, DeltaM (SD) = -0.91 (2.15), p = .001, and DeltaM (SD) = 0.24 (0.68), p = .04, respectively.\nIn healthy older women, participation in new stimulating activities contributes to cognitive fitness and might delay cognitive decline. Exercise and computer classes seem to generate equivalent beneficial effects.', 'title': 'Complex mental and physical activity in older women and cognitive performance: a 6-month randomized controlled trial.', 'date': '2010-04-27'}}",1.0,Psychiatry & Neurology
104,"Is speed of processing at the end of intervention higher, lower, or the same when comparing computerised cognitive training (CCT) of at least 12 weeks to no intervention?",uncertain effect,very low,yes,"['23531885', '25511081']",32104914,2020,"{'23531885': {'article_id': '23531885', 'content': 'Cognitive training and aerobic training are known to improve cognitive functions. To examine the separate and combined effects of such training on cognitive performance, four groups of healthy older adults embarked on a 4 months cognitive and/or mild aerobic training. A first group [n = 33, mean age = 80 (66-90)] engaged in cognitive training, a second [n = 29, mean age = 81 (65-89)] in mild aerobic training, a third [n = 29, mean age = 79 (70-93)] in the combination of both, and a fourth [n = 31, mean age = 79 (71-92)] control group engaged in book-reading activity. The outcome was a well-validated multi-domain computerized cognitive evaluation for older adults. The results indicate that, when compared to older adults who did not engage in cognitive training (the mild aerobic and control groups) older adults who engaged in cognitive training (separate or combined training groups) showed significant improvement in cognitive performance on Hand-Eye Coordination, Global Visual Memory (GVM; working memory and long-term memory), Speed of Information Processing, Visual Scanning, and Naming. Indeed, individuals who did not engage in cognitive training showed no such improvements. Those results suggest that cognitive training is effective in improving cognitive performance and that it (and not mild aerobic training) is driving the improvement in the combined condition. Results are discussed in terms of the special circumstances of aerobic and cognitive training for older adults who are above 80 years of age.', 'title': 'Does combined cognitive training and physical activity training enhance cognitive abilities more than either alone? A four-condition randomized controlled trial among healthy older adults.', 'date': '2013-03-28'}, '25511081': {'article_id': '25511081', 'content': ""Exercise interventions often do not combine physical and cognitive training. However, this combination is assumed to be more beneficial in improving walking and cognitive functioning compared to isolated cognitive or physical training.\nA multicenter parallel randomized controlled trial was conducted to compare a motor to a cognitive-motor exercise program. A total of 182 eligible residents of homes-for-the-aged (n\u2009=\u2009159) or elderly living in the vicinity of the homes (n\u2009=\u200923) were randomly assigned to either strength-balance (SB) or strength-balance-cognitive (SBC) training. Both groups conducted similar strength-balance training during 12 weeks. SBC additionally absolved computerized cognitive training. Outcomes were dual task costs of walking, physical performance, simple reaction time, executive functions, divided attention, fear of falling and fall rate. Participants were analysed with an intention to treat approach.\nThe 182 participants (mean age\u2009±\u2009SD: 81.5\u2009±\u20097.3 years) were allocated to either SB (n\u2009=\u200998) or SBC (n\u2009=\u200984). The attrition rate was 14.3%. Interaction effects were observed for dual task costs of step length (preferred walking speed: F(1,174)\u2009=\u20094.94, p\u2009=\u20090.028, η2\u2009=\u20090.027, fast walking speed: F(1,166)\u2009=\u20096.14, p\u2009=\u20090.009, η2\u2009=\u20090.040) and dual task costs of the standard deviation of step length (F(1,166)\u2009=\u20096.14, p\u2009=\u20090.014, η2\u2009=\u20090.036), in favor of SBC. Significant interactions in favor of SBC revealed for in gait initiation (F(1,166)\u2009=\u20099.16, p\u2009=\u20090.003, η2\u2009=\u20090.052), 'reaction time' (F(1,180)\u2009=\u20095.243, p\u2009=\u20090.023, η²\u2009=\u20090.028) & 'missed answers' (F(1,180)\u2009=\u200911.839, p\u2009=\u20090.001, η²\u2009=\u20090.062) as part of the test for divided attention. Within-group comparison revealed significant improvements in dual task costs of walking (preferred speed; velocity (p\u2009=\u20090.002), step time (p\u2009=\u20090.018), step length (p\u2009=\u20090.028), fast speed; velocity (p\u2009<\u20090.001), step time (p\u2009=\u20090.035), step length (p\u2009=\u20090.001)), simple reaction time (p\u2009<\u20090.001), executive functioning (Trail making test B; p\u2009<\u20090.001), divided attention (p\u2009<\u20090.001), fear of falling (p\u2009<\u20090.001), and fall rate (p\u2009<\u20090.001).\nCombining strength-balance training with specific cognitive training has a positive additional effect on dual task costs of walking, gait initiation, and divided attention. The findings further confirm previous research showing that strength-balance training improves executive functions and reduces falls.\nThis trial has been registered under ISRCTN75134517."", 'title': 'Strength-balance supplemented with computerized cognitive training to improve dual task gait and divided attention in older adults: a multicenter randomized-controlled trial.', 'date': '2014-12-17'}}",0.0,Psychiatry & Neurology
105,"Is working memory at the end of intervention higher, lower, or the same when comparing computerised cognitive training (CCT) of at least 12 weeks to no intervention?",no difference,low,yes,['23531885'],32104914,2020,"{'23531885': {'article_id': '23531885', 'content': 'Cognitive training and aerobic training are known to improve cognitive functions. To examine the separate and combined effects of such training on cognitive performance, four groups of healthy older adults embarked on a 4 months cognitive and/or mild aerobic training. A first group [n = 33, mean age = 80 (66-90)] engaged in cognitive training, a second [n = 29, mean age = 81 (65-89)] in mild aerobic training, a third [n = 29, mean age = 79 (70-93)] in the combination of both, and a fourth [n = 31, mean age = 79 (71-92)] control group engaged in book-reading activity. The outcome was a well-validated multi-domain computerized cognitive evaluation for older adults. The results indicate that, when compared to older adults who did not engage in cognitive training (the mild aerobic and control groups) older adults who engaged in cognitive training (separate or combined training groups) showed significant improvement in cognitive performance on Hand-Eye Coordination, Global Visual Memory (GVM; working memory and long-term memory), Speed of Information Processing, Visual Scanning, and Naming. Indeed, individuals who did not engage in cognitive training showed no such improvements. Those results suggest that cognitive training is effective in improving cognitive performance and that it (and not mild aerobic training) is driving the improvement in the combined condition. Results are discussed in terms of the special circumstances of aerobic and cognitive training for older adults who are above 80 years of age.', 'title': 'Does combined cognitive training and physical activity training enhance cognitive abilities more than either alone? A four-condition randomized controlled trial among healthy older adults.', 'date': '2013-03-28'}}",0.0,Psychiatry & Neurology
230,"Is global cognitive function at the end of intervention higher, lower, or the same when comparing computerised cognitive training (CCT) of at least 12 weeks to no intervention?",insufficient data,,no,"['23531885', '25511081', '20418350']",32104914,2020,"{'23531885': {'article_id': '23531885', 'content': 'Cognitive training and aerobic training are known to improve cognitive functions. To examine the separate and combined effects of such training on cognitive performance, four groups of healthy older adults embarked on a 4 months cognitive and/or mild aerobic training. A first group [n = 33, mean age = 80 (66-90)] engaged in cognitive training, a second [n = 29, mean age = 81 (65-89)] in mild aerobic training, a third [n = 29, mean age = 79 (70-93)] in the combination of both, and a fourth [n = 31, mean age = 79 (71-92)] control group engaged in book-reading activity. The outcome was a well-validated multi-domain computerized cognitive evaluation for older adults. The results indicate that, when compared to older adults who did not engage in cognitive training (the mild aerobic and control groups) older adults who engaged in cognitive training (separate or combined training groups) showed significant improvement in cognitive performance on Hand-Eye Coordination, Global Visual Memory (GVM; working memory and long-term memory), Speed of Information Processing, Visual Scanning, and Naming. Indeed, individuals who did not engage in cognitive training showed no such improvements. Those results suggest that cognitive training is effective in improving cognitive performance and that it (and not mild aerobic training) is driving the improvement in the combined condition. Results are discussed in terms of the special circumstances of aerobic and cognitive training for older adults who are above 80 years of age.', 'title': 'Does combined cognitive training and physical activity training enhance cognitive abilities more than either alone? A four-condition randomized controlled trial among healthy older adults.', 'date': '2013-03-28'}, '25511081': {'article_id': '25511081', 'content': ""Exercise interventions often do not combine physical and cognitive training. However, this combination is assumed to be more beneficial in improving walking and cognitive functioning compared to isolated cognitive or physical training.\nA multicenter parallel randomized controlled trial was conducted to compare a motor to a cognitive-motor exercise program. A total of 182 eligible residents of homes-for-the-aged (n\u2009=\u2009159) or elderly living in the vicinity of the homes (n\u2009=\u200923) were randomly assigned to either strength-balance (SB) or strength-balance-cognitive (SBC) training. Both groups conducted similar strength-balance training during 12 weeks. SBC additionally absolved computerized cognitive training. Outcomes were dual task costs of walking, physical performance, simple reaction time, executive functions, divided attention, fear of falling and fall rate. Participants were analysed with an intention to treat approach.\nThe 182 participants (mean age\u2009±\u2009SD: 81.5\u2009±\u20097.3 years) were allocated to either SB (n\u2009=\u200998) or SBC (n\u2009=\u200984). The attrition rate was 14.3%. Interaction effects were observed for dual task costs of step length (preferred walking speed: F(1,174)\u2009=\u20094.94, p\u2009=\u20090.028, η2\u2009=\u20090.027, fast walking speed: F(1,166)\u2009=\u20096.14, p\u2009=\u20090.009, η2\u2009=\u20090.040) and dual task costs of the standard deviation of step length (F(1,166)\u2009=\u20096.14, p\u2009=\u20090.014, η2\u2009=\u20090.036), in favor of SBC. Significant interactions in favor of SBC revealed for in gait initiation (F(1,166)\u2009=\u20099.16, p\u2009=\u20090.003, η2\u2009=\u20090.052), 'reaction time' (F(1,180)\u2009=\u20095.243, p\u2009=\u20090.023, η²\u2009=\u20090.028) & 'missed answers' (F(1,180)\u2009=\u200911.839, p\u2009=\u20090.001, η²\u2009=\u20090.062) as part of the test for divided attention. Within-group comparison revealed significant improvements in dual task costs of walking (preferred speed; velocity (p\u2009=\u20090.002), step time (p\u2009=\u20090.018), step length (p\u2009=\u20090.028), fast speed; velocity (p\u2009<\u20090.001), step time (p\u2009=\u20090.035), step length (p\u2009=\u20090.001)), simple reaction time (p\u2009<\u20090.001), executive functioning (Trail making test B; p\u2009<\u20090.001), divided attention (p\u2009<\u20090.001), fear of falling (p\u2009<\u20090.001), and fall rate (p\u2009<\u20090.001).\nCombining strength-balance training with specific cognitive training has a positive additional effect on dual task costs of walking, gait initiation, and divided attention. The findings further confirm previous research showing that strength-balance training improves executive functions and reduces falls.\nThis trial has been registered under ISRCTN75134517."", 'title': 'Strength-balance supplemented with computerized cognitive training to improve dual task gait and divided attention in older adults: a multicenter randomized-controlled trial.', 'date': '2014-12-17'}, '20418350': {'article_id': '20418350', 'content': 'Several reports suggest beneficial impacts of either physical or mental activity on cognitive function in old age. However, the differential effects of complex mental and physical activities on cognitive performance in humans remain to be clarified.\nThis randomized controlled trial evaluates a cognitive and a physical standardized 6-month activity intervention (3 x 1.5 h/wk) conducted in Berlin (Germany). Two hundred fifty nine healthy women aged 70-93 years were randomized to a computer course (n = 92), an exercise course (n = 91), or a control group (n = 76), of whom 230 completed the 6-month assessment. Group differences in change over a period of 6 months in episodic memory (story recall, possible range, 0-21; word recall, possible range, 0-16), executive control (working memory, ie, time quotient of Trail Making Tests B/A), and verbal fluency were evaluated by analyses of covariance (intention to treat) adjusting for baseline, fluid intelligence, and educational level.\nIn contrast to the control group, both the exercise group, DeltaM (SD) = 2.09 (2.66), p < .001, and the computer group, DeltaM (SD) =1.89 (2.88), p < .001, showed improved delayed story recall. They maintained performance in delayed word recall and working memory (time measure) as opposed to the control group that showed a decline, DeltaM (SD) = -0.91 (2.15), p = .001, and DeltaM (SD) = 0.24 (0.68), p = .04, respectively.\nIn healthy older women, participation in new stimulating activities contributes to cognitive fitness and might delay cognitive decline. Exercise and computer classes seem to generate equivalent beneficial effects.', 'title': 'Complex mental and physical activity in older women and cognitive performance: a 6-month randomized controlled trial.', 'date': '2010-04-27'}}",0.666666667,Psychiatry & Neurology
231,"Is the rate of activities of daily living higher, lower, or the same when comparing computerised cognitive training (CCT) of at least 12 weeks to no intervention?",insufficient data,,no,"['23531885', '25511081', '20418350']",32104914,2020,"{'23531885': {'article_id': '23531885', 'content': 'Cognitive training and aerobic training are known to improve cognitive functions. To examine the separate and combined effects of such training on cognitive performance, four groups of healthy older adults embarked on a 4 months cognitive and/or mild aerobic training. A first group [n = 33, mean age = 80 (66-90)] engaged in cognitive training, a second [n = 29, mean age = 81 (65-89)] in mild aerobic training, a third [n = 29, mean age = 79 (70-93)] in the combination of both, and a fourth [n = 31, mean age = 79 (71-92)] control group engaged in book-reading activity. The outcome was a well-validated multi-domain computerized cognitive evaluation for older adults. The results indicate that, when compared to older adults who did not engage in cognitive training (the mild aerobic and control groups) older adults who engaged in cognitive training (separate or combined training groups) showed significant improvement in cognitive performance on Hand-Eye Coordination, Global Visual Memory (GVM; working memory and long-term memory), Speed of Information Processing, Visual Scanning, and Naming. Indeed, individuals who did not engage in cognitive training showed no such improvements. Those results suggest that cognitive training is effective in improving cognitive performance and that it (and not mild aerobic training) is driving the improvement in the combined condition. Results are discussed in terms of the special circumstances of aerobic and cognitive training for older adults who are above 80 years of age.', 'title': 'Does combined cognitive training and physical activity training enhance cognitive abilities more than either alone? A four-condition randomized controlled trial among healthy older adults.', 'date': '2013-03-28'}, '25511081': {'article_id': '25511081', 'content': ""Exercise interventions often do not combine physical and cognitive training. However, this combination is assumed to be more beneficial in improving walking and cognitive functioning compared to isolated cognitive or physical training.\nA multicenter parallel randomized controlled trial was conducted to compare a motor to a cognitive-motor exercise program. A total of 182 eligible residents of homes-for-the-aged (n\u2009=\u2009159) or elderly living in the vicinity of the homes (n\u2009=\u200923) were randomly assigned to either strength-balance (SB) or strength-balance-cognitive (SBC) training. Both groups conducted similar strength-balance training during 12 weeks. SBC additionally absolved computerized cognitive training. Outcomes were dual task costs of walking, physical performance, simple reaction time, executive functions, divided attention, fear of falling and fall rate. Participants were analysed with an intention to treat approach.\nThe 182 participants (mean age\u2009±\u2009SD: 81.5\u2009±\u20097.3 years) were allocated to either SB (n\u2009=\u200998) or SBC (n\u2009=\u200984). The attrition rate was 14.3%. Interaction effects were observed for dual task costs of step length (preferred walking speed: F(1,174)\u2009=\u20094.94, p\u2009=\u20090.028, η2\u2009=\u20090.027, fast walking speed: F(1,166)\u2009=\u20096.14, p\u2009=\u20090.009, η2\u2009=\u20090.040) and dual task costs of the standard deviation of step length (F(1,166)\u2009=\u20096.14, p\u2009=\u20090.014, η2\u2009=\u20090.036), in favor of SBC. Significant interactions in favor of SBC revealed for in gait initiation (F(1,166)\u2009=\u20099.16, p\u2009=\u20090.003, η2\u2009=\u20090.052), 'reaction time' (F(1,180)\u2009=\u20095.243, p\u2009=\u20090.023, η²\u2009=\u20090.028) & 'missed answers' (F(1,180)\u2009=\u200911.839, p\u2009=\u20090.001, η²\u2009=\u20090.062) as part of the test for divided attention. Within-group comparison revealed significant improvements in dual task costs of walking (preferred speed; velocity (p\u2009=\u20090.002), step time (p\u2009=\u20090.018), step length (p\u2009=\u20090.028), fast speed; velocity (p\u2009<\u20090.001), step time (p\u2009=\u20090.035), step length (p\u2009=\u20090.001)), simple reaction time (p\u2009<\u20090.001), executive functioning (Trail making test B; p\u2009<\u20090.001), divided attention (p\u2009<\u20090.001), fear of falling (p\u2009<\u20090.001), and fall rate (p\u2009<\u20090.001).\nCombining strength-balance training with specific cognitive training has a positive additional effect on dual task costs of walking, gait initiation, and divided attention. The findings further confirm previous research showing that strength-balance training improves executive functions and reduces falls.\nThis trial has been registered under ISRCTN75134517."", 'title': 'Strength-balance supplemented with computerized cognitive training to improve dual task gait and divided attention in older adults: a multicenter randomized-controlled trial.', 'date': '2014-12-17'}, '20418350': {'article_id': '20418350', 'content': 'Several reports suggest beneficial impacts of either physical or mental activity on cognitive function in old age. However, the differential effects of complex mental and physical activities on cognitive performance in humans remain to be clarified.\nThis randomized controlled trial evaluates a cognitive and a physical standardized 6-month activity intervention (3 x 1.5 h/wk) conducted in Berlin (Germany). Two hundred fifty nine healthy women aged 70-93 years were randomized to a computer course (n = 92), an exercise course (n = 91), or a control group (n = 76), of whom 230 completed the 6-month assessment. Group differences in change over a period of 6 months in episodic memory (story recall, possible range, 0-21; word recall, possible range, 0-16), executive control (working memory, ie, time quotient of Trail Making Tests B/A), and verbal fluency were evaluated by analyses of covariance (intention to treat) adjusting for baseline, fluid intelligence, and educational level.\nIn contrast to the control group, both the exercise group, DeltaM (SD) = 2.09 (2.66), p < .001, and the computer group, DeltaM (SD) =1.89 (2.88), p < .001, showed improved delayed story recall. They maintained performance in delayed word recall and working memory (time measure) as opposed to the control group that showed a decline, DeltaM (SD) = -0.91 (2.15), p = .001, and DeltaM (SD) = 0.24 (0.68), p = .04, respectively.\nIn healthy older women, participation in new stimulating activities contributes to cognitive fitness and might delay cognitive decline. Exercise and computer classes seem to generate equivalent beneficial effects.', 'title': 'Complex mental and physical activity in older women and cognitive performance: a 6-month randomized controlled trial.', 'date': '2010-04-27'}}",1.0,Psychiatry & Neurology
232,"Is the rate of activities of daily living higher, lower, or the same when comparing computerised cognitive training (CCT) of at least 12 weeks to active control interventions?",insufficient data,,no,"['27698558', '29261218', '21615936', '26417460', '21311196', '23531885']",32104914,2020,"{'27698558': {'article_id': '27698558', 'content': 'Physical exercise and cognitive training have been shown to enhance cognition among older adults. However, few studies have looked at the potential synergetic effects of combining physical and cognitive training in a single study. Prior trials on combined training have led to interesting yet equivocal results. The aim of this study was to examine the effects of combined physical and cognitive interventions on physical fitness and neuropsychological performance in healthy older adults.\nSeventy-six participants were randomly assigned to one of four training combinations using a 2×2 factorial design. The physical intervention was a mixed aerobic and resistance training program, and the cognitive intervention was a dual-task (DT) training program. Stretching and toning exercises and computer lessons were used as active control conditions. Physical and cognitive measures were collected pre- and postintervention.\nAll groups showed equivalent improvements in measures of functional mobility. The aerobic-strength condition led to larger effect size in lower body strength, independently of cognitive training. All groups showed improved speed of processing and inhibition abilities, but only participants who took part in the DT training, independently of physical training, showed increased task-switching abilities. The level of functional mobility after intervention was significantly associated with task-switching abilities.\nCombined training did not yield synergetic effects. However, DT training did lead to transfer effects on executive performance in neuropsychological tests. Both aerobic-resistance training and stretching-toning exercises can improve functional mobility in older adults.', 'title': 'Effects of combined physical and cognitive training on fitness and neuropsychological outcomes in healthy older adults.', 'date': '2016-10-05'}, '29261218': {'article_id': '29261218', 'content': 'Home-based computerised cognitive training (CCT) is ineffective at enhancing global cognition, a key marker of cognitive ageing.\nTo test the effectiveness of supervised, group-based, multidomain CCT on global cognition in older adults and to characterise the dose-response relationship during and after training.\nA randomised, double-blind, longitudinal, active-controlled trial.\nCommunity-based training centre in Sydney, Australia Participants: Eighty nondemented community-dwelling older adults (mean age = 72.1, 68.8% females) with multiple dementia risk factors but no major neuropsychiatric or sensory disorder. Of the 80 participants admitted to the study, 65 completed post-training assessment and 55 were followed up one year after training cessation.\nThirty-six group-based sessions over three months of either CCT targeting memory, speed, attention, language and reasoning tasks, or active control training comprising audiovisual educational exercises.\nPrimary outcome was change from baseline in global cognition as defined by a composite score of memory, speed and executive function. Secondary outcome was 15-month change in Bayer Activities of Daily Living from baseline to one year post-training.\nIntention-to-treat analyses revealed significant effects on global cognition in the cognitive training group compared to active control after three weeks of training (ES = 0.33, P=.039) that increased after 3 months of training (ES = 0.49, P=.003) and persisted three months after training cessation (ES = 0.30, P=0.023). Significant and durable improvements were also noted in memory and processing speed. Dose-response characteristics differed among cognitive domains. Training effects waned gradually but residual gains were noted twelve months post-training. No significant effects on activities of daily living were noted and there were no adverse effects.\nIn older adults with multiple dementia risk factors, group-based CCT is a safe and effective intervention for enhancing overall cognition, memory and processing speed. Dose-response relationships vary for each cognitive domain, vital information for clinical and community implementation and further trial design.', 'title': 'The Timecourse of Global Cognitive Gains from Supervised Computer-Assisted Cognitive Training: A Randomised, Active-Controlled Trial in Elderly with Multiple Dementia Risk Factors.', 'date': '2014-01-01'}, '21615936': {'article_id': '21615936', 'content': 'The efficacy of non-pharmacological intervention approaches such as physical activity, strength, and cognitive training for improving brain health has not been established. Before definitive trials are mounted, important design questions on participation/adherence, training and interventions effects must be answered to more fully inform a full-scale trial.\nSHARP-P was a single-blinded randomized controlled pilot trial of a 4-month physical activity training intervention (PA) and/or cognitive training intervention (CT) in a 2 × 2 factorial design with a health education control condition in 73 community-dwelling persons, aged 70-85 years, who were at risk for cognitive decline but did not have mild cognitive impairment.\nIntervention attendance rates were higher in the CT and PACT groups: CT: 96%, PA: 76%, PACT: 90% (p=0.004), the interventions produced marked changes in cognitive and physical performance measures (p≤0.05), and retention rates exceeded 90%. There were no statistically significant differences in 4-month changes in composite scores of cognitive, executive, and episodic memory function among arms. Four-month improvements in the composite measure increased with age among participants assigned to physical activity training but decreased with age for other participants (intervention*age interaction p=0.01). Depending on the choice of outcome, two-armed full-scale trials may require fewer than 1,000 participants (continuous outcome) or 2,000 participants (categorical outcome).\nGood levels of participation, adherence, and retention appear to be achievable for participants through age 85 years. Care should be taken to ensure that an attention control condition does not attenuate intervention effects. Depending on the choice of outcome measures, the necessary sample sizes to conduct four-year trials appear to be feasible.\nClinicaltrials.gov Identifier: NCT00688155.', 'title': 'Designing clinical trials for assessing the effects of cognitive training and physical activity interventions on cognitive outcomes: the Seniors Health and Activity Research Program Pilot (SHARP-P) study, a randomized controlled trial.', 'date': '2011-05-28'}, '26417460': {'article_id': '26417460', 'content': 'Increasing research has evidenced that our brain retains a capacity to change in response to experience until late adulthood. This implies that cognitive training can possibly ameliorate age-associated cognitive decline by inducing training-specific neural plastic changes at both neural and behavioral levels. This longitudinal study examined the behavioral effects of a systematic thirteen-week cognitive training program on attention and working memory of older adults who were at risk of cognitive decline. These older adults were randomly assigned to the Cognitive Training Group (n = 109) and the Active Control Group (n = 100). Findings clearly indicated that training induced improvement in auditory and visual-spatial attention and working memory. The training effect was specific to the experience provided because no significant difference in verbal and visual-spatial memory between the two groups was observed. This pattern of findings is consistent with the prediction and the principle of experience-dependent neuroplasticity. Findings of our study provided further support to the notion that the neural plastic potential continues until older age. The baseline cognitive status did not correlate with pre- versus posttraining changes to any cognitive variables studied, suggesting that the initial cognitive status may not limit the neuroplastic potential of the brain at an old age.', 'title': 'Neural Plastic Effects of Cognitive Training on Aging Brain.', 'date': '2015-09-30'}, '21311196': {'article_id': '21311196', 'content': 'Many studies have suggested that cognitive training can result in cognitive gains in healthy older adults. We investigated whether personalized computerized cognitive training provides greater benefits than those obtained by playing conventional computer games.\nThis was a randomized double-blind interventional study. Self-referred healthy older adults (n = 155, 68 ± 7 years old) were assigned to either a personalized, computerized cognitive training or to a computer games group. Cognitive performance was assessed at baseline and after 3 months by a neuropsychological assessment battery. Differences in cognitive performance scores between and within groups were evaluated using mixed effects models in 2 approaches: adherence only (AO; n = 121) and intention to treat (ITT; n = 155).\nBoth groups improved in cognitive performance. The improvement in the personalized cognitive training group was significant (p < 0.03, AO and ITT approaches) in all 8 cognitive domains. However, in the computer games group it was significant (p < 0.05) in only 4 (AO) or 6 domains (ITT). In the AO analysis, personalized cognitive training was significantly more effective than playing games in improving visuospatial working memory (p = 0.0001), visuospatial learning (p = 0.0012) and focused attention (p = 0.0019).\nPersonalized, computerized cognitive training appears to be more effective than computer games in improving cognitive performance in healthy older adults. Further studies are needed to evaluate the ecological validity of these findings.', 'title': 'Computer-based, personalized cognitive training versus classical computer games: a randomized double-blind prospective trial of cognitive stimulation.', 'date': '2011-02-12'}, '23531885': {'article_id': '23531885', 'content': 'Cognitive training and aerobic training are known to improve cognitive functions. To examine the separate and combined effects of such training on cognitive performance, four groups of healthy older adults embarked on a 4 months cognitive and/or mild aerobic training. A first group [n = 33, mean age = 80 (66-90)] engaged in cognitive training, a second [n = 29, mean age = 81 (65-89)] in mild aerobic training, a third [n = 29, mean age = 79 (70-93)] in the combination of both, and a fourth [n = 31, mean age = 79 (71-92)] control group engaged in book-reading activity. The outcome was a well-validated multi-domain computerized cognitive evaluation for older adults. The results indicate that, when compared to older adults who did not engage in cognitive training (the mild aerobic and control groups) older adults who engaged in cognitive training (separate or combined training groups) showed significant improvement in cognitive performance on Hand-Eye Coordination, Global Visual Memory (GVM; working memory and long-term memory), Speed of Information Processing, Visual Scanning, and Naming. Indeed, individuals who did not engage in cognitive training showed no such improvements. Those results suggest that cognitive training is effective in improving cognitive performance and that it (and not mild aerobic training) is driving the improvement in the combined condition. Results are discussed in terms of the special circumstances of aerobic and cognitive training for older adults who are above 80 years of age.', 'title': 'Does combined cognitive training and physical activity training enhance cognitive abilities more than either alone? A four-condition randomized controlled trial among healthy older adults.', 'date': '2013-03-28'}}",0.833333333,Psychiatry & Neurology
233,"Is the risk of adverse effects higher, lower, or the same when comparing computerised cognitive training (CCT) of at least 12 weeks to no intervention?",insufficient data,,no,"['23531885', '25511081', '20418350']",32104914,2020,"{'23531885': {'article_id': '23531885', 'content': 'Cognitive training and aerobic training are known to improve cognitive functions. To examine the separate and combined effects of such training on cognitive performance, four groups of healthy older adults embarked on a 4 months cognitive and/or mild aerobic training. A first group [n = 33, mean age = 80 (66-90)] engaged in cognitive training, a second [n = 29, mean age = 81 (65-89)] in mild aerobic training, a third [n = 29, mean age = 79 (70-93)] in the combination of both, and a fourth [n = 31, mean age = 79 (71-92)] control group engaged in book-reading activity. The outcome was a well-validated multi-domain computerized cognitive evaluation for older adults. The results indicate that, when compared to older adults who did not engage in cognitive training (the mild aerobic and control groups) older adults who engaged in cognitive training (separate or combined training groups) showed significant improvement in cognitive performance on Hand-Eye Coordination, Global Visual Memory (GVM; working memory and long-term memory), Speed of Information Processing, Visual Scanning, and Naming. Indeed, individuals who did not engage in cognitive training showed no such improvements. Those results suggest that cognitive training is effective in improving cognitive performance and that it (and not mild aerobic training) is driving the improvement in the combined condition. Results are discussed in terms of the special circumstances of aerobic and cognitive training for older adults who are above 80 years of age.', 'title': 'Does combined cognitive training and physical activity training enhance cognitive abilities more than either alone? A four-condition randomized controlled trial among healthy older adults.', 'date': '2013-03-28'}, '25511081': {'article_id': '25511081', 'content': ""Exercise interventions often do not combine physical and cognitive training. However, this combination is assumed to be more beneficial in improving walking and cognitive functioning compared to isolated cognitive or physical training.\nA multicenter parallel randomized controlled trial was conducted to compare a motor to a cognitive-motor exercise program. A total of 182 eligible residents of homes-for-the-aged (n\u2009=\u2009159) or elderly living in the vicinity of the homes (n\u2009=\u200923) were randomly assigned to either strength-balance (SB) or strength-balance-cognitive (SBC) training. Both groups conducted similar strength-balance training during 12 weeks. SBC additionally absolved computerized cognitive training. Outcomes were dual task costs of walking, physical performance, simple reaction time, executive functions, divided attention, fear of falling and fall rate. Participants were analysed with an intention to treat approach.\nThe 182 participants (mean age\u2009±\u2009SD: 81.5\u2009±\u20097.3 years) were allocated to either SB (n\u2009=\u200998) or SBC (n\u2009=\u200984). The attrition rate was 14.3%. Interaction effects were observed for dual task costs of step length (preferred walking speed: F(1,174)\u2009=\u20094.94, p\u2009=\u20090.028, η2\u2009=\u20090.027, fast walking speed: F(1,166)\u2009=\u20096.14, p\u2009=\u20090.009, η2\u2009=\u20090.040) and dual task costs of the standard deviation of step length (F(1,166)\u2009=\u20096.14, p\u2009=\u20090.014, η2\u2009=\u20090.036), in favor of SBC. Significant interactions in favor of SBC revealed for in gait initiation (F(1,166)\u2009=\u20099.16, p\u2009=\u20090.003, η2\u2009=\u20090.052), 'reaction time' (F(1,180)\u2009=\u20095.243, p\u2009=\u20090.023, η²\u2009=\u20090.028) & 'missed answers' (F(1,180)\u2009=\u200911.839, p\u2009=\u20090.001, η²\u2009=\u20090.062) as part of the test for divided attention. Within-group comparison revealed significant improvements in dual task costs of walking (preferred speed; velocity (p\u2009=\u20090.002), step time (p\u2009=\u20090.018), step length (p\u2009=\u20090.028), fast speed; velocity (p\u2009<\u20090.001), step time (p\u2009=\u20090.035), step length (p\u2009=\u20090.001)), simple reaction time (p\u2009<\u20090.001), executive functioning (Trail making test B; p\u2009<\u20090.001), divided attention (p\u2009<\u20090.001), fear of falling (p\u2009<\u20090.001), and fall rate (p\u2009<\u20090.001).\nCombining strength-balance training with specific cognitive training has a positive additional effect on dual task costs of walking, gait initiation, and divided attention. The findings further confirm previous research showing that strength-balance training improves executive functions and reduces falls.\nThis trial has been registered under ISRCTN75134517."", 'title': 'Strength-balance supplemented with computerized cognitive training to improve dual task gait and divided attention in older adults: a multicenter randomized-controlled trial.', 'date': '2014-12-17'}, '20418350': {'article_id': '20418350', 'content': 'Several reports suggest beneficial impacts of either physical or mental activity on cognitive function in old age. However, the differential effects of complex mental and physical activities on cognitive performance in humans remain to be clarified.\nThis randomized controlled trial evaluates a cognitive and a physical standardized 6-month activity intervention (3 x 1.5 h/wk) conducted in Berlin (Germany). Two hundred fifty nine healthy women aged 70-93 years were randomized to a computer course (n = 92), an exercise course (n = 91), or a control group (n = 76), of whom 230 completed the 6-month assessment. Group differences in change over a period of 6 months in episodic memory (story recall, possible range, 0-21; word recall, possible range, 0-16), executive control (working memory, ie, time quotient of Trail Making Tests B/A), and verbal fluency were evaluated by analyses of covariance (intention to treat) adjusting for baseline, fluid intelligence, and educational level.\nIn contrast to the control group, both the exercise group, DeltaM (SD) = 2.09 (2.66), p < .001, and the computer group, DeltaM (SD) =1.89 (2.88), p < .001, showed improved delayed story recall. They maintained performance in delayed word recall and working memory (time measure) as opposed to the control group that showed a decline, DeltaM (SD) = -0.91 (2.15), p = .001, and DeltaM (SD) = 0.24 (0.68), p = .04, respectively.\nIn healthy older women, participation in new stimulating activities contributes to cognitive fitness and might delay cognitive decline. Exercise and computer classes seem to generate equivalent beneficial effects.', 'title': 'Complex mental and physical activity in older women and cognitive performance: a 6-month randomized controlled trial.', 'date': '2010-04-27'}}",1.0,Psychiatry & Neurology
234,"Is the risk of adverse effects higher, lower, or the same when comparing computerised cognitive training (CCT) of at least 12 weeks to active control interventions?",insufficient data,,no,"['27698558', '29261218', '21615936', '26417460', '21311196', '23531885']",32104914,2020,"{'27698558': {'article_id': '27698558', 'content': 'Physical exercise and cognitive training have been shown to enhance cognition among older adults. However, few studies have looked at the potential synergetic effects of combining physical and cognitive training in a single study. Prior trials on combined training have led to interesting yet equivocal results. The aim of this study was to examine the effects of combined physical and cognitive interventions on physical fitness and neuropsychological performance in healthy older adults.\nSeventy-six participants were randomly assigned to one of four training combinations using a 2×2 factorial design. The physical intervention was a mixed aerobic and resistance training program, and the cognitive intervention was a dual-task (DT) training program. Stretching and toning exercises and computer lessons were used as active control conditions. Physical and cognitive measures were collected pre- and postintervention.\nAll groups showed equivalent improvements in measures of functional mobility. The aerobic-strength condition led to larger effect size in lower body strength, independently of cognitive training. All groups showed improved speed of processing and inhibition abilities, but only participants who took part in the DT training, independently of physical training, showed increased task-switching abilities. The level of functional mobility after intervention was significantly associated with task-switching abilities.\nCombined training did not yield synergetic effects. However, DT training did lead to transfer effects on executive performance in neuropsychological tests. Both aerobic-resistance training and stretching-toning exercises can improve functional mobility in older adults.', 'title': 'Effects of combined physical and cognitive training on fitness and neuropsychological outcomes in healthy older adults.', 'date': '2016-10-05'}, '29261218': {'article_id': '29261218', 'content': 'Home-based computerised cognitive training (CCT) is ineffective at enhancing global cognition, a key marker of cognitive ageing.\nTo test the effectiveness of supervised, group-based, multidomain CCT on global cognition in older adults and to characterise the dose-response relationship during and after training.\nA randomised, double-blind, longitudinal, active-controlled trial.\nCommunity-based training centre in Sydney, Australia Participants: Eighty nondemented community-dwelling older adults (mean age = 72.1, 68.8% females) with multiple dementia risk factors but no major neuropsychiatric or sensory disorder. Of the 80 participants admitted to the study, 65 completed post-training assessment and 55 were followed up one year after training cessation.\nThirty-six group-based sessions over three months of either CCT targeting memory, speed, attention, language and reasoning tasks, or active control training comprising audiovisual educational exercises.\nPrimary outcome was change from baseline in global cognition as defined by a composite score of memory, speed and executive function. Secondary outcome was 15-month change in Bayer Activities of Daily Living from baseline to one year post-training.\nIntention-to-treat analyses revealed significant effects on global cognition in the cognitive training group compared to active control after three weeks of training (ES = 0.33, P=.039) that increased after 3 months of training (ES = 0.49, P=.003) and persisted three months after training cessation (ES = 0.30, P=0.023). Significant and durable improvements were also noted in memory and processing speed. Dose-response characteristics differed among cognitive domains. Training effects waned gradually but residual gains were noted twelve months post-training. No significant effects on activities of daily living were noted and there were no adverse effects.\nIn older adults with multiple dementia risk factors, group-based CCT is a safe and effective intervention for enhancing overall cognition, memory and processing speed. Dose-response relationships vary for each cognitive domain, vital information for clinical and community implementation and further trial design.', 'title': 'The Timecourse of Global Cognitive Gains from Supervised Computer-Assisted Cognitive Training: A Randomised, Active-Controlled Trial in Elderly with Multiple Dementia Risk Factors.', 'date': '2014-01-01'}, '21615936': {'article_id': '21615936', 'content': 'The efficacy of non-pharmacological intervention approaches such as physical activity, strength, and cognitive training for improving brain health has not been established. Before definitive trials are mounted, important design questions on participation/adherence, training and interventions effects must be answered to more fully inform a full-scale trial.\nSHARP-P was a single-blinded randomized controlled pilot trial of a 4-month physical activity training intervention (PA) and/or cognitive training intervention (CT) in a 2 × 2 factorial design with a health education control condition in 73 community-dwelling persons, aged 70-85 years, who were at risk for cognitive decline but did not have mild cognitive impairment.\nIntervention attendance rates were higher in the CT and PACT groups: CT: 96%, PA: 76%, PACT: 90% (p=0.004), the interventions produced marked changes in cognitive and physical performance measures (p≤0.05), and retention rates exceeded 90%. There were no statistically significant differences in 4-month changes in composite scores of cognitive, executive, and episodic memory function among arms. Four-month improvements in the composite measure increased with age among participants assigned to physical activity training but decreased with age for other participants (intervention*age interaction p=0.01). Depending on the choice of outcome, two-armed full-scale trials may require fewer than 1,000 participants (continuous outcome) or 2,000 participants (categorical outcome).\nGood levels of participation, adherence, and retention appear to be achievable for participants through age 85 years. Care should be taken to ensure that an attention control condition does not attenuate intervention effects. Depending on the choice of outcome measures, the necessary sample sizes to conduct four-year trials appear to be feasible.\nClinicaltrials.gov Identifier: NCT00688155.', 'title': 'Designing clinical trials for assessing the effects of cognitive training and physical activity interventions on cognitive outcomes: the Seniors Health and Activity Research Program Pilot (SHARP-P) study, a randomized controlled trial.', 'date': '2011-05-28'}, '26417460': {'article_id': '26417460', 'content': 'Increasing research has evidenced that our brain retains a capacity to change in response to experience until late adulthood. This implies that cognitive training can possibly ameliorate age-associated cognitive decline by inducing training-specific neural plastic changes at both neural and behavioral levels. This longitudinal study examined the behavioral effects of a systematic thirteen-week cognitive training program on attention and working memory of older adults who were at risk of cognitive decline. These older adults were randomly assigned to the Cognitive Training Group (n = 109) and the Active Control Group (n = 100). Findings clearly indicated that training induced improvement in auditory and visual-spatial attention and working memory. The training effect was specific to the experience provided because no significant difference in verbal and visual-spatial memory between the two groups was observed. This pattern of findings is consistent with the prediction and the principle of experience-dependent neuroplasticity. Findings of our study provided further support to the notion that the neural plastic potential continues until older age. The baseline cognitive status did not correlate with pre- versus posttraining changes to any cognitive variables studied, suggesting that the initial cognitive status may not limit the neuroplastic potential of the brain at an old age.', 'title': 'Neural Plastic Effects of Cognitive Training on Aging Brain.', 'date': '2015-09-30'}, '21311196': {'article_id': '21311196', 'content': 'Many studies have suggested that cognitive training can result in cognitive gains in healthy older adults. We investigated whether personalized computerized cognitive training provides greater benefits than those obtained by playing conventional computer games.\nThis was a randomized double-blind interventional study. Self-referred healthy older adults (n = 155, 68 ± 7 years old) were assigned to either a personalized, computerized cognitive training or to a computer games group. Cognitive performance was assessed at baseline and after 3 months by a neuropsychological assessment battery. Differences in cognitive performance scores between and within groups were evaluated using mixed effects models in 2 approaches: adherence only (AO; n = 121) and intention to treat (ITT; n = 155).\nBoth groups improved in cognitive performance. The improvement in the personalized cognitive training group was significant (p < 0.03, AO and ITT approaches) in all 8 cognitive domains. However, in the computer games group it was significant (p < 0.05) in only 4 (AO) or 6 domains (ITT). In the AO analysis, personalized cognitive training was significantly more effective than playing games in improving visuospatial working memory (p = 0.0001), visuospatial learning (p = 0.0012) and focused attention (p = 0.0019).\nPersonalized, computerized cognitive training appears to be more effective than computer games in improving cognitive performance in healthy older adults. Further studies are needed to evaluate the ecological validity of these findings.', 'title': 'Computer-based, personalized cognitive training versus classical computer games: a randomized double-blind prospective trial of cognitive stimulation.', 'date': '2011-02-12'}, '23531885': {'article_id': '23531885', 'content': 'Cognitive training and aerobic training are known to improve cognitive functions. To examine the separate and combined effects of such training on cognitive performance, four groups of healthy older adults embarked on a 4 months cognitive and/or mild aerobic training. A first group [n = 33, mean age = 80 (66-90)] engaged in cognitive training, a second [n = 29, mean age = 81 (65-89)] in mild aerobic training, a third [n = 29, mean age = 79 (70-93)] in the combination of both, and a fourth [n = 31, mean age = 79 (71-92)] control group engaged in book-reading activity. The outcome was a well-validated multi-domain computerized cognitive evaluation for older adults. The results indicate that, when compared to older adults who did not engage in cognitive training (the mild aerobic and control groups) older adults who engaged in cognitive training (separate or combined training groups) showed significant improvement in cognitive performance on Hand-Eye Coordination, Global Visual Memory (GVM; working memory and long-term memory), Speed of Information Processing, Visual Scanning, and Naming. Indeed, individuals who did not engage in cognitive training showed no such improvements. Those results suggest that cognitive training is effective in improving cognitive performance and that it (and not mild aerobic training) is driving the improvement in the combined condition. Results are discussed in terms of the special circumstances of aerobic and cognitive training for older adults who are above 80 years of age.', 'title': 'Does combined cognitive training and physical activity training enhance cognitive abilities more than either alone? A four-condition randomized controlled trial among healthy older adults.', 'date': '2013-03-28'}}",1.0,Psychiatry & Neurology
235,"Is quality of life higher, lower, or the same when comparing computerised cognitive training (CCT) of at least 12 weeks to no intervention?",insufficient data,,no,"['23531885', '25511081', '20418350']",32104914,2020,"{'23531885': {'article_id': '23531885', 'content': 'Cognitive training and aerobic training are known to improve cognitive functions. To examine the separate and combined effects of such training on cognitive performance, four groups of healthy older adults embarked on a 4 months cognitive and/or mild aerobic training. A first group [n = 33, mean age = 80 (66-90)] engaged in cognitive training, a second [n = 29, mean age = 81 (65-89)] in mild aerobic training, a third [n = 29, mean age = 79 (70-93)] in the combination of both, and a fourth [n = 31, mean age = 79 (71-92)] control group engaged in book-reading activity. The outcome was a well-validated multi-domain computerized cognitive evaluation for older adults. The results indicate that, when compared to older adults who did not engage in cognitive training (the mild aerobic and control groups) older adults who engaged in cognitive training (separate or combined training groups) showed significant improvement in cognitive performance on Hand-Eye Coordination, Global Visual Memory (GVM; working memory and long-term memory), Speed of Information Processing, Visual Scanning, and Naming. Indeed, individuals who did not engage in cognitive training showed no such improvements. Those results suggest that cognitive training is effective in improving cognitive performance and that it (and not mild aerobic training) is driving the improvement in the combined condition. Results are discussed in terms of the special circumstances of aerobic and cognitive training for older adults who are above 80 years of age.', 'title': 'Does combined cognitive training and physical activity training enhance cognitive abilities more than either alone? A four-condition randomized controlled trial among healthy older adults.', 'date': '2013-03-28'}, '25511081': {'article_id': '25511081', 'content': ""Exercise interventions often do not combine physical and cognitive training. However, this combination is assumed to be more beneficial in improving walking and cognitive functioning compared to isolated cognitive or physical training.\nA multicenter parallel randomized controlled trial was conducted to compare a motor to a cognitive-motor exercise program. A total of 182 eligible residents of homes-for-the-aged (n\u2009=\u2009159) or elderly living in the vicinity of the homes (n\u2009=\u200923) were randomly assigned to either strength-balance (SB) or strength-balance-cognitive (SBC) training. Both groups conducted similar strength-balance training during 12 weeks. SBC additionally absolved computerized cognitive training. Outcomes were dual task costs of walking, physical performance, simple reaction time, executive functions, divided attention, fear of falling and fall rate. Participants were analysed with an intention to treat approach.\nThe 182 participants (mean age\u2009±\u2009SD: 81.5\u2009±\u20097.3 years) were allocated to either SB (n\u2009=\u200998) or SBC (n\u2009=\u200984). The attrition rate was 14.3%. Interaction effects were observed for dual task costs of step length (preferred walking speed: F(1,174)\u2009=\u20094.94, p\u2009=\u20090.028, η2\u2009=\u20090.027, fast walking speed: F(1,166)\u2009=\u20096.14, p\u2009=\u20090.009, η2\u2009=\u20090.040) and dual task costs of the standard deviation of step length (F(1,166)\u2009=\u20096.14, p\u2009=\u20090.014, η2\u2009=\u20090.036), in favor of SBC. Significant interactions in favor of SBC revealed for in gait initiation (F(1,166)\u2009=\u20099.16, p\u2009=\u20090.003, η2\u2009=\u20090.052), 'reaction time' (F(1,180)\u2009=\u20095.243, p\u2009=\u20090.023, η²\u2009=\u20090.028) & 'missed answers' (F(1,180)\u2009=\u200911.839, p\u2009=\u20090.001, η²\u2009=\u20090.062) as part of the test for divided attention. Within-group comparison revealed significant improvements in dual task costs of walking (preferred speed; velocity (p\u2009=\u20090.002), step time (p\u2009=\u20090.018), step length (p\u2009=\u20090.028), fast speed; velocity (p\u2009<\u20090.001), step time (p\u2009=\u20090.035), step length (p\u2009=\u20090.001)), simple reaction time (p\u2009<\u20090.001), executive functioning (Trail making test B; p\u2009<\u20090.001), divided attention (p\u2009<\u20090.001), fear of falling (p\u2009<\u20090.001), and fall rate (p\u2009<\u20090.001).\nCombining strength-balance training with specific cognitive training has a positive additional effect on dual task costs of walking, gait initiation, and divided attention. The findings further confirm previous research showing that strength-balance training improves executive functions and reduces falls.\nThis trial has been registered under ISRCTN75134517."", 'title': 'Strength-balance supplemented with computerized cognitive training to improve dual task gait and divided attention in older adults: a multicenter randomized-controlled trial.', 'date': '2014-12-17'}, '20418350': {'article_id': '20418350', 'content': 'Several reports suggest beneficial impacts of either physical or mental activity on cognitive function in old age. However, the differential effects of complex mental and physical activities on cognitive performance in humans remain to be clarified.\nThis randomized controlled trial evaluates a cognitive and a physical standardized 6-month activity intervention (3 x 1.5 h/wk) conducted in Berlin (Germany). Two hundred fifty nine healthy women aged 70-93 years were randomized to a computer course (n = 92), an exercise course (n = 91), or a control group (n = 76), of whom 230 completed the 6-month assessment. Group differences in change over a period of 6 months in episodic memory (story recall, possible range, 0-21; word recall, possible range, 0-16), executive control (working memory, ie, time quotient of Trail Making Tests B/A), and verbal fluency were evaluated by analyses of covariance (intention to treat) adjusting for baseline, fluid intelligence, and educational level.\nIn contrast to the control group, both the exercise group, DeltaM (SD) = 2.09 (2.66), p < .001, and the computer group, DeltaM (SD) =1.89 (2.88), p < .001, showed improved delayed story recall. They maintained performance in delayed word recall and working memory (time measure) as opposed to the control group that showed a decline, DeltaM (SD) = -0.91 (2.15), p = .001, and DeltaM (SD) = 0.24 (0.68), p = .04, respectively.\nIn healthy older women, participation in new stimulating activities contributes to cognitive fitness and might delay cognitive decline. Exercise and computer classes seem to generate equivalent beneficial effects.', 'title': 'Complex mental and physical activity in older women and cognitive performance: a 6-month randomized controlled trial.', 'date': '2010-04-27'}}",1.0,Psychiatry & Neurology
236,"Is quality of life higher, lower, or the same when comparing computerised cognitive training (CCT) of at least 12 weeks to active control interventions?",insufficient data,,no,"['27698558', '29261218', '21615936', '26417460', '21311196', '23531885']",32104914,2020,"{'27698558': {'article_id': '27698558', 'content': 'Physical exercise and cognitive training have been shown to enhance cognition among older adults. However, few studies have looked at the potential synergetic effects of combining physical and cognitive training in a single study. Prior trials on combined training have led to interesting yet equivocal results. The aim of this study was to examine the effects of combined physical and cognitive interventions on physical fitness and neuropsychological performance in healthy older adults.\nSeventy-six participants were randomly assigned to one of four training combinations using a 2×2 factorial design. The physical intervention was a mixed aerobic and resistance training program, and the cognitive intervention was a dual-task (DT) training program. Stretching and toning exercises and computer lessons were used as active control conditions. Physical and cognitive measures were collected pre- and postintervention.\nAll groups showed equivalent improvements in measures of functional mobility. The aerobic-strength condition led to larger effect size in lower body strength, independently of cognitive training. All groups showed improved speed of processing and inhibition abilities, but only participants who took part in the DT training, independently of physical training, showed increased task-switching abilities. The level of functional mobility after intervention was significantly associated with task-switching abilities.\nCombined training did not yield synergetic effects. However, DT training did lead to transfer effects on executive performance in neuropsychological tests. Both aerobic-resistance training and stretching-toning exercises can improve functional mobility in older adults.', 'title': 'Effects of combined physical and cognitive training on fitness and neuropsychological outcomes in healthy older adults.', 'date': '2016-10-05'}, '29261218': {'article_id': '29261218', 'content': 'Home-based computerised cognitive training (CCT) is ineffective at enhancing global cognition, a key marker of cognitive ageing.\nTo test the effectiveness of supervised, group-based, multidomain CCT on global cognition in older adults and to characterise the dose-response relationship during and after training.\nA randomised, double-blind, longitudinal, active-controlled trial.\nCommunity-based training centre in Sydney, Australia Participants: Eighty nondemented community-dwelling older adults (mean age = 72.1, 68.8% females) with multiple dementia risk factors but no major neuropsychiatric or sensory disorder. Of the 80 participants admitted to the study, 65 completed post-training assessment and 55 were followed up one year after training cessation.\nThirty-six group-based sessions over three months of either CCT targeting memory, speed, attention, language and reasoning tasks, or active control training comprising audiovisual educational exercises.\nPrimary outcome was change from baseline in global cognition as defined by a composite score of memory, speed and executive function. Secondary outcome was 15-month change in Bayer Activities of Daily Living from baseline to one year post-training.\nIntention-to-treat analyses revealed significant effects on global cognition in the cognitive training group compared to active control after three weeks of training (ES = 0.33, P=.039) that increased after 3 months of training (ES = 0.49, P=.003) and persisted three months after training cessation (ES = 0.30, P=0.023). Significant and durable improvements were also noted in memory and processing speed. Dose-response characteristics differed among cognitive domains. Training effects waned gradually but residual gains were noted twelve months post-training. No significant effects on activities of daily living were noted and there were no adverse effects.\nIn older adults with multiple dementia risk factors, group-based CCT is a safe and effective intervention for enhancing overall cognition, memory and processing speed. Dose-response relationships vary for each cognitive domain, vital information for clinical and community implementation and further trial design.', 'title': 'The Timecourse of Global Cognitive Gains from Supervised Computer-Assisted Cognitive Training: A Randomised, Active-Controlled Trial in Elderly with Multiple Dementia Risk Factors.', 'date': '2014-01-01'}, '21615936': {'article_id': '21615936', 'content': 'The efficacy of non-pharmacological intervention approaches such as physical activity, strength, and cognitive training for improving brain health has not been established. Before definitive trials are mounted, important design questions on participation/adherence, training and interventions effects must be answered to more fully inform a full-scale trial.\nSHARP-P was a single-blinded randomized controlled pilot trial of a 4-month physical activity training intervention (PA) and/or cognitive training intervention (CT) in a 2 × 2 factorial design with a health education control condition in 73 community-dwelling persons, aged 70-85 years, who were at risk for cognitive decline but did not have mild cognitive impairment.\nIntervention attendance rates were higher in the CT and PACT groups: CT: 96%, PA: 76%, PACT: 90% (p=0.004), the interventions produced marked changes in cognitive and physical performance measures (p≤0.05), and retention rates exceeded 90%. There were no statistically significant differences in 4-month changes in composite scores of cognitive, executive, and episodic memory function among arms. Four-month improvements in the composite measure increased with age among participants assigned to physical activity training but decreased with age for other participants (intervention*age interaction p=0.01). Depending on the choice of outcome, two-armed full-scale trials may require fewer than 1,000 participants (continuous outcome) or 2,000 participants (categorical outcome).\nGood levels of participation, adherence, and retention appear to be achievable for participants through age 85 years. Care should be taken to ensure that an attention control condition does not attenuate intervention effects. Depending on the choice of outcome measures, the necessary sample sizes to conduct four-year trials appear to be feasible.\nClinicaltrials.gov Identifier: NCT00688155.', 'title': 'Designing clinical trials for assessing the effects of cognitive training and physical activity interventions on cognitive outcomes: the Seniors Health and Activity Research Program Pilot (SHARP-P) study, a randomized controlled trial.', 'date': '2011-05-28'}, '26417460': {'article_id': '26417460', 'content': 'Increasing research has evidenced that our brain retains a capacity to change in response to experience until late adulthood. This implies that cognitive training can possibly ameliorate age-associated cognitive decline by inducing training-specific neural plastic changes at both neural and behavioral levels. This longitudinal study examined the behavioral effects of a systematic thirteen-week cognitive training program on attention and working memory of older adults who were at risk of cognitive decline. These older adults were randomly assigned to the Cognitive Training Group (n = 109) and the Active Control Group (n = 100). Findings clearly indicated that training induced improvement in auditory and visual-spatial attention and working memory. The training effect was specific to the experience provided because no significant difference in verbal and visual-spatial memory between the two groups was observed. This pattern of findings is consistent with the prediction and the principle of experience-dependent neuroplasticity. Findings of our study provided further support to the notion that the neural plastic potential continues until older age. The baseline cognitive status did not correlate with pre- versus posttraining changes to any cognitive variables studied, suggesting that the initial cognitive status may not limit the neuroplastic potential of the brain at an old age.', 'title': 'Neural Plastic Effects of Cognitive Training on Aging Brain.', 'date': '2015-09-30'}, '21311196': {'article_id': '21311196', 'content': 'Many studies have suggested that cognitive training can result in cognitive gains in healthy older adults. We investigated whether personalized computerized cognitive training provides greater benefits than those obtained by playing conventional computer games.\nThis was a randomized double-blind interventional study. Self-referred healthy older adults (n = 155, 68 ± 7 years old) were assigned to either a personalized, computerized cognitive training or to a computer games group. Cognitive performance was assessed at baseline and after 3 months by a neuropsychological assessment battery. Differences in cognitive performance scores between and within groups were evaluated using mixed effects models in 2 approaches: adherence only (AO; n = 121) and intention to treat (ITT; n = 155).\nBoth groups improved in cognitive performance. The improvement in the personalized cognitive training group was significant (p < 0.03, AO and ITT approaches) in all 8 cognitive domains. However, in the computer games group it was significant (p < 0.05) in only 4 (AO) or 6 domains (ITT). In the AO analysis, personalized cognitive training was significantly more effective than playing games in improving visuospatial working memory (p = 0.0001), visuospatial learning (p = 0.0012) and focused attention (p = 0.0019).\nPersonalized, computerized cognitive training appears to be more effective than computer games in improving cognitive performance in healthy older adults. Further studies are needed to evaluate the ecological validity of these findings.', 'title': 'Computer-based, personalized cognitive training versus classical computer games: a randomized double-blind prospective trial of cognitive stimulation.', 'date': '2011-02-12'}, '23531885': {'article_id': '23531885', 'content': 'Cognitive training and aerobic training are known to improve cognitive functions. To examine the separate and combined effects of such training on cognitive performance, four groups of healthy older adults embarked on a 4 months cognitive and/or mild aerobic training. A first group [n = 33, mean age = 80 (66-90)] engaged in cognitive training, a second [n = 29, mean age = 81 (65-89)] in mild aerobic training, a third [n = 29, mean age = 79 (70-93)] in the combination of both, and a fourth [n = 31, mean age = 79 (71-92)] control group engaged in book-reading activity. The outcome was a well-validated multi-domain computerized cognitive evaluation for older adults. The results indicate that, when compared to older adults who did not engage in cognitive training (the mild aerobic and control groups) older adults who engaged in cognitive training (separate or combined training groups) showed significant improvement in cognitive performance on Hand-Eye Coordination, Global Visual Memory (GVM; working memory and long-term memory), Speed of Information Processing, Visual Scanning, and Naming. Indeed, individuals who did not engage in cognitive training showed no such improvements. Those results suggest that cognitive training is effective in improving cognitive performance and that it (and not mild aerobic training) is driving the improvement in the combined condition. Results are discussed in terms of the special circumstances of aerobic and cognitive training for older adults who are above 80 years of age.', 'title': 'Does combined cognitive training and physical activity training enhance cognitive abilities more than either alone? A four-condition randomized controlled trial among healthy older adults.', 'date': '2013-03-28'}}",1.0,Psychiatry & Neurology
106,"Is the risk of cholera at two-year follow-up higher, lower, or the same when comparing two doses of Dukoral with or without a booster dose to placebo?",lower,high,no,"['7967990', '10823767']",38197546,2024,"{'7967990': {'article_id': '7967990', 'content': 'The cholera epidemic in South America has reinforced the need for safe and effective oral vaccines. In a randomised, double-blind, placebo-controlled efficacy trial among 1563 Peruvian military recruits we have investigated the protective efficacy of an oral inactivated whole-cell/recombinant-B-subunit (WC/rBS) cholera vaccine. Participants were given two oral doses of cholera vaccine or Escherichia coli K12 placebo, with an interval of 7-14 days. 1426 (91%) subjects received the two prescribed doses and were followed up for a mean of 18 weeks (median 21 weeks). After vaccination, Vibrio cholerae O1 El Tor Ogawa was isolated from 17 subjects with diarrhoea. 16 of the cholera cases occurred 2 weeks or longer after the second dose of vaccine (14 placebo recipients, 2 vaccinees). We also detected 14 symptomless infections (11 [7 placebo recipients, 4 vaccinees]) 2 weeks or longer after the second dose. The vaccine had significant protective efficacy against cholera (86% [95% CI 37-97], p < 0.01) but not against symptomless infection (42% [-96 to 85]). All cholera cases were in people of blood group O, who made up 76% of the study population (p < 0.01). Two doses of WC/rBS vaccine, given 1 to 2 weeks apart, provide rapid, short-term protection against symptomatic cholera in adult South Americans, who are predominantly of blood group O. Long-term efficacy studies in Peruvian adults and children are under way.', 'title': 'Protective efficacy of oral whole-cell/recombinant-B-subunit cholera vaccine in Peruvian military recruits.', 'date': '1994-11-05'}, '10823767': {'article_id': '10823767', 'content': 'The protective efficacy of an oral inactivated whole cell Vibrio cholerae plus recombinant B subunit cholera vaccine was determined against El Tor cholera among Peruvian children and adults (2-65 years old) in a randomized, double-blind manner. Study subjects received 2 doses of vaccine or placebo 2 weeks apart, followed by a booster dose 10 months later. Surveillance for cholera was performed actively, with 2 visits per week to each household, and passively, at a local hospital. Stool samples were collected during diarrhea episodes and were cultured for V. cholerae. A total of 17,799 persons received 2 doses of vaccine or placebo, and 14,997 of these persons received the booster dose. After 2 doses (first surveillance period), V. cholerae biotype O1 was isolated from 17 vaccinees and 16 placebo recipients, demonstrating vaccine efficacy (VE) of -4%. After 3 doses (second surveillance period), V. cholerae O1 was isolated from 13 vaccinees and 32 placebo recipients, demonstrating VE of 61% (95% confidence interval ¿CI, 28%-79%). In the second surveillance period, the VE for illness requiring hospitalization was 82% (95% CI, 27%-96%). VE was also higher for persons >15 years old (VE, 72%; 95% CI, 28%-89%).', 'title': 'Two-year study of the protective efficacy of the oral whole cell plus recombinant B subunit cholera vaccine in Peru.', 'date': '2000-05-24'}}",0.0,"Public Health, Epidemiology & Health Systems"
107,"Is the risk of cholera at two-year follow-up higher, lower, or the same when comparing two doses of Shanchol to placebo?",lower,moderate,no,"['19819004', '26164097']",38197546,2024,"{'19819004': {'article_id': '19819004', 'content': 'Oral cholera vaccines consisting of killed whole cells have been available for many years, but they have not been used extensively in populations with endemic disease. An inexpensive, locally produced oral killed-whole-cell vaccine has been used in high-risk areas in Vietnam. To expand the use of this vaccine, it was modified to comply with WHO standards. We assessed the efficacy and safety of this modified vaccine in a population with endemic cholera.\nIn this double-blind trial, 107 774 non-pregnant residents of Kolkata, India, aged 1 year or older, were cluster-randomised by dwelling to receive two doses of either modified killed-whole-cell cholera vaccine (n=52 212; 1966 clusters) or heat-killed Escherichia coli K12 placebo (n=55 562; 1967 clusters), both delivered orally. Randomisation was done by computer-generated sequence in blocks of four. The primary endpoint was prevention of episodes of culture-confirmed Vibrio cholerae O1 diarrhoea severe enough for the patient to seek treatment in a health-care facility. We undertook an interim, per-protocol analysis at 2 years of follow-up that included individuals who received two completely ingested doses of vaccine or placebo. We assessed first episodes of cholera that occurred between 14 days and 730 days after receipt of the second dose. This study is registered with ClinicalTrials.gov, number NCT00289224.\n31 932 participants assigned to vaccine (1721 clusters) and 34 968 assigned to placebo (1757 clusters) received two doses of study treatment. There were 20 episodes of cholera in the vaccine group and 68 episodes in the placebo group (protective efficacy 67%; one-tailed 99% CI, lower bound 35%, p<0.0001). The vaccine protected individuals in age-groups 1.0-4.9 years, 5.0-14.9 years, and 15 years and older, and protective efficacy did not differ significantly between age-groups (p=0.28). We recorded no vaccine-related serious adverse events.\nThis modified killed-whole-cell oral vaccine, compliant with WHO standards, is safe, provides protection against clinically significant cholera in an endemic setting, and can be used in children aged 1.0-4.9 years, who are at highest risk of developing cholera in endemic settings.\nBill & Melinda Gates Foundation, Swedish International Development Cooperation Agency, Governments of South Korea, Sweden, and Kuwait.', 'title': 'Efficacy and safety of a modified killed-whole-cell oral cholera vaccine in India: an interim analysis of a cluster-randomised, double-blind, placebo-controlled trial.', 'date': '2009-10-13'}, '26164097': {'article_id': '26164097', 'content': 'Cholera is endemic in Bangladesh with epidemics occurring each year. The decision to use a cheap oral killed whole-cell cholera vaccine to control the disease depends on the feasibility and effectiveness of vaccination when delivered in a public health setting. We therefore assessed the feasibility and protective effect of delivering such a vaccine through routine government services in urban Bangladesh and evaluated the benefit of adding behavioural interventions to encourage safe drinking water and hand washing to vaccination in this setting.\nWe did this cluster-randomised open-label trial in Dhaka, Bangladesh. We randomly assigned 90 clusters (1:1:1) to vaccination only, vaccination and behavioural change, or no intervention. The primary outcome was overall protective effectiveness, assessed as the risk of severely dehydrating cholera during 2 years after vaccination for all individuals present at time of the second dose. This study is registered with ClinicalTrials.gov, number NCT01339845.\nOf 268,896 people present at baseline, we analysed 267,270: 94,675 assigned to vaccination only, 92,539 assigned to vaccination and behavioural change, and 80,056 assigned to non-intervention. Vaccine coverage was 65% in the vaccination only group and 66% in the vaccination and behavioural change group. Overall protective effectiveness was 37% (95% CI lower bound 18%; p=0·002) in the vaccination group and 45% (95% CI lower bound 24%; p=0·001) in the vaccination and behavioural change group. We recorded no vaccine-related serious adverse events.\nOur findings provide the first indication of the effect of delivering an oral killed whole-cell cholera vaccine to poor urban populations with endemic cholera using routine government services and will help policy makers to formulate vaccination strategies to reduce the burden of severely dehydrating cholera in such populations.\nBill & Melinda Gates Foundation.', 'title': 'Feasibility and effectiveness of oral cholera vaccine in an urban endemic setting in Bangladesh: a cluster randomised open-label trial.', 'date': '2015-07-15'}}",1.0,"Public Health, Epidemiology & Health Systems"
108,"Is the risk of cholera at five-year follow-up higher, lower, or the same when comparing two doses of Shanchol to placebo?",lower,high,no,"['24140390', '29463233']",38197546,2024,"{'24140390': {'article_id': '24140390', 'content': 'Efficacy and safety of a two-dose regimen of bivalent killed whole-cell oral cholera vaccine (Shantha Biotechnics, Hyderabad, India) to 3 years is established, but long-term efficacy is not. We aimed to assess protective efficacy up to 5 years in a slum area of Kolkata, India.\nIn our double-blind, cluster-randomised, placebo-controlled trial, we assessed incidence of cholera in non-pregnant individuals older than 1 year residing in 3933 dwellings (clusters) in Kolkata, India. We randomly allocated participants, by dwelling, to receive two oral doses of modified killed bivalent whole-cell cholera vaccine or heat-killed Escherichia coli K12 placebo, 14 days apart. Randomisation was done by use of a computer-generated sequence in blocks of four. The primary endpoint was prevention of episodes of culture-confirmed Vibrio cholerae O1 diarrhoea severe enough for patients to seek treatment in a health-care facility. We identified culture-confirmed cholera cases among participants seeking treatment for diarrhoea at a study clinic or government hospital between 14 days and 1825 days after receipt of the second dose. We assessed vaccine protection in a per-protocol population of participants who had completely ingested two doses of assigned study treatment.\n69 of 31\u2008932 recipients of vaccine and 219 of 34\u2008968 recipients of placebo developed cholera during 5 year follow-up (incidence 2·2 per 1000 in the vaccine group and 6·3 per 1000 in the placebo group). Cumulative protective efficacy of the vaccine at 5 years was 65% (95% CI 52-74; p<0·0001), and point estimates by year of follow-up suggested no evidence of decline in protective efficacy.\nSustained protection for 5 years at the level we reported has not been noted previously with other oral cholera vaccines. Established long-term efficacy of this vaccine could assist policy makers formulate rational vaccination strategies to reduce overall cholera burden in endemic settings.\nBill & Melinda Gates Foundation and the governments of South Korea and Sweden.', 'title': '5 year efficacy of a bivalent killed whole-cell oral cholera vaccine in Kolkata, India: a cluster-randomised, double-blind, placebo-controlled trial.', 'date': '2013-10-22'}, '29463233': {'article_id': '29463233', 'content': ""Oral cholera vaccine (OCV) is a feasible tool to prevent or mitigate cholera outbreaks. A better understanding of the vaccine's efficacy among different age groups and how rapidly its protection wanes could help guide vaccination policy.\nTo estimate the level and duration of OCV efficacy, we re-analyzed data from a previously published cluster-randomized, double-blind, placebo controlled trial with five years of follow-up. We used a Cox proportional hazards model and modeled the potentially time-dependent effect of age categories on both vaccine efficacy and risk of infection in the placebo group. In addition, we investigated the impact of an outbreak period on model estimation.\nVaccine efficacy was 38% (95% CI: -2%,62%) for those vaccinated from ages 1 to under 5 years old, 85% (95% CI: 67%,93%) for those 5 to under 15 years, and 69% (95% CI: 49%,81%) for those vaccinated at ages 15 years and older. Among adult vaccinees, efficacy did not appear to wane during the trial, but there was insufficient data to assess the waning of efficacy among child vaccinees.\nThrough this re-analysis we were able to detect a statistically significant difference in OCV efficacy when the vaccine was administered to children under 5 years old vs. children 5 years and older. The estimated efficacies are more similar to the previously published analysis based on the first two years of follow-up than the analysis based on all five years.\nClinicalTrials.gov identifier NCT00289224."", 'title': 'Efficacy of a bivalent killed whole-cell cholera vaccine over five years: a re-analysis of a cluster-randomized trial.', 'date': '2018-02-22'}}",1.0,"Public Health, Epidemiology & Health Systems"
109,"Is the risk of cholera at two-year follow-up higher, lower, or the same when comparing one dose of Shanchol to placebo?",lower,high,no,['29550406'],38197546,2024,"{'29550406': {'article_id': '29550406', 'content': 'A single-dose regimen of inactivated whole-cell oral cholera vaccine (OCV) is attractive because it reduces logistical challenges for vaccination and could enable more people to be vaccinated. Previously, we reported the efficacy of a single dose of an OCV vaccine during the 6 months following dosing. Herein, we report the results of 2 years of follow-up.\nIn this placebo-controlled, double-blind trial done in Dhaka, Bangladesh, individuals aged 1 year or older with no history of receipt of OCV were randomly assigned to receive a single dose of inactivated OCV or oral placebo. The primary endpoint was a confirmed episode of non-bloody diarrhoea for which the onset was at least 7 days after dosing and a faecal culture was positive for Vibrio cholerae O1 or O139. Passive surveillance for diarrhoea was done in 13 hospitals or major clinics located in or near the study area for 2 years after the last administered dose. We assessed the protective efficacy of the OCV against culture-confirmed cholera occurring 7-730 days after dosing with both crude and multivariable per-protocol analyses. This trial is registered at ClinicalTrials.gov, number NCT02027207.\nBetween Jan 10, 2014, and Feb 4, 2014, 205\u2008513 people were randomly assigned to receive either vaccine or placebo, of whom 204\u2008700 (102\u2008552 vaccine recipients and 102\u2008148 placebo recipients) were included in the per-protocol analysis. 287 first episodes of cholera (109 among vaccine recipients and 178 among placebo recipients) were detected during the 2-year follow-up; 138 of these episodes (46 in vaccine recipients and 92 in placebo recipients) were associated with severe dehydration. The overall incidence rates of initial cholera episodes were 0·22 (95% CI 0·18 to 0·27) per 100\u2008000 person-days in vaccine recipients versus 0·36 (0·31 to 0·42) per 100\u2008000 person-days in placebo recipients (adjusted protective efficacy 39%, 95% CI 23 to 52). The overall incidence of severe cholera was 0·09 (0·07 to 0·12) per 100\u2008000 person-days versus 0·19 (0·15 to 0·23; adjusted protective efficacy 50%, 29 to 65). Vaccine protective efficacy was 52% (8 to 75) against all cholera episodes and 71% (27 to 88) against severe cholera episodes in participants aged 5 years to younger than 15 years. For participants aged 15 years or older, vaccine protective efficacy was 59% (42 to 71) against all cholera episodes and 59% (35 to 74) against severe cholera. The protection in the older age groups was sustained throughout the 2-year follow-up. In participants younger than 5 years, the vaccine did not show protection against either all cholera episodes (protective efficacy -13%, -68 to 25) or severe cholera episodes (-44%, -220 to 35).\nA single dose of the inactivated whole-cell OCV offered protection to older children and adults that was sustained for at least 2 years. The absence of protection of young children might reflect a lesser degree of pre-existing natural immunity in this age group.\nBill & Melinda Gates Foundation to the International Vaccine Institute.', 'title': 'Efficacy of a single-dose regimen of inactivated whole-cell oral cholera vaccine: results from 2 years of follow-up of a randomised trial.', 'date': '2018-03-20'}}",1.0,"Public Health, Epidemiology & Health Systems"
110,"Is the risk of severe dehydrating cholera at two-year follow-up higher, lower, or the same when comparing one dose of Shanchol to placebo?",lower,high,no,['29550406'],38197546,2024,"{'29550406': {'article_id': '29550406', 'content': 'A single-dose regimen of inactivated whole-cell oral cholera vaccine (OCV) is attractive because it reduces logistical challenges for vaccination and could enable more people to be vaccinated. Previously, we reported the efficacy of a single dose of an OCV vaccine during the 6 months following dosing. Herein, we report the results of 2 years of follow-up.\nIn this placebo-controlled, double-blind trial done in Dhaka, Bangladesh, individuals aged 1 year or older with no history of receipt of OCV were randomly assigned to receive a single dose of inactivated OCV or oral placebo. The primary endpoint was a confirmed episode of non-bloody diarrhoea for which the onset was at least 7 days after dosing and a faecal culture was positive for Vibrio cholerae O1 or O139. Passive surveillance for diarrhoea was done in 13 hospitals or major clinics located in or near the study area for 2 years after the last administered dose. We assessed the protective efficacy of the OCV against culture-confirmed cholera occurring 7-730 days after dosing with both crude and multivariable per-protocol analyses. This trial is registered at ClinicalTrials.gov, number NCT02027207.\nBetween Jan 10, 2014, and Feb 4, 2014, 205\u2008513 people were randomly assigned to receive either vaccine or placebo, of whom 204\u2008700 (102\u2008552 vaccine recipients and 102\u2008148 placebo recipients) were included in the per-protocol analysis. 287 first episodes of cholera (109 among vaccine recipients and 178 among placebo recipients) were detected during the 2-year follow-up; 138 of these episodes (46 in vaccine recipients and 92 in placebo recipients) were associated with severe dehydration. The overall incidence rates of initial cholera episodes were 0·22 (95% CI 0·18 to 0·27) per 100\u2008000 person-days in vaccine recipients versus 0·36 (0·31 to 0·42) per 100\u2008000 person-days in placebo recipients (adjusted protective efficacy 39%, 95% CI 23 to 52). The overall incidence of severe cholera was 0·09 (0·07 to 0·12) per 100\u2008000 person-days versus 0·19 (0·15 to 0·23; adjusted protective efficacy 50%, 29 to 65). Vaccine protective efficacy was 52% (8 to 75) against all cholera episodes and 71% (27 to 88) against severe cholera episodes in participants aged 5 years to younger than 15 years. For participants aged 15 years or older, vaccine protective efficacy was 59% (42 to 71) against all cholera episodes and 59% (35 to 74) against severe cholera. The protection in the older age groups was sustained throughout the 2-year follow-up. In participants younger than 5 years, the vaccine did not show protection against either all cholera episodes (protective efficacy -13%, -68 to 25) or severe cholera episodes (-44%, -220 to 35).\nA single dose of the inactivated whole-cell OCV offered protection to older children and adults that was sustained for at least 2 years. The absence of protection of young children might reflect a lesser degree of pre-existing natural immunity in this age group.\nBill & Melinda Gates Foundation to the International Vaccine Institute.', 'title': 'Efficacy of a single-dose regimen of inactivated whole-cell oral cholera vaccine: results from 2 years of follow-up of a randomised trial.', 'date': '2018-03-20'}}",1.0,"Public Health, Epidemiology & Health Systems"
111,"Is vision-related quality of life higher, lower, or the same when comparing telerehabilitation with a therapist to self-guided training?",no difference,very low,yes,['34081648'],36637057,2023,"{'34081648': {'article_id': '34081648', 'content': ""Head-mounted low vision devices have become a viable alternative to enhance residual vision. This study supports the use of a head-mounted display to improve aspects of functional vision and quality of life. Much is still unknown regarding the required frequency, duration, or potential effectiveness of this telerehabilitation training protocol or what characteristics best identify optimal users.\nA randomized study explored the effect of telerehabilitation on quality of life and functional vision in individuals with low vision using a head-mounted display.\nWe recruited 57 participants (age, 21 to 82 years; mean, 54.5 years) among new prospective eSight Eyewear users, randomized 1:1 into two parallel groups; the experimental group received the telerehabilitation training provided by a low vision therapist, whereas the control group received the self-training standard offered by the device manufacturer and without involvement of a low vision therapist. The primary outcome measures were the impact of telerehabilitation on validated measures of assistive technology-related quality of life: the Psychosocial Impact of Assistive Devices Scale and the Quebec User Evaluation of Satisfaction with Assistive Technology scale. Exploratory outcomes were the assessment of self-reported functional vision using the Veterans Affairs Low Vision Visual Functioning Questionnaire-48 and cybersickness associated with head-mounted display use with the Simulator Sickness Questionnaire.\nAssistive technology-related quality of life was improved when measured by the satisfaction scale but not the psychosocial scale within the first 3 months, independently of training type. Overall, functional vision improvement was observed within the first 2 weeks of device use and maintained during the 6-month study, independently of group type. Cybersickness outcomes were similar between training groups and did not change significantly for 6 months.\neSight Eyewear, either with telerehabilitation or with the manufacturer self-training comparison, improved functional vision and increased users' quality of life within the initial 3 months of device training and practice."", 'title': 'Head-mounted Visual Assistive Technology-related Quality of Life Changes after Telerehabilitation.', 'date': '2021-06-04'}}",1.0,Other
112,"Is device abandonment after 2 weeks higher, lower, or the same when comparing telerehabilitation with a therapist to self-guided training?",lower,very low,yes,['34081649'],36637057,2023,"{'34081649': {'article_id': '34081649', 'content': ""A recent trend in low vision rehabilitation has been the use of portable head-mounted displays to enhance residual vision. Our study confirms the feasibility of telerehabilitation and informs the development of evidence-based recommendations to improve telerehabilitation interventions to reduce device abandonment.\nTo develop evidence-based recommendations for telerehabilitation, we conducted a feasibility study in preparation for a future randomized trial on the use of head-mounted displays.\nWe recruited novice eSight Eyewear users, randomized 1:1: the experimental group received telerehabilitation by a low vision therapist using video conferencing; the control group completed at home self-training provided by the device manufacturer. The primary feasibility outcomes were whether the recruitment goal of 60 participants (30/group) was attainable within 1 year and how participants judged the accessibility and acceptability of the telerehabilitation. An exploratory outcome was the impact of telerehabilitation on eSight Eyewear use behavior.\nAmong 333 eSight users, 57 participants were enrolled, of which 35% withdrew from the study, whereas the remainder completed the 6-month follow-up. The withdrawal rate was higher in the control group but did not differ significantly from the experimental group. High accessibility (93% of participants accessed the platform) and global acceptability (100% overall satisfaction) were reported among those who completed the telerehabilitation protocol. The therapist had no difficulty judging the participants' reading performances qualitatively while participants used their device to read their eSkills and VisExc guides. Most participants improved their daily activities, based on qualitative reports of the attained goals. Seventy-nine percent of individuals declined to participate, whereas 16% of participants decided not to use eSight Eyewear anymore.\nThe data demonstrated the feasibility of a randomized controlled telerehabilitation study for people with low vision using a head-mounted display. Positive feedback from the participants and the therapist suggests the potential value of this modality for low vision services."", 'title': 'Personalized Telerehabilitation for a Head-mounted Low Vision Aid: A Randomized Feasibility Study.', 'date': '2021-06-04'}}",0.0,Other
113,"Is device abandonment after 3 months higher, lower, or the same when comparing telerehabilitation with a therapist to self-guided training?",no difference,very low,yes,['34081649'],36637057,2023,"{'34081649': {'article_id': '34081649', 'content': ""A recent trend in low vision rehabilitation has been the use of portable head-mounted displays to enhance residual vision. Our study confirms the feasibility of telerehabilitation and informs the development of evidence-based recommendations to improve telerehabilitation interventions to reduce device abandonment.\nTo develop evidence-based recommendations for telerehabilitation, we conducted a feasibility study in preparation for a future randomized trial on the use of head-mounted displays.\nWe recruited novice eSight Eyewear users, randomized 1:1: the experimental group received telerehabilitation by a low vision therapist using video conferencing; the control group completed at home self-training provided by the device manufacturer. The primary feasibility outcomes were whether the recruitment goal of 60 participants (30/group) was attainable within 1 year and how participants judged the accessibility and acceptability of the telerehabilitation. An exploratory outcome was the impact of telerehabilitation on eSight Eyewear use behavior.\nAmong 333 eSight users, 57 participants were enrolled, of which 35% withdrew from the study, whereas the remainder completed the 6-month follow-up. The withdrawal rate was higher in the control group but did not differ significantly from the experimental group. High accessibility (93% of participants accessed the platform) and global acceptability (100% overall satisfaction) were reported among those who completed the telerehabilitation protocol. The therapist had no difficulty judging the participants' reading performances qualitatively while participants used their device to read their eSkills and VisExc guides. Most participants improved their daily activities, based on qualitative reports of the attained goals. Seventy-nine percent of individuals declined to participate, whereas 16% of participants decided not to use eSight Eyewear anymore.\nThe data demonstrated the feasibility of a randomized controlled telerehabilitation study for people with low vision using a head-mounted display. Positive feedback from the participants and the therapist suggests the potential value of this modality for low vision services."", 'title': 'Personalized Telerehabilitation for a Head-mounted Low Vision Aid: A Randomized Feasibility Study.', 'date': '2021-06-04'}}",0.0,Other
114,"Is weight by 30 days of age higher, lower, or the same when comparing carbohydrate supplementation to no supplementation?",higher,very low,yes,['25538834'],32898300,2020,"{'25538834': {'article_id': '25538834', 'content': 'Necrotizing enterocolitis (NEC) is one of the most destructive diseases associated with conditions of neonatal prematurity. Supplementation with enteral prebiotics may reduce the incidence of NEC, especially in infants who fed exclusively with breast-milk. Therefore, we compared the efficacy and safety of enteral supplementation of a prebiotic mixture (short chain galacto-oligosaccharides/long chain fructo-oligosaccharides [SCGOS/LCFOS]) versus no intervention on incidence of NEC in preterm infants.\nIn a single-center randomized control trial 75 preterm infants (birth weight [BW] ≤1500 g, gestational age ≤34 weeks and were not fed with formula) on 30 ml/kg/day volume of breast-milk were randomly allocated to have enteral supplementation with a prebiotic mixture (SCGOS/LCFOS; 9:1) or not receive any prebiotic. The incidence of suspected NEC, feeding intolerance, time to full enteral feeds, duration of hospitalization were investigated.\nDifferences in demographic characteristics were not statistically important. SCGOS/LCFOS mixture significantly reduced the incidence of suspected NEC, (1 [4.0%] vs. 11 [22.0%]; hazard ratio: 0.49 [95% confidence interval: 0.29-0.84]; P = 0.002), and time to full enteral feeds (11 [7-21] vs. 14 [8-36] days; P - 0.02]. Also duration of hospitalization was meaningfully shorter in the prebiotic group (16 [9-45] vs. 25 [11-80]; P - 0.004]. Prebiotic oligosaccharides were well tolerated by very low BW (VLBW) infants.\nEnteral supplementation with prebiotic significantly reduced the incidence of NEC in VLBW infants who were fed exclusively breast-milk. This finding suggests that it might have been the complete removal of formula which caused a synergistic effect between nonhuman neutral oligosaccharides (prebiotic) and human oligosaccharides.', 'title': 'The Effect of Neutral Oligosaccharides on Reducing the Incidence of Necrotizing Enterocolitis in Preterm infants: A Randomized Clinical Trial.', 'date': '2014-12-30'}}",0.0,Pediatrics & Neonatology
115,"Is the risk of feeding intolerance higher, lower, or the same when comparing carbohydrate supplementation to no supplementation?",no difference,very low,yes,['25538834'],32898300,2020,"{'25538834': {'article_id': '25538834', 'content': 'Necrotizing enterocolitis (NEC) is one of the most destructive diseases associated with conditions of neonatal prematurity. Supplementation with enteral prebiotics may reduce the incidence of NEC, especially in infants who fed exclusively with breast-milk. Therefore, we compared the efficacy and safety of enteral supplementation of a prebiotic mixture (short chain galacto-oligosaccharides/long chain fructo-oligosaccharides [SCGOS/LCFOS]) versus no intervention on incidence of NEC in preterm infants.\nIn a single-center randomized control trial 75 preterm infants (birth weight [BW] ≤1500 g, gestational age ≤34 weeks and were not fed with formula) on 30 ml/kg/day volume of breast-milk were randomly allocated to have enteral supplementation with a prebiotic mixture (SCGOS/LCFOS; 9:1) or not receive any prebiotic. The incidence of suspected NEC, feeding intolerance, time to full enteral feeds, duration of hospitalization were investigated.\nDifferences in demographic characteristics were not statistically important. SCGOS/LCFOS mixture significantly reduced the incidence of suspected NEC, (1 [4.0%] vs. 11 [22.0%]; hazard ratio: 0.49 [95% confidence interval: 0.29-0.84]; P = 0.002), and time to full enteral feeds (11 [7-21] vs. 14 [8-36] days; P - 0.02]. Also duration of hospitalization was meaningfully shorter in the prebiotic group (16 [9-45] vs. 25 [11-80]; P - 0.004]. Prebiotic oligosaccharides were well tolerated by very low BW (VLBW) infants.\nEnteral supplementation with prebiotic significantly reduced the incidence of NEC in VLBW infants who were fed exclusively breast-milk. This finding suggests that it might have been the complete removal of formula which caused a synergistic effect between nonhuman neutral oligosaccharides (prebiotic) and human oligosaccharides.', 'title': 'The Effect of Neutral Oligosaccharides on Reducing the Incidence of Necrotizing Enterocolitis in Preterm infants: A Randomized Clinical Trial.', 'date': '2014-12-30'}}",1.0,Emergency Medicine & Critical Care
116,"Is the risk of necrotising enterocolitis (NEC) higher, lower, or the same when comparing carbohydrate supplementation to no supplementation?",no difference,very low,no,['25538834'],32898300,2020,"{'25538834': {'article_id': '25538834', 'content': 'Necrotizing enterocolitis (NEC) is one of the most destructive diseases associated with conditions of neonatal prematurity. Supplementation with enteral prebiotics may reduce the incidence of NEC, especially in infants who fed exclusively with breast-milk. Therefore, we compared the efficacy and safety of enteral supplementation of a prebiotic mixture (short chain galacto-oligosaccharides/long chain fructo-oligosaccharides [SCGOS/LCFOS]) versus no intervention on incidence of NEC in preterm infants.\nIn a single-center randomized control trial 75 preterm infants (birth weight [BW] ≤1500 g, gestational age ≤34 weeks and were not fed with formula) on 30 ml/kg/day volume of breast-milk were randomly allocated to have enteral supplementation with a prebiotic mixture (SCGOS/LCFOS; 9:1) or not receive any prebiotic. The incidence of suspected NEC, feeding intolerance, time to full enteral feeds, duration of hospitalization were investigated.\nDifferences in demographic characteristics were not statistically important. SCGOS/LCFOS mixture significantly reduced the incidence of suspected NEC, (1 [4.0%] vs. 11 [22.0%]; hazard ratio: 0.49 [95% confidence interval: 0.29-0.84]; P = 0.002), and time to full enteral feeds (11 [7-21] vs. 14 [8-36] days; P - 0.02]. Also duration of hospitalization was meaningfully shorter in the prebiotic group (16 [9-45] vs. 25 [11-80]; P - 0.004]. Prebiotic oligosaccharides were well tolerated by very low BW (VLBW) infants.\nEnteral supplementation with prebiotic significantly reduced the incidence of NEC in VLBW infants who were fed exclusively breast-milk. This finding suggests that it might have been the complete removal of formula which caused a synergistic effect between nonhuman neutral oligosaccharides (prebiotic) and human oligosaccharides.', 'title': 'The Effect of Neutral Oligosaccharides on Reducing the Incidence of Necrotizing Enterocolitis in Preterm infants: A Randomized Clinical Trial.', 'date': '2014-12-30'}}",0.0,Pediatrics & Neonatology
117,"Is the risk of severe post-partum haemorrhage (PPH) higher, lower, or the same when comparing oxytocin to no intervention?",no difference,very low,yes,['24130463'],27078125,2016,"{'24130463': {'article_id': '24130463', 'content': ""Oxytocin (10 IU) is the drug of choice for prevention of postpartum hemorrhage (PPH). Its use has generally been restricted to medically trained staff in health facilities. We assessed the effectiveness, safety, and feasibility of PPH prevention using oxytocin injected by peripheral health care providers without midwifery skills at home births.\nThis community-based, cluster-randomized trial was conducted in four rural districts in Ghana. We randomly allocated 54 community health officers (stratified on district and catchment area distance to a health facility: ≥10 km versus <10 km) to intervention (one injection of oxytocin [10 IU] one minute after birth) and control (no provision of prophylactic oxytocin) arms. Births attended by a community health officer constituted a cluster. Our primary outcome was PPH, using multiple definitions; (PPH-1) blood loss ≥500 mL; (PPH-2) PPH-1 plus women who received early treatment for PPH; and (PPH-3) PPH-2 plus any other women referred to hospital for postpartum bleeding. Unsafe practice is defined as oxytocin use before delivery of the baby. We enrolled 689 and 897 women, respectively, into oxytocin and control arms of the trial from April 2011 to November 2012. In oxytocin and control arms, respectively, PPH-1 rates were 2.6% versus 5.5% (RR: 0.49; 95% CI: 0.27-0.88); PPH-2 rates were 3.8% versus 10.8% (RR: 0.35; 95% CI: 0.18-0.63), and PPH-3 rates were similar to those of PPH-2. Compared to women in control clusters, those in the intervention clusters lost 45.1 mL (17.7-72.6) less blood. There were no cases of oxytocin use before delivery of the baby and no major adverse events requiring notification of the institutional review boards. Limitations include an unblinded trial and imbalanced numbers of participants, favoring controls.\nMaternal health care planners can consider adapting this model to extend the use of oxytocin into peripheral settings including, in some contexts, home births.\nClinicalTrials.gov NCT01108289 Please see later in the article for the Editors' Summary."", 'title': 'Effect on postpartum hemorrhage of prophylactic oxytocin (10 IU) by injection by community health officers in Ghana: a community-based, cluster-randomized trial.', 'date': '2013-10-17'}}",0.0,Obstetrics & Gynecology
118,"Is the risk of PPH (> 500 mL)  higher, lower, or the same when comparing oxytocin to no intervention?",lower,low,no,['24130463'],27078125,2016,"{'24130463': {'article_id': '24130463', 'content': ""Oxytocin (10 IU) is the drug of choice for prevention of postpartum hemorrhage (PPH). Its use has generally been restricted to medically trained staff in health facilities. We assessed the effectiveness, safety, and feasibility of PPH prevention using oxytocin injected by peripheral health care providers without midwifery skills at home births.\nThis community-based, cluster-randomized trial was conducted in four rural districts in Ghana. We randomly allocated 54 community health officers (stratified on district and catchment area distance to a health facility: ≥10 km versus <10 km) to intervention (one injection of oxytocin [10 IU] one minute after birth) and control (no provision of prophylactic oxytocin) arms. Births attended by a community health officer constituted a cluster. Our primary outcome was PPH, using multiple definitions; (PPH-1) blood loss ≥500 mL; (PPH-2) PPH-1 plus women who received early treatment for PPH; and (PPH-3) PPH-2 plus any other women referred to hospital for postpartum bleeding. Unsafe practice is defined as oxytocin use before delivery of the baby. We enrolled 689 and 897 women, respectively, into oxytocin and control arms of the trial from April 2011 to November 2012. In oxytocin and control arms, respectively, PPH-1 rates were 2.6% versus 5.5% (RR: 0.49; 95% CI: 0.27-0.88); PPH-2 rates were 3.8% versus 10.8% (RR: 0.35; 95% CI: 0.18-0.63), and PPH-3 rates were similar to those of PPH-2. Compared to women in control clusters, those in the intervention clusters lost 45.1 mL (17.7-72.6) less blood. There were no cases of oxytocin use before delivery of the baby and no major adverse events requiring notification of the institutional review boards. Limitations include an unblinded trial and imbalanced numbers of participants, favoring controls.\nMaternal health care planners can consider adapting this model to extend the use of oxytocin into peripheral settings including, in some contexts, home births.\nClinicalTrials.gov NCT01108289 Please see later in the article for the Editors' Summary."", 'title': 'Effect on postpartum hemorrhage of prophylactic oxytocin (10 IU) by injection by community health officers in Ghana: a community-based, cluster-randomized trial.', 'date': '2013-10-17'}}",1.0,Obstetrics & Gynecology
119,"Is transfer or referral of the mother to a healthcare facility higher, lower, or the same when comparing oxytocin to no intervention?",no difference,low,yes,['24130463'],27078125,2016,"{'24130463': {'article_id': '24130463', 'content': ""Oxytocin (10 IU) is the drug of choice for prevention of postpartum hemorrhage (PPH). Its use has generally been restricted to medically trained staff in health facilities. We assessed the effectiveness, safety, and feasibility of PPH prevention using oxytocin injected by peripheral health care providers without midwifery skills at home births.\nThis community-based, cluster-randomized trial was conducted in four rural districts in Ghana. We randomly allocated 54 community health officers (stratified on district and catchment area distance to a health facility: ≥10 km versus <10 km) to intervention (one injection of oxytocin [10 IU] one minute after birth) and control (no provision of prophylactic oxytocin) arms. Births attended by a community health officer constituted a cluster. Our primary outcome was PPH, using multiple definitions; (PPH-1) blood loss ≥500 mL; (PPH-2) PPH-1 plus women who received early treatment for PPH; and (PPH-3) PPH-2 plus any other women referred to hospital for postpartum bleeding. Unsafe practice is defined as oxytocin use before delivery of the baby. We enrolled 689 and 897 women, respectively, into oxytocin and control arms of the trial from April 2011 to November 2012. In oxytocin and control arms, respectively, PPH-1 rates were 2.6% versus 5.5% (RR: 0.49; 95% CI: 0.27-0.88); PPH-2 rates were 3.8% versus 10.8% (RR: 0.35; 95% CI: 0.18-0.63), and PPH-3 rates were similar to those of PPH-2. Compared to women in control clusters, those in the intervention clusters lost 45.1 mL (17.7-72.6) less blood. There were no cases of oxytocin use before delivery of the baby and no major adverse events requiring notification of the institutional review boards. Limitations include an unblinded trial and imbalanced numbers of participants, favoring controls.\nMaternal health care planners can consider adapting this model to extend the use of oxytocin into peripheral settings including, in some contexts, home births.\nClinicalTrials.gov NCT01108289 Please see later in the article for the Editors' Summary."", 'title': 'Effect on postpartum hemorrhage of prophylactic oxytocin (10 IU) by injection by community health officers in Ghana: a community-based, cluster-randomized trial.', 'date': '2013-10-17'}}",0.0,Obstetrics & Gynecology
120,"Is the risk of stillbirth higher, lower, or the same when comparing oxytocin to no intervention?",no difference,low,yes,['24130463'],27078125,2016,"{'24130463': {'article_id': '24130463', 'content': ""Oxytocin (10 IU) is the drug of choice for prevention of postpartum hemorrhage (PPH). Its use has generally been restricted to medically trained staff in health facilities. We assessed the effectiveness, safety, and feasibility of PPH prevention using oxytocin injected by peripheral health care providers without midwifery skills at home births.\nThis community-based, cluster-randomized trial was conducted in four rural districts in Ghana. We randomly allocated 54 community health officers (stratified on district and catchment area distance to a health facility: ≥10 km versus <10 km) to intervention (one injection of oxytocin [10 IU] one minute after birth) and control (no provision of prophylactic oxytocin) arms. Births attended by a community health officer constituted a cluster. Our primary outcome was PPH, using multiple definitions; (PPH-1) blood loss ≥500 mL; (PPH-2) PPH-1 plus women who received early treatment for PPH; and (PPH-3) PPH-2 plus any other women referred to hospital for postpartum bleeding. Unsafe practice is defined as oxytocin use before delivery of the baby. We enrolled 689 and 897 women, respectively, into oxytocin and control arms of the trial from April 2011 to November 2012. In oxytocin and control arms, respectively, PPH-1 rates were 2.6% versus 5.5% (RR: 0.49; 95% CI: 0.27-0.88); PPH-2 rates were 3.8% versus 10.8% (RR: 0.35; 95% CI: 0.18-0.63), and PPH-3 rates were similar to those of PPH-2. Compared to women in control clusters, those in the intervention clusters lost 45.1 mL (17.7-72.6) less blood. There were no cases of oxytocin use before delivery of the baby and no major adverse events requiring notification of the institutional review boards. Limitations include an unblinded trial and imbalanced numbers of participants, favoring controls.\nMaternal health care planners can consider adapting this model to extend the use of oxytocin into peripheral settings including, in some contexts, home births.\nClinicalTrials.gov NCT01108289 Please see later in the article for the Editors' Summary."", 'title': 'Effect on postpartum hemorrhage of prophylactic oxytocin (10 IU) by injection by community health officers in Ghana: a community-based, cluster-randomized trial.', 'date': '2013-10-17'}}",1.0,Obstetrics & Gynecology
121,"Is the risk of early infant death (0-3 days) higher, lower, or the same when comparing oxytocin to no intervention?",no difference,low,yes,['24130463'],27078125,2016,"{'24130463': {'article_id': '24130463', 'content': ""Oxytocin (10 IU) is the drug of choice for prevention of postpartum hemorrhage (PPH). Its use has generally been restricted to medically trained staff in health facilities. We assessed the effectiveness, safety, and feasibility of PPH prevention using oxytocin injected by peripheral health care providers without midwifery skills at home births.\nThis community-based, cluster-randomized trial was conducted in four rural districts in Ghana. We randomly allocated 54 community health officers (stratified on district and catchment area distance to a health facility: ≥10 km versus <10 km) to intervention (one injection of oxytocin [10 IU] one minute after birth) and control (no provision of prophylactic oxytocin) arms. Births attended by a community health officer constituted a cluster. Our primary outcome was PPH, using multiple definitions; (PPH-1) blood loss ≥500 mL; (PPH-2) PPH-1 plus women who received early treatment for PPH; and (PPH-3) PPH-2 plus any other women referred to hospital for postpartum bleeding. Unsafe practice is defined as oxytocin use before delivery of the baby. We enrolled 689 and 897 women, respectively, into oxytocin and control arms of the trial from April 2011 to November 2012. In oxytocin and control arms, respectively, PPH-1 rates were 2.6% versus 5.5% (RR: 0.49; 95% CI: 0.27-0.88); PPH-2 rates were 3.8% versus 10.8% (RR: 0.35; 95% CI: 0.18-0.63), and PPH-3 rates were similar to those of PPH-2. Compared to women in control clusters, those in the intervention clusters lost 45.1 mL (17.7-72.6) less blood. There were no cases of oxytocin use before delivery of the baby and no major adverse events requiring notification of the institutional review boards. Limitations include an unblinded trial and imbalanced numbers of participants, favoring controls.\nMaternal health care planners can consider adapting this model to extend the use of oxytocin into peripheral settings including, in some contexts, home births.\nClinicalTrials.gov NCT01108289 Please see later in the article for the Editors' Summary."", 'title': 'Effect on postpartum hemorrhage of prophylactic oxytocin (10 IU) by injection by community health officers in Ghana: a community-based, cluster-randomized trial.', 'date': '2013-10-17'}}",0.0,Obstetrics & Gynecology
226,"Is maternal anaemia higher, lower, or the same when comparing oxytocin to no intervention?",insufficient data,,no,['24130463'],27078125,2016,"{'24130463': {'article_id': '24130463', 'content': ""Oxytocin (10 IU) is the drug of choice for prevention of postpartum hemorrhage (PPH). Its use has generally been restricted to medically trained staff in health facilities. We assessed the effectiveness, safety, and feasibility of PPH prevention using oxytocin injected by peripheral health care providers without midwifery skills at home births.\nThis community-based, cluster-randomized trial was conducted in four rural districts in Ghana. We randomly allocated 54 community health officers (stratified on district and catchment area distance to a health facility: ≥10 km versus <10 km) to intervention (one injection of oxytocin [10 IU] one minute after birth) and control (no provision of prophylactic oxytocin) arms. Births attended by a community health officer constituted a cluster. Our primary outcome was PPH, using multiple definitions; (PPH-1) blood loss ≥500 mL; (PPH-2) PPH-1 plus women who received early treatment for PPH; and (PPH-3) PPH-2 plus any other women referred to hospital for postpartum bleeding. Unsafe practice is defined as oxytocin use before delivery of the baby. We enrolled 689 and 897 women, respectively, into oxytocin and control arms of the trial from April 2011 to November 2012. In oxytocin and control arms, respectively, PPH-1 rates were 2.6% versus 5.5% (RR: 0.49; 95% CI: 0.27-0.88); PPH-2 rates were 3.8% versus 10.8% (RR: 0.35; 95% CI: 0.18-0.63), and PPH-3 rates were similar to those of PPH-2. Compared to women in control clusters, those in the intervention clusters lost 45.1 mL (17.7-72.6) less blood. There were no cases of oxytocin use before delivery of the baby and no major adverse events requiring notification of the institutional review boards. Limitations include an unblinded trial and imbalanced numbers of participants, favoring controls.\nMaternal health care planners can consider adapting this model to extend the use of oxytocin into peripheral settings including, in some contexts, home births.\nClinicalTrials.gov NCT01108289 Please see later in the article for the Editors' Summary."", 'title': 'Effect on postpartum hemorrhage of prophylactic oxytocin (10 IU) by injection by community health officers in Ghana: a community-based, cluster-randomized trial.', 'date': '2013-10-17'}}",1.0,Obstetrics & Gynecology
227,"Is neonatal death within 28 days higher, lower, or the same when comparing oxytocin to no intervention?",insufficient data,,no,['24130463'],27078125,2016,"{'24130463': {'article_id': '24130463', 'content': ""Oxytocin (10 IU) is the drug of choice for prevention of postpartum hemorrhage (PPH). Its use has generally been restricted to medically trained staff in health facilities. We assessed the effectiveness, safety, and feasibility of PPH prevention using oxytocin injected by peripheral health care providers without midwifery skills at home births.\nThis community-based, cluster-randomized trial was conducted in four rural districts in Ghana. We randomly allocated 54 community health officers (stratified on district and catchment area distance to a health facility: ≥10 km versus <10 km) to intervention (one injection of oxytocin [10 IU] one minute after birth) and control (no provision of prophylactic oxytocin) arms. Births attended by a community health officer constituted a cluster. Our primary outcome was PPH, using multiple definitions; (PPH-1) blood loss ≥500 mL; (PPH-2) PPH-1 plus women who received early treatment for PPH; and (PPH-3) PPH-2 plus any other women referred to hospital for postpartum bleeding. Unsafe practice is defined as oxytocin use before delivery of the baby. We enrolled 689 and 897 women, respectively, into oxytocin and control arms of the trial from April 2011 to November 2012. In oxytocin and control arms, respectively, PPH-1 rates were 2.6% versus 5.5% (RR: 0.49; 95% CI: 0.27-0.88); PPH-2 rates were 3.8% versus 10.8% (RR: 0.35; 95% CI: 0.18-0.63), and PPH-3 rates were similar to those of PPH-2. Compared to women in control clusters, those in the intervention clusters lost 45.1 mL (17.7-72.6) less blood. There were no cases of oxytocin use before delivery of the baby and no major adverse events requiring notification of the institutional review boards. Limitations include an unblinded trial and imbalanced numbers of participants, favoring controls.\nMaternal health care planners can consider adapting this model to extend the use of oxytocin into peripheral settings including, in some contexts, home births.\nClinicalTrials.gov NCT01108289 Please see later in the article for the Editors' Summary."", 'title': 'Effect on postpartum hemorrhage of prophylactic oxytocin (10 IU) by injection by community health officers in Ghana: a community-based, cluster-randomized trial.', 'date': '2013-10-17'}}",1.0,Pediatrics & Neonatology
228,"Is the rate of breastfeeding higher, lower, or the same when comparing oxytocin to no intervention?",insufficient data,,no,['24130463'],27078125,2016,"{'24130463': {'article_id': '24130463', 'content': ""Oxytocin (10 IU) is the drug of choice for prevention of postpartum hemorrhage (PPH). Its use has generally been restricted to medically trained staff in health facilities. We assessed the effectiveness, safety, and feasibility of PPH prevention using oxytocin injected by peripheral health care providers without midwifery skills at home births.\nThis community-based, cluster-randomized trial was conducted in four rural districts in Ghana. We randomly allocated 54 community health officers (stratified on district and catchment area distance to a health facility: ≥10 km versus <10 km) to intervention (one injection of oxytocin [10 IU] one minute after birth) and control (no provision of prophylactic oxytocin) arms. Births attended by a community health officer constituted a cluster. Our primary outcome was PPH, using multiple definitions; (PPH-1) blood loss ≥500 mL; (PPH-2) PPH-1 plus women who received early treatment for PPH; and (PPH-3) PPH-2 plus any other women referred to hospital for postpartum bleeding. Unsafe practice is defined as oxytocin use before delivery of the baby. We enrolled 689 and 897 women, respectively, into oxytocin and control arms of the trial from April 2011 to November 2012. In oxytocin and control arms, respectively, PPH-1 rates were 2.6% versus 5.5% (RR: 0.49; 95% CI: 0.27-0.88); PPH-2 rates were 3.8% versus 10.8% (RR: 0.35; 95% CI: 0.18-0.63), and PPH-3 rates were similar to those of PPH-2. Compared to women in control clusters, those in the intervention clusters lost 45.1 mL (17.7-72.6) less blood. There were no cases of oxytocin use before delivery of the baby and no major adverse events requiring notification of the institutional review boards. Limitations include an unblinded trial and imbalanced numbers of participants, favoring controls.\nMaternal health care planners can consider adapting this model to extend the use of oxytocin into peripheral settings including, in some contexts, home births.\nClinicalTrials.gov NCT01108289 Please see later in the article for the Editors' Summary."", 'title': 'Effect on postpartum hemorrhage of prophylactic oxytocin (10 IU) by injection by community health officers in Ghana: a community-based, cluster-randomized trial.', 'date': '2013-10-17'}}",1.0,Obstetrics & Gynecology
229,"Is the rate of breastfeeding higher, lower, or the same when comparing oxytocin to no intervention?",insufficient data,,no,['24130463'],27078125,2016,"{'24130463': {'article_id': '24130463', 'content': ""Oxytocin (10 IU) is the drug of choice for prevention of postpartum hemorrhage (PPH). Its use has generally been restricted to medically trained staff in health facilities. We assessed the effectiveness, safety, and feasibility of PPH prevention using oxytocin injected by peripheral health care providers without midwifery skills at home births.\nThis community-based, cluster-randomized trial was conducted in four rural districts in Ghana. We randomly allocated 54 community health officers (stratified on district and catchment area distance to a health facility: ≥10 km versus <10 km) to intervention (one injection of oxytocin [10 IU] one minute after birth) and control (no provision of prophylactic oxytocin) arms. Births attended by a community health officer constituted a cluster. Our primary outcome was PPH, using multiple definitions; (PPH-1) blood loss ≥500 mL; (PPH-2) PPH-1 plus women who received early treatment for PPH; and (PPH-3) PPH-2 plus any other women referred to hospital for postpartum bleeding. Unsafe practice is defined as oxytocin use before delivery of the baby. We enrolled 689 and 897 women, respectively, into oxytocin and control arms of the trial from April 2011 to November 2012. In oxytocin and control arms, respectively, PPH-1 rates were 2.6% versus 5.5% (RR: 0.49; 95% CI: 0.27-0.88); PPH-2 rates were 3.8% versus 10.8% (RR: 0.35; 95% CI: 0.18-0.63), and PPH-3 rates were similar to those of PPH-2. Compared to women in control clusters, those in the intervention clusters lost 45.1 mL (17.7-72.6) less blood. There were no cases of oxytocin use before delivery of the baby and no major adverse events requiring notification of the institutional review boards. Limitations include an unblinded trial and imbalanced numbers of participants, favoring controls.\nMaternal health care planners can consider adapting this model to extend the use of oxytocin into peripheral settings including, in some contexts, home births.\nClinicalTrials.gov NCT01108289 Please see later in the article for the Editors' Summary."", 'title': 'Effect on postpartum hemorrhage of prophylactic oxytocin (10 IU) by injection by community health officers in Ghana: a community-based, cluster-randomized trial.', 'date': '2013-10-17'}}",1.0,Obstetrics & Gynecology
122,"Is the risk of gingivitis higher, lower, or the same when comparing scaling and polishing at a reguarly-scheduled 6 month interval to no yearly-or-shorter schedule for scaling and polishing?",no difference,high,no,"['22204658', '29984691']",30590875,2018,"{'22204658': {'article_id': '22204658', 'content': 'Practice-based general dental practitioners routinely provide ""scale and polish"" or ""oral prophylaxis"" to patients attending their practices. Despite its routine provision, there is no evidence to support the clinical effectiveness of single-visit scale and polish, nor the frequency at which it should be provided. A recent systematic review recommended that future trials investigating scale and polish should involve dental practice patients.\nA practice-based parallel randomised controlled trial with 24-month follow-up was conducted. Healthy adults (Basic Periodontal Examination [BPE] codes <3) were randomly assigned to 3 groups (6-month, 12-month, or 24-month interval between scale and polish). The primary outcome was gingival bleeding with the hypothesis that 6-monthly scale and polish would result in lower prevalence than 12-month or 24-month frequency. Follow-up measurements were recorded by examiners blinded to the allocation. 125, 122 and 122 participants were randomised to the 6-month, 12-month and 24-month groups respectively. Complete data set analyses were conducted for 307 participants: 107, 100, and 100 in the 6-month, 12-month and 24-month groups respectively. Chi-square test and ANOVA were used to compare treatment groups at follow-up. Logistic regression and ANCOVA were used to estimate the relationship between outcome and treatment group, adjusted for baseline values. Multiple imputation analyses were also carried out for participants with incomplete data sets.\nPrevalence of gingival bleeding at follow-up was 78.5% (6-month), 78% (12-month) and 82% (24-month) (p = 0.746). There were no statistically significant differences between groups with respect to follow-up prevalence of plaque and calculus. Statistically significant differences detected in the amount (millimetres) of calculus were too small to be clinically significant. Seventeen (4.6%) participants were withdrawn from the trial to receive additional treatment.\nThis trial could not identify any differences in outcomes for single-visit scale and polish provided at 6, 12 and 24 month frequencies for healthy patients (with no significant periodontal disease). However, this is the first trial of scale and polish which has been conducted in a general practice setting and the results are not conclusive. Larger trials with more comprehensive measurement and long-term follow up need to be undertaken to provide a firm evidence base for this intervention. This trial informs the design of future practice-based trials on this subject.', 'title': 'Clinical outcomes of single-visit oral prophylaxis: a practice-based randomised controlled trial.', 'date': '2011-12-30'}, '29984691': {'article_id': '29984691', 'content': ""Periodontal disease is preventable but remains the most common oral disease worldwide, with major health and economic implications. Stakeholders lack reliable evidence of the relative clinical effectiveness and cost-effectiveness of different types of oral hygiene advice (OHA) and the optimal frequency of periodontal instrumentation (PI).\nTo test clinical effectiveness and assess the economic value of the following strategies: personalised OHA versus routine OHA, 12-monthly PI (scale and polish) compared with 6-monthly PI, and no PI compared with 6-monthly PI.\nMulticentre, pragmatic split-plot, randomised open trial with a cluster factorial design and blinded outcome evaluation with 3 years' follow-up and a within-trial cost-benefit analysis. NHS and participant costs were combined with benefits [willingness to pay (WTP)] estimated from a discrete choice experiment (DCE).\nUK dental practices.\nAdult dentate NHS patients, regular attenders, with Basic Periodontal Examination (BPE) scores of 0, 1, 2 or 3.\nPractices were randomised to provide routine or personalised OHA. Within each practice, participants were randomised to the following groups: no PI, 12-monthly PI or 6-monthly PI (current practice).\nClinical - gingival inflammation/bleeding on probing at the gingival margin (3 years). Patient - oral hygiene self-efficacy (3 years). Economic - net benefits (mean WTP minus mean costs).\nA total of 63 dental practices and 1877 participants were recruited. The mean number of teeth and percentage of bleeding sites was 24 and 33%, respectively. Two-thirds of participants had BPE scores of ≤ 2. Under intention-to-treat analysis, there was no evidence of a difference in gingival inflammation/bleeding between the 6-monthly PI group and the no-PI group [difference 0.87%, 95% confidence interval (CI) -1.6% to 3.3%; \nBeing a pragmatic trial, we did not deny PIs to the no-PI group; there was clear separation in the mean number of PIs between groups.\nThere was no additional benefit from scheduling 6-monthly or 12-monthly PIs over not providing this treatment unless desired or recommended, and no difference between OHA delivery for gingival inflammation/bleeding and patient-centred outcomes. However, participants valued, and were willing to pay for, both interventions, with greater financial value placed on PI than on OHA.\nAssess the clinical effectiveness and cost-effectiveness of providing multifaceted periodontal care packages in primary dental care for those with periodontitis.\nCurrent Controlled Trials ISRCTN56465715.\nThis project was funded by the National Institute for Health Research (NIHR) Health Technology Assessment programme and will be published in full in "", 'title': 'Improving the Quality of Dentistry (IQuaD): a cluster factorial randomised controlled trial comparing the effectiveness and cost-benefit of oral hygiene advice and/or periodontal instrumentation with routine care for the prevention and management of periodontal disease in dentate adults attending dental primary care.', 'date': '2018-07-10'}}",0.5,Dentistry & Oral Health
123,"Is the risk of gingivitis higher, lower, or the same when comparing scaling and polishing at a reguarly-scheduled 12 month interval to no yearly-or-shorter schedule for scaling and polishing?",no difference,high,no,"['22204658', '29984691']",30590875,2018,"{'22204658': {'article_id': '22204658', 'content': 'Practice-based general dental practitioners routinely provide ""scale and polish"" or ""oral prophylaxis"" to patients attending their practices. Despite its routine provision, there is no evidence to support the clinical effectiveness of single-visit scale and polish, nor the frequency at which it should be provided. A recent systematic review recommended that future trials investigating scale and polish should involve dental practice patients.\nA practice-based parallel randomised controlled trial with 24-month follow-up was conducted. Healthy adults (Basic Periodontal Examination [BPE] codes <3) were randomly assigned to 3 groups (6-month, 12-month, or 24-month interval between scale and polish). The primary outcome was gingival bleeding with the hypothesis that 6-monthly scale and polish would result in lower prevalence than 12-month or 24-month frequency. Follow-up measurements were recorded by examiners blinded to the allocation. 125, 122 and 122 participants were randomised to the 6-month, 12-month and 24-month groups respectively. Complete data set analyses were conducted for 307 participants: 107, 100, and 100 in the 6-month, 12-month and 24-month groups respectively. Chi-square test and ANOVA were used to compare treatment groups at follow-up. Logistic regression and ANCOVA were used to estimate the relationship between outcome and treatment group, adjusted for baseline values. Multiple imputation analyses were also carried out for participants with incomplete data sets.\nPrevalence of gingival bleeding at follow-up was 78.5% (6-month), 78% (12-month) and 82% (24-month) (p = 0.746). There were no statistically significant differences between groups with respect to follow-up prevalence of plaque and calculus. Statistically significant differences detected in the amount (millimetres) of calculus were too small to be clinically significant. Seventeen (4.6%) participants were withdrawn from the trial to receive additional treatment.\nThis trial could not identify any differences in outcomes for single-visit scale and polish provided at 6, 12 and 24 month frequencies for healthy patients (with no significant periodontal disease). However, this is the first trial of scale and polish which has been conducted in a general practice setting and the results are not conclusive. Larger trials with more comprehensive measurement and long-term follow up need to be undertaken to provide a firm evidence base for this intervention. This trial informs the design of future practice-based trials on this subject.', 'title': 'Clinical outcomes of single-visit oral prophylaxis: a practice-based randomised controlled trial.', 'date': '2011-12-30'}, '29984691': {'article_id': '29984691', 'content': ""Periodontal disease is preventable but remains the most common oral disease worldwide, with major health and economic implications. Stakeholders lack reliable evidence of the relative clinical effectiveness and cost-effectiveness of different types of oral hygiene advice (OHA) and the optimal frequency of periodontal instrumentation (PI).\nTo test clinical effectiveness and assess the economic value of the following strategies: personalised OHA versus routine OHA, 12-monthly PI (scale and polish) compared with 6-monthly PI, and no PI compared with 6-monthly PI.\nMulticentre, pragmatic split-plot, randomised open trial with a cluster factorial design and blinded outcome evaluation with 3 years' follow-up and a within-trial cost-benefit analysis. NHS and participant costs were combined with benefits [willingness to pay (WTP)] estimated from a discrete choice experiment (DCE).\nUK dental practices.\nAdult dentate NHS patients, regular attenders, with Basic Periodontal Examination (BPE) scores of 0, 1, 2 or 3.\nPractices were randomised to provide routine or personalised OHA. Within each practice, participants were randomised to the following groups: no PI, 12-monthly PI or 6-monthly PI (current practice).\nClinical - gingival inflammation/bleeding on probing at the gingival margin (3 years). Patient - oral hygiene self-efficacy (3 years). Economic - net benefits (mean WTP minus mean costs).\nA total of 63 dental practices and 1877 participants were recruited. The mean number of teeth and percentage of bleeding sites was 24 and 33%, respectively. Two-thirds of participants had BPE scores of ≤ 2. Under intention-to-treat analysis, there was no evidence of a difference in gingival inflammation/bleeding between the 6-monthly PI group and the no-PI group [difference 0.87%, 95% confidence interval (CI) -1.6% to 3.3%; \nBeing a pragmatic trial, we did not deny PIs to the no-PI group; there was clear separation in the mean number of PIs between groups.\nThere was no additional benefit from scheduling 6-monthly or 12-monthly PIs over not providing this treatment unless desired or recommended, and no difference between OHA delivery for gingival inflammation/bleeding and patient-centred outcomes. However, participants valued, and were willing to pay for, both interventions, with greater financial value placed on PI than on OHA.\nAssess the clinical effectiveness and cost-effectiveness of providing multifaceted periodontal care packages in primary dental care for those with periodontitis.\nCurrent Controlled Trials ISRCTN56465715.\nThis project was funded by the National Institute for Health Research (NIHR) Health Technology Assessment programme and will be published in full in "", 'title': 'Improving the Quality of Dentistry (IQuaD): a cluster factorial randomised controlled trial comparing the effectiveness and cost-benefit of oral hygiene advice and/or periodontal instrumentation with routine care for the prevention and management of periodontal disease in dentate adults attending dental primary care.', 'date': '2018-07-10'}}",0.5,Dentistry & Oral Health
124,"Is the risk of gingivitis higher, lower, or the same when comparing scaling and polishing at a 6 month interval to scaling and polishing at a 12 month interval?",no difference,high,no,"['22204658', '29984691']",30590875,2018,"{'22204658': {'article_id': '22204658', 'content': 'Practice-based general dental practitioners routinely provide ""scale and polish"" or ""oral prophylaxis"" to patients attending their practices. Despite its routine provision, there is no evidence to support the clinical effectiveness of single-visit scale and polish, nor the frequency at which it should be provided. A recent systematic review recommended that future trials investigating scale and polish should involve dental practice patients.\nA practice-based parallel randomised controlled trial with 24-month follow-up was conducted. Healthy adults (Basic Periodontal Examination [BPE] codes <3) were randomly assigned to 3 groups (6-month, 12-month, or 24-month interval between scale and polish). The primary outcome was gingival bleeding with the hypothesis that 6-monthly scale and polish would result in lower prevalence than 12-month or 24-month frequency. Follow-up measurements were recorded by examiners blinded to the allocation. 125, 122 and 122 participants were randomised to the 6-month, 12-month and 24-month groups respectively. Complete data set analyses were conducted for 307 participants: 107, 100, and 100 in the 6-month, 12-month and 24-month groups respectively. Chi-square test and ANOVA were used to compare treatment groups at follow-up. Logistic regression and ANCOVA were used to estimate the relationship between outcome and treatment group, adjusted for baseline values. Multiple imputation analyses were also carried out for participants with incomplete data sets.\nPrevalence of gingival bleeding at follow-up was 78.5% (6-month), 78% (12-month) and 82% (24-month) (p = 0.746). There were no statistically significant differences between groups with respect to follow-up prevalence of plaque and calculus. Statistically significant differences detected in the amount (millimetres) of calculus were too small to be clinically significant. Seventeen (4.6%) participants were withdrawn from the trial to receive additional treatment.\nThis trial could not identify any differences in outcomes for single-visit scale and polish provided at 6, 12 and 24 month frequencies for healthy patients (with no significant periodontal disease). However, this is the first trial of scale and polish which has been conducted in a general practice setting and the results are not conclusive. Larger trials with more comprehensive measurement and long-term follow up need to be undertaken to provide a firm evidence base for this intervention. This trial informs the design of future practice-based trials on this subject.', 'title': 'Clinical outcomes of single-visit oral prophylaxis: a practice-based randomised controlled trial.', 'date': '2011-12-30'}, '29984691': {'article_id': '29984691', 'content': ""Periodontal disease is preventable but remains the most common oral disease worldwide, with major health and economic implications. Stakeholders lack reliable evidence of the relative clinical effectiveness and cost-effectiveness of different types of oral hygiene advice (OHA) and the optimal frequency of periodontal instrumentation (PI).\nTo test clinical effectiveness and assess the economic value of the following strategies: personalised OHA versus routine OHA, 12-monthly PI (scale and polish) compared with 6-monthly PI, and no PI compared with 6-monthly PI.\nMulticentre, pragmatic split-plot, randomised open trial with a cluster factorial design and blinded outcome evaluation with 3 years' follow-up and a within-trial cost-benefit analysis. NHS and participant costs were combined with benefits [willingness to pay (WTP)] estimated from a discrete choice experiment (DCE).\nUK dental practices.\nAdult dentate NHS patients, regular attenders, with Basic Periodontal Examination (BPE) scores of 0, 1, 2 or 3.\nPractices were randomised to provide routine or personalised OHA. Within each practice, participants were randomised to the following groups: no PI, 12-monthly PI or 6-monthly PI (current practice).\nClinical - gingival inflammation/bleeding on probing at the gingival margin (3 years). Patient - oral hygiene self-efficacy (3 years). Economic - net benefits (mean WTP minus mean costs).\nA total of 63 dental practices and 1877 participants were recruited. The mean number of teeth and percentage of bleeding sites was 24 and 33%, respectively. Two-thirds of participants had BPE scores of ≤ 2. Under intention-to-treat analysis, there was no evidence of a difference in gingival inflammation/bleeding between the 6-monthly PI group and the no-PI group [difference 0.87%, 95% confidence interval (CI) -1.6% to 3.3%; \nBeing a pragmatic trial, we did not deny PIs to the no-PI group; there was clear separation in the mean number of PIs between groups.\nThere was no additional benefit from scheduling 6-monthly or 12-monthly PIs over not providing this treatment unless desired or recommended, and no difference between OHA delivery for gingival inflammation/bleeding and patient-centred outcomes. However, participants valued, and were willing to pay for, both interventions, with greater financial value placed on PI than on OHA.\nAssess the clinical effectiveness and cost-effectiveness of providing multifaceted periodontal care packages in primary dental care for those with periodontitis.\nCurrent Controlled Trials ISRCTN56465715.\nThis project was funded by the National Institute for Health Research (NIHR) Health Technology Assessment programme and will be published in full in "", 'title': 'Improving the Quality of Dentistry (IQuaD): a cluster factorial randomised controlled trial comparing the effectiveness and cost-benefit of oral hygiene advice and/or periodontal instrumentation with routine care for the prevention and management of periodontal disease in dentate adults attending dental primary care.', 'date': '2018-07-10'}}",1.0,Dentistry & Oral Health
125,"Is the likelihood of 4‐point improvement in SELENA‐SLEDAI higher, lower, or the same when comparing belimumab to placebo?",higher,high,no,"['29295825', '28118533', '21296403', '22127708']",33631841,2021,"{'29295825': {'article_id': '29295825', 'content': 'Intravenous belimumab plus standard of care (SoC) is approved in the USA and Europe for treatment of active, autoantibody-positive systemic lupus erythematosus (SLE).\nThis phase III, multicentre, randomised, double-blind, placebo-controlled study (BEL113750; NCT01345253) was conducted in 49 centres across China, Japan and South Korea (May 2011\nThe modified intent-to-treat population included 677 patients (belimumab n=451, placebo n=226). At Week 52, the SRI4 response rate was higher with belimumab versus placebo (53.8% vs 40.1%; OR: 1.99 (95% CI: 1.40, 2.82; P=0.0001)). The percentages of patients with a ≥4\u2009point reduction in SELENA-SLEDAI and an SRI7 response were significantly greater for belimumab versus placebo. Patients in the belimumab group had a 50% lower risk of experiencing a severe flare than those receiving placebo (P=0.0004). In patients with baseline prednisone dose >7.5\u2009mg/day, there was a significant reduction in steroid use favouring belimumab (P=0.0228). The incidence of adverse events was similar between groups.\nIn patients with SLE from North East Asia, belimumab significantly improved disease activity, while reducing prednisone use, with no new safety issues.', 'title': 'A pivotal phase III, randomised, placebo-controlled study of belimumab in patients with systemic lupus erythematosus located in China, Japan and South Korea.', 'date': '2018-01-04'}, '28118533': {'article_id': '28118533', 'content': 'To assess the efficacy and safety of subcutaneous (SC) belimumab in patients with systemic lupus erythematosus (SLE).\nPatients with moderate-to-severe SLE (score of ≥8 on the Safety of Estrogens in Lupus Erythematosus National Assessment [SELENA] version of the SLE Disease Activity Index [SLEDAI]) were randomized 2:1 to receive weekly SC belimumab 200 mg or placebo by prefilled syringe in addition to standard SLE therapy for 52 weeks. The primary end point was the SLE Responder Index (SRI4) at week 52. Secondary end points were reduction in the corticosteroid dosage and time to severe flare. Safety was assessed according to the adverse events (AEs) reported and the laboratory test results.\nOf 839 patients randomized, 836 (556 in the belimumab group and 280 in the placebo group) received treatment. A total of 159 patients withdrew before the end of the study. At entry, mean SELENA-SLEDAI scores were 10.5 in the belimumab group and 10.3 in the placebo group. More patients who received belimumab were SRI4 responders than those who received placebo (61.4% versus 48.4%; odds ratio [OR] 1.68 [95% confidence interval (95% CI) 1.25-2.25]; P\u2009=\u20090.0006). In the belimumab group, both time to and risk of severe flare were improved (median 171.0 days versus 118.0 days; hazard ratio 0.51 [95% CI 0.35-0.74]; P\u2009=\u20090.0004), and more patients were able to reduce their corticosteroid dosage by ≥25% (to ≤7.5 mg/day) during weeks 40-52 (18.2% versus 11.9%; OR 1.65 [95% CI 0.95-2.84]; P\u2009=\u20090.0732), compared with placebo. AE incidence was comparable between treatment groups; serious AEs were reported by 10.8% of patients taking belimumab and 15.7% of those taking placebo. A worsening of IgG hypoglobulinemia by ≥2 grades occurred in 0.9% of patients taking belimumab and 1.4% of those taking placebo.\nIn patients with moderate-to-severe SLE, weekly SC doses of belimumab 200 mg plus standard SLE therapy significantly improved their SRI4 response, decreased severe disease flares as compared with placebo, and had a safety profile similar to placebo plus standard SLE therapy.', 'title': 'Efficacy and Safety of Subcutaneous Belimumab in Systemic Lupus Erythematosus: A Fifty-Two-Week Randomized, Double-Blind, Placebo-Controlled Study.', 'date': '2017-01-25'}, '21296403': {'article_id': '21296403', 'content': ""Systemic lupus erythematosus is a heterogeneous autoimmune disease that is associated with B-cell hyperactivity, autoantibodies, and increased concentrations of B-lymphocyte stimulator (BLyS). The efficacy and safety of the fully human monoclonal antibody belimumab (BLyS-specific inhibitor) was assessed in patients with active systemic lupus erythematosus.\nPatients (aged ≥18 years) who were seropositive with scores of at least 6 on the Safety of Estrogens in Lupus Erythematosus National Assessment-Systemic Lupus Erythematosus Disease Activity Index (SELENA-SLEDAI) were enrolled in a multicentre phase 3 study, which was done in Latin America, Asia-Pacific, and eastern Europe. Patients were randomly assigned by use of a central interactive voice response system in a 1:1:1 ratio to belimumab 1 mg/kg or 10 mg/kg, or placebo by intravenous infusion in 1 h on days 0, 14, and 28, and then every 28 days until 48 weeks, with standard of care. Patients, investigators, study coordinators, and sponsors were masked to treatment assignment. Primary efficacy endpoint was improvement in the Systemic Lupus Erythematosus Responder Index (SRI) at week 52 (reduction ≥4 points in SELENA-SLEDAI score; no new British Isles Lupus Assessment Group [BILAG] A organ domain score and no more than 1 new B organ domain score; and no worsening [<0·3 increase] in Physician's Global Assessment [PGA] score) versus baseline. Method of analysis was by modified intention to treat. This trial is registered with ClinicalTrials.gov, number NCT00424476.\n867 patients were randomly assigned to belimumab 1 mg/kg (n=289) or 10 mg/kg (n=290), or placebo (n=288). 865 were treated and analysed in the belimumab (1 mg/kg, n=288; 10 mg/kg, n=290) and placebo groups (n=287). Significantly higher SRI rates were noted with belimumab 1 mg/kg (148 [51%], odds ratio 1·55 [95% CI 1·10-2·19]; p=0·0129) and 10 mg/kg (167 [58%], 1·83 [1·30-2·59]; p=0·0006) than with placebo (125 [44%]) at week 52. More patients had their SELENA-SLEDAI score reduced by at least 4 points during 52 weeks with belimumab 1 mg/kg (153 [53%], 1·51 [1·07-2·14]; p=0·0189) and 10 mg/kg (169 [58%], 1·71 [1·21-2·41]; p=0·0024) than with placebo (132 [46%]). More patients given belimumab 1 mg/kg (226 [78%], 1·38 [0·93-2·04]; p=0·1064) and 10 mg/kg (236 [81%], 1·62 [1·09-2·42]; p=0·0181) had no new BILAG A or no more than 1 new B flare than did those in the placebo group (210 [73%]). No worsening in PGA score was noted in more patients with belimumab 1 mg/kg (227 [79%], 1·68 [1·15-2·47]; p=0·0078) and 10 mg/kg (231 [80%], 1·74 [1·18-2·55]; p=0·0048) than with placebo (199 [69%]). Rates of adverse events were similar in the groups given belimumab 1 mg/kg and 10 mg/kg, and placebo: serious infection was reported in 22 (8%), 13 (4%), and 17 (6%) patients, respectively, and severe or serious hypersensitivity reactions on an infusion day were reported in two (<1%), two (<1%), and no patients, respectively. No malignant diseases were reported.\nBelimumab has the potential to be the first targeted biological treatment that is approved specifically for systemic lupus erythematosus, providing a new option for the management of this important prototypic autoimmune disease.\nHuman Genome Sciences and GlaxoSmithKline."", 'title': 'Efficacy and safety of belimumab in patients with active systemic lupus erythematosus: a randomised, placebo-controlled, phase 3 trial.', 'date': '2011-02-08'}, '22127708': {'article_id': '22127708', 'content': ""To assess the efficacy/safety of the B lymphocyte stimulator inhibitor belimumab plus standard therapy compared with placebo plus standard therapy in active systemic lupus erythematosus (SLE).\nIn a phase III, multicenter, randomized, placebo-controlled trial, 819 antinuclear antibody-positive or anti-double-stranded DNA-positive SLE patients with scores ≥6 on the Safety of Estrogens in Lupus Erythematosus National Assessment (SELENA) version of the SLE Disease Activity Index (SLEDAI) were randomized in a 1:1:1 ratio to receive 1 mg/kg belimumab, 10 mg/kg belimumab, or placebo intravenously on days 0, 14, and 28 and then every 28 days for 72 weeks. The primary efficacy end point was the SLE Responder Index (SRI) response rate at week 52 (an SRI response was defined as a ≥4-point reduction in SELENA-SLEDAI score, no new British Isles Lupus Assessment Group [BILAG] A organ domain score and no more than 1 new BILAG B score, and no worsening in physician's global assessment score versus baseline).\nBelimumab at 10 mg/kg plus standard therapy met the primary efficacy end point, generating a significantly greater SRI response at week 52 compared with placebo (43.2% versus 33.5%; P = 0.017). The rate with 1 mg/kg belimumab was 40.6% (P = 0.089). Response rates at week 76 were 32.4%, 39.1%, and 38.5% with placebo, 1 mg/kg belimumab, and 10 mg/kg belimumab, respectively. In post hoc sensitivity analyses evaluating higher SELENA-SLEDAI score thresholds, 10 mg/kg belimumab achieved better discrimination at weeks 52 and 76. Risk of severe flares over 76 weeks (based on the modified SLE Flare Index) was reduced with 1 mg/kg belimumab (34%) (P = 0.023) and 10 mg/kg belimumab (23%) (P = 0.13). Serious and severe adverse events, including infections, laboratory abnormalities, malignancies, and deaths, were comparable across groups.\nBelimumab plus standard therapy significantly improved SRI response rate, reduced SLE disease activity and severe flares, and was generally well tolerated in SLE."", 'title': 'A phase III, randomized, placebo-controlled study of belimumab, a monoclonal antibody that inhibits B lymphocyte stimulator, in patients with systemic lupus erythematosus.', 'date': '2011-12-01'}}",1.0,Internal Medicine & Subspecialties
126,"Is the risk of one or more serious adverse events higher, lower, or the same when comparing belimumab to placebo?",no difference,low,yes,"['19714604', '28118533', '22127708', '21296403', '29295825']",33631841,2021,"{'19714604': {'article_id': '19714604', 'content': ""To assess the safety, tolerability, biologic activity, and efficacy of belimumab in combination with standard of care therapy (SOC) in patients with active systemic lupus erythematosus (SLE).\nPatients with a Safety of Estrogens in Lupus Erythematosus: National Assessment (SELENA) version of the Systemic Lupus Erythematosus Disease Activity Index (SLEDAI) score >/=4 (n = 449) were randomly assigned to belimumab (1, 4, or 10 mg/kg) or placebo in a 52-week study. Coprimary end points were the percent change in the SELENA-SLEDAI score at week 24 and the time to first SLE flare.\nSignificant differences between the treatment and placebo groups were not attained for either primary end point, and no dose response was observed. Reductions in SELENA-SLEDAI scores from baseline were 19.5% in the combined belimumab group versus 17.2% in the placebo group. The median time to first SLE flare was 67 days in the combined belimumab group versus 83 days in the placebo group. However, the median time to first SLE flare during weeks 24-52 was significantly longer with belimumab treatment (154 versus 108 days; P = 0.0361). In the subgroup (71.5%) of serologically active patients (antinuclear antibody titer >/=1:80 and/or anti-double-stranded DNA [anti-dsDNA] >/=30 IU/ml), belimumab treatment resulted in significantly better responses at week 52 than placebo for SELENA-SLEDAI score (-28.8% versus -14.2%; P = 0.0435), physician's global assessment (-32.7% versus -10.7%; P = 0.0011), and Short Form 36 physical component score (+3.0 versus +1.2 points; P = 0.0410). Treatment with belimumab resulted in a 63-71% reduction of naive, activated, and plasmacytoid CD20+ B cells, and a 29.4% reduction in anti-dsDNA titers (P = 0.0017) by week 52. The rates of adverse events and serious adverse events were similar in the belimumab and placebo groups.\nBelimumab was biologically active and well tolerated. The effect of belimumab on the reduction of SLE disease activity or flares was not significant. However, serologically active SLE patients responded significantly better to belimumab therapy plus SOC than to SOC alone."", 'title': 'A phase II, randomized, double-blind, placebo-controlled, dose-ranging study of belimumab in patients with active systemic lupus erythematosus.', 'date': '2009-08-29'}, '28118533': {'article_id': '28118533', 'content': 'To assess the efficacy and safety of subcutaneous (SC) belimumab in patients with systemic lupus erythematosus (SLE).\nPatients with moderate-to-severe SLE (score of ≥8 on the Safety of Estrogens in Lupus Erythematosus National Assessment [SELENA] version of the SLE Disease Activity Index [SLEDAI]) were randomized 2:1 to receive weekly SC belimumab 200 mg or placebo by prefilled syringe in addition to standard SLE therapy for 52 weeks. The primary end point was the SLE Responder Index (SRI4) at week 52. Secondary end points were reduction in the corticosteroid dosage and time to severe flare. Safety was assessed according to the adverse events (AEs) reported and the laboratory test results.\nOf 839 patients randomized, 836 (556 in the belimumab group and 280 in the placebo group) received treatment. A total of 159 patients withdrew before the end of the study. At entry, mean SELENA-SLEDAI scores were 10.5 in the belimumab group and 10.3 in the placebo group. More patients who received belimumab were SRI4 responders than those who received placebo (61.4% versus 48.4%; odds ratio [OR] 1.68 [95% confidence interval (95% CI) 1.25-2.25]; P\u2009=\u20090.0006). In the belimumab group, both time to and risk of severe flare were improved (median 171.0 days versus 118.0 days; hazard ratio 0.51 [95% CI 0.35-0.74]; P\u2009=\u20090.0004), and more patients were able to reduce their corticosteroid dosage by ≥25% (to ≤7.5 mg/day) during weeks 40-52 (18.2% versus 11.9%; OR 1.65 [95% CI 0.95-2.84]; P\u2009=\u20090.0732), compared with placebo. AE incidence was comparable between treatment groups; serious AEs were reported by 10.8% of patients taking belimumab and 15.7% of those taking placebo. A worsening of IgG hypoglobulinemia by ≥2 grades occurred in 0.9% of patients taking belimumab and 1.4% of those taking placebo.\nIn patients with moderate-to-severe SLE, weekly SC doses of belimumab 200 mg plus standard SLE therapy significantly improved their SRI4 response, decreased severe disease flares as compared with placebo, and had a safety profile similar to placebo plus standard SLE therapy.', 'title': 'Efficacy and Safety of Subcutaneous Belimumab in Systemic Lupus Erythematosus: A Fifty-Two-Week Randomized, Double-Blind, Placebo-Controlled Study.', 'date': '2017-01-25'}, '22127708': {'article_id': '22127708', 'content': ""To assess the efficacy/safety of the B lymphocyte stimulator inhibitor belimumab plus standard therapy compared with placebo plus standard therapy in active systemic lupus erythematosus (SLE).\nIn a phase III, multicenter, randomized, placebo-controlled trial, 819 antinuclear antibody-positive or anti-double-stranded DNA-positive SLE patients with scores ≥6 on the Safety of Estrogens in Lupus Erythematosus National Assessment (SELENA) version of the SLE Disease Activity Index (SLEDAI) were randomized in a 1:1:1 ratio to receive 1 mg/kg belimumab, 10 mg/kg belimumab, or placebo intravenously on days 0, 14, and 28 and then every 28 days for 72 weeks. The primary efficacy end point was the SLE Responder Index (SRI) response rate at week 52 (an SRI response was defined as a ≥4-point reduction in SELENA-SLEDAI score, no new British Isles Lupus Assessment Group [BILAG] A organ domain score and no more than 1 new BILAG B score, and no worsening in physician's global assessment score versus baseline).\nBelimumab at 10 mg/kg plus standard therapy met the primary efficacy end point, generating a significantly greater SRI response at week 52 compared with placebo (43.2% versus 33.5%; P = 0.017). The rate with 1 mg/kg belimumab was 40.6% (P = 0.089). Response rates at week 76 were 32.4%, 39.1%, and 38.5% with placebo, 1 mg/kg belimumab, and 10 mg/kg belimumab, respectively. In post hoc sensitivity analyses evaluating higher SELENA-SLEDAI score thresholds, 10 mg/kg belimumab achieved better discrimination at weeks 52 and 76. Risk of severe flares over 76 weeks (based on the modified SLE Flare Index) was reduced with 1 mg/kg belimumab (34%) (P = 0.023) and 10 mg/kg belimumab (23%) (P = 0.13). Serious and severe adverse events, including infections, laboratory abnormalities, malignancies, and deaths, were comparable across groups.\nBelimumab plus standard therapy significantly improved SRI response rate, reduced SLE disease activity and severe flares, and was generally well tolerated in SLE."", 'title': 'A phase III, randomized, placebo-controlled study of belimumab, a monoclonal antibody that inhibits B lymphocyte stimulator, in patients with systemic lupus erythematosus.', 'date': '2011-12-01'}, '21296403': {'article_id': '21296403', 'content': ""Systemic lupus erythematosus is a heterogeneous autoimmune disease that is associated with B-cell hyperactivity, autoantibodies, and increased concentrations of B-lymphocyte stimulator (BLyS). The efficacy and safety of the fully human monoclonal antibody belimumab (BLyS-specific inhibitor) was assessed in patients with active systemic lupus erythematosus.\nPatients (aged ≥18 years) who were seropositive with scores of at least 6 on the Safety of Estrogens in Lupus Erythematosus National Assessment-Systemic Lupus Erythematosus Disease Activity Index (SELENA-SLEDAI) were enrolled in a multicentre phase 3 study, which was done in Latin America, Asia-Pacific, and eastern Europe. Patients were randomly assigned by use of a central interactive voice response system in a 1:1:1 ratio to belimumab 1 mg/kg or 10 mg/kg, or placebo by intravenous infusion in 1 h on days 0, 14, and 28, and then every 28 days until 48 weeks, with standard of care. Patients, investigators, study coordinators, and sponsors were masked to treatment assignment. Primary efficacy endpoint was improvement in the Systemic Lupus Erythematosus Responder Index (SRI) at week 52 (reduction ≥4 points in SELENA-SLEDAI score; no new British Isles Lupus Assessment Group [BILAG] A organ domain score and no more than 1 new B organ domain score; and no worsening [<0·3 increase] in Physician's Global Assessment [PGA] score) versus baseline. Method of analysis was by modified intention to treat. This trial is registered with ClinicalTrials.gov, number NCT00424476.\n867 patients were randomly assigned to belimumab 1 mg/kg (n=289) or 10 mg/kg (n=290), or placebo (n=288). 865 were treated and analysed in the belimumab (1 mg/kg, n=288; 10 mg/kg, n=290) and placebo groups (n=287). Significantly higher SRI rates were noted with belimumab 1 mg/kg (148 [51%], odds ratio 1·55 [95% CI 1·10-2·19]; p=0·0129) and 10 mg/kg (167 [58%], 1·83 [1·30-2·59]; p=0·0006) than with placebo (125 [44%]) at week 52. More patients had their SELENA-SLEDAI score reduced by at least 4 points during 52 weeks with belimumab 1 mg/kg (153 [53%], 1·51 [1·07-2·14]; p=0·0189) and 10 mg/kg (169 [58%], 1·71 [1·21-2·41]; p=0·0024) than with placebo (132 [46%]). More patients given belimumab 1 mg/kg (226 [78%], 1·38 [0·93-2·04]; p=0·1064) and 10 mg/kg (236 [81%], 1·62 [1·09-2·42]; p=0·0181) had no new BILAG A or no more than 1 new B flare than did those in the placebo group (210 [73%]). No worsening in PGA score was noted in more patients with belimumab 1 mg/kg (227 [79%], 1·68 [1·15-2·47]; p=0·0078) and 10 mg/kg (231 [80%], 1·74 [1·18-2·55]; p=0·0048) than with placebo (199 [69%]). Rates of adverse events were similar in the groups given belimumab 1 mg/kg and 10 mg/kg, and placebo: serious infection was reported in 22 (8%), 13 (4%), and 17 (6%) patients, respectively, and severe or serious hypersensitivity reactions on an infusion day were reported in two (<1%), two (<1%), and no patients, respectively. No malignant diseases were reported.\nBelimumab has the potential to be the first targeted biological treatment that is approved specifically for systemic lupus erythematosus, providing a new option for the management of this important prototypic autoimmune disease.\nHuman Genome Sciences and GlaxoSmithKline."", 'title': 'Efficacy and safety of belimumab in patients with active systemic lupus erythematosus: a randomised, placebo-controlled, phase 3 trial.', 'date': '2011-02-08'}, '29295825': {'article_id': '29295825', 'content': 'Intravenous belimumab plus standard of care (SoC) is approved in the USA and Europe for treatment of active, autoantibody-positive systemic lupus erythematosus (SLE).\nThis phase III, multicentre, randomised, double-blind, placebo-controlled study (BEL113750; NCT01345253) was conducted in 49 centres across China, Japan and South Korea (May 2011\nThe modified intent-to-treat population included 677 patients (belimumab n=451, placebo n=226). At Week 52, the SRI4 response rate was higher with belimumab versus placebo (53.8% vs 40.1%; OR: 1.99 (95% CI: 1.40, 2.82; P=0.0001)). The percentages of patients with a ≥4\u2009point reduction in SELENA-SLEDAI and an SRI7 response were significantly greater for belimumab versus placebo. Patients in the belimumab group had a 50% lower risk of experiencing a severe flare than those receiving placebo (P=0.0004). In patients with baseline prednisone dose >7.5\u2009mg/day, there was a significant reduction in steroid use favouring belimumab (P=0.0228). The incidence of adverse events was similar between groups.\nIn patients with SLE from North East Asia, belimumab significantly improved disease activity, while reducing prednisone use, with no new safety issues.', 'title': 'A pivotal phase III, randomised, placebo-controlled study of belimumab in patients with systemic lupus erythematosus located in China, Japan and South Korea.', 'date': '2018-01-04'}}",0.6,Internal Medicine & Subspecialties
127,"Is the risk of one or more serious infections higher, lower, or the same when comparing belimumab to placebo?",no difference,moderate,yes,"['19714604', '28118533', '22127708', '21296403']",33631841,2021,"{'19714604': {'article_id': '19714604', 'content': ""To assess the safety, tolerability, biologic activity, and efficacy of belimumab in combination with standard of care therapy (SOC) in patients with active systemic lupus erythematosus (SLE).\nPatients with a Safety of Estrogens in Lupus Erythematosus: National Assessment (SELENA) version of the Systemic Lupus Erythematosus Disease Activity Index (SLEDAI) score >/=4 (n = 449) were randomly assigned to belimumab (1, 4, or 10 mg/kg) or placebo in a 52-week study. Coprimary end points were the percent change in the SELENA-SLEDAI score at week 24 and the time to first SLE flare.\nSignificant differences between the treatment and placebo groups were not attained for either primary end point, and no dose response was observed. Reductions in SELENA-SLEDAI scores from baseline were 19.5% in the combined belimumab group versus 17.2% in the placebo group. The median time to first SLE flare was 67 days in the combined belimumab group versus 83 days in the placebo group. However, the median time to first SLE flare during weeks 24-52 was significantly longer with belimumab treatment (154 versus 108 days; P = 0.0361). In the subgroup (71.5%) of serologically active patients (antinuclear antibody titer >/=1:80 and/or anti-double-stranded DNA [anti-dsDNA] >/=30 IU/ml), belimumab treatment resulted in significantly better responses at week 52 than placebo for SELENA-SLEDAI score (-28.8% versus -14.2%; P = 0.0435), physician's global assessment (-32.7% versus -10.7%; P = 0.0011), and Short Form 36 physical component score (+3.0 versus +1.2 points; P = 0.0410). Treatment with belimumab resulted in a 63-71% reduction of naive, activated, and plasmacytoid CD20+ B cells, and a 29.4% reduction in anti-dsDNA titers (P = 0.0017) by week 52. The rates of adverse events and serious adverse events were similar in the belimumab and placebo groups.\nBelimumab was biologically active and well tolerated. The effect of belimumab on the reduction of SLE disease activity or flares was not significant. However, serologically active SLE patients responded significantly better to belimumab therapy plus SOC than to SOC alone."", 'title': 'A phase II, randomized, double-blind, placebo-controlled, dose-ranging study of belimumab in patients with active systemic lupus erythematosus.', 'date': '2009-08-29'}, '28118533': {'article_id': '28118533', 'content': 'To assess the efficacy and safety of subcutaneous (SC) belimumab in patients with systemic lupus erythematosus (SLE).\nPatients with moderate-to-severe SLE (score of ≥8 on the Safety of Estrogens in Lupus Erythematosus National Assessment [SELENA] version of the SLE Disease Activity Index [SLEDAI]) were randomized 2:1 to receive weekly SC belimumab 200 mg or placebo by prefilled syringe in addition to standard SLE therapy for 52 weeks. The primary end point was the SLE Responder Index (SRI4) at week 52. Secondary end points were reduction in the corticosteroid dosage and time to severe flare. Safety was assessed according to the adverse events (AEs) reported and the laboratory test results.\nOf 839 patients randomized, 836 (556 in the belimumab group and 280 in the placebo group) received treatment. A total of 159 patients withdrew before the end of the study. At entry, mean SELENA-SLEDAI scores were 10.5 in the belimumab group and 10.3 in the placebo group. More patients who received belimumab were SRI4 responders than those who received placebo (61.4% versus 48.4%; odds ratio [OR] 1.68 [95% confidence interval (95% CI) 1.25-2.25]; P\u2009=\u20090.0006). In the belimumab group, both time to and risk of severe flare were improved (median 171.0 days versus 118.0 days; hazard ratio 0.51 [95% CI 0.35-0.74]; P\u2009=\u20090.0004), and more patients were able to reduce their corticosteroid dosage by ≥25% (to ≤7.5 mg/day) during weeks 40-52 (18.2% versus 11.9%; OR 1.65 [95% CI 0.95-2.84]; P\u2009=\u20090.0732), compared with placebo. AE incidence was comparable between treatment groups; serious AEs were reported by 10.8% of patients taking belimumab and 15.7% of those taking placebo. A worsening of IgG hypoglobulinemia by ≥2 grades occurred in 0.9% of patients taking belimumab and 1.4% of those taking placebo.\nIn patients with moderate-to-severe SLE, weekly SC doses of belimumab 200 mg plus standard SLE therapy significantly improved their SRI4 response, decreased severe disease flares as compared with placebo, and had a safety profile similar to placebo plus standard SLE therapy.', 'title': 'Efficacy and Safety of Subcutaneous Belimumab in Systemic Lupus Erythematosus: A Fifty-Two-Week Randomized, Double-Blind, Placebo-Controlled Study.', 'date': '2017-01-25'}, '22127708': {'article_id': '22127708', 'content': ""To assess the efficacy/safety of the B lymphocyte stimulator inhibitor belimumab plus standard therapy compared with placebo plus standard therapy in active systemic lupus erythematosus (SLE).\nIn a phase III, multicenter, randomized, placebo-controlled trial, 819 antinuclear antibody-positive or anti-double-stranded DNA-positive SLE patients with scores ≥6 on the Safety of Estrogens in Lupus Erythematosus National Assessment (SELENA) version of the SLE Disease Activity Index (SLEDAI) were randomized in a 1:1:1 ratio to receive 1 mg/kg belimumab, 10 mg/kg belimumab, or placebo intravenously on days 0, 14, and 28 and then every 28 days for 72 weeks. The primary efficacy end point was the SLE Responder Index (SRI) response rate at week 52 (an SRI response was defined as a ≥4-point reduction in SELENA-SLEDAI score, no new British Isles Lupus Assessment Group [BILAG] A organ domain score and no more than 1 new BILAG B score, and no worsening in physician's global assessment score versus baseline).\nBelimumab at 10 mg/kg plus standard therapy met the primary efficacy end point, generating a significantly greater SRI response at week 52 compared with placebo (43.2% versus 33.5%; P = 0.017). The rate with 1 mg/kg belimumab was 40.6% (P = 0.089). Response rates at week 76 were 32.4%, 39.1%, and 38.5% with placebo, 1 mg/kg belimumab, and 10 mg/kg belimumab, respectively. In post hoc sensitivity analyses evaluating higher SELENA-SLEDAI score thresholds, 10 mg/kg belimumab achieved better discrimination at weeks 52 and 76. Risk of severe flares over 76 weeks (based on the modified SLE Flare Index) was reduced with 1 mg/kg belimumab (34%) (P = 0.023) and 10 mg/kg belimumab (23%) (P = 0.13). Serious and severe adverse events, including infections, laboratory abnormalities, malignancies, and deaths, were comparable across groups.\nBelimumab plus standard therapy significantly improved SRI response rate, reduced SLE disease activity and severe flares, and was generally well tolerated in SLE."", 'title': 'A phase III, randomized, placebo-controlled study of belimumab, a monoclonal antibody that inhibits B lymphocyte stimulator, in patients with systemic lupus erythematosus.', 'date': '2011-12-01'}, '21296403': {'article_id': '21296403', 'content': ""Systemic lupus erythematosus is a heterogeneous autoimmune disease that is associated with B-cell hyperactivity, autoantibodies, and increased concentrations of B-lymphocyte stimulator (BLyS). The efficacy and safety of the fully human monoclonal antibody belimumab (BLyS-specific inhibitor) was assessed in patients with active systemic lupus erythematosus.\nPatients (aged ≥18 years) who were seropositive with scores of at least 6 on the Safety of Estrogens in Lupus Erythematosus National Assessment-Systemic Lupus Erythematosus Disease Activity Index (SELENA-SLEDAI) were enrolled in a multicentre phase 3 study, which was done in Latin America, Asia-Pacific, and eastern Europe. Patients were randomly assigned by use of a central interactive voice response system in a 1:1:1 ratio to belimumab 1 mg/kg or 10 mg/kg, or placebo by intravenous infusion in 1 h on days 0, 14, and 28, and then every 28 days until 48 weeks, with standard of care. Patients, investigators, study coordinators, and sponsors were masked to treatment assignment. Primary efficacy endpoint was improvement in the Systemic Lupus Erythematosus Responder Index (SRI) at week 52 (reduction ≥4 points in SELENA-SLEDAI score; no new British Isles Lupus Assessment Group [BILAG] A organ domain score and no more than 1 new B organ domain score; and no worsening [<0·3 increase] in Physician's Global Assessment [PGA] score) versus baseline. Method of analysis was by modified intention to treat. This trial is registered with ClinicalTrials.gov, number NCT00424476.\n867 patients were randomly assigned to belimumab 1 mg/kg (n=289) or 10 mg/kg (n=290), or placebo (n=288). 865 were treated and analysed in the belimumab (1 mg/kg, n=288; 10 mg/kg, n=290) and placebo groups (n=287). Significantly higher SRI rates were noted with belimumab 1 mg/kg (148 [51%], odds ratio 1·55 [95% CI 1·10-2·19]; p=0·0129) and 10 mg/kg (167 [58%], 1·83 [1·30-2·59]; p=0·0006) than with placebo (125 [44%]) at week 52. More patients had their SELENA-SLEDAI score reduced by at least 4 points during 52 weeks with belimumab 1 mg/kg (153 [53%], 1·51 [1·07-2·14]; p=0·0189) and 10 mg/kg (169 [58%], 1·71 [1·21-2·41]; p=0·0024) than with placebo (132 [46%]). More patients given belimumab 1 mg/kg (226 [78%], 1·38 [0·93-2·04]; p=0·1064) and 10 mg/kg (236 [81%], 1·62 [1·09-2·42]; p=0·0181) had no new BILAG A or no more than 1 new B flare than did those in the placebo group (210 [73%]). No worsening in PGA score was noted in more patients with belimumab 1 mg/kg (227 [79%], 1·68 [1·15-2·47]; p=0·0078) and 10 mg/kg (231 [80%], 1·74 [1·18-2·55]; p=0·0048) than with placebo (199 [69%]). Rates of adverse events were similar in the groups given belimumab 1 mg/kg and 10 mg/kg, and placebo: serious infection was reported in 22 (8%), 13 (4%), and 17 (6%) patients, respectively, and severe or serious hypersensitivity reactions on an infusion day were reported in two (<1%), two (<1%), and no patients, respectively. No malignant diseases were reported.\nBelimumab has the potential to be the first targeted biological treatment that is approved specifically for systemic lupus erythematosus, providing a new option for the management of this important prototypic autoimmune disease.\nHuman Genome Sciences and GlaxoSmithKline."", 'title': 'Efficacy and safety of belimumab in patients with active systemic lupus erythematosus: a randomised, placebo-controlled, phase 3 trial.', 'date': '2011-02-08'}}",0.25,Internal Medicine & Subspecialties
128,"Is the incidence of critical illness polyneuropathy/myopathy (CIP/CIM) higher, lower, or the same when comparing intensive insulin therapy (IIT) to conventional insulin therapy (CIT)?",lower,moderate,no,"['17138955', '15851721']",24477672,2014,"{'17138955': {'article_id': '17138955', 'content': 'Critical illness polyneuropathy/myopathy causes limb and respiratory muscle weakness, prolongs mechanical ventilation, and extends hospitalization of intensive care patients. Besides controlling risk factors, no specific prevention or treatment exists. Recently, intensive insulin therapy prevented critical illness polyneuropathy in a surgical intensive care unit.\nTo investigate the impact of intensive insulin therapy on polyneuropathy/myopathy and treatment with prolonged mechanical ventilation in medical patients in the intensive care unit for at least 7 days.\nThis was a prospectively planned subanalysis of a randomized controlled trial evaluating the effect of intensive insulin versus conventional therapy on morbidity and mortality in critically ill medical patients. All patients who were still in intensive care on Day 7 were screened weekly by electroneuromyography. The effect of intensive insulin therapy on critical illness polyneuropathy/myopathy and the relationship with duration of mechanical ventilation were assessed.\nIndependent of risk factors, intensive insulin therapy reduced incidence of critical illness polyneuropathy/myopathy (107/212 [50.5%] to 81/208 [38.9%], p = 0.02). Treatment with prolonged (> or = 14 d) mechanical ventilation was reduced from 99 of 212 (46.7%) to 72 of 208 (34.6%) (p = 0.01). This was statistically only partially explained by prevention of critical illness polyneuropathy/myopathy.\nIn a subset of medical patients in the intensive care unit for at least 7 days, enrolled in a randomized controlled trial of intensive insulin therapy, those assigned to intensive insulin therapy had a reduced incidence of critical illness polyneuropathy/myopathy and were treated with prolonged mechanical ventilation less frequently.', 'title': 'Impact of intensive insulin therapy on neuromuscular complications and ventilator dependency in the medical intensive care unit.', 'date': '2006-12-02'}, '15851721': {'article_id': '15851721', 'content': 'To investigate the effectiveness of maintaining blood glucose levels below 6.1 mmol/L with insulin as prevention of secondary injury to the central and peripheral nervous systems of intensive care patients.\nThe authors studied the effect of intensive insulin therapy on critical illness polyneuropathy (CIPNP), assessed by weekly EMG screening, and its impact on mechanical ventilation dependency, as a prospectively planned subanalysis of a large randomized, controlled trial of 1,548 intensive care patients. In the 63 patients admitted with isolated brain injury, the authors studied the impact of insulin therapy on intracranial pressure, diabetes insipidus, seizures, and long-term rehabilitation at 6 and 12 months follow-up.\nIntensive insulin therapy reduced ventilation dependency (p = 0.0007; Mantel-Cox log rank test) and the risk of CIPNP (p < 0.0001). The risk of CIPNP among the 405 long-stay (> or =7 days in intensive care unit) patients was lowered by 49% (p < 0.0001). Of all metabolic and clinical effects of insulin therapy, and corrected for known risk factors, the level of glycemic control independently explained this benefit (OR for CIPNP 1.26 [1.09 to 1.46] per mmol blood glucose, p = 0.002). In turn, prevention of CIPNP explained the ability of intensive insulin therapy to reduce the risk of prolonged mechanical ventilation (OR 3.75 [1.49 to 9.39], p = 0.005). In isolated brain injury patients, intensive insulin therapy reduced mean (p = 0.003) and maximal (p < 0.0001) intracranial pressure while identical cerebral perfusion pressures were obtained with eightfold less vasopressors (p = 0.01). Seizures (p < 0.0001) and diabetes insipidus (p = 0.06) occurred less frequently. At 12 months follow-up, more brain-injured survivors in the intensive insulin group were able to care for most of their own needs (p = 0.05).\nPreventing even moderate hyperglycemia with insulin during intensive care protected the central and peripheral nervous systems, with clinical consequences such as shortening of intensive care dependency and possibly better long-term rehabilitation.', 'title': 'Insulin therapy protects the central and peripheral nervous system of intensive care patients.', 'date': '2005-04-27'}}",1.0,Emergency Medicine & Critical Care
129,"Is duration of mechanical ventilation higher, lower, or the same when comparing intensive insulin therapy (IIT) to conventional insulin therapy (CIT)?",lower,high,no,"['17138955', '15851721']",24477672,2014,"{'17138955': {'article_id': '17138955', 'content': 'Critical illness polyneuropathy/myopathy causes limb and respiratory muscle weakness, prolongs mechanical ventilation, and extends hospitalization of intensive care patients. Besides controlling risk factors, no specific prevention or treatment exists. Recently, intensive insulin therapy prevented critical illness polyneuropathy in a surgical intensive care unit.\nTo investigate the impact of intensive insulin therapy on polyneuropathy/myopathy and treatment with prolonged mechanical ventilation in medical patients in the intensive care unit for at least 7 days.\nThis was a prospectively planned subanalysis of a randomized controlled trial evaluating the effect of intensive insulin versus conventional therapy on morbidity and mortality in critically ill medical patients. All patients who were still in intensive care on Day 7 were screened weekly by electroneuromyography. The effect of intensive insulin therapy on critical illness polyneuropathy/myopathy and the relationship with duration of mechanical ventilation were assessed.\nIndependent of risk factors, intensive insulin therapy reduced incidence of critical illness polyneuropathy/myopathy (107/212 [50.5%] to 81/208 [38.9%], p = 0.02). Treatment with prolonged (> or = 14 d) mechanical ventilation was reduced from 99 of 212 (46.7%) to 72 of 208 (34.6%) (p = 0.01). This was statistically only partially explained by prevention of critical illness polyneuropathy/myopathy.\nIn a subset of medical patients in the intensive care unit for at least 7 days, enrolled in a randomized controlled trial of intensive insulin therapy, those assigned to intensive insulin therapy had a reduced incidence of critical illness polyneuropathy/myopathy and were treated with prolonged mechanical ventilation less frequently.', 'title': 'Impact of intensive insulin therapy on neuromuscular complications and ventilator dependency in the medical intensive care unit.', 'date': '2006-12-02'}, '15851721': {'article_id': '15851721', 'content': 'To investigate the effectiveness of maintaining blood glucose levels below 6.1 mmol/L with insulin as prevention of secondary injury to the central and peripheral nervous systems of intensive care patients.\nThe authors studied the effect of intensive insulin therapy on critical illness polyneuropathy (CIPNP), assessed by weekly EMG screening, and its impact on mechanical ventilation dependency, as a prospectively planned subanalysis of a large randomized, controlled trial of 1,548 intensive care patients. In the 63 patients admitted with isolated brain injury, the authors studied the impact of insulin therapy on intracranial pressure, diabetes insipidus, seizures, and long-term rehabilitation at 6 and 12 months follow-up.\nIntensive insulin therapy reduced ventilation dependency (p = 0.0007; Mantel-Cox log rank test) and the risk of CIPNP (p < 0.0001). The risk of CIPNP among the 405 long-stay (> or =7 days in intensive care unit) patients was lowered by 49% (p < 0.0001). Of all metabolic and clinical effects of insulin therapy, and corrected for known risk factors, the level of glycemic control independently explained this benefit (OR for CIPNP 1.26 [1.09 to 1.46] per mmol blood glucose, p = 0.002). In turn, prevention of CIPNP explained the ability of intensive insulin therapy to reduce the risk of prolonged mechanical ventilation (OR 3.75 [1.49 to 9.39], p = 0.005). In isolated brain injury patients, intensive insulin therapy reduced mean (p = 0.003) and maximal (p < 0.0001) intracranial pressure while identical cerebral perfusion pressures were obtained with eightfold less vasopressors (p = 0.01). Seizures (p < 0.0001) and diabetes insipidus (p = 0.06) occurred less frequently. At 12 months follow-up, more brain-injured survivors in the intensive insulin group were able to care for most of their own needs (p = 0.05).\nPreventing even moderate hyperglycemia with insulin during intensive care protected the central and peripheral nervous systems, with clinical consequences such as shortening of intensive care dependency and possibly better long-term rehabilitation.', 'title': 'Insulin therapy protects the central and peripheral nervous system of intensive care patients.', 'date': '2005-04-27'}}",0.5,Emergency Medicine & Critical Care
130,"Is mortality rate at 180 days higher, lower, or the same when comparing corticosteroids to placebo?",no difference,high,no,['16625008'],24477672,2014,"{'16625008': {'article_id': '16625008', 'content': ""Persistent acute respiratory distress syndrome (ARDS) is characterized by excessive fibroproliferation, ongoing inflammation, prolonged mechanical ventilation, and a substantial risk of death. Because previous reports suggested that corticosteroids may improve survival, we performed a multicenter, randomized controlled trial of corticosteroids in patients with persistent ARDS.\nWe randomly assigned 180 patients with ARDS of at least seven days' duration to receive either methylprednisolone or placebo in a double-blind fashion. The primary end point was mortality at 60 days. Secondary end points included the number of ventilator-free days and organ-failure-free days, biochemical markers of inflammation and fibroproliferation, and infectious complications.\nAt 60 days, the hospital mortality rate was 28.6 percent in the placebo group (95 percent confidence interval, 20.3 to 38.6 percent) and 29.2 percent in the methylprednisolone group (95 percent confidence interval, 20.8 to 39.4 percent; P=1.0); at 180 days, the rates were 31.9 percent (95 percent confidence interval, 23.2 to 42.0 percent) and 31.5 percent (95 percent confidence interval, 22.8 to 41.7 percent; P=1.0), respectively. Methylprednisolone was associated with significantly increased 60- and 180-day mortality rates among patients enrolled at least 14 days after the onset of ARDS. Methylprednisolone increased the number of ventilator-free and shock-free days during the first 28 days in association with an improvement in oxygenation, respiratory-system compliance, and blood pressure with fewer days of vasopressor therapy. As compared with placebo, methylprednisolone did not increase the rate of infectious complications but was associated with a higher rate of neuromuscular weakness.\nThese results do not support the routine use of methylprednisolone for persistent ARDS despite the improvement in cardiopulmonary physiology. In addition, starting methylprednisolone therapy more than two weeks after the onset of ARDS may increase the risk of death. (ClinicalTrials.gov number, NCT00295269.)."", 'title': 'Efficacy and safety of corticosteroids for persistent acute respiratory distress syndrome.', 'date': '2006-04-21'}}",1.0,"Public Health, Epidemiology & Health Systems"
131,"Is the incidence of critical illness polyneuropathy/myopathy (CIP/CIM) higher, lower, or the same when comparing electrical muscle stimulation (EMS) to no stimulation?",no difference,very low,no,['20426834'],24477672,2014,"{'20426834': {'article_id': '20426834', 'content': 'Critical illness polyneuromyopathy (CIPNM) is a common complication of critical illness presenting with muscle weakness and is associated with increased duration of mechanical ventilation and weaning period. No preventive tool and no specific treatment have been proposed so far for CIPNM. Electrical muscle stimulation (EMS) has been shown to be beneficial in patients with severe chronic heart failure and chronic obstructive pulmonary disease. Aim of our study was to assess the efficacy of EMS in preventing CIPNM in critically ill patients.\nOne hundred and forty consecutive critically ill patients with an APACHE II score >or= 13 were randomly assigned after stratification to the EMS group (n = 68) (age:61 +/- 19 years) (APACHE II:18 +/- 4, SOFA:9 +/- 3) or to the control group (n = 72) (age:58 +/- 18 years) (APACHE II:18 +/- 5, SOFA:9 +/- 3). Patients of the EMS group received daily EMS sessions. CIPNM was diagnosed clinically with the medical research council (MRC) scale for muscle strength (maximum score 60, <48/60 cut off for diagnosis) by two unblinded independent investigators. Duration of weaning from mechanical ventilation and intensive care unit (ICU) stay were recorded.\nFifty two patients could be finally evaluated with MRC; 24 in the EMS group and 28 in the control group. CIPNM was diagnosed in 3 patients in the EMS group as compared to 11 patients in the control group (OR = 0.22; CI: 0.05 to 0.92, P = 0.04). The MRC score was significantly higher in patients of the EMS group as compared to the control group [58 (33 to 60) vs. 52 (2 to 60) respectively, median (range), P = 0.04). The weaning period was statistically significantly shorter in patients of the EMS group vs. the control group [1 (0 to 10) days vs. 3 (0 to 44) days, respectively, median (range), P = 0.003].\nThis study suggests that daily EMS sessions prevent the development of CIPNM in critically ill patients and also result in shorter duration of weaning. Further studies should evaluate which patients benefit more from EMS and explore the EMS characteristics most appropriate for preventing CIPNM.\nClinicalTrials.gov NCT00882830.', 'title': 'Electrical muscle stimulation prevents critical illness polyneuromyopathy: a randomized parallel intervention trial.', 'date': '2010-04-30'}}",0.0,Emergency Medicine & Critical Care
237,"Is duration of ICU stay higher, lower, or the same when comparing early physical therapy to control?",no difference,,yes,['19446324'],24477672,2014,"{'19446324': {'article_id': '19446324', 'content': 'Long-term complications of critical illness include intensive care unit (ICU)-acquired weakness and neuropsychiatric disease. Immobilisation secondary to sedation might potentiate these problems. We assessed the efficacy of combining daily interruption of sedation with physical and occupational therapy on functional outcomes in patients receiving mechanical ventilation in intensive care.\nSedated adults (>/=18 years of age) in the ICU who had been on mechanical ventilation for less than 72 h, were expected to continue for at least 24 h, and who met criteria for baseline functional independence were eligible for enrolment in this randomised controlled trial at two university hospitals. We randomly assigned 104 patients by computer-generated, permuted block randomisation to early exercise and mobilisation (physical and occupational therapy) during periods of daily interruption of sedation (intervention; n=49) or to daily interruption of sedation with therapy as ordered by the primary care team (control; n=55). The primary endpoint-the number of patients returning to independent functional status at hospital discharge-was defined as the ability to perform six activities of daily living and the ability to walk independently. Therapists who undertook patient assessments were blinded to treatment assignment. Secondary endpoints included duration of delirium and ventilator-free days during the first 28 days of hospital stay. Analysis was by intention to treat. This trial is registered with ClinicalTrials.gov, number NCT00322010.\nAll 104 patients were included in the analysis. Return to independent functional status at hospital discharge occurred in 29 (59%) patients in the intervention group compared with 19 (35%) patients in the control group (p=0.02; odds ratio 2.7 [95% CI 1.2-6.1]). Patients in the intervention group had shorter duration of delirium (median 2.0 days, IQR 0.0-6.0 vs 4.0 days, 2.0-8.0; p=0.02), and more ventilator-free days (23.5 days, 7.4-25.6 vs 21.1 days, 0.0-23.8; p=0.05) during the 28-day follow-up period than did controls. There was one serious adverse event in 498 therapy sessions (desaturation less than 80%). Discontinuation of therapy as a result of patient instability occurred in 19 (4%) of all sessions, most commonly for perceived patient-ventilator asynchrony.\nA strategy for whole-body rehabilitation-consisting of interruption of sedation and physical and occupational therapy in the earliest days of critical illness-was safe and well tolerated, and resulted in better functional outcomes at hospital discharge, a shorter duration of delirium, and more ventilator-free days compared with standard care.\nNone.', 'title': 'Early physical and occupational therapy in mechanically ventilated, critically ill patients: a randomised controlled trial.', 'date': '2009-05-19'}}",1.0,Emergency Medicine & Critical Care
238,"Is duration of mechanical ventilation higher, lower, or the same when comparing early physical therapy to control?",lower,,no,['19446324'],24477672,2014,"{'19446324': {'article_id': '19446324', 'content': 'Long-term complications of critical illness include intensive care unit (ICU)-acquired weakness and neuropsychiatric disease. Immobilisation secondary to sedation might potentiate these problems. We assessed the efficacy of combining daily interruption of sedation with physical and occupational therapy on functional outcomes in patients receiving mechanical ventilation in intensive care.\nSedated adults (>/=18 years of age) in the ICU who had been on mechanical ventilation for less than 72 h, were expected to continue for at least 24 h, and who met criteria for baseline functional independence were eligible for enrolment in this randomised controlled trial at two university hospitals. We randomly assigned 104 patients by computer-generated, permuted block randomisation to early exercise and mobilisation (physical and occupational therapy) during periods of daily interruption of sedation (intervention; n=49) or to daily interruption of sedation with therapy as ordered by the primary care team (control; n=55). The primary endpoint-the number of patients returning to independent functional status at hospital discharge-was defined as the ability to perform six activities of daily living and the ability to walk independently. Therapists who undertook patient assessments were blinded to treatment assignment. Secondary endpoints included duration of delirium and ventilator-free days during the first 28 days of hospital stay. Analysis was by intention to treat. This trial is registered with ClinicalTrials.gov, number NCT00322010.\nAll 104 patients were included in the analysis. Return to independent functional status at hospital discharge occurred in 29 (59%) patients in the intervention group compared with 19 (35%) patients in the control group (p=0.02; odds ratio 2.7 [95% CI 1.2-6.1]). Patients in the intervention group had shorter duration of delirium (median 2.0 days, IQR 0.0-6.0 vs 4.0 days, 2.0-8.0; p=0.02), and more ventilator-free days (23.5 days, 7.4-25.6 vs 21.1 days, 0.0-23.8; p=0.05) during the 28-day follow-up period than did controls. There was one serious adverse event in 498 therapy sessions (desaturation less than 80%). Discontinuation of therapy as a result of patient instability occurred in 19 (4%) of all sessions, most commonly for perceived patient-ventilator asynchrony.\nA strategy for whole-body rehabilitation-consisting of interruption of sedation and physical and occupational therapy in the earliest days of critical illness-was safe and well tolerated, and resulted in better functional outcomes at hospital discharge, a shorter duration of delirium, and more ventilator-free days compared with standard care.\nNone.', 'title': 'Early physical and occupational therapy in mechanically ventilated, critically ill patients: a randomised controlled trial.', 'date': '2009-05-19'}}",1.0,Emergency Medicine & Critical Care
239,"Is mortality rate at 30 days higher, lower, or the same when comparing early physical therapy to control?",insufficient data,,no,['19446324'],24477672,2014,"{'19446324': {'article_id': '19446324', 'content': 'Long-term complications of critical illness include intensive care unit (ICU)-acquired weakness and neuropsychiatric disease. Immobilisation secondary to sedation might potentiate these problems. We assessed the efficacy of combining daily interruption of sedation with physical and occupational therapy on functional outcomes in patients receiving mechanical ventilation in intensive care.\nSedated adults (>/=18 years of age) in the ICU who had been on mechanical ventilation for less than 72 h, were expected to continue for at least 24 h, and who met criteria for baseline functional independence were eligible for enrolment in this randomised controlled trial at two university hospitals. We randomly assigned 104 patients by computer-generated, permuted block randomisation to early exercise and mobilisation (physical and occupational therapy) during periods of daily interruption of sedation (intervention; n=49) or to daily interruption of sedation with therapy as ordered by the primary care team (control; n=55). The primary endpoint-the number of patients returning to independent functional status at hospital discharge-was defined as the ability to perform six activities of daily living and the ability to walk independently. Therapists who undertook patient assessments were blinded to treatment assignment. Secondary endpoints included duration of delirium and ventilator-free days during the first 28 days of hospital stay. Analysis was by intention to treat. This trial is registered with ClinicalTrials.gov, number NCT00322010.\nAll 104 patients were included in the analysis. Return to independent functional status at hospital discharge occurred in 29 (59%) patients in the intervention group compared with 19 (35%) patients in the control group (p=0.02; odds ratio 2.7 [95% CI 1.2-6.1]). Patients in the intervention group had shorter duration of delirium (median 2.0 days, IQR 0.0-6.0 vs 4.0 days, 2.0-8.0; p=0.02), and more ventilator-free days (23.5 days, 7.4-25.6 vs 21.1 days, 0.0-23.8; p=0.05) during the 28-day follow-up period than did controls. There was one serious adverse event in 498 therapy sessions (desaturation less than 80%). Discontinuation of therapy as a result of patient instability occurred in 19 (4%) of all sessions, most commonly for perceived patient-ventilator asynchrony.\nA strategy for whole-body rehabilitation-consisting of interruption of sedation and physical and occupational therapy in the earliest days of critical illness-was safe and well tolerated, and resulted in better functional outcomes at hospital discharge, a shorter duration of delirium, and more ventilator-free days compared with standard care.\nNone.', 'title': 'Early physical and occupational therapy in mechanically ventilated, critically ill patients: a randomised controlled trial.', 'date': '2009-05-19'}}",1.0,"Public Health, Epidemiology & Health Systems"
240,"Is mortality rate at 180 days higher, lower, or the same when comparing early physical therapy to control?",insufficient data,,no,['19446324'],24477672,2014,"{'19446324': {'article_id': '19446324', 'content': 'Long-term complications of critical illness include intensive care unit (ICU)-acquired weakness and neuropsychiatric disease. Immobilisation secondary to sedation might potentiate these problems. We assessed the efficacy of combining daily interruption of sedation with physical and occupational therapy on functional outcomes in patients receiving mechanical ventilation in intensive care.\nSedated adults (>/=18 years of age) in the ICU who had been on mechanical ventilation for less than 72 h, were expected to continue for at least 24 h, and who met criteria for baseline functional independence were eligible for enrolment in this randomised controlled trial at two university hospitals. We randomly assigned 104 patients by computer-generated, permuted block randomisation to early exercise and mobilisation (physical and occupational therapy) during periods of daily interruption of sedation (intervention; n=49) or to daily interruption of sedation with therapy as ordered by the primary care team (control; n=55). The primary endpoint-the number of patients returning to independent functional status at hospital discharge-was defined as the ability to perform six activities of daily living and the ability to walk independently. Therapists who undertook patient assessments were blinded to treatment assignment. Secondary endpoints included duration of delirium and ventilator-free days during the first 28 days of hospital stay. Analysis was by intention to treat. This trial is registered with ClinicalTrials.gov, number NCT00322010.\nAll 104 patients were included in the analysis. Return to independent functional status at hospital discharge occurred in 29 (59%) patients in the intervention group compared with 19 (35%) patients in the control group (p=0.02; odds ratio 2.7 [95% CI 1.2-6.1]). Patients in the intervention group had shorter duration of delirium (median 2.0 days, IQR 0.0-6.0 vs 4.0 days, 2.0-8.0; p=0.02), and more ventilator-free days (23.5 days, 7.4-25.6 vs 21.1 days, 0.0-23.8; p=0.05) during the 28-day follow-up period than did controls. There was one serious adverse event in 498 therapy sessions (desaturation less than 80%). Discontinuation of therapy as a result of patient instability occurred in 19 (4%) of all sessions, most commonly for perceived patient-ventilator asynchrony.\nA strategy for whole-body rehabilitation-consisting of interruption of sedation and physical and occupational therapy in the earliest days of critical illness-was safe and well tolerated, and resulted in better functional outcomes at hospital discharge, a shorter duration of delirium, and more ventilator-free days compared with standard care.\nNone.', 'title': 'Early physical and occupational therapy in mechanically ventilated, critically ill patients: a randomised controlled trial.', 'date': '2009-05-19'}}",1.0,Family Medicine & Preventive Care
241,"Is mortality rate at 180 days higher, lower, or the same when comparing electrical muscle stimulation (EMS) to no stimulation?",insufficient data,,no,['20426834'],24477672,2014,"{'20426834': {'article_id': '20426834', 'content': 'Critical illness polyneuromyopathy (CIPNM) is a common complication of critical illness presenting with muscle weakness and is associated with increased duration of mechanical ventilation and weaning period. No preventive tool and no specific treatment have been proposed so far for CIPNM. Electrical muscle stimulation (EMS) has been shown to be beneficial in patients with severe chronic heart failure and chronic obstructive pulmonary disease. Aim of our study was to assess the efficacy of EMS in preventing CIPNM in critically ill patients.\nOne hundred and forty consecutive critically ill patients with an APACHE II score >or= 13 were randomly assigned after stratification to the EMS group (n = 68) (age:61 +/- 19 years) (APACHE II:18 +/- 4, SOFA:9 +/- 3) or to the control group (n = 72) (age:58 +/- 18 years) (APACHE II:18 +/- 5, SOFA:9 +/- 3). Patients of the EMS group received daily EMS sessions. CIPNM was diagnosed clinically with the medical research council (MRC) scale for muscle strength (maximum score 60, <48/60 cut off for diagnosis) by two unblinded independent investigators. Duration of weaning from mechanical ventilation and intensive care unit (ICU) stay were recorded.\nFifty two patients could be finally evaluated with MRC; 24 in the EMS group and 28 in the control group. CIPNM was diagnosed in 3 patients in the EMS group as compared to 11 patients in the control group (OR = 0.22; CI: 0.05 to 0.92, P = 0.04). The MRC score was significantly higher in patients of the EMS group as compared to the control group [58 (33 to 60) vs. 52 (2 to 60) respectively, median (range), P = 0.04). The weaning period was statistically significantly shorter in patients of the EMS group vs. the control group [1 (0 to 10) days vs. 3 (0 to 44) days, respectively, median (range), P = 0.003].\nThis study suggests that daily EMS sessions prevent the development of CIPNM in critically ill patients and also result in shorter duration of weaning. Further studies should evaluate which patients benefit more from EMS and explore the EMS characteristics most appropriate for preventing CIPNM.\nClinicalTrials.gov NCT00882830.', 'title': 'Electrical muscle stimulation prevents critical illness polyneuromyopathy: a randomized parallel intervention trial.', 'date': '2010-04-30'}}",1.0,Emergency Medicine & Critical Care
242,"Is mortality rate at 30 days higher, lower, or the same when comparing electrical muscle stimulation (EMS) to no stimulation?",insufficient data,,no,['20426834'],24477672,2014,"{'20426834': {'article_id': '20426834', 'content': 'Critical illness polyneuromyopathy (CIPNM) is a common complication of critical illness presenting with muscle weakness and is associated with increased duration of mechanical ventilation and weaning period. No preventive tool and no specific treatment have been proposed so far for CIPNM. Electrical muscle stimulation (EMS) has been shown to be beneficial in patients with severe chronic heart failure and chronic obstructive pulmonary disease. Aim of our study was to assess the efficacy of EMS in preventing CIPNM in critically ill patients.\nOne hundred and forty consecutive critically ill patients with an APACHE II score >or= 13 were randomly assigned after stratification to the EMS group (n = 68) (age:61 +/- 19 years) (APACHE II:18 +/- 4, SOFA:9 +/- 3) or to the control group (n = 72) (age:58 +/- 18 years) (APACHE II:18 +/- 5, SOFA:9 +/- 3). Patients of the EMS group received daily EMS sessions. CIPNM was diagnosed clinically with the medical research council (MRC) scale for muscle strength (maximum score 60, <48/60 cut off for diagnosis) by two unblinded independent investigators. Duration of weaning from mechanical ventilation and intensive care unit (ICU) stay were recorded.\nFifty two patients could be finally evaluated with MRC; 24 in the EMS group and 28 in the control group. CIPNM was diagnosed in 3 patients in the EMS group as compared to 11 patients in the control group (OR = 0.22; CI: 0.05 to 0.92, P = 0.04). The MRC score was significantly higher in patients of the EMS group as compared to the control group [58 (33 to 60) vs. 52 (2 to 60) respectively, median (range), P = 0.04). The weaning period was statistically significantly shorter in patients of the EMS group vs. the control group [1 (0 to 10) days vs. 3 (0 to 44) days, respectively, median (range), P = 0.003].\nThis study suggests that daily EMS sessions prevent the development of CIPNM in critically ill patients and also result in shorter duration of weaning. Further studies should evaluate which patients benefit more from EMS and explore the EMS characteristics most appropriate for preventing CIPNM.\nClinicalTrials.gov NCT00882830.', 'title': 'Electrical muscle stimulation prevents critical illness polyneuromyopathy: a randomized parallel intervention trial.', 'date': '2010-04-30'}}",1.0,Surgery
132,"Is the rate of all‐cause hospital admission higher, lower, or the same when comparing nurse-led titration (NLT) to usual care?",lower,high,no,"['16908918', '25248944', '23150980']",26689943,2015,"{'16908918': {'article_id': '16908918', 'content': ""Despite therapies proven effective for heart failure with systolic dysfunction, the condition continues to cause substantial hospitalization, disability, and death, especially among African- American and other nonwhite populations.\nTo compare the effects of a nurse-led intervention focused on specific management problems versus usual care among ethnically diverse patients with systolic dysfunction in ambulatory care practices.\nRandomized effectiveness trial conducted from September 2000 to September 2002.\nThe 4 hospitals in Harlem, New York.\n406 adults (45.8% were non-Hispanic black adults, 32.5% were Hispanic adults, 46.3% were women, and 36.7% were > or =65 years of age) who met eligibility criteria: systolic dysfunction, English- or Spanish-language speakers, community-dwelling patients, and ambulatory care practice patients.\nDuring a 12-month intervention, bilingual nurses counseled patients on diet, medication adherence, and self-management of symptoms through an initial visit and regularly scheduled follow-up telephone calls and facilitated evidence-based changes to medications in discussions with patients' clinicians.\nHospitalizations (in 406 of 406 patients during follow-up) and self-reported functioning (in 286 of 406 patients during follow-up) at 12 months.\nAt 12 months, nurse management patients had had fewer hospitalizations (143 hospitalizations vs. 180 hospitalizations; adjusted difference, -0.13 hospitalization/person-year [95% CI, -0.25 to -0.001 hospitalization/person-year]) than usual care patients. They also had better functioning: The Short Form-12 physical component score was 39.9 versus 36.3, respectively (difference, 3.6 [CI, 1.2 to 6.1]), and the Minnesota Living with Heart Failure Questionnaire score was 38.6 versus 47.3, respectively (difference, -8.8 [CI, -15.3 to -2.2]). Through 12 months, 22 deaths occurred in each group and percentages of patients who were hospitalized at least once were similar in each group (30.5% of nurse management patients vs. 36.5% of control patients; adjusted difference, -7.1 percentage points [CI, -16.9 to 2.6 percentage points]).\nThree nurses at 4 hospitals delivered interventions in this modest-sized trial, and 75% of the participants were from 1 site. It is not clear which aspects of the complex intervention accounted for the results.\nNurse management can improve functioning and modestly lower hospitalizations in ethnically diverse ambulatory care patients who have heart failure with systolic dysfunction. Sustaining improved functioning may require continuing nurse contact."", 'title': 'Effects of nurse management on the quality of heart failure care in minority communities: a randomized trial.', 'date': '2006-08-16'}, '25248944': {'article_id': '25248944', 'content': 'Beta-adrenergic blockade has been shown to improve left ventricular function, reduce hospital admissions and improve survival in chronic heart failure with reduced ejection fraction (HFrEF), with mortality reduction starting early after beta-adrenergic receptor blocker initiation and being dose-related. The aim of this pilot study was to determine the effectiveness of a nurse-led titration clinic in improving the time required for patients to reach optimal doses of the beta-adrenergic receptor blocking agents.\nWe conducted a prospective pilot randomized controlled trial. Twenty eight patients with CHF were randomized to optimisation of beta-adrenergic receptor blocker therapy over six months by either a nurse-led titration (NLT) clinic, led by a nurse specialist with the support of a cardiologist in a CHF clinic, or by their primary care physician (usual care (UC)). The primary endpoint was time to maximal beta-adrenergic receptor blocker dose. The secondary end-point was the proportion of patients reaching the target dose of beta-adrenergic receptor blocker by six months.\nThe patients were predominantly men (72%), age 67 ± 16 years; New York Heart Association (NYHA) functional class I (32%), II (44%) and III (20%); baseline left ventricular ejection fraction 33 ± 10%, and a low mean Charlson co-morbidity score of 2.5 ± 1.4. The time to maximum dose was shorter in the NLT group compared to the UC group (90 ± 14 vs 166 ± 8 days, p < 0.0005). At six months, in the NLT group there were nine patients (82%) on high dose and one patient (9%) on low dose beta-adrenergic receptor blocker compared to the UC group with five (42%) patients reaching maximum dose and five (42%) patients on low dose (p = 0.04). The patients allocated to the NLT group also had significantly less worsening of depression between baseline and six months (p = 0.006).\nA NLT clinic improves optimisation of beta-adrenergic receptor blocker therapy through increasing the proportion of patients reaching maximal dose and facilitating rapid up-titration of beta-adrenergic receptor blocker agents in patients with chronic HFrEF.\nAustralian Clinical Trials Registry (ACTRN012606000383561).', 'title': 'A nurse-led up-titration clinic improves chronic heart failure optimization of beta-adrenergic receptor blocking therapy--a randomized controlled trial.', 'date': '2014-09-25'}, '23150980': {'article_id': '23150980', 'content': ""Many older people in long-term care do not receive evidence-based diagnosis or management for heart failure; it is not known whether this can be achieved for this population. We initiated an onsite heart failure service, compared with 'usual care' with the aim of establishing the feasibility of accurate diagnosis and appropriate management.\nA pilot randomised controlled trial which randomised residents from 33 care facilities in North-East England with left ventricular systolic dysfunction (LVSD) to usual care or an onsite heart failure service. The primary outcome was the optimum prescription of angiotensin-converting enzyme inhibitors and beta-adrenergic antagonists at 6 months.\nOf 399 echocardiographically-screened residents aged 65-100 years, 30 subjects with LVSD were eligible; 28 (93%) consented and were randomised (HF service: 16; routine care: 12). Groups were similar at baseline; six month follow-up was completed for 25 patients (89%); 3 (11%) patients died. Results for the primary outcome were not statistically significant but there was a consistent pattern of increased drug use and titration to optimum dose in the intervention group (21% compared to 0% receiving routine care, p=0.250). Hospitalisation rates, quality of life and mortality at 6 months were similar between groups.\nThis study demonstrated the feasibility of an on-site heart failure service for older long-term care populations. Optimisation of medication appeared possible without adversely affecting quality of life; this questions clinicians' concerns about adverse effects in this group. This has international implications for managing such patients. These methods should be replicated in a large-scale study to quantify the scale of benefit.\nISRCTN19781227 http://www.controlled-trials.com/ISRCTN19781227"", 'title': 'Feasibility of evidence-based diagnosis and management of heart failure in older people in care: a pilot randomised controlled trial.', 'date': '2012-11-16'}}",0.0,Family Medicine & Preventive Care
133,"Is time to complete ulcer healing higher, lower, or the same when comparing combined endovenous ablation and compression to compression therapy without or with deferred endovenous treatment?",lower,high,no,"['29688123', '32928070']",37497816,2023,"{'29688123': {'article_id': '29688123', 'content': 'Venous disease is the most common cause of leg ulceration. Although compression therapy improves venous ulcer healing, it does not treat the underlying causes of venous hypertension. Treatment of superficial venous reflux has been shown to reduce the rate of ulcer recurrence, but the effect of early endovenous ablation of superficial venous reflux on ulcer healing remains unclear.\nIn a trial conducted at 20 centers in the United Kingdom, we randomly assigned 450 patients with venous leg ulcers to receive compression therapy and undergo early endovenous ablation of superficial venous reflux within 2 weeks after randomization (early-intervention group) or to receive compression therapy alone, with consideration of endovenous ablation deferred until after the ulcer was healed or until 6 months after randomization if the ulcer was unhealed (deferred-intervention group). The primary outcome was the time to ulcer healing. Secondary outcomes were the rate of ulcer healing at 24 weeks, the rate of ulcer recurrence, the length of time free from ulcers (ulcer-free time) during the first year after randomization, and patient-reported health-related quality of life.\nPatient and clinical characteristics at baseline were similar in the two treatment groups. The time to ulcer healing was shorter in the early-intervention group than in the deferred-intervention group; more patients had healed ulcers with early intervention (hazard ratio for ulcer healing, 1.38; 95% confidence interval [CI], 1.13 to 1.68; P=0.001). The median time to ulcer healing was 56 days (95% CI, 49 to 66) in the early-intervention group and 82 days (95% CI, 69 to 92) in the deferred-intervention group. The rate of ulcer healing at 24 weeks was 85.6% in the early-intervention group and 76.3% in the deferred-intervention group. The median ulcer-free time during the first year after trial enrollment was 306 days (interquartile range, 240 to 328) in the early-intervention group and 278 days (interquartile range, 175 to 324) in the deferred-intervention group (P=0.002). The most common procedural complications of endovenous ablation were pain and deep-vein thrombosis.\nEarly endovenous ablation of superficial venous reflux resulted in faster healing of venous leg ulcers and more time free from ulcers than deferred endovenous ablation. (Funded by the National Institute for Health Research Health Technology Assessment Program; EVRA Current Controlled Trials number, ISRCTN02335796 .).', 'title': 'A Randomized Trial of Early Endovenous Ablation in Venous Ulceration.', 'date': '2018-04-25'}, '32928070': {'article_id': '32928070', 'content': 'To investigate whether radiofrequency endovenous ablation (RFA) of saphenous and perforating veins increases venous leg ulcer (VLU) healing rates and prevents ulcer recurrence.\nThis prospective, open-label, randomized, controlled, single-center trial recruited 56 patients with VLU divided into: compression alone (CR, N\u2009=\u200929) and RFA plus compression (RF, N\u2009=\u200927). Primary endpoints were ulcer recurrence rate at 12\u2009months; and ulcer healing rates at 6, 12, and 24\u2009weeks. Secondary endpoints were ulcer healing velocity; and Venous Clinical Severity Score (VCSS).\nRecurrence was lower in the RF group (p\u2009<\u2009.001), as well as mean VCSS after treatment (p\u2009=\u2009.001). There were no significant between-group differences in healing rates. Healing velocity was faster in the RF group (p\u2009=\u20090.049). In the RF group, 2 participants had type 1 endovenous heat-induced thrombosis (EHIT).\nRFA plus compression is an excellent treatment for VLU because of its safety, effectiveness, and impact on ulcer recurrence reduction and clinical outcome.', 'title': 'A randomized clinical trial of the effects of saphenous and perforating veins radiofrequency ablation on venous ulcer healing (VUERT trial).', 'date': '2020-09-16'}}",0.5,Surgery
134,"Is ulcer recurrence higher, lower, or the same when comparing combined endovenous ablation and compression to compression therapy without or with deferred endovenous treatment?",uncertain effect,low,yes,"['32965493', '32928070']",37497816,2023,"{'32965493': {'article_id': '32965493', 'content': ""One-year outcomes from the Early Venous Reflux Ablation (EVRA) randomized trial showed accelerated venous leg ulcer healing and greater ulcer-free time for participants who are treated with early endovenous ablation of lower extremity superficial reflux.\nTo evaluate the clinical and cost-effectiveness of early endovenous ablation of superficial venous reflux in patients with venous leg ulceration.\nBetween October 24, 2013, and September 27, 2016, the EVRA randomized clinical trial enrolled 450 participants (450 legs) with venous leg ulceration of less than 6 months' duration and superficial venous reflux. Initially, 6555 patients were assessed for eligibility, and 6105 were excluded for reasons including ulcer duration greater than 6 months, healed ulcer by the time of randomization, deep venous occlusive disease, and insufficient superficial venous reflux to warrant ablation therapy, among others. A total of 426 of 450 participants (94.7%) from the vascular surgery departments of 20 hospitals in the United Kingdom were included in the analysis for ulcer recurrence. Surgeons, participants, and follow-up assessors were not blinded to the treatment group. Data were analyzed from August 11 to November 4, 2019.\nPatients were randomly assigned to receive compression therapy with early endovenous ablation within 2 weeks of randomization (early intervention, n\u2009=\u2009224) or compression with deferred endovenous treatment of superficial venous reflux (deferred intervention, n\u2009=\u2009226). Endovenous modality and strategy were left to the preference of the treating clinical team.\nThe primary outcome for the extended phase was time to first ulcer recurrence. Secondary outcomes included ulcer recurrence rate and cost-effectiveness.\nThe early-intervention group consisted of 224 participants (mean [SD] age, 67.0 [15.5] years; 127 men [56.7%]; 206 White participants [92%]). The deferred-intervention group consisted of 226 participants (mean [SD] age, 68.9 [14.0] years; 120 men [53.1%]; 208 White participants [92%]). Of the 426 participants whose leg ulcer had healed, 121 (28.4%) experienced at least 1 recurrence during follow-up. There was no clear difference in time to first ulcer recurrence between the 2 groups (hazard ratio, 0.82; 95% CI, 0.57-1.17; P\u2009=\u2009.28). Ulcers recurred at a lower rate of 0.11 per person-year in the early-intervention group compared with 0.16 per person-year in the deferred-intervention group (incidence rate ratio, 0.658; 95% CI, 0.480-0.898; P\u2009=\u2009.003). Time to ulcer healing was shorter in the early-intervention group for primary ulcers (hazard ratio, 1.36; 95% CI, 1.12-1.64; P\u2009=\u2009.002). At 3 years, early intervention was 91.6% likely to be cost-effective at a willingness to pay of £20\u202f000 ($26\u202f283) per quality-adjusted life year and 90.8% likely at a threshold of £35\u202f000 ($45\u202f995) per quality-adjusted life year.\nEarly endovenous ablation of superficial venous reflux was highly likely to be cost-effective over a 3-year horizon compared with deferred intervention. Early intervention accelerated the healing of venous leg ulcers and reduced the overall incidence of ulcer recurrence.\nClinicalTrials.gov identifier: ISRCTN02335796."", 'title': 'Long-term Clinical and Cost-effectiveness of Early Endovenous Ablation in Venous Ulceration: A Randomized Clinical Trial.', 'date': '2020-09-24'}, '32928070': {'article_id': '32928070', 'content': 'To investigate whether radiofrequency endovenous ablation (RFA) of saphenous and perforating veins increases venous leg ulcer (VLU) healing rates and prevents ulcer recurrence.\nThis prospective, open-label, randomized, controlled, single-center trial recruited 56 patients with VLU divided into: compression alone (CR, N\u2009=\u200929) and RFA plus compression (RF, N\u2009=\u200927). Primary endpoints were ulcer recurrence rate at 12\u2009months; and ulcer healing rates at 6, 12, and 24\u2009weeks. Secondary endpoints were ulcer healing velocity; and Venous Clinical Severity Score (VCSS).\nRecurrence was lower in the RF group (p\u2009<\u2009.001), as well as mean VCSS after treatment (p\u2009=\u2009.001). There were no significant between-group differences in healing rates. Healing velocity was faster in the RF group (p\u2009=\u20090.049). In the RF group, 2 participants had type 1 endovenous heat-induced thrombosis (EHIT).\nRFA plus compression is an excellent treatment for VLU because of its safety, effectiveness, and impact on ulcer recurrence reduction and clinical outcome.', 'title': 'A randomized clinical trial of the effects of saphenous and perforating veins radiofrequency ablation on venous ulcer healing (VUERT trial).', 'date': '2020-09-16'}}",0.0,Surgery
135,"Is the risk of stroke higher, lower, or the same when comparing rivaroxaban to placebo?",lower,moderate,yes,['31461239'],34002371,2021,"{'31461239': {'article_id': '31461239', 'content': 'Stroke is often a devastating event among patients with heart failure with reduced ejection (HFrEF). In COMMANDER HF, rivaroxaban 2.5\u2009mg b.i.d. did not reduce the composite of first occurrence of death, stroke, or myocardial infarction compared with placebo in patients with HFrEF, coronary artery disease (CAD), and sinus rhythm. We now examine the incidence, timing, type, severity, and predictors of stroke or a transient ischaemic attack (TIA), and seek to establish the net clinical benefit of treatment with low-dose rivaroxaban.\nIn this double-blind, randomized trial, 5022 patients who had HFrEF(≤40%), elevated natriuretic peptides, CAD, and who were in sinus rhythm were treated with rivaroxaban 2.5\u2009mg b.i.d. or placebo in addition to antiplatelet therapy, after an episode of worsening HF. The primary neurological outcome for this post hoc analysis was time to first event of any stroke or TIA. Over a median follow-up of 20.5 (25th-75th percentiles 20.0-20.9) months, 150 all-cause stroke (127) or TIA (23) events occurred (ischaemic stroke in 82% and haemorrhagic stroke in 11% of stroke events). Overall, 47.5% of first-time strokes were either disabling (16.5%) or fatal (31%). Prior stroke, low body mass index, geographic region, and the CHA2DS2-VASc score were predictors of stroke/TIA. Rivaroxaban significantly reduced the primary neurological endpoint of all-cause stroke or TIA compared with placebo by 32% (1.29 events vs. 1.90 events per 100 patient-years), adjusted for the time from index HF event to randomization and stratified by geographic region (adjusted hazard ratio 0.68, 95% confidence interval 0.49-0.94), with a number needed to treat of 164 patients per year to prevent one stroke/TIA event. The principal safety endpoint of fatal bleeding or bleeding into a critical space, occurred at a similar rate on rivaroxaban and placebo (0.44 events vs. 0.55 events per 100 patient-years).\nPatients with HFrEF and CAD are at risk for stroke or TIA in the period following an episode of worsening heart failure in the absence of atrial fibrillation. Most strokes are of ischaemic origin and nearly half are either disabling or fatal. Rivaroxaban at a dose of 2.5\u2009mg b.i.d. reduced rates of stroke or TIA compared with placebo in this population.\nCOMMANDER HF (A Study to Assess the Effectiveness and Safety of Rivaroxaban in Reducing the Risk of Death, Myocardial Infarction, or Stroke in Participants with Heart Failure and Coronary Artery Disease Following an Episode of Decompensated Heart Failure); ClinicalTrials.gov NCT01877915.', 'title': 'A comprehensive analysis of the effects of rivaroxaban on stroke or transient ischaemic attack in patients with heart failure, coronary artery disease, and sinus rhythm: the COMMANDER HF trial.', 'date': '2019-08-29'}}",1.0,Internal Medicine & Subspecialties
136,"Is the proportion of children wearing spectacles higher, lower, or the same when comparing vision screening with free spectacles to vision screening with spectacles by prescription only?",higher,high,no,"['18156372', '25249453']",29446439,2018,"{'18156372': {'article_id': '18156372', 'content': 'To compare whether free spectacles or only a prescription for spectacles influences wearing rates among Tanzanian students with un/undercorrected refractive error (RE).\nCluster randomised trial.\n37 secondary schools in Dar es Salaam, Tanzania.\nDistance visual acuity was measured in 6,904 year-1 students (90.2% response rate; median age 14 years; range 11-25 years) using a Snellen E-chart. 135 had RE requiring correction.\nSchools were randomly allocated to free spectacles (arm A) or prescription only (arm B).\nSpectacle use at 3 months.\nThe prevalence of un/undercorrected RE was 1.8% (95% CI: 1.5 to 2.2%). At 3 months, 27/58 (47%) students in arm A were wearing spectacles or had them at school compared with 13/50 (26%) in arm B (adjusted OR 2.4, 95% CI 1.0 to 6.7). Free spectacles and myopia were independently associated with spectacle use.\nThe low prevalence of un/undercorrected RE and poor uptake of spectacles, even when provided free, raises doubts about the value of vision-screening programmes in Tanzanian secondary schools. Policy decisions on school vision screening in middle- and low-income countries should take account of the cost-effectiveness as well as competing demands for scarce resources.', 'title': 'Two strategies for correcting refractive errors in school students in Tanzania: randomised comparison, with implications for screening programmes.', 'date': '2007-12-25'}, '25249453': {'article_id': '25249453', 'content': ""To assess the effect of provision of free glasses on academic performance in rural Chinese children with myopia.\nCluster randomized, investigator masked, controlled trial.\n252 primary schools in two prefectures in western China, 2012-13.\n3177 of 19,934 children in fourth and fifth grades (mean age 10.5 years) with visual acuity <6/12 in either eye without glasses correctable to >6/12 with glasses. 3052 (96.0%) completed the study.\nChildren were randomized by school (84 schools per arm) to one of three interventions at the beginning of the school year: prescription for glasses only (control group), vouchers for free glasses at a local facility, or free glasses provided in class.\nSpectacle wear at endline examination and end of year score on a specially designed mathematics test, adjusted for baseline score and expressed in standard deviations.\nAmong 3177 eligible children, 1036 (32.6%) were randomized to control, 988 (31.1%) to vouchers, and 1153 (36.3%) to free glasses in class. All eligible children would benefit from glasses, but only 15% wore them at baseline. At closeout glasses wear was 41% (observed) and 68% (self reported) in the free glasses group, and 26% (observed) and 37% (self reported) in the controls. Effect on test score was 0.11 SD (95% confidence interval 0.01 to 0.21) when the free glasses group was compared with the control group. The adjusted effect of providing free glasses (0.10, 0.002 to 0.19) was greater than parental education (0.03, -0.04 to 0.09) or family wealth (0.01, -0.06 to 0.08). This difference between groups was significant, but was smaller than the prespecified 0.20 SD difference that the study was powered to detect.\nThe provision of free glasses to Chinese children with myopia improves children's performance on mathematics testing to a statistically significant degree, despite imperfect compliance, although the observed difference between groups was smaller than the study was originally designed to detect. Myopia is common and rarely corrected in this setting.Trial Registration Current Controlled Trials ISRCTN03252665."", 'title': ""Effect of providing free glasses on children's educational outcomes in China: cluster randomized controlled trial."", 'date': '2014-09-25'}}",1.0,Pediatrics & Neonatology
137,"Is educational attainment higher, lower, or the same when comparing vision screening with free spectacles to vision screening with spectacles by prescription only?",higher,low,yes,['25249453'],29446439,2018,"{'25249453': {'article_id': '25249453', 'content': ""To assess the effect of provision of free glasses on academic performance in rural Chinese children with myopia.\nCluster randomized, investigator masked, controlled trial.\n252 primary schools in two prefectures in western China, 2012-13.\n3177 of 19,934 children in fourth and fifth grades (mean age 10.5 years) with visual acuity <6/12 in either eye without glasses correctable to >6/12 with glasses. 3052 (96.0%) completed the study.\nChildren were randomized by school (84 schools per arm) to one of three interventions at the beginning of the school year: prescription for glasses only (control group), vouchers for free glasses at a local facility, or free glasses provided in class.\nSpectacle wear at endline examination and end of year score on a specially designed mathematics test, adjusted for baseline score and expressed in standard deviations.\nAmong 3177 eligible children, 1036 (32.6%) were randomized to control, 988 (31.1%) to vouchers, and 1153 (36.3%) to free glasses in class. All eligible children would benefit from glasses, but only 15% wore them at baseline. At closeout glasses wear was 41% (observed) and 68% (self reported) in the free glasses group, and 26% (observed) and 37% (self reported) in the controls. Effect on test score was 0.11 SD (95% confidence interval 0.01 to 0.21) when the free glasses group was compared with the control group. The adjusted effect of providing free glasses (0.10, 0.002 to 0.19) was greater than parental education (0.03, -0.04 to 0.09) or family wealth (0.01, -0.06 to 0.08). This difference between groups was significant, but was smaller than the prespecified 0.20 SD difference that the study was powered to detect.\nThe provision of free glasses to Chinese children with myopia improves children's performance on mathematics testing to a statistically significant degree, despite imperfect compliance, although the observed difference between groups was smaller than the study was originally designed to detect. Myopia is common and rarely corrected in this setting.Trial Registration Current Controlled Trials ISRCTN03252665."", 'title': ""Effect of providing free glasses on children's educational outcomes in China: cluster randomized controlled trial."", 'date': '2014-09-25'}}",0.0,"Public Health, Epidemiology & Health Systems"
138,"Is the risk of major bleeding higher, lower, or the same when comparing aspirin to placebo?",higher,high,no,['30221596'],32352165,2020,"{'30221596': {'article_id': '30221596', 'content': 'Information on the use of aspirin to increase healthy independent life span in older persons is limited. Whether 5 years of daily low-dose aspirin therapy would extend disability-free life in healthy seniors is unclear.\nFrom 2010 through 2014, we enrolled community-dwelling persons in Australia and the United States who were 70 years of age or older (or ≥65 years of age among blacks and Hispanics in the United States) and did not have cardiovascular disease, dementia, or physical disability. Participants were randomly assigned to receive 100 mg per day of enteric-coated aspirin or placebo orally. The primary end point was a composite of death, dementia, or persistent physical disability. Secondary end points reported in this article included the individual components of the primary end point and major hemorrhage.\nA total of 19,114 persons with a median age of 74 years were enrolled, of whom 9525 were randomly assigned to receive aspirin and 9589 to receive placebo. A total of 56.4% of the participants were women, 8.7% were nonwhite, and 11.0% reported previous regular aspirin use. The trial was terminated at a median of 4.7 years of follow-up after a determination was made that there would be no benefit with continued aspirin use with regard to the primary end point. The rate of the composite of death, dementia, or persistent physical disability was 21.5 events per 1000 person-years in the aspirin group and 21.2 per 1000 person-years in the placebo group (hazard ratio, 1.01; 95% confidence interval [CI], 0.92 to 1.11; P=0.79). The rate of adherence to the assigned intervention was 62.1% in the aspirin group and 64.1% in the placebo group in the final year of trial participation. Differences between the aspirin group and the placebo group were not substantial with regard to the secondary individual end points of death from any cause (12.7 events per 1000 person-years in the aspirin group and 11.1 events per 1000 person-years in the placebo group), dementia, or persistent physical disability. The rate of major hemorrhage was higher in the aspirin group than in the placebo group (3.8% vs. 2.8%; hazard ratio, 1.38; 95% CI, 1.18 to 1.62; P<0.001).\nAspirin use in healthy elderly persons did not prolong disability-free survival over a period of 5 years but led to a higher rate of major hemorrhage than placebo. (Funded by the National Institute on Aging and others; ASPREE ClinicalTrials.gov number, NCT01038583 .).', 'title': 'Effect of Aspirin on Disability-free Survival in the Healthy Elderly.', 'date': '2018-09-18'}}",1.0,Family Medicine & Preventive Care
139,"Is the risk of dementia higher, lower, or the same when comparing aspirin to placebo?",no difference,high,no,['30221596'],32352165,2020,"{'30221596': {'article_id': '30221596', 'content': 'Information on the use of aspirin to increase healthy independent life span in older persons is limited. Whether 5 years of daily low-dose aspirin therapy would extend disability-free life in healthy seniors is unclear.\nFrom 2010 through 2014, we enrolled community-dwelling persons in Australia and the United States who were 70 years of age or older (or ≥65 years of age among blacks and Hispanics in the United States) and did not have cardiovascular disease, dementia, or physical disability. Participants were randomly assigned to receive 100 mg per day of enteric-coated aspirin or placebo orally. The primary end point was a composite of death, dementia, or persistent physical disability. Secondary end points reported in this article included the individual components of the primary end point and major hemorrhage.\nA total of 19,114 persons with a median age of 74 years were enrolled, of whom 9525 were randomly assigned to receive aspirin and 9589 to receive placebo. A total of 56.4% of the participants were women, 8.7% were nonwhite, and 11.0% reported previous regular aspirin use. The trial was terminated at a median of 4.7 years of follow-up after a determination was made that there would be no benefit with continued aspirin use with regard to the primary end point. The rate of the composite of death, dementia, or persistent physical disability was 21.5 events per 1000 person-years in the aspirin group and 21.2 per 1000 person-years in the placebo group (hazard ratio, 1.01; 95% confidence interval [CI], 0.92 to 1.11; P=0.79). The rate of adherence to the assigned intervention was 62.1% in the aspirin group and 64.1% in the placebo group in the final year of trial participation. Differences between the aspirin group and the placebo group were not substantial with regard to the secondary individual end points of death from any cause (12.7 events per 1000 person-years in the aspirin group and 11.1 events per 1000 person-years in the placebo group), dementia, or persistent physical disability. The rate of major hemorrhage was higher in the aspirin group than in the placebo group (3.8% vs. 2.8%; hazard ratio, 1.38; 95% CI, 1.18 to 1.62; P<0.001).\nAspirin use in healthy elderly persons did not prolong disability-free survival over a period of 5 years but led to a higher rate of major hemorrhage than placebo. (Funded by the National Institute on Aging and others; ASPREE ClinicalTrials.gov number, NCT01038583 .).', 'title': 'Effect of Aspirin on Disability-free Survival in the Healthy Elderly.', 'date': '2018-09-18'}}",1.0,Psychiatry & Neurology
140,"Is the risk of mortality higher, lower, or the same when comparing aspirin to placebo?",higher,high,no,['30221596'],32352165,2020,"{'30221596': {'article_id': '30221596', 'content': 'Information on the use of aspirin to increase healthy independent life span in older persons is limited. Whether 5 years of daily low-dose aspirin therapy would extend disability-free life in healthy seniors is unclear.\nFrom 2010 through 2014, we enrolled community-dwelling persons in Australia and the United States who were 70 years of age or older (or ≥65 years of age among blacks and Hispanics in the United States) and did not have cardiovascular disease, dementia, or physical disability. Participants were randomly assigned to receive 100 mg per day of enteric-coated aspirin or placebo orally. The primary end point was a composite of death, dementia, or persistent physical disability. Secondary end points reported in this article included the individual components of the primary end point and major hemorrhage.\nA total of 19,114 persons with a median age of 74 years were enrolled, of whom 9525 were randomly assigned to receive aspirin and 9589 to receive placebo. A total of 56.4% of the participants were women, 8.7% were nonwhite, and 11.0% reported previous regular aspirin use. The trial was terminated at a median of 4.7 years of follow-up after a determination was made that there would be no benefit with continued aspirin use with regard to the primary end point. The rate of the composite of death, dementia, or persistent physical disability was 21.5 events per 1000 person-years in the aspirin group and 21.2 per 1000 person-years in the placebo group (hazard ratio, 1.01; 95% confidence interval [CI], 0.92 to 1.11; P=0.79). The rate of adherence to the assigned intervention was 62.1% in the aspirin group and 64.1% in the placebo group in the final year of trial participation. Differences between the aspirin group and the placebo group were not substantial with regard to the secondary individual end points of death from any cause (12.7 events per 1000 person-years in the aspirin group and 11.1 events per 1000 person-years in the placebo group), dementia, or persistent physical disability. The rate of major hemorrhage was higher in the aspirin group than in the placebo group (3.8% vs. 2.8%; hazard ratio, 1.38; 95% CI, 1.18 to 1.62; P<0.001).\nAspirin use in healthy elderly persons did not prolong disability-free survival over a period of 5 years but led to a higher rate of major hemorrhage than placebo. (Funded by the National Institute on Aging and others; ASPREE ClinicalTrials.gov number, NCT01038583 .).', 'title': 'Effect of Aspirin on Disability-free Survival in the Healthy Elderly.', 'date': '2018-09-18'}}",0.0,Family Medicine & Preventive Care
141,"Is the incidence of myocardial infarction higher, lower, or the same when comparing NSAIDs to placebo?",no difference,moderate,yes,['17111043'],32352165,2020,"{'17111043': {'article_id': '17111043', 'content': ""The Alzheimer's Disease Anti-inflammatory Prevention Trial (ADAPT) was designed to evaluate the conventional NSAID naproxen sodium and the selective COX-2 inhibitor celecoxib for primary prevention of Alzheimer's dementia (AD). On 17 December 2004, after the Adenoma Prevention with Celecoxib (APC) trial reported increased cardiovascular risks with celecoxib, the ADAPT Steering Committee suspended treatment and enrollment. This paper reports on cardiovascular and cerebrovascular events in ADAPT.\nADAPT is a randomized, placebo-controlled, parallel chemoprevention trial with 1-46 mo of follow-up.\nThe trial was conducted at six field sites in the United States: Baltimore, Maryland; Boston, Massachusetts; Rochester, New York; Seattle, Washington; Sun City, Arizona; and Tampa, Florida.\nThe 2,528 participants were aged 70 y and older with a family history of AD.\nStudy treatments were celecoxib (200 mg b.i.d.), naproxen sodium (220 mg b.i.d.), and placebo.\nOutcome measures were deaths, along with nonfatal myocardial infarction (MI), stroke, congestive heart failure (CHF), transient ischemic attack (TIA), and antihypertensive treatment recorded from structured interviews at scheduled intervals. Cox proportional hazards regression was used to analyze these events individually and in several composites.\nCounts (with 3-y incidence) of participants who experienced cardiovascular or cerebrovascular death, MI, stroke, CHF, or TIA in the celecoxib-, naproxen-, and placebo-treated groups were 28/717 (5.54%), 40/713 (8.25%), and 37/1070 (5.68%), respectively. This yielded a hazard ratio (95% confidence interval [CI]) for celecoxib of 1.10 (0.67-1.79) and for naproxen of 1.63 (1.04-2.55). Antihypertensive treatment was initiated in 160/440 (47.43%), 147/427 (45.00%), and 164/644 (34.08%). This yielded hazard ratios (CIs) of 1.56 for celecoxib (1.26-1.94) and 1.40 for naproxen (1.12-1.75).\nFor celecoxib, ADAPT data do not show the same level of risk as those of the APC trial. The data for naproxen, although not definitive, are suggestive of increased cardiovascular and cerebrovascular risk."", 'title': ""Cardiovascular and cerebrovascular events in the randomized, controlled Alzheimer's Disease Anti-Inflammatory Prevention Trial (ADAPT)."", 'date': '2006-11-18'}}",0.0,Internal Medicine & Subspecialties
142,"Is the risk of mortality higher, lower, or the same when comparing NSAIDs to placebo?",no difference,moderate,yes,['17111043'],32352165,2020,"{'17111043': {'article_id': '17111043', 'content': ""The Alzheimer's Disease Anti-inflammatory Prevention Trial (ADAPT) was designed to evaluate the conventional NSAID naproxen sodium and the selective COX-2 inhibitor celecoxib for primary prevention of Alzheimer's dementia (AD). On 17 December 2004, after the Adenoma Prevention with Celecoxib (APC) trial reported increased cardiovascular risks with celecoxib, the ADAPT Steering Committee suspended treatment and enrollment. This paper reports on cardiovascular and cerebrovascular events in ADAPT.\nADAPT is a randomized, placebo-controlled, parallel chemoprevention trial with 1-46 mo of follow-up.\nThe trial was conducted at six field sites in the United States: Baltimore, Maryland; Boston, Massachusetts; Rochester, New York; Seattle, Washington; Sun City, Arizona; and Tampa, Florida.\nThe 2,528 participants were aged 70 y and older with a family history of AD.\nStudy treatments were celecoxib (200 mg b.i.d.), naproxen sodium (220 mg b.i.d.), and placebo.\nOutcome measures were deaths, along with nonfatal myocardial infarction (MI), stroke, congestive heart failure (CHF), transient ischemic attack (TIA), and antihypertensive treatment recorded from structured interviews at scheduled intervals. Cox proportional hazards regression was used to analyze these events individually and in several composites.\nCounts (with 3-y incidence) of participants who experienced cardiovascular or cerebrovascular death, MI, stroke, CHF, or TIA in the celecoxib-, naproxen-, and placebo-treated groups were 28/717 (5.54%), 40/713 (8.25%), and 37/1070 (5.68%), respectively. This yielded a hazard ratio (95% confidence interval [CI]) for celecoxib of 1.10 (0.67-1.79) and for naproxen of 1.63 (1.04-2.55). Antihypertensive treatment was initiated in 160/440 (47.43%), 147/427 (45.00%), and 164/644 (34.08%). This yielded hazard ratios (CIs) of 1.56 for celecoxib (1.26-1.94) and 1.40 for naproxen (1.12-1.75).\nFor celecoxib, ADAPT data do not show the same level of risk as those of the APC trial. The data for naproxen, although not definitive, are suggestive of increased cardiovascular and cerebrovascular risk."", 'title': ""Cardiovascular and cerebrovascular events in the randomized, controlled Alzheimer's Disease Anti-Inflammatory Prevention Trial (ADAPT)."", 'date': '2006-11-18'}}",1.0,Internal Medicine & Subspecialties
143,"Is the incidence of Alzheimer's disease higher, lower, or the same when comparing NSAIDs to placebo?",no difference,moderate,yes,['17111043'],32352165,2020,"{'17111043': {'article_id': '17111043', 'content': ""The Alzheimer's Disease Anti-inflammatory Prevention Trial (ADAPT) was designed to evaluate the conventional NSAID naproxen sodium and the selective COX-2 inhibitor celecoxib for primary prevention of Alzheimer's dementia (AD). On 17 December 2004, after the Adenoma Prevention with Celecoxib (APC) trial reported increased cardiovascular risks with celecoxib, the ADAPT Steering Committee suspended treatment and enrollment. This paper reports on cardiovascular and cerebrovascular events in ADAPT.\nADAPT is a randomized, placebo-controlled, parallel chemoprevention trial with 1-46 mo of follow-up.\nThe trial was conducted at six field sites in the United States: Baltimore, Maryland; Boston, Massachusetts; Rochester, New York; Seattle, Washington; Sun City, Arizona; and Tampa, Florida.\nThe 2,528 participants were aged 70 y and older with a family history of AD.\nStudy treatments were celecoxib (200 mg b.i.d.), naproxen sodium (220 mg b.i.d.), and placebo.\nOutcome measures were deaths, along with nonfatal myocardial infarction (MI), stroke, congestive heart failure (CHF), transient ischemic attack (TIA), and antihypertensive treatment recorded from structured interviews at scheduled intervals. Cox proportional hazards regression was used to analyze these events individually and in several composites.\nCounts (with 3-y incidence) of participants who experienced cardiovascular or cerebrovascular death, MI, stroke, CHF, or TIA in the celecoxib-, naproxen-, and placebo-treated groups were 28/717 (5.54%), 40/713 (8.25%), and 37/1070 (5.68%), respectively. This yielded a hazard ratio (95% confidence interval [CI]) for celecoxib of 1.10 (0.67-1.79) and for naproxen of 1.63 (1.04-2.55). Antihypertensive treatment was initiated in 160/440 (47.43%), 147/427 (45.00%), and 164/644 (34.08%). This yielded hazard ratios (CIs) of 1.56 for celecoxib (1.26-1.94) and 1.40 for naproxen (1.12-1.75).\nFor celecoxib, ADAPT data do not show the same level of risk as those of the APC trial. The data for naproxen, although not definitive, are suggestive of increased cardiovascular and cerebrovascular risk."", 'title': ""Cardiovascular and cerebrovascular events in the randomized, controlled Alzheimer's Disease Anti-Inflammatory Prevention Trial (ADAPT)."", 'date': '2006-11-18'}}",0.0,Psychiatry & Neurology
144,"Is the risk of stroke higher, lower, or the same when comparing NSAIDs to placebo?",no difference,moderate,yes,['17111043'],32352165,2020,"{'17111043': {'article_id': '17111043', 'content': ""The Alzheimer's Disease Anti-inflammatory Prevention Trial (ADAPT) was designed to evaluate the conventional NSAID naproxen sodium and the selective COX-2 inhibitor celecoxib for primary prevention of Alzheimer's dementia (AD). On 17 December 2004, after the Adenoma Prevention with Celecoxib (APC) trial reported increased cardiovascular risks with celecoxib, the ADAPT Steering Committee suspended treatment and enrollment. This paper reports on cardiovascular and cerebrovascular events in ADAPT.\nADAPT is a randomized, placebo-controlled, parallel chemoprevention trial with 1-46 mo of follow-up.\nThe trial was conducted at six field sites in the United States: Baltimore, Maryland; Boston, Massachusetts; Rochester, New York; Seattle, Washington; Sun City, Arizona; and Tampa, Florida.\nThe 2,528 participants were aged 70 y and older with a family history of AD.\nStudy treatments were celecoxib (200 mg b.i.d.), naproxen sodium (220 mg b.i.d.), and placebo.\nOutcome measures were deaths, along with nonfatal myocardial infarction (MI), stroke, congestive heart failure (CHF), transient ischemic attack (TIA), and antihypertensive treatment recorded from structured interviews at scheduled intervals. Cox proportional hazards regression was used to analyze these events individually and in several composites.\nCounts (with 3-y incidence) of participants who experienced cardiovascular or cerebrovascular death, MI, stroke, CHF, or TIA in the celecoxib-, naproxen-, and placebo-treated groups were 28/717 (5.54%), 40/713 (8.25%), and 37/1070 (5.68%), respectively. This yielded a hazard ratio (95% confidence interval [CI]) for celecoxib of 1.10 (0.67-1.79) and for naproxen of 1.63 (1.04-2.55). Antihypertensive treatment was initiated in 160/440 (47.43%), 147/427 (45.00%), and 164/644 (34.08%). This yielded hazard ratios (CIs) of 1.56 for celecoxib (1.26-1.94) and 1.40 for naproxen (1.12-1.75).\nFor celecoxib, ADAPT data do not show the same level of risk as those of the APC trial. The data for naproxen, although not definitive, are suggestive of increased cardiovascular and cerebrovascular risk."", 'title': ""Cardiovascular and cerebrovascular events in the randomized, controlled Alzheimer's Disease Anti-Inflammatory Prevention Trial (ADAPT)."", 'date': '2006-11-18'}}",0.0,Psychiatry & Neurology
145,"Is the incidence of Alzheimer's disease in patients with mild cognitive impairment higher, lower, or the same when comparing rofecoxib to placebo?",higher,moderate,no,['15742005'],32352165,2020,"{'15742005': {'article_id': '15742005', 'content': ""Inflammatory mechanisms have been implicated in Alzheimer's disease (AD) and might be mediated via the COX-2 enzyme. Previous studies with the selective COX-2 inhibitors, rofecoxib and celecoxib, have shown that they do not alter the progression of AD. We conducted a double-blind study to investigate whether rofecoxib could delay a diagnosis of AD in patients with mild cognitive impairment (MCI), a group with an expected annual AD diagnosis rate of 10-15%. MCI patients > or =65 years were randomized to rofecoxib 25 mg (N=725) or placebo (N=732) daily for up to 4 years. The primary end point was the percentage of patients with a clinical diagnosis of AD. The estimated annual AD diagnosis rate was lower than the anticipated 10-15%: 6.4% in the rofecoxib group vs 4.5% in the placebo group (rofecoxib : placebo hazard ratio=1.46 (95% CI: 1.09, 1.94), p=0.011). Analyses of secondary end points, including measures of cognition (eg the cognitive subscale of the AD Assessment Scale (ADAS-Cog)) and global function (eg the Clinical Dementia Rating (CDR)), did not demonstrate differences between treatment groups. There was also no consistent evidence that rofecoxib differed from placebo in post hoc analyses comparing ADAS-Cog and CDR-sum of boxes scores in overlapping subgroups of patients who had Mini Mental State Exam scores of 24-26 in the present MCI study and in a previous AD treatment study with a similar design. The results from this MCI study did not support the hypothesis that rofecoxib would delay a diagnosis of AD. In conjunction with the lack of effects observed in previous AD studies, the findings suggest that inhibition of COX-2 is not a useful therapeutic approach in AD."", 'title': 'A randomized, double-blind, study of rofecoxib in patients with mild cognitive impairment.', 'date': '2005-03-03'}}",1.0,Psychiatry & Neurology
146,"Is the overall risk of developing breast cancer higher, lower, or the same when comparing tamoxifen to placebo?",lower,moderate,no,"['16288118', '25497694']",31032883,2019,"{'16288118': {'article_id': '16288118', 'content': 'Initial findings from the National Surgical Adjuvant Breast and Bowel Project Breast Cancer Prevention Trial (P-1) demonstrated that tamoxifen reduced the risk of estrogen receptor-positive tumors and osteoporotic fractures in women at increased risk for breast cancer. Side effects of varying clinical significance were observed. The trial was unblinded because of the positive results, and follow-up continued. This report updates our initial findings.\nWomen (n = 13,388) were randomly assigned to receive placebo or tamoxifen for 5 years. Rates of breast cancer and other events were compared by the use of risk ratios (RRs) and 95% confidence intervals (CIs). Estimates of the net benefit from 5 years of tamoxifen therapy were compared by age, race, and categories of predicted breast cancer risk. Statistical tests were two-sided.\nAfter 7 years of follow-up, the cumulative rate of invasive breast cancer was reduced from 42.5 per 1000 women in the placebo group to 24.8 per 1000 women in the tamoxifen group (RR = 0.57, 95% CI = 0.46 to 0.70) and the cumulative rate of noninvasive breast cancer was reduced from 15.8 per 1000 women in the placebo group to 10.2 per 1000 women in the tamoxifen group (RR = 0.63, 95% CI = 0.45 to 0.89). These reductions were similar to those seen in the initial report. Tamoxifen led to a 32% reduction in osteoporotic fractures (RR = 0.68, 95% CI = 0.51 to 0.92). Relative risks of stroke, deep-vein thrombosis, and cataracts (which increased with tamoxifen) and of ischemic heart disease and death (which were not changed with tamoxifen) were also similar to those initially reported. Risks of pulmonary embolism were approximately 11% lower than in the original report, and risks of endometrial cancer were about 29% higher, but these differences were not statistically significant. The net benefit achieved with tamoxifen varied according to age, race, and level of breast cancer risk.\nDespite the potential bias caused by the unblinding of the P-1 trial, the magnitudes of all beneficial and undesirable treatment effects of tamoxifen were similar to those initially reported, with notable reductions in breast cancer and increased risks of thromboembolic events and endometrial cancer. Readily identifiable subsets of individuals comprising 2.5 million women could derive a net benefit from the drug.', 'title': 'Tamoxifen for the prevention of breast cancer: current status of the National Surgical Adjuvant Breast and Bowel Project P-1 study.', 'date': '2005-11-17'}, '25497694': {'article_id': '25497694', 'content': 'Four previously published randomised clinical trials have shown that tamoxifen can reduce the risk of breast cancer in healthy women at increased risk of breast cancer in the first 10 years of follow-up. We report the long-term follow-up of the IBIS-I trial, in which the participants and investigators remain largely masked to treatment allocation.\nIn the IBIS-I randomised controlled trial, premenopausal and postmenopausal women 35-70 years of age deemed to be at an increased risk of developing breast cancer were randomly assigned (1:1) to receive oral tamoxifen 20 mg daily or matching placebo for 5 years. Patients were randomly assigned to the two treatment groups by telephone or fax according to a block randomisation schedule (permuted block sizes of six or ten). Patients and investigators were masked to treatment assignment by use of central randomisation and coded drug supply. The primary endpoint was the occurrence of breast cancer (invasive breast cancer and ductal carcinoma in situ), analysed by intention to treat. Cox proportional hazard models were used to assess breast cancer occurrence and mortality. The trial is closed to recruitment and active treatment is completed, but long-term follow-up is ongoing. This trial is registered with controlledtrials.com, number ISRCTN91879928.\nBetween April 14, 1992, and March 30, 2001, 7154 eligible women recruited from genetics clinics and breast care clinics in eight countries were enrolled into the IBIS-I trial and were randomly allocated to the two treatment groups: 3579 to tamoxifen and 3575 to placebo. After a median follow up of 16.0 years (IQR 14.1-17.6), 601 breast cancers have been reported (251 [7.0%] in 3579 patients in the tamoxifen group vs 350 [9.8%] in 3575 women in the placebo group; hazard ratio [HR] 0.71 [95% CI 0.60-0.83], p<0.0001). The risk of developing breast cancer was similar between years 0-10 (226 [6.3%] in 3575 women in the placebo group vs 163 [4.6%] in 3579 women in the tamoxifen group; hazard ratio [HR] 0.72 [95% CI 0.59-0.88], p=0.001) and after 10 years (124 [3.8%] in 3295 women vs 88 [2.6%] in 3343, respectively; HR 0.69 [0.53-0.91], p=0.009). The greatest reduction in risk was seen in invasive oestrogen receptor-positive breast cancer (HR 0.66 [95% CI 0.54-0.81], p<0.0001) and ductal carcinoma in situ (0.65 [0.43-1.00], p=0.05), but no effect was noted for invasive oestrogen receptor-negative breast cancer (HR 1.05 [95% CI 0.71-1.57], p=0.8).\nThese results show that tamoxifen offers a very long period of protection after treatment cessation, and thus substantially improves the benefit-to-harm ratio of the drug for breast cancer prevention.\nCancer Research UK (UK) and the National Health and Medical Research Council (Australia).', 'title': 'Tamoxifen for prevention of breast cancer: extended long-term follow-up of the IBIS-I breast cancer prevention trial.', 'date': '2014-12-17'}}",1.0,Oncology & Hematology
147,"Is the risk of developing invasive breast cancer higher, lower, or the same when comparing tamoxifen to placebo?",lower,moderate,no,"['16288118', '25497694', '17312305']",31032883,2019,"{'16288118': {'article_id': '16288118', 'content': 'Initial findings from the National Surgical Adjuvant Breast and Bowel Project Breast Cancer Prevention Trial (P-1) demonstrated that tamoxifen reduced the risk of estrogen receptor-positive tumors and osteoporotic fractures in women at increased risk for breast cancer. Side effects of varying clinical significance were observed. The trial was unblinded because of the positive results, and follow-up continued. This report updates our initial findings.\nWomen (n = 13,388) were randomly assigned to receive placebo or tamoxifen for 5 years. Rates of breast cancer and other events were compared by the use of risk ratios (RRs) and 95% confidence intervals (CIs). Estimates of the net benefit from 5 years of tamoxifen therapy were compared by age, race, and categories of predicted breast cancer risk. Statistical tests were two-sided.\nAfter 7 years of follow-up, the cumulative rate of invasive breast cancer was reduced from 42.5 per 1000 women in the placebo group to 24.8 per 1000 women in the tamoxifen group (RR = 0.57, 95% CI = 0.46 to 0.70) and the cumulative rate of noninvasive breast cancer was reduced from 15.8 per 1000 women in the placebo group to 10.2 per 1000 women in the tamoxifen group (RR = 0.63, 95% CI = 0.45 to 0.89). These reductions were similar to those seen in the initial report. Tamoxifen led to a 32% reduction in osteoporotic fractures (RR = 0.68, 95% CI = 0.51 to 0.92). Relative risks of stroke, deep-vein thrombosis, and cataracts (which increased with tamoxifen) and of ischemic heart disease and death (which were not changed with tamoxifen) were also similar to those initially reported. Risks of pulmonary embolism were approximately 11% lower than in the original report, and risks of endometrial cancer were about 29% higher, but these differences were not statistically significant. The net benefit achieved with tamoxifen varied according to age, race, and level of breast cancer risk.\nDespite the potential bias caused by the unblinding of the P-1 trial, the magnitudes of all beneficial and undesirable treatment effects of tamoxifen were similar to those initially reported, with notable reductions in breast cancer and increased risks of thromboembolic events and endometrial cancer. Readily identifiable subsets of individuals comprising 2.5 million women could derive a net benefit from the drug.', 'title': 'Tamoxifen for the prevention of breast cancer: current status of the National Surgical Adjuvant Breast and Bowel Project P-1 study.', 'date': '2005-11-17'}, '25497694': {'article_id': '25497694', 'content': 'Four previously published randomised clinical trials have shown that tamoxifen can reduce the risk of breast cancer in healthy women at increased risk of breast cancer in the first 10 years of follow-up. We report the long-term follow-up of the IBIS-I trial, in which the participants and investigators remain largely masked to treatment allocation.\nIn the IBIS-I randomised controlled trial, premenopausal and postmenopausal women 35-70 years of age deemed to be at an increased risk of developing breast cancer were randomly assigned (1:1) to receive oral tamoxifen 20 mg daily or matching placebo for 5 years. Patients were randomly assigned to the two treatment groups by telephone or fax according to a block randomisation schedule (permuted block sizes of six or ten). Patients and investigators were masked to treatment assignment by use of central randomisation and coded drug supply. The primary endpoint was the occurrence of breast cancer (invasive breast cancer and ductal carcinoma in situ), analysed by intention to treat. Cox proportional hazard models were used to assess breast cancer occurrence and mortality. The trial is closed to recruitment and active treatment is completed, but long-term follow-up is ongoing. This trial is registered with controlledtrials.com, number ISRCTN91879928.\nBetween April 14, 1992, and March 30, 2001, 7154 eligible women recruited from genetics clinics and breast care clinics in eight countries were enrolled into the IBIS-I trial and were randomly allocated to the two treatment groups: 3579 to tamoxifen and 3575 to placebo. After a median follow up of 16.0 years (IQR 14.1-17.6), 601 breast cancers have been reported (251 [7.0%] in 3579 patients in the tamoxifen group vs 350 [9.8%] in 3575 women in the placebo group; hazard ratio [HR] 0.71 [95% CI 0.60-0.83], p<0.0001). The risk of developing breast cancer was similar between years 0-10 (226 [6.3%] in 3575 women in the placebo group vs 163 [4.6%] in 3579 women in the tamoxifen group; hazard ratio [HR] 0.72 [95% CI 0.59-0.88], p=0.001) and after 10 years (124 [3.8%] in 3295 women vs 88 [2.6%] in 3343, respectively; HR 0.69 [0.53-0.91], p=0.009). The greatest reduction in risk was seen in invasive oestrogen receptor-positive breast cancer (HR 0.66 [95% CI 0.54-0.81], p<0.0001) and ductal carcinoma in situ (0.65 [0.43-1.00], p=0.05), but no effect was noted for invasive oestrogen receptor-negative breast cancer (HR 1.05 [95% CI 0.71-1.57], p=0.8).\nThese results show that tamoxifen offers a very long period of protection after treatment cessation, and thus substantially improves the benefit-to-harm ratio of the drug for breast cancer prevention.\nCancer Research UK (UK) and the National Health and Medical Research Council (Australia).', 'title': 'Tamoxifen for prevention of breast cancer: extended long-term follow-up of the IBIS-I breast cancer prevention trial.', 'date': '2014-12-17'}, '17312305': {'article_id': '17312305', 'content': 'Several clinical trials have reported an early reduction in breast cancer incidence in healthy women using tamoxifen to reduce their risk of breast cancer but have not reported longer follow-up data for the evaluation of breast cancer prevention. We report the blinded 20-year follow-up (median follow-up = 13 years) of the Royal Marsden trial to identify any long-term prevention of breast cancer associated with tamoxifen treatment.\nWe randomly assigned 2494 healthy women to oral tamoxifen (20 mg/day) or placebo for 8 years. The primary outcome was occurrence of invasive breast cancer. A secondary planned analysis of estrogen receptor (ER)-positive invasive breast cancer was also done. Survival was assessed by use of a Cox proportional hazards model in both univariate and multivariable analyses. The durability of the treatment effect was assessed by use of a Cox regression analysis. All statistical tests were two-sided.\nAmong the 2471 eligible participants (1238 participants in the tamoxifen arm and 1233 participants in the placebo arm), 186 developed invasive breast cancer (82 on tamoxifen and 104 on placebo; hazard ratio [HR] = 0.78, 95% confidence interval [CI] = 0.58 to 1.04; P = .1). Of these 186 cancers, 139 were ER positive (53 on tamoxifen and 86 on placebo; HR = 0.61, 95% CI = 0.43 to 0.86; P = .005). The risk of ER-positive breast cancer was not statistically significantly lower in the tamoxifen arm than in the placebo arm during the 8-year treatment period (30 cancers in the tamoxifen arm and 39 in the placebo arm; HR = 0.77, 95% CI = 0.48 to 1.23; P = .3) but was statistically significantly lower in the posttreatment period (23 in the tamoxifen arm and 47 in the placebo arm; HR = 0.48, 95% CI = 0.29 to 0.79; P = .004). Fifty-four participants in each arm have died from any cause (HR = 0.99, 95% CI = 0.68 to 1.44; P = .95). The adverse event profiles for both arms were similar to those previously reported and occurred predominantly during the treatment period.\nA statistically significant reduction in the incidence of ER-positive breast cancer was observed in the tamoxifen arm that occurred predominantly during the post treatment follow-up, indicating long-term prevention of estrogen-dependent breast cancer by tamoxifen.', 'title': 'Twenty-year follow-up of the Royal Marsden randomized, double-blinded tamoxifen breast cancer prevention trial.', 'date': '2007-02-22'}}",0.666666667,Oncology & Hematology
148,"Is the overall risk of breast cancer higher, lower, or the same when comparing aromatase inhibitors (AIs) to placebo?",lower,high,no,"['21639806', '24333009']",31032883,2019,"{'21639806': {'article_id': '21639806', 'content': 'Tamoxifen and raloxifene have limited patient acceptance for primary prevention of breast cancer. Aromatase inhibitors prevent more contralateral breast cancers and cause fewer side effects than tamoxifen in patients with early-stage breast cancer.\nIn a randomized, placebo-controlled, double-blind trial of exemestane designed to detect a 65% relative reduction in invasive breast cancer, eligible postmenopausal women 35 years of age or older had at least one of the following risk factors: 60 years of age or older; Gail 5-year risk score greater than 1.66% (chances in 100 of invasive breast cancer developing within 5 years); prior atypical ductal or lobular hyperplasia or lobular carcinoma in situ; or ductal carcinoma in situ with mastectomy. Toxic effects and health-related and menopause-specific qualities of life were measured.\nA total of 4560 women for whom the median age was 62.5 years and the median Gail risk score was 2.3% were randomly assigned to either exemestane or placebo. At a median follow-up of 35 months, 11 invasive breast cancers were detected in those given exemestane and in 32 of those given placebo, with a 65% relative reduction in the annual incidence of invasive breast cancer (0.19% vs. 0.55%; hazard ratio, 0.35; 95% confidence interval [CI], 0.18 to 0.70; P=0.002). The annual incidence of invasive plus noninvasive (ductal carcinoma in situ) breast cancers was 0.35% on exemestane and 0.77% on placebo (hazard ratio, 0.47; 95% CI, 0.27 to 0.79; P=0.004). Adverse events occurred in 88% of the exemestane group and 85% of the placebo group (P=0.003), with no significant differences between the two groups in terms of skeletal fractures, cardiovascular events, other cancers, or treatment-related deaths. Minimal quality-of-life differences were observed.\nExemestane significantly reduced invasive breast cancers in postmenopausal women who were at moderately increased risk for breast cancer. During a median follow-up period of 3 years, exemestane was associated with no serious toxic effects and only minimal changes in health-related quality of life. (Funded by Pfizer and others; NCIC CTG MAP.3 ClinicalTrials.gov number, NCT00083174.).', 'title': 'Exemestane for breast-cancer prevention in postmenopausal women.', 'date': '2011-06-07'}, '24333009': {'article_id': '24333009', 'content': 'Aromatase inhibitors effectively prevent breast cancer recurrence and development of new contralateral tumours in postmenopausal women. We assessed the efficacy and safety of the aromatase inhibitor anastrozole for prevention of breast cancer in postmenopausal women who are at high risk of the disease.\nBetween Feb 2, 2003, and Jan 31, 2012, we recruited postmenopausal women aged 40-70 years from 18 countries into an international, double-blind, randomised placebo-controlled trial. To be eligible, women had to be at increased risk of breast cancer (judged on the basis of specific criteria). Eligible women were randomly assigned (1:1) by central computer allocation to receive 1 mg oral anastrozole or matching placebo every day for 5 years. Randomisation was stratified by country and was done with blocks (size six, eight, or ten). All trial personnel, participants, and clinicians were masked to treatment allocation; only the trial statistician was unmasked. The primary endpoint was histologically confirmed breast cancer (invasive cancers or non-invasive ductal carcinoma in situ). Analyses were done by intention to treat. This trial is registered, number ISRCTN31488319.\n1920 women were randomly assigned to receive anastrozole and 1944 to placebo. After a median follow-up of 5·0 years (IQR 3·0-7·1), 40 women in the anastrozole group (2%) and 85 in the placebo group (4%) had developed breast cancer (hazard ratio 0·47, 95% CI 0·32-0·68, p<0·0001). The predicted cumulative incidence of all breast cancers after 7 years was 5·6% in the placebo group and 2·8% in the anastrozole group. 18 deaths were reported in the anastrozole group and 17 in the placebo group, and no specific causes were more common in one group than the other (p=0·836).\nAnastrozole effectively reduces incidence of breast cancer in high-risk postmenopausal women. This finding, along with the fact that most of the side-effects associated with oestrogen deprivation were not attributable to treatment, provides support for the use of anastrozole in postmenopausal women at high risk of breast cancer.\nCancer Research UK, the National Health and Medical Research Council Australia, Sanofi-Aventis, and AstraZeneca.', 'title': 'Anastrozole for prevention of breast cancer in high-risk postmenopausal women (IBIS-II): an international, double-blind, randomised placebo-controlled trial.', 'date': '2013-12-18'}}",1.0,Oncology & Hematology
149,"Is the overall risk of developing breast cancer higher, lower, or the same when comparing raloxifene to tamoxifen?",higher,moderate,no,['20404000'],31032883,2019,"{'20404000': {'article_id': '20404000', 'content': 'The selective estrogen-receptor modulator (SERM) tamoxifen became the first U.S. Food and Drug Administration (FDA)-approved agent for reducing breast cancer risk but did not gain wide acceptance for prevention, largely because it increased endometrial cancer and thromboembolic events. The FDA approved the SERM raloxifene for breast cancer risk reduction following its demonstrated effectiveness in preventing invasive breast cancer in the Study of Tamoxifen and Raloxifene (STAR). Raloxifene caused less toxicity (versus tamoxifen), including reduced thromboembolic events and endometrial cancer. In this report, we present an updated analysis with an 81-month median follow-up. STAR women were randomly assigned to receive either tamoxifen (20 mg/d) or raloxifene (60 mg/d) for 5 years. The risk ratio (RR; raloxifene:tamoxifen) for invasive breast cancer was 1.24 (95% confidence interval [CI], 1.05-1.47) and for noninvasive disease, 1.22 (95% CI, 0.95-1.59). Compared with initial results, the RRs widened for invasive and narrowed for noninvasive breast cancer. Toxicity RRs (raloxifene:tamoxifen) were 0.55 (95% CI, 0.36-0.83; P = 0.003) for endometrial cancer (this difference was not significant in the initial results), 0.19 (95% CI, 0.12-0.29) for uterine hyperplasia, and 0.75 (95% CI, 0.60-0.93) for thromboembolic events. There were no significant mortality differences. Long-term raloxifene retained 76% of the effectiveness of tamoxifen in preventing invasive disease and grew closer over time to tamoxifen in preventing noninvasive disease, with far less toxicity (e.g., highly significantly less endometrial cancer). These results have important public health implications and clarify that both raloxifene and tamoxifen are good preventive choices for postmenopausal women with elevated risk for breast cancer.', 'title': 'Update of the National Surgical Adjuvant Breast and Bowel Project Study of Tamoxifen and Raloxifene (STAR) P-2 Trial: Preventing breast cancer.', 'date': '2010-04-21'}}",1.0,Oncology & Hematology
150,"Is the risk of developing invasive breast cancer higher, lower, or the same when comparing raloxifene to tamoxifen?",higher,moderate,no,['20404000'],31032883,2019,"{'20404000': {'article_id': '20404000', 'content': 'The selective estrogen-receptor modulator (SERM) tamoxifen became the first U.S. Food and Drug Administration (FDA)-approved agent for reducing breast cancer risk but did not gain wide acceptance for prevention, largely because it increased endometrial cancer and thromboembolic events. The FDA approved the SERM raloxifene for breast cancer risk reduction following its demonstrated effectiveness in preventing invasive breast cancer in the Study of Tamoxifen and Raloxifene (STAR). Raloxifene caused less toxicity (versus tamoxifen), including reduced thromboembolic events and endometrial cancer. In this report, we present an updated analysis with an 81-month median follow-up. STAR women were randomly assigned to receive either tamoxifen (20 mg/d) or raloxifene (60 mg/d) for 5 years. The risk ratio (RR; raloxifene:tamoxifen) for invasive breast cancer was 1.24 (95% confidence interval [CI], 1.05-1.47) and for noninvasive disease, 1.22 (95% CI, 0.95-1.59). Compared with initial results, the RRs widened for invasive and narrowed for noninvasive breast cancer. Toxicity RRs (raloxifene:tamoxifen) were 0.55 (95% CI, 0.36-0.83; P = 0.003) for endometrial cancer (this difference was not significant in the initial results), 0.19 (95% CI, 0.12-0.29) for uterine hyperplasia, and 0.75 (95% CI, 0.60-0.93) for thromboembolic events. There were no significant mortality differences. Long-term raloxifene retained 76% of the effectiveness of tamoxifen in preventing invasive disease and grew closer over time to tamoxifen in preventing noninvasive disease, with far less toxicity (e.g., highly significantly less endometrial cancer). These results have important public health implications and clarify that both raloxifene and tamoxifen are good preventive choices for postmenopausal women with elevated risk for breast cancer.', 'title': 'Update of the National Surgical Adjuvant Breast and Bowel Project Study of Tamoxifen and Raloxifene (STAR) P-2 Trial: Preventing breast cancer.', 'date': '2010-04-21'}}",1.0,Oncology & Hematology
151,"Is the likelihood of good functional outcome higher, lower, or the same when comparing endovascular thrombectomy to control?",higher,high,no,"['29129157', '29364767']",34850380,2021,"{'29129157': {'article_id': '29129157', 'content': 'The effect of endovascular thrombectomy that is performed more than 6 hours after the onset of ischemic stroke is uncertain. Patients with a clinical deficit that is disproportionately severe relative to the infarct volume may benefit from late thrombectomy.\nWe enrolled patients with occlusion of the intracranial internal carotid artery or proximal middle cerebral artery who had last been known to be well 6 to 24 hours earlier and who had a mismatch between the severity of the clinical deficit and the infarct volume, with mismatch criteria defined according to age (<80 years or ≥80 years). Patients were randomly assigned to thrombectomy plus standard care (the thrombectomy group) or to standard care alone (the control group). The coprimary end points were the mean score for disability on the utility-weighted modified Rankin scale (which ranges from 0 [death] to 10 [no symptoms or disability]) and the rate of functional independence (a score of 0, 1, or 2 on the modified Rankin scale, which ranges from 0 to 6, with higher scores indicating more severe disability) at 90 days.\nA total of 206 patients were enrolled; 107 were assigned to the thrombectomy group and 99 to the control group. At 31 months, enrollment in the trial was stopped because of the results of a prespecified interim analysis. The mean score on the utility-weighted modified Rankin scale at 90 days was 5.5 in the thrombectomy group as compared with 3.4 in the control group (adjusted difference [Bayesian analysis], 2.0 points; 95% credible interval, 1.1 to 3.0; posterior probability of superiority, >0.999), and the rate of functional independence at 90 days was 49% in the thrombectomy group as compared with 13% in the control group (adjusted difference, 33 percentage points; 95% credible interval, 24 to 44; posterior probability of superiority, >0.999). The rate of symptomatic intracranial hemorrhage did not differ significantly between the two groups (6% in the thrombectomy group and 3% in the control group, P=0.50), nor did 90-day mortality (19% and 18%, respectively; P=1.00).\nAmong patients with acute stroke who had last been known to be well 6 to 24 hours earlier and who had a mismatch between clinical deficit and infarct, outcomes for disability at 90 days were better with thrombectomy plus standard care than with standard care alone. (Funded by Stryker Neurovascular; DAWN ClinicalTrials.gov number, NCT02142283 .).', 'title': 'Thrombectomy 6 to 24 Hours after Stroke with a Mismatch between Deficit and Infarct.', 'date': '2017-11-14'}, '29364767': {'article_id': '29364767', 'content': 'Thrombectomy is currently recommended for eligible patients with stroke who are treated within 6 hours after the onset of symptoms.\nWe conducted a multicenter, randomized, open-label trial, with blinded outcome assessment, of thrombectomy in patients 6 to 16 hours after they were last known to be well and who had remaining ischemic brain tissue that was not yet infarcted. Patients with proximal middle-cerebral-artery or internal-carotid-artery occlusion, an initial infarct size of less than 70 ml, and a ratio of the volume of ischemic tissue on perfusion imaging to infarct volume of 1.8 or more were randomly assigned to endovascular therapy (thrombectomy) plus standard medical therapy (endovascular-therapy group) or standard medical therapy alone (medical-therapy group). The primary outcome was the ordinal score on the modified Rankin scale (range, 0 to 6, with higher scores indicating greater disability) at day 90.\nThe trial was conducted at 38 U.S. centers and terminated early for efficacy after 182 patients had undergone randomization (92 to the endovascular-therapy group and 90 to the medical-therapy group). Endovascular therapy plus medical therapy, as compared with medical therapy alone, was associated with a favorable shift in the distribution of functional outcomes on the modified Rankin scale at 90 days (odds ratio, 2.77; P<0.001) and a higher percentage of patients who were functionally independent, defined as a score on the modified Rankin scale of 0 to 2 (45% vs. 17%, P<0.001). The 90-day mortality rate was 14% in the endovascular-therapy group and 26% in the medical-therapy group (P=0.05), and there was no significant between-group difference in the frequency of symptomatic intracranial hemorrhage (7% and 4%, respectively; P=0.75) or of serious adverse events (43% and 53%, respectively; P=0.18).\nEndovascular thrombectomy for ischemic stroke 6 to 16 hours after a patient was last known to be well plus standard medical therapy resulted in better functional outcomes than standard medical therapy alone among patients with proximal middle-cerebral-artery or internal-carotid-artery occlusion and a region of tissue that was ischemic but not yet infarcted. (Funded by the National Institute of Neurological Disorders and Stroke; DEFUSE 3 ClinicalTrials.gov number, NCT02586415 .).', 'title': 'Thrombectomy for Stroke at 6 to 16 Hours with Selection by Perfusion Imaging.', 'date': '2018-01-25'}}",1.0,Surgery
152,"Is the risk of death at 90 days higher, lower, or the same when comparing endovascular thrombectomy to control?",no difference,high,no,"['29129157', '29364767']",34850380,2021,"{'29129157': {'article_id': '29129157', 'content': 'The effect of endovascular thrombectomy that is performed more than 6 hours after the onset of ischemic stroke is uncertain. Patients with a clinical deficit that is disproportionately severe relative to the infarct volume may benefit from late thrombectomy.\nWe enrolled patients with occlusion of the intracranial internal carotid artery or proximal middle cerebral artery who had last been known to be well 6 to 24 hours earlier and who had a mismatch between the severity of the clinical deficit and the infarct volume, with mismatch criteria defined according to age (<80 years or ≥80 years). Patients were randomly assigned to thrombectomy plus standard care (the thrombectomy group) or to standard care alone (the control group). The coprimary end points were the mean score for disability on the utility-weighted modified Rankin scale (which ranges from 0 [death] to 10 [no symptoms or disability]) and the rate of functional independence (a score of 0, 1, or 2 on the modified Rankin scale, which ranges from 0 to 6, with higher scores indicating more severe disability) at 90 days.\nA total of 206 patients were enrolled; 107 were assigned to the thrombectomy group and 99 to the control group. At 31 months, enrollment in the trial was stopped because of the results of a prespecified interim analysis. The mean score on the utility-weighted modified Rankin scale at 90 days was 5.5 in the thrombectomy group as compared with 3.4 in the control group (adjusted difference [Bayesian analysis], 2.0 points; 95% credible interval, 1.1 to 3.0; posterior probability of superiority, >0.999), and the rate of functional independence at 90 days was 49% in the thrombectomy group as compared with 13% in the control group (adjusted difference, 33 percentage points; 95% credible interval, 24 to 44; posterior probability of superiority, >0.999). The rate of symptomatic intracranial hemorrhage did not differ significantly between the two groups (6% in the thrombectomy group and 3% in the control group, P=0.50), nor did 90-day mortality (19% and 18%, respectively; P=1.00).\nAmong patients with acute stroke who had last been known to be well 6 to 24 hours earlier and who had a mismatch between clinical deficit and infarct, outcomes for disability at 90 days were better with thrombectomy plus standard care than with standard care alone. (Funded by Stryker Neurovascular; DAWN ClinicalTrials.gov number, NCT02142283 .).', 'title': 'Thrombectomy 6 to 24 Hours after Stroke with a Mismatch between Deficit and Infarct.', 'date': '2017-11-14'}, '29364767': {'article_id': '29364767', 'content': 'Thrombectomy is currently recommended for eligible patients with stroke who are treated within 6 hours after the onset of symptoms.\nWe conducted a multicenter, randomized, open-label trial, with blinded outcome assessment, of thrombectomy in patients 6 to 16 hours after they were last known to be well and who had remaining ischemic brain tissue that was not yet infarcted. Patients with proximal middle-cerebral-artery or internal-carotid-artery occlusion, an initial infarct size of less than 70 ml, and a ratio of the volume of ischemic tissue on perfusion imaging to infarct volume of 1.8 or more were randomly assigned to endovascular therapy (thrombectomy) plus standard medical therapy (endovascular-therapy group) or standard medical therapy alone (medical-therapy group). The primary outcome was the ordinal score on the modified Rankin scale (range, 0 to 6, with higher scores indicating greater disability) at day 90.\nThe trial was conducted at 38 U.S. centers and terminated early for efficacy after 182 patients had undergone randomization (92 to the endovascular-therapy group and 90 to the medical-therapy group). Endovascular therapy plus medical therapy, as compared with medical therapy alone, was associated with a favorable shift in the distribution of functional outcomes on the modified Rankin scale at 90 days (odds ratio, 2.77; P<0.001) and a higher percentage of patients who were functionally independent, defined as a score on the modified Rankin scale of 0 to 2 (45% vs. 17%, P<0.001). The 90-day mortality rate was 14% in the endovascular-therapy group and 26% in the medical-therapy group (P=0.05), and there was no significant between-group difference in the frequency of symptomatic intracranial hemorrhage (7% and 4%, respectively; P=0.75) or of serious adverse events (43% and 53%, respectively; P=0.18).\nEndovascular thrombectomy for ischemic stroke 6 to 16 hours after a patient was last known to be well plus standard medical therapy resulted in better functional outcomes than standard medical therapy alone among patients with proximal middle-cerebral-artery or internal-carotid-artery occlusion and a region of tissue that was ischemic but not yet infarcted. (Funded by the National Institute of Neurological Disorders and Stroke; DEFUSE 3 ClinicalTrials.gov number, NCT02586415 .).', 'title': 'Thrombectomy for Stroke at 6 to 16 Hours with Selection by Perfusion Imaging.', 'date': '2018-01-25'}}",0.5,Surgery
153,"Is the likelihood of good functional outcome higher, lower, or the same when comparing intravenous thrombolytic treatment to control?",higher,high,no,"['30947642', '31067369', '21808985', '32248771', '29766770']",34850380,2021,"{'30947642': {'article_id': '30947642', 'content': 'Intravenous thrombolysis with alteplase within a time window up to 4.5\u2009h is the only approved pharmacological treatment for acute ischemic stroke. We studied whether acute ischemic stroke patients with penumbral tissue identified on magnetic resonance imaging 4.5-9\u2009h after symptom onset benefit from intravenous thrombolysis compared to placebo.\nAcute ischemic stroke patients with salvageable brain tissue identified on a magnetic resonance imaging were randomly assigned to receive standard dose alteplase or placebo. The primary end point was disability at 90 days assessed by the modified Rankin scale, which has a range of 0-6 (with 0 indicating no symptoms at all and 6 indicating death). Safety end points included death, symptomatic intracranial hemorrhage, and other serious adverse events.\nThe trial was stopped early for slow recruitment after the enrollment of 119 (61 alteplase, 58 placebo) of 264 patients planned. Median time to intravenous thrombolysis was 7\u2009h 42\u2009min. The primary endpoint showed no significant difference in the modified Rankin scale distribution at day 90 (odds ratio alteplase versus placebo, 1.20; 95% CI, 0.63-2.27, P\u2009=\u20090.58). One symptomatic intracranial hemorrhage occurred in the alteplase group. Mortality at 90 days did not differ significantly between the two groups (11.5 and 6.8%, respectively; P\u2009=\u20090.53).\nIntravenous alteplase administered between 4.5 and 9\u2009h after the onset of symptoms in patients with salvageable tissue did not result in a significant benefit over placebo. (Supported by Boehringer Ingelheim, Germany; ISRCTN 71616222).', 'title': 'Extending the time window for intravenous thrombolysis in acute ischemic stroke using magnetic resonance imaging-based patient selection.', 'date': '2019-04-06'}, '31067369': {'article_id': '31067369', 'content': 'The time to initiate intravenous thrombolysis for acute ischemic stroke is generally limited to within 4.5 hours after the onset of symptoms. Some trials have suggested that the treatment window may be extended in patients who are shown to have ischemic but not yet infarcted brain tissue on imaging.\nWe conducted a multicenter, randomized, placebo-controlled trial involving patients with ischemic stroke who had hypoperfused but salvageable regions of brain detected on automated perfusion imaging. The patients were randomly assigned to receive intravenous alteplase or placebo between 4.5 and 9.0 hours after the onset of stroke or on awakening with stroke (if within 9 hours from the midpoint of sleep). The primary outcome was a score of 0 or 1 on the modified Rankin scale, on which scores range from 0 (no symptoms) to 6 (death), at 90 days. The risk ratio for the primary outcome was adjusted for age and clinical severity at baseline.\nAfter 225 of the planned 310 patients had been enrolled, the trial was terminated because of a loss of equipoise after the publication of positive results from a previous trial. A total of 113 patients were randomly assigned to the alteplase group and 112 to the placebo group. The primary outcome occurred in 40 patients (35.4%) in the alteplase group and in 33 patients (29.5%) in the placebo group (adjusted risk ratio, 1.44; 95% confidence interval [CI], 1.01 to 2.06; P\u2009=\u20090.04). Symptomatic intracerebral hemorrhage occurred in 7 patients (6.2%) in the alteplase group and in 1 patient (0.9%) in the placebo group (adjusted risk ratio, 7.22; 95% CI, 0.97 to 53.5; P\u2009=\u20090.05). A secondary ordinal analysis of the distribution of scores on the modified Rankin scale did not show a significant between-group difference in functional improvement at 90 days.\nAmong the patients in this trial who had ischemic stroke and salvageable brain tissue, the use of alteplase between 4.5 and 9.0 hours after stroke onset or at the time the patient awoke with stroke symptoms resulted in a higher percentage of patients with no or minor neurologic deficits than the use of placebo. There were more cases of symptomatic cerebral hemorrhage in the alteplase group than in the placebo group. (Funded by the Australian National Health and Medical Research Council and others; EXTEND ClinicalTrials.gov numbers, NCT00887328 and NCT01580839.).', 'title': 'Thrombolysis Guided by Perfusion Imaging up to 9 Hours after Onset of Stroke.', 'date': '2019-05-09'}, '21808985': {'article_id': '21808985', 'content': 'Patients with unknown stroke onset are generally excluded from acute recanalisation treatments. We designed a pilot study to assess feasibility of a trial of perfusion computed tomography (PCT)-guided thrombolysis in patients with ischemic tissue at risk of infarction and unknown stroke onset.\nPatients with a supratentorial stroke of unknown onset in the middle cerebral artery territory and significant volume of at-risk tissue on PCT were randomized to intravenous thrombolysis with alteplase (0.9\xa0mg/kg) or placebo. Feasibility endpoints were randomization and blinded treatment of patients within 2\xa0h after hospital arrival, and the correct application (estimation) of the perfusion imaging criteria.\nAt baseline, there was a trend towards older age [69.5 (57-78) vs. 49 (44-78)\u2009years] in the thrombolysis group (n\u2009=\u20096) compared to placebo (n\u2009=\u20096). Regarding feasibility, hospital arrival to treatment delay was above the allowed 2\xa0h in three patients (25%). There were two protocol violations (17%) regarding PCT, both underestimating the predicted infarct in patients randomized in the placebo group. No symptomatic hemorrhage or death occurred during the first 7\xa0days. Three of the four (75%) and one of the five (20%) patients were recanalized in the thrombolysis and placebo group respectively. The volume of non-infarcted at-risk tissue was 84 (44-206)\u2009cm(3) in the treatment arm and 29 (8-105)\u2009cm(3) in the placebo arm.\nThis pilot study shows that a randomized PCT-guided thrombolysis trial in patients with stroke of unknown onset may be feasible if issues such as treatment delays and reliable identification of tissue at risk of infarction tissue are resolved. Safety and efficiency of such an approach need to be established.', 'title': 'Perfusion-CT guided intravenous thrombolysis in patients with unknown-onset stroke: a randomized, double-blind, placebo-controlled, pilot feasibility trial.', 'date': '2011-08-03'}, '32248771': {'article_id': '32248771', 'content': 'Background and Purpose- We assessed whether lower-dose alteplase at 0.6 mg/kg is efficacious and safe for acute fluid-attenuated inversion recovery-negative stroke with unknown time of onset. Methods- This was an investigator-initiated, multicenter, randomized, open-label, blinded-end point trial. Patients met the standard indication criteria for intravenous thrombolysis other than a time last-known-well >4.5 hours (eg, wake-up stroke). Patients were randomly assigned (1:1) to receive alteplase at 0.6 mg/kg or standard medical treatment if magnetic resonance imaging showed acute ischemic lesion on diffusion-weighted imaging and no marked corresponding hyperintensity on fluid-attenuated inversion recovery. The primary outcome was a favorable outcome (90-day modified Rankin Scale score of 0-1). Results- Following the early stop and positive results of the WAKE-UP trial (Efficacy and Safety of MRI-Based Thrombolysis in Wake-Up Stroke), this trial was prematurely terminated with 131 of the anticipated 300 patients (55 women; mean age, 74.4±12.2 years). Favorable outcome was comparable between the alteplase group (32/68, 47.1%) and the control group (28/58, 48.3%; relative risk [RR], 0.97 [95% CI, 0.68-1.41]; ', 'title': 'Thrombolysis With Alteplase at 0.6 mg/kg for Stroke With Unknown Time of Onset: A Randomized Controlled Trial.', 'date': '2020-04-07'}, '29766770': {'article_id': '29766770', 'content': 'Under current guidelines, intravenous thrombolysis is used to treat acute stroke only if it can be ascertained that the time since the onset of symptoms was less than 4.5 hours. We sought to determine whether patients with stroke with an unknown time of onset and features suggesting recent cerebral infarction on magnetic resonance imaging (MRI) would benefit from thrombolysis with the use of intravenous alteplase.\nIn a multicenter trial, we randomly assigned patients who had an unknown time of onset of stroke to receive either intravenous alteplase or placebo. All the patients had an ischemic lesion that was visible on MRI diffusion-weighted imaging but no parenchymal hyperintensity on fluid-attenuated inversion recovery (FLAIR), which indicated that the stroke had occurred approximately within the previous 4.5 hours. We excluded patients for whom thrombectomy was planned. The primary end point was favorable outcome, as defined by a score of 0 or 1 on the modified Rankin scale of neurologic disability (which ranges from 0 [no symptoms] to 6 [death]) at 90 days. A secondary outcome was the likelihood that alteplase would lead to lower ordinal scores on the modified Rankin scale than would placebo (shift analysis).\nThe trial was stopped early owing to cessation of funding after the enrollment of 503 of an anticipated 800 patients. Of these patients, 254 were randomly assigned to receive alteplase and 249 to receive placebo. A favorable outcome at 90 days was reported in 131 of 246 patients (53.3%) in the alteplase group and in 102 of 244 patients (41.8%) in the placebo group (adjusted odds ratio, 1.61; 95% confidence interval [CI], 1.09 to 2.36; P=0.02). The median score on the modified Rankin scale at 90 days was 1 in the alteplase group and 2 in the placebo group (adjusted common odds ratio, 1.62; 95% CI, 1.17 to 2.23; P=0.003). There were 10 deaths (4.1%) in the alteplase group and 3 (1.2%) in the placebo group (odds ratio, 3.38; 95% CI, 0.92 to 12.52; P=0.07). The rate of symptomatic intracranial hemorrhage was 2.0% in the alteplase group and 0.4% in the placebo group (odds ratio, 4.95; 95% CI, 0.57 to 42.87; P=0.15).\nIn patients with acute stroke with an unknown time of onset, intravenous alteplase guided by a mismatch between diffusion-weighted imaging and FLAIR in the region of ischemia resulted in a significantly better functional outcome and numerically more intracranial hemorrhages than placebo at 90 days. (Funded by the European Union Seventh Framework Program; WAKE-UP ClinicalTrials.gov number, NCT01525290; and EudraCT number, 2011-005906-32 .).', 'title': 'MRI-Guided Thrombolysis for Stroke with Unknown Time of Onset.', 'date': '2018-05-17'}}",0.4,Emergency Medicine & Critical Care
154,"Is the risk of death at 90 days higher, lower, or the same when comparing intravenous thrombolytic treatment to control?",no difference,high,no,"['30947642', '31067369', '21808985', '32248771', '29766770']",34850380,2021,"{'30947642': {'article_id': '30947642', 'content': 'Intravenous thrombolysis with alteplase within a time window up to 4.5\u2009h is the only approved pharmacological treatment for acute ischemic stroke. We studied whether acute ischemic stroke patients with penumbral tissue identified on magnetic resonance imaging 4.5-9\u2009h after symptom onset benefit from intravenous thrombolysis compared to placebo.\nAcute ischemic stroke patients with salvageable brain tissue identified on a magnetic resonance imaging were randomly assigned to receive standard dose alteplase or placebo. The primary end point was disability at 90 days assessed by the modified Rankin scale, which has a range of 0-6 (with 0 indicating no symptoms at all and 6 indicating death). Safety end points included death, symptomatic intracranial hemorrhage, and other serious adverse events.\nThe trial was stopped early for slow recruitment after the enrollment of 119 (61 alteplase, 58 placebo) of 264 patients planned. Median time to intravenous thrombolysis was 7\u2009h 42\u2009min. The primary endpoint showed no significant difference in the modified Rankin scale distribution at day 90 (odds ratio alteplase versus placebo, 1.20; 95% CI, 0.63-2.27, P\u2009=\u20090.58). One symptomatic intracranial hemorrhage occurred in the alteplase group. Mortality at 90 days did not differ significantly between the two groups (11.5 and 6.8%, respectively; P\u2009=\u20090.53).\nIntravenous alteplase administered between 4.5 and 9\u2009h after the onset of symptoms in patients with salvageable tissue did not result in a significant benefit over placebo. (Supported by Boehringer Ingelheim, Germany; ISRCTN 71616222).', 'title': 'Extending the time window for intravenous thrombolysis in acute ischemic stroke using magnetic resonance imaging-based patient selection.', 'date': '2019-04-06'}, '31067369': {'article_id': '31067369', 'content': 'The time to initiate intravenous thrombolysis for acute ischemic stroke is generally limited to within 4.5 hours after the onset of symptoms. Some trials have suggested that the treatment window may be extended in patients who are shown to have ischemic but not yet infarcted brain tissue on imaging.\nWe conducted a multicenter, randomized, placebo-controlled trial involving patients with ischemic stroke who had hypoperfused but salvageable regions of brain detected on automated perfusion imaging. The patients were randomly assigned to receive intravenous alteplase or placebo between 4.5 and 9.0 hours after the onset of stroke or on awakening with stroke (if within 9 hours from the midpoint of sleep). The primary outcome was a score of 0 or 1 on the modified Rankin scale, on which scores range from 0 (no symptoms) to 6 (death), at 90 days. The risk ratio for the primary outcome was adjusted for age and clinical severity at baseline.\nAfter 225 of the planned 310 patients had been enrolled, the trial was terminated because of a loss of equipoise after the publication of positive results from a previous trial. A total of 113 patients were randomly assigned to the alteplase group and 112 to the placebo group. The primary outcome occurred in 40 patients (35.4%) in the alteplase group and in 33 patients (29.5%) in the placebo group (adjusted risk ratio, 1.44; 95% confidence interval [CI], 1.01 to 2.06; P\u2009=\u20090.04). Symptomatic intracerebral hemorrhage occurred in 7 patients (6.2%) in the alteplase group and in 1 patient (0.9%) in the placebo group (adjusted risk ratio, 7.22; 95% CI, 0.97 to 53.5; P\u2009=\u20090.05). A secondary ordinal analysis of the distribution of scores on the modified Rankin scale did not show a significant between-group difference in functional improvement at 90 days.\nAmong the patients in this trial who had ischemic stroke and salvageable brain tissue, the use of alteplase between 4.5 and 9.0 hours after stroke onset or at the time the patient awoke with stroke symptoms resulted in a higher percentage of patients with no or minor neurologic deficits than the use of placebo. There were more cases of symptomatic cerebral hemorrhage in the alteplase group than in the placebo group. (Funded by the Australian National Health and Medical Research Council and others; EXTEND ClinicalTrials.gov numbers, NCT00887328 and NCT01580839.).', 'title': 'Thrombolysis Guided by Perfusion Imaging up to 9 Hours after Onset of Stroke.', 'date': '2019-05-09'}, '21808985': {'article_id': '21808985', 'content': 'Patients with unknown stroke onset are generally excluded from acute recanalisation treatments. We designed a pilot study to assess feasibility of a trial of perfusion computed tomography (PCT)-guided thrombolysis in patients with ischemic tissue at risk of infarction and unknown stroke onset.\nPatients with a supratentorial stroke of unknown onset in the middle cerebral artery territory and significant volume of at-risk tissue on PCT were randomized to intravenous thrombolysis with alteplase (0.9\xa0mg/kg) or placebo. Feasibility endpoints were randomization and blinded treatment of patients within 2\xa0h after hospital arrival, and the correct application (estimation) of the perfusion imaging criteria.\nAt baseline, there was a trend towards older age [69.5 (57-78) vs. 49 (44-78)\u2009years] in the thrombolysis group (n\u2009=\u20096) compared to placebo (n\u2009=\u20096). Regarding feasibility, hospital arrival to treatment delay was above the allowed 2\xa0h in three patients (25%). There were two protocol violations (17%) regarding PCT, both underestimating the predicted infarct in patients randomized in the placebo group. No symptomatic hemorrhage or death occurred during the first 7\xa0days. Three of the four (75%) and one of the five (20%) patients were recanalized in the thrombolysis and placebo group respectively. The volume of non-infarcted at-risk tissue was 84 (44-206)\u2009cm(3) in the treatment arm and 29 (8-105)\u2009cm(3) in the placebo arm.\nThis pilot study shows that a randomized PCT-guided thrombolysis trial in patients with stroke of unknown onset may be feasible if issues such as treatment delays and reliable identification of tissue at risk of infarction tissue are resolved. Safety and efficiency of such an approach need to be established.', 'title': 'Perfusion-CT guided intravenous thrombolysis in patients with unknown-onset stroke: a randomized, double-blind, placebo-controlled, pilot feasibility trial.', 'date': '2011-08-03'}, '32248771': {'article_id': '32248771', 'content': 'Background and Purpose- We assessed whether lower-dose alteplase at 0.6 mg/kg is efficacious and safe for acute fluid-attenuated inversion recovery-negative stroke with unknown time of onset. Methods- This was an investigator-initiated, multicenter, randomized, open-label, blinded-end point trial. Patients met the standard indication criteria for intravenous thrombolysis other than a time last-known-well >4.5 hours (eg, wake-up stroke). Patients were randomly assigned (1:1) to receive alteplase at 0.6 mg/kg or standard medical treatment if magnetic resonance imaging showed acute ischemic lesion on diffusion-weighted imaging and no marked corresponding hyperintensity on fluid-attenuated inversion recovery. The primary outcome was a favorable outcome (90-day modified Rankin Scale score of 0-1). Results- Following the early stop and positive results of the WAKE-UP trial (Efficacy and Safety of MRI-Based Thrombolysis in Wake-Up Stroke), this trial was prematurely terminated with 131 of the anticipated 300 patients (55 women; mean age, 74.4±12.2 years). Favorable outcome was comparable between the alteplase group (32/68, 47.1%) and the control group (28/58, 48.3%; relative risk [RR], 0.97 [95% CI, 0.68-1.41]; ', 'title': 'Thrombolysis With Alteplase at 0.6 mg/kg for Stroke With Unknown Time of Onset: A Randomized Controlled Trial.', 'date': '2020-04-07'}, '29766770': {'article_id': '29766770', 'content': 'Under current guidelines, intravenous thrombolysis is used to treat acute stroke only if it can be ascertained that the time since the onset of symptoms was less than 4.5 hours. We sought to determine whether patients with stroke with an unknown time of onset and features suggesting recent cerebral infarction on magnetic resonance imaging (MRI) would benefit from thrombolysis with the use of intravenous alteplase.\nIn a multicenter trial, we randomly assigned patients who had an unknown time of onset of stroke to receive either intravenous alteplase or placebo. All the patients had an ischemic lesion that was visible on MRI diffusion-weighted imaging but no parenchymal hyperintensity on fluid-attenuated inversion recovery (FLAIR), which indicated that the stroke had occurred approximately within the previous 4.5 hours. We excluded patients for whom thrombectomy was planned. The primary end point was favorable outcome, as defined by a score of 0 or 1 on the modified Rankin scale of neurologic disability (which ranges from 0 [no symptoms] to 6 [death]) at 90 days. A secondary outcome was the likelihood that alteplase would lead to lower ordinal scores on the modified Rankin scale than would placebo (shift analysis).\nThe trial was stopped early owing to cessation of funding after the enrollment of 503 of an anticipated 800 patients. Of these patients, 254 were randomly assigned to receive alteplase and 249 to receive placebo. A favorable outcome at 90 days was reported in 131 of 246 patients (53.3%) in the alteplase group and in 102 of 244 patients (41.8%) in the placebo group (adjusted odds ratio, 1.61; 95% confidence interval [CI], 1.09 to 2.36; P=0.02). The median score on the modified Rankin scale at 90 days was 1 in the alteplase group and 2 in the placebo group (adjusted common odds ratio, 1.62; 95% CI, 1.17 to 2.23; P=0.003). There were 10 deaths (4.1%) in the alteplase group and 3 (1.2%) in the placebo group (odds ratio, 3.38; 95% CI, 0.92 to 12.52; P=0.07). The rate of symptomatic intracranial hemorrhage was 2.0% in the alteplase group and 0.4% in the placebo group (odds ratio, 4.95; 95% CI, 0.57 to 42.87; P=0.15).\nIn patients with acute stroke with an unknown time of onset, intravenous alteplase guided by a mismatch between diffusion-weighted imaging and FLAIR in the region of ischemia resulted in a significantly better functional outcome and numerically more intracranial hemorrhages than placebo at 90 days. (Funded by the European Union Seventh Framework Program; WAKE-UP ClinicalTrials.gov number, NCT01525290; and EudraCT number, 2011-005906-32 .).', 'title': 'MRI-Guided Thrombolysis for Stroke with Unknown Time of Onset.', 'date': '2018-05-17'}}",0.6,Emergency Medicine & Critical Care
155,"Is survival to hospital discharge higher, lower, or the same when comparing untrained bystander CPR with continuous chest compression to untrained bystander CPR with chest compression interrupted with pauses for rescue breathing?",higher,high,no,"['10824072', '20818863', '20818864']",28349529,2017,"{'10824072': {'article_id': '10824072', 'content': 'Despite extensive training of citizens of Seattle in cardiopulmonary resuscitation (CPR), bystanders do not perform CPR in almost half of witnessed cardiac arrests. Instructions in chest compression plus mouth-to-mouth ventilation given by dispatchers over the telephone can require 2.4 minutes. In experimental studies, chest compression alone is associated with survival rates similar to those with chest compression plus mouth-to-mouth ventilation. We conducted a randomized study to compare CPR by chest compression alone with CPR by chest compression plus mouth-to-mouth ventilation.\nThe setting of the trial was an urban, fire-department-based, emergency-medical-care system with central dispatching. In a randomized manner, telephone dispatchers gave bystanders at the scene of apparent cardiac arrest instructions in either chest compression alone or chest compression plus mouth-to-mouth ventilation. The primary end point was survival to hospital discharge.\nData were analyzed for 241 patients randomly assigned to receive chest compression alone and 279 assigned to chest compression plus mouth-to-mouth ventilation. Complete instructions were delivered in 62 percent of episodes for the group receiving chest compression plus mouth-to-mouth ventilation and 81 percent of episodes for the group receiving chest compression alone (P=0.005). Instructions for compression required 1.4 minutes less to complete than instructions for compression plus mouth-to-mouth ventilation. Survival to hospital discharge was better among patients assigned to chest compression alone than among those assigned to chest compression plus mouth-to-mouth ventilation (14.6 percent vs. 10.4 percent), but the difference was not statistically significant (P=0.18).\nThe outcome after CPR with chest compression alone is similar to that after chest compression with mouth-to-mouth ventilation, and chest compression alone may be the preferred approach for bystanders inexperienced in CPR.', 'title': 'Cardiopulmonary resuscitation by chest compression alone or with mouth-to-mouth ventilation.', 'date': '2000-05-29'}, '20818863': {'article_id': '20818863', 'content': 'The role of rescue breathing in cardiopulmonary resuscitation (CPR) performed by a layperson is uncertain. We hypothesized that the dispatcher instructions to bystanders to provide chest compression alone would result in improved survival as compared with instructions to provide chest compression plus rescue breathing.\nWe conducted a multicenter, randomized trial of dispatcher instructions to bystanders for performing CPR. The patients were persons 18 years of age or older with out-of-hospital cardiac arrest for whom dispatchers initiated CPR instruction to bystanders. Patients were randomly assigned to receive chest compression alone or chest compression plus rescue breathing. The primary outcome was survival to hospital discharge. Secondary outcomes included a favorable neurologic outcome at discharge.\nOf the 1941 patients who met the inclusion criteria, 981 were randomly assigned to receive chest compression alone and 960 to receive chest compression plus rescue breathing. We observed no significant difference between the two groups in the proportion of patients who survived to hospital discharge (12.5% with chest compression alone and 11.0% with chest compression plus rescue breathing, P=0.31) or in the proportion who survived with a favorable neurologic outcome in the two sites that assessed this secondary outcome (14.4% and 11.5%, respectively; P=0.13). Prespecified subgroup analyses showed a trend toward a higher proportion of patients surviving to hospital discharge with chest compression alone as compared with chest compression plus rescue breathing for patients with a cardiac cause of arrest (15.5% vs. 12.3%, P=0.09) and for those with shockable rhythms (31.9% vs. 25.7%, P=0.09).\nDispatcher instruction consisting of chest compression alone did not increase the survival rate overall, although there was a trend toward better outcomes in key clinical subgroups. The results support a strategy for CPR performed by laypersons that emphasizes chest compression and minimizes the role of rescue breathing. (Funded in part by the Laerdal Foundation for Acute Medicine and the Medic One Foundation; ClinicalTrials.gov number, NCT00219687.)', 'title': 'CPR with chest compression alone or with rescue breathing.', 'date': '2010-09-08'}, '20818864': {'article_id': '20818864', 'content': 'Emergency medical dispatchers give instructions on how to perform cardiopulmonary resuscitation (CPR) over the telephone to callers requesting help for a patient with suspected cardiac arrest, before the arrival of emergency medical services (EMS) personnel. A previous study indicated that instructions to perform CPR consisting of only chest compression result in a treatment efficacy that is similar or even superior to that associated with instructions given to perform standard CPR, which consists of both compression and ventilation. That study, however, was not powered to assess a possible difference in survival. The aim of this prospective, randomized study was to evaluate the possible superiority of compression-only CPR over standard CPR with respect to survival.\nPatients with suspected, witnessed, out-of-hospital cardiac arrest were randomly assigned to undergo either compression-only CPR or standard CPR. The primary end point was 30-day survival.\nData for the primary analysis were collected from February 2005 through January 2009 for a total of 1276 patients. Of these, 620 patients had been assigned to receive compression-only CPR and 656 patients had been assigned to receive standard CPR. The rate of 30-day survival was similar in the two groups: 8.7% (54 of 620 patients) in the group receiving compression-only CPR and 7.0% (46 of 656 patients) in the group receiving standard CPR (absolute difference for compression-only vs. standard CPR, 1.7 percentage points; 95% confidence interval, -1.2 to 4.6; P=0.29).\nThis prospective, randomized study showed no significant difference with respect to survival at 30 days between instructions given by an emergency medical dispatcher, before the arrival of EMS personnel, for compression-only CPR and instructions for standard CPR in patients with suspected, witnessed, out-of-hospital cardiac arrest. (Funded by the Swedish Heart–Lung Foundation and others; Karolinska Clinical Trial Registration number, CT20080012.)', 'title': 'Compression-only CPR or standard CPR in out-of-hospital cardiac arrest.', 'date': '2010-09-08'}}",0.0,Emergency Medicine & Critical Care
156,"Is the likelihood of survival to hospital discharge higher, lower, or the same when comparing continuous chest compression CPR with asynchronous rescue breathing to interrupted chest compression plus rescue breathing?",lower,moderate,no,['26550795'],28349529,2017,"{'26550795': {'article_id': '26550795', 'content': 'During cardiopulmonary resuscitation (CPR) in patients with out-of-hospital cardiac arrest, the interruption of manual chest compressions for rescue breathing reduces blood flow and possibly survival. We assessed whether outcomes after continuous compressions with positive-pressure ventilation differed from those after compressions that were interrupted for ventilations at a ratio of 30 compressions to two ventilations.\nThis cluster-randomized trial with crossover included 114 emergency medical service (EMS) agencies. Adults with non-trauma-related cardiac arrest who were treated by EMS providers received continuous chest compressions (intervention group) or interrupted chest compressions (control group). The primary outcome was the rate of survival to hospital discharge. Secondary outcomes included the modified Rankin scale score (on a scale from 0 to 6, with a score of ≤3 indicating favorable neurologic function). CPR process was measured to assess compliance.\nOf 23,711 patients included in the primary analysis, 12,653 were assigned to the intervention group and 11,058 to the control group. A total of 1129 of 12,613 patients with available data (9.0%) in the intervention group and 1072 of 11,035 with available data (9.7%) in the control group survived until discharge (difference, -0.7 percentage points; 95% confidence interval [CI], -1.5 to 0.1; P=0.07); 7.0% of the patients in the intervention group and 7.7% of those in the control group survived with favorable neurologic function at discharge (difference, -0.6 percentage points; 95% CI, -1.4 to 0.1, P=0.09). Hospital-free survival was significantly shorter in the intervention group than in the control group (mean difference, -0.2 days; 95% CI, -0.3 to -0.1; P=0.004).\nIn patients with out-of-hospital cardiac arrest, continuous chest compressions during CPR performed by EMS providers did not result in significantly higher rates of survival or favorable neurologic function than did interrupted chest compressions. (Funded by the National Heart, Lung, and Blood Institute and others; ROC CCC ClinicalTrials.gov number, NCT01372748.).', 'title': 'Trial of Continuous or Interrupted Chest Compressions during CPR.', 'date': '2015-11-10'}}",0.0,Emergency Medicine & Critical Care
157,"Is the likelihood of favorable neurological outcome higher, lower, or the same when comparing continuous chest compression CPR with asynchronous rescue breathing to interrupted chest compression plus rescue breathing?",no difference,high,no,['26550795'],28349529,2017,"{'26550795': {'article_id': '26550795', 'content': 'During cardiopulmonary resuscitation (CPR) in patients with out-of-hospital cardiac arrest, the interruption of manual chest compressions for rescue breathing reduces blood flow and possibly survival. We assessed whether outcomes after continuous compressions with positive-pressure ventilation differed from those after compressions that were interrupted for ventilations at a ratio of 30 compressions to two ventilations.\nThis cluster-randomized trial with crossover included 114 emergency medical service (EMS) agencies. Adults with non-trauma-related cardiac arrest who were treated by EMS providers received continuous chest compressions (intervention group) or interrupted chest compressions (control group). The primary outcome was the rate of survival to hospital discharge. Secondary outcomes included the modified Rankin scale score (on a scale from 0 to 6, with a score of ≤3 indicating favorable neurologic function). CPR process was measured to assess compliance.\nOf 23,711 patients included in the primary analysis, 12,653 were assigned to the intervention group and 11,058 to the control group. A total of 1129 of 12,613 patients with available data (9.0%) in the intervention group and 1072 of 11,035 with available data (9.7%) in the control group survived until discharge (difference, -0.7 percentage points; 95% confidence interval [CI], -1.5 to 0.1; P=0.07); 7.0% of the patients in the intervention group and 7.7% of those in the control group survived with favorable neurologic function at discharge (difference, -0.6 percentage points; 95% CI, -1.4 to 0.1, P=0.09). Hospital-free survival was significantly shorter in the intervention group than in the control group (mean difference, -0.2 days; 95% CI, -0.3 to -0.1; P=0.004).\nIn patients with out-of-hospital cardiac arrest, continuous chest compressions during CPR performed by EMS providers did not result in significantly higher rates of survival or favorable neurologic function than did interrupted chest compressions. (Funded by the National Heart, Lung, and Blood Institute and others; ROC CCC ClinicalTrials.gov number, NCT01372748.).', 'title': 'Trial of Continuous or Interrupted Chest Compressions during CPR.', 'date': '2015-11-10'}}",1.0,Emergency Medicine & Critical Care
158,"Is pain relief higher, lower, or the same when comparing 120 mg etoricoxib to placebo?",higher,high,no,"['15333415', '21188849', '15100590', '15220011', '15881486', '16192529']",24809657,2014,"{'15333415': {'article_id': '15333415', 'content': 'Our objective in this study was to compare the analgesic effects of etoricoxib and oxycodone/acetaminophen in a postoperative dental pain model. Patients experiencing moderate to severe pain after extraction of two or more third molars were randomized to single doses of etoricoxib 120 mg (n = 100), oxycodone/acetaminophen 10/650 mg (n = 100), or placebo (n = 25). The primary end-point was total pain relief over 6 h. Other end-points included patient global assessment of response to therapy; onset, peak, and duration of effect; and rescue opioid analgesic use. Active treatments were statistically significantly superior to placebo for all efficacy measures. Total pain relief over 6 h for etoricoxib was significantly more than for oxycodone/acetaminophen (P < 0.001). Patient global assessment of response to therapy at 6 and 24 h was superior for etoricoxib. Both drugs achieved rapid onset, although the time was faster for oxycodone/acetaminophen by 5 min. The peak effect was similar for both drugs. Compared with oxycodone/acetaminophen patients, etoricoxib patients experienced a longer analgesic duration, had a smaller percentage requiring rescue opioids during 6 and 24 h, and required less rescue analgesia during 6 and 24 h. Oxycodone/acetaminophen treatment resulted in more frequent adverse events (AEs), drug-related AEs, nausea, and vomiting compared with etoricoxib treatment. In conclusion, etoricoxib 120 mg provided superior overall efficacy compared with oxycodone/acetaminophen 10/650 mg and was associated with significantly fewer AEs.', 'title': 'The analgesic efficacy of etoricoxib compared with oxycodone/acetaminophen in an acute postoperative pain model: a randomized, double-blind clinical trial.', 'date': '2004-08-31'}, '21188849': {'article_id': '21188849', 'content': 'This study was conducted to evaluate the dose range of etoricoxib in acute pain using the postoperative dental pain model further.\nThis double-blind, randomized controlled study evaluated etoricoxib (90 and 120 mg), ibuprofen (600 mg), and acetaminophen (600 mg/codeine) (60 mg, (A/C)) in patients aged ≥ 18 years with moderate or severe pain after surgical extraction of ≥ 2 third molars (≥ 1 impacted). The patients reported pain intensity and pain relief over 24 hours. The primary efficacy endpoint was total pain relief over 6 hours (TOPAR6). Adverse events were evaluated throughout the study.\nThere were 588 patients randomized to placebo (n=46),etoricoxib (90 mg (n=191)), etoricoxib (120 mg (n=97)), ibuprofen(2400 mg (n=192)), and A/C (n=62). The overall analgesic effect (TOPAR6) of etoricoxib (90, 120 mg) was significantly greater than that of placebo (P ≤ 0.001), and not inferior to that of ibuprofen; no discernible difference was observed between etoricoxib 90 and 120 mg. Both etoricoxib doses were superior to A/C (P ≤ 0.001). Etoricoxib (90 and 120 mg) and ibuprofen(2400 mg) were generally well tolerated and had a similar incidence of adverse events (AEs). A/C was associated with significantly more AEs that led to discontinuation (ie, nausea and vomiting).\nEtoricoxib (90 and 120 mg) showed similar efficacy in the postoperative dental pain model, which was noninferior to ibuprofen and superior to A/C. A higher number of tooth extractions or a higher mean impaction score may have led to a greater separation in efficacy between the 2 etoricoxib doses.', 'title': 'Evaluation of the dose range of etoricoxib in an acute pain setting using the postoperative dental pain model.', 'date': '2010-12-29'}, '15100590': {'article_id': '15100590', 'content': ""To compare the overall analgesic effect, including time to onset, peak and duration of effect for etoricoxib 120 mg, a new COX-2 selective inhibitor, in patients with acute pain to that of placebo. Naproxen sodium 550 mg and acetaminophen/codeine 600/60 mg were the active comparators.\nA total of 201 patients with moderate to severe pain following surgical extraction of > or = 2 third molars, of which at least the mandibular tooth was impacted, were randomly allocated to receive single oral doses of placebo (n = 50), etoricoxib 120 mg (n = 50), naproxen sodium 550 mg (n = 51), or acetaminophen/codeine 600/60 mg (n = 50). The endpoints included total pain relief over 8 hours (TOPAR8, primary end point), sum of pain intensity difference over 8 hours, patient's global evaluation, onset, peak, and duration of analgesia.\nEtoricoxib 120 mg had a significantly greater least squares (LS) mean TOPAR8 score than placebo (20.9 vs 5.4; P < 0.001) and acetaminophen/codeine 600/60 mg (20.9 vs 11.5; P < 0.001), and a similar LS mean TOPAR8 score to naproxen sodium 550 mg (20.9 vs 21.3). All three active treatments had rapid onset of analgesia, median time approximately 30 minutes. The duration of analgesic effect, defined as median time to rescue medication use, was >24 hours for etoricoxib, 20.8 hours for naproxen sodium, 3.6 hours for acetaminophen/codeine, and 1.6 hours for placebo.\nEtoricoxib is a new COX-2 selective inhibitor under development for treatment of osteoarthritis, rheumatoid arthritis, and acute pain. In this study, etoricoxib 120 mg provided rapid and long-lasting pain relief to patients with moderate-to-severe postdental surgery pain. Etoricoxib was generally well tolerated."", 'title': 'A randomized, double-blind, parallel-group study comparing the analgesic effect of etoricoxib to placebo, naproxen sodium, and acetaminophen with codeine using the dental impaction pain model.', 'date': '2004-04-22'}, '15220011': {'article_id': '15220011', 'content': ""Patients experiencing acute pain after surgery, including dental surgery, often require analgesia. Ideally, the chosen analgesic should have a rapid onset and sustained effect. Etoricoxib is a new cyclooxygenase-2-selective inhibitor that has demonstrated analgesic efficacy in the treatment of acute pain with a rapid onset and long-lasting pain relief.\nThe goal of this study was to determine the analgesic effect of single oral doses of etoricoxib 60, 120, 180, and 240 mg compared with placebo in the treatment of pain after dental surgery. Ibuprofen was used as an active control.\nThis was a randomized, double-blind, parallel-group, single-dose, placebo- and active comparator-controlled study performed at a single center. It consisted of 3 visits (prestudy, treatment, and poststudy). Eligible patients were aged > or =16 years with moderate or severe pain after surgical extraction of > or =2 third molars, of which > or =1 was an impacted mandibular molar. Patients were assessed over 24 hours and reported pain intensity and pain relied at 14 predefined time points. Plasma samples for a pharmacokinetic/pharmacodynamic analysis were collected from a subset of patients at baseline and the 14 predefined time points. The end points included total pain relief over 8 hours (TOPAR8, the primary end point), sum of pain intensity difference over 8 hours, patient's global evaluation of treatment, median time to onset of pain relief (2-stopwatch method), peak pain relief, and duration of analgesic effect (median time to use of rescue medication). Adverse events were collected up to 14 days postdose.\nThree hundred ninety-eight (63.1% women, 36.9% men; mean age, 21.1 years; 72.1% white, 27.9% other; mean number of third molars removed, 3.5; 65.2% experiencing moderate pain) were randomly allocated to receive etoricoxib 60 mg (n = 75), etoricoxib 120 mg (n = 76), etoricoxib 180 mg (n = 74), etoricoxib 240 mg (n = 76), ibuprofen 400 mg (n = 48), and placebo (n = 49). All active treatments had significantly greater overall analgesic effect (TOPAR8) compared with placebo (P < or 0.001). Patients who received etoricoxib 120 and 180 mg had significantly higher TOPAR8 scores than those who received etoricoxib 60 mg ( P < = 0.001) and ibuprofen (P < 0.05 etoricoxib 120 mg; P < or = 0.001 etoricoxib 180 mg). Least-squares mean TOPAR8 scores for etoricoxib 60, 120, 180, and 240 mg, ibuprofen, and placebo were 16.0, 22.0, 23.5, 20.7, 18.6, and 5.2, respectively. The median time to onset of analgesia was 24 minutes for etoricoxib 120, 180, and 240 mg, and 30 minutes for etoricoxib 60 mg and ibuprofen. There were no significant differences in the onset of analgesia between etoricoxib 120, 180, and 240 mg and ibuprofen. The duration of analgesic effect was >24 hours for etoricoxib 120, 180, and 240 mg, and 12.1 hours for etoricoxib 60 mg. The duration of effect was significantly longer with all 4 etoricoxib doses compared with ibuprofen (10.1 hours; P < 0.05 etoricoxib 60 mg; < or = 0.001etoricoxib 120, 180, and 240 mg) and compared with placebo (2.1 hours; P < = 0.001). In the pharmacokinetic/pharmacodynamic analysis (n approximately 120), there was a linear relationship between plasma etoricoxib concentrations and pain relief scores up to the maximum observed concentration, followed by a decline in plasma concentrations with persistent analgesia. The most common adverse events were postextraction alveolitis and nausea.\nIn this dose-ranging study, etoricoxib 120 mg was determined to be the minimum dose that had maximal efficacy in patients with moderate to severe acute pain associated with dental surgery. Both etoricoxib and ibuprofen were generally well tolerated."", 'title': 'Etoricoxib in acute pain associated with dental surgery: a randomized, double-blind, placebo- and active comparator-controlled dose-ranging study.', 'date': '2004-06-29'}, '15881486': {'article_id': '15881486', 'content': 'To compare the analgesic effect of single doses of etoricoxib 120 mg, oxycodone/ acetaminophen 10 mg/650 mg and codeine/ acetaminophen 60 mg/600 mg in acute pain using the dental impaction model.\nIn this randomized, double-blind, placebo-controlled, parallel-group study, patients reported pain intensity and pain relief (16 times) and global scores (twice) during a 24-h period. The primary endpoint was the overall analgesic effect, total pain relief over 6 h (TOPAR6). Other endpoints were patient global evaluation, time to onset (2-stopwatch method), duration of analgesic effect (median time to and amount of rescue medication use). Tolerability was evaluated by overall and opioid-related (nausea and vomiting) adverse experiences.\n302 patients (mean age 23; 63% women; 63 % White) were randomized to etoricoxib 120 mg, oxycodone/acetaminophen 10 mg/650 mg, codeine/acetaminophen 60 mg/600 mg, and placebo (2:2:1:1). Etoricoxib demonstrated significantly greater overall analgesic efficacy (TOPAR6) (13.2 units) versus oxycodone/acetaminophen (10.2 units); and codeine/acetaminophen (6.0 units); p < 0.001 for all. All active treatments were superior to placebo. Median time to onset was significantly (p < 0.001) shorter for oxycodone/acetaminophen (20 min) and numerically but not significantly shorter (p = 0.259) for codeine/acetaminophen (26 min) compared with etoricoxib (40 min). Etoricoxib (24 h) had a significantly longer lasting analgesic effect than oxycodone/acetaminophen (5.3 h), codeine/acetaminophen (2.7 h), and placebo (1.7 h) (p < 0.001 for all). Etoricoxib patients experienced fewer clinical adverse experiences than patients on oxycodone/acetaminophen and codeine/acetaminophen, specifically, significantly (p < 0.05) fewer episodes of nausea.\nEtoricoxib 120 mg provided superior overall analgesic effect with a smaller percentage of patients experiencing nausea versus both oxycodone/acetaminophen 10 mg/650 mg and codeine/acetaminophen 60 mg/600 mg.', 'title': 'The analgesic effect of etoricoxib relative to that of cetaminophen analgesics: a randomized, controlled single-dose study in acute dental impaction pain.', 'date': '2005-05-11'}, '16192529': {'article_id': '16192529', 'content': 'In this randomized, double-blind, placebo-controlled, multicenter study we assessed the analgesic effect of etoricoxib (a new cyclooxygenase-2 inhibitor) in patients having had knee or hip replacement surgery. A total of 228 patients with moderate or severe pain were randomly allocated within 72 h after surgery to receive etoricoxib 120 mg, controlled-release naproxen sodium 1100 mg, or placebo (1:1:1) on day 1 followed by etoricoxib and placebo (1:2) on days 2 to 7. Patients reported pain scores, rescue (opioid-combination) medication use, and the response to study drug. On day 1, etoricoxib provided an analgesic effect superior to placebo and similar to controlled-release naproxen sodium as demonstrated by the total pain relief score over 8 h, the primary end-point; least-squares mean scores were 11.0, 11.5, and 5.6, respectively (P < 0.001 versus placebo). Similarly, a larger percentage of patients receiving etoricoxib and naproxen sodium than those receiving placebo reported good to excellent responses to study drug: 53%, 60%, and 26% respectively. On days 2-7, etoricoxib demonstrated a significant reduction of rescue medication use, 35% (P < 0.001 versus placebo). The clinical relevance of the decrease was confirmed by Patient\'s Global Evaluation (P < 0.05 versus placebo). Patients receiving etoricoxib also experienced significantly less ""worst"" and ""average"" pain than did those on placebo. Etoricoxib was generally well tolerated in this study; the incidence of adverse experiences was infrequent and similar across treatment groups. In summary, etoricoxib provided analgesia that was similar to controlled-release naproxen sodium on day 1 and superior to placebo with reduced supplemental opioid use over 7 days.\nIn a postsurgery setting (knee and hip replacements), etoricoxib 120 mg provided analgesia superior to placebo and similar to controlled-release naproxen sodium 1100 mg. Patients receiving etoricoxib suffered less pain and took less opioid rescue medication compared with patients on placebo.', 'title': 'Etoricoxib provides analgesic efficacy to patients after knee or hip replacement surgery: a randomized, double-blind, placebo-controlled study.', 'date': '2005-09-30'}}",1.0,Surgery
159,"Is overall survival higher, lower, or the same when comparing chemoradiotherapy plus esophagectomy to chemoradiotherapy alone?",no difference,high,no,"['17401004', '15800321']",28829911,2017,"{'17401004': {'article_id': '17401004', 'content': 'Uncontrolled studies suggest that chemoradiation has similar efficacy as surgery for esophageal cancer. Therefore, a randomized trial was carried out to compare, in responders only, chemoradiation alone with chemoradiation followed by surgery in patients with locally advanced tumors.\nEligible patients had operable T3N0-1M0 thoracic esophageal cancer. Patients received two cycles of fluorouracil (FU) and cisplatin (days 1 to 5 and 22 to 26) and either conventional (46 Gy in 4.5 weeks) or split-course (15 Gy, days 1 to 5 and 22 to 26) concomitant radiotherapy. Patients with response and no contraindication to either treatment were randomly assigned to surgery (arm A) or continuation of chemoradiation (arm B; three cycles of FU/cisplatin and either conventional [20 Gy] or split-course [15 Gy] radiotherapy). Chemoradiation was considered equivalent to surgery if the difference in 2-year survival rate was less than 10%.\nOf 444 eligible patients, 259 were randomly assigned; 230 patients (88.8%) had epidermoid cancer, and 29 (11.2%) had glandular carcinoma. Two-year survival rate was 34% in arm A versus 40% in arm B (hazard ratio for arm B v arm A = 0.90; adjusted P = .44). Median survival time was 17.7 months in arm A compared with 19.3 months in arm B. Two-year local control rate was 66.4% in arm A compared with 57.0% in arm B, and stents were less required in the surgery arm (5% in arm A v 32% in arm B; P < .001). The 3-month mortality rate was 9.3% in arm A compared with 0.8% in arm B (P = .002). Cumulative hospital stay was 68 days in arm A compared with 52 days in arm B (P = .02).\nOur data suggest that, in patients with locally advanced thoracic esophageal cancers, especially epidermoid, who respond to chemoradiation, there is no benefit for the addition of surgery after chemoradiation compared with the continuation of additional chemoradiation.', 'title': 'Chemoradiation followed by surgery compared with chemoradiation alone in squamous cancer of the esophagus: FFCD 9102.', 'date': '2007-04-03'}, '15800321': {'article_id': '15800321', 'content': 'Combined chemoradiotherapy with and without surgery are widely accepted alternatives for the curative treatment of patients with locally advanced esophageal cancer. The value of adding surgery to chemotherapy and radiotherapy is unknown.\nPatients with locally advanced squamous cell carcinoma (SCC) of the esophagus were randomly allocated to either induction chemotherapy followed by chemoradiotherapy (40 Gy) followed by surgery (arm A), or the same induction chemotherapy followed by chemoradiotherapy (at least 65 Gy) without surgery (arm B). Primary outcome was overall survival time.\nThe median observation time was 6 years. The analysis of 172 eligible, randomized patients (86 patients per arm) showed overall survival to be equivalent between the two treatment groups (log-rank test for equivalence, P < .05). Local progression-free survival was better in the surgery group (2-year progression-free survival, 64.3%; 95% CI, 52.1% to 76.5%) than in the chemoradiotherapy group (2-year progression-free survival, 40.7%; 95% CI, 28.9% to 52.5%; hazard ratio [HR] for arm B v arm A, 2.1; 95% CI, 1.3 to 3.5; P = .003). Treatment-related mortality was significantly increased in the surgery group than in the chemoradiotherapy group (12.8% v 3.5%, respectively; P = .03). Cox regression analysis revealed clinical tumor response to induction chemotherapy to be the single independent prognostic factor for overall survival (HR, 0.30; 95% CI, 0.19 to 0.47; P < .0001).\nAdding surgery to chemoradiotherapy improves local tumor control but does not increase survival of patients with locally advanced esophageal SCC. Tumor response to induction chemotherapy identifies a favorable prognostic group within these high-risk patients, regardless of the treatment group.', 'title': 'Chemoradiation with and without surgery in patients with locally advanced squamous cell carcinoma of the esophagus.', 'date': '2005-04-01'}}",1.0,Surgery
160,"Is the risk of treatment‐related mortality higher, lower, or the same when comparing chemoradiotherapy plus esophagectomy to chemoradiotherapy alone?",higher,low,no,"['17401004', '15800321']",28829911,2017,"{'17401004': {'article_id': '17401004', 'content': 'Uncontrolled studies suggest that chemoradiation has similar efficacy as surgery for esophageal cancer. Therefore, a randomized trial was carried out to compare, in responders only, chemoradiation alone with chemoradiation followed by surgery in patients with locally advanced tumors.\nEligible patients had operable T3N0-1M0 thoracic esophageal cancer. Patients received two cycles of fluorouracil (FU) and cisplatin (days 1 to 5 and 22 to 26) and either conventional (46 Gy in 4.5 weeks) or split-course (15 Gy, days 1 to 5 and 22 to 26) concomitant radiotherapy. Patients with response and no contraindication to either treatment were randomly assigned to surgery (arm A) or continuation of chemoradiation (arm B; three cycles of FU/cisplatin and either conventional [20 Gy] or split-course [15 Gy] radiotherapy). Chemoradiation was considered equivalent to surgery if the difference in 2-year survival rate was less than 10%.\nOf 444 eligible patients, 259 were randomly assigned; 230 patients (88.8%) had epidermoid cancer, and 29 (11.2%) had glandular carcinoma. Two-year survival rate was 34% in arm A versus 40% in arm B (hazard ratio for arm B v arm A = 0.90; adjusted P = .44). Median survival time was 17.7 months in arm A compared with 19.3 months in arm B. Two-year local control rate was 66.4% in arm A compared with 57.0% in arm B, and stents were less required in the surgery arm (5% in arm A v 32% in arm B; P < .001). The 3-month mortality rate was 9.3% in arm A compared with 0.8% in arm B (P = .002). Cumulative hospital stay was 68 days in arm A compared with 52 days in arm B (P = .02).\nOur data suggest that, in patients with locally advanced thoracic esophageal cancers, especially epidermoid, who respond to chemoradiation, there is no benefit for the addition of surgery after chemoradiation compared with the continuation of additional chemoradiation.', 'title': 'Chemoradiation followed by surgery compared with chemoradiation alone in squamous cancer of the esophagus: FFCD 9102.', 'date': '2007-04-03'}, '15800321': {'article_id': '15800321', 'content': 'Combined chemoradiotherapy with and without surgery are widely accepted alternatives for the curative treatment of patients with locally advanced esophageal cancer. The value of adding surgery to chemotherapy and radiotherapy is unknown.\nPatients with locally advanced squamous cell carcinoma (SCC) of the esophagus were randomly allocated to either induction chemotherapy followed by chemoradiotherapy (40 Gy) followed by surgery (arm A), or the same induction chemotherapy followed by chemoradiotherapy (at least 65 Gy) without surgery (arm B). Primary outcome was overall survival time.\nThe median observation time was 6 years. The analysis of 172 eligible, randomized patients (86 patients per arm) showed overall survival to be equivalent between the two treatment groups (log-rank test for equivalence, P < .05). Local progression-free survival was better in the surgery group (2-year progression-free survival, 64.3%; 95% CI, 52.1% to 76.5%) than in the chemoradiotherapy group (2-year progression-free survival, 40.7%; 95% CI, 28.9% to 52.5%; hazard ratio [HR] for arm B v arm A, 2.1; 95% CI, 1.3 to 3.5; P = .003). Treatment-related mortality was significantly increased in the surgery group than in the chemoradiotherapy group (12.8% v 3.5%, respectively; P = .03). Cox regression analysis revealed clinical tumor response to induction chemotherapy to be the single independent prognostic factor for overall survival (HR, 0.30; 95% CI, 0.19 to 0.47; P < .0001).\nAdding surgery to chemoradiotherapy improves local tumor control but does not increase survival of patients with locally advanced esophageal SCC. Tumor response to induction chemotherapy identifies a favorable prognostic group within these high-risk patients, regardless of the treatment group.', 'title': 'Chemoradiation with and without surgery in patients with locally advanced squamous cell carcinoma of the esophagus.', 'date': '2005-04-01'}}",1.0,Surgery
161,"Is short-term quality of life higher, lower, or the same when comparing chemoradiotherapy plus esophagectomy to chemoradiotherapy alone?",lower,very low,no,['16524973'],28829911,2017,"{'16524973': {'article_id': '16524973', 'content': 'The aim of the study was to compare the longitudinal quality of life (QoL) between chemoradiation with or without surgery in patients with locally advanced squamous resectable esophageal cancer included in a randomized multicenter phase III trial (FFCD 9102).\nAll patients with locally advanced resectable (T3-4 N0-1 M0) epidermoid or glandular esophageal cancer (n = 451) received induction chemoradiation. Responders (n = 259) were randomized between surgery (arm A) and continuation of chemoradiation (arm B). The Spitzer QoL Index was scored (0-10) at inclusion and at each follow-up, every 3 months during 2 years. QoL at baseline and longitudinal changes were respectively compared with univariate ANOVA and mixed-model analysis of variance for repeated measurements. The time interval between the follow-up was assessed and the same analyses were performed among survivors with 2 years of follow-up.\nThe squamous histology was predominant in both arms. The mean QoL score decreased between baseline and the first follow-up and between the first and the second follow-ups. QoL scores at the first follow-up were comparatively worse in arm A than in arm B (7.52 versus 8.45, P < 0.01), whereas the longitudinal QoL study showed no difference between treatments (adjusted P = 0.26). Furthermore, the longitudinal QoL was not different (adjusted P = 0.23) among survivors with 2 years of follow-up.\nAmong patients responding to induction chemoradiation, surgery and continuation of chemoradiation had the same impact on QoL in patients with locally advanced, resectable esophageal cancer although a significantly greater decrease in the Spitzer Index was observed in the postoperative period.', 'title': 'A comparative longitudinal quality of life study using the Spitzer quality of life index in a randomized multicenter phase III trial (FFCD 9102): chemoradiation followed by surgery compared with chemoradiation alone in locally advanced squamous resectable thoracic esophageal cancer.', 'date': '2006-03-10'}}",1.0,Surgery
162,"Is neonatal mortality higher, lower, or the same when comparing community-led chlorhexidine skin cleansing to community-led usual skin care?",no difference,high,no,['17210728'],25739381,2015,"{'17210728': {'article_id': '17210728', 'content': 'Hospital-based data from Africa suggest that newborn skin-cleansing with chlorhexidine may reduce neonatal mortality. Evaluation of this intervention in the communities where most births occur in the home has not been done. Our objective was to assess the efficacy of a 1-time skin-cleansing of newborn infants with 0.25% chlorhexidine on neonatal mortality.\nThe design was a community-based, placebo-controlled, cluster-randomized trial in Sarlahi District in southern Nepal. Newborn infants were cleansed with infant wipes that contained 0.25% chlorhexidine or placebo solution as soon as possible after delivery in the home (median: 5.8 hours). The primary outcome was all-cause mortality by 28 days. After the completion of the randomized phase, all newborns in study clusters were converted to chlorhexidine treatment for the subsequent 9 months.\nA total of 17,530 live births occurred in the enrolled sectors, 8650 and 8880 in the chlorhexidine and placebo groups, respectively. Baseline characteristics were similar in the treatment groups. Intention-to-treat analysis among all live births showed no impact of the intervention on neonatal mortality. Among live-born infants who actually received their assigned treatment (98.7%), there was a nonsignificant 11% lower neonatal mortality rate among those who were treated with chlorhexidine compared with placebo. Low birth weight infants had a statistically significant 28% reduction in neonatal mortality; there was no significant difference among infants who were born weighing > or = 2500 g. After conversion to active treatment in the placebo clusters, there was a 37% reduction in mortality among low birth weight infants in the placebo clusters versus no change in the chlorhexidine clusters.\nNewborn skin-wiping with chlorhexidine solution once, soon after birth, reduced neonatal mortality only among low birth weight infants. Evidence from additional trials is needed to determine whether this inexpensive and simple intervention could improve survival significantly among low birth weight infants in settings where home delivery is common and hygiene practices are poor.', 'title': 'Impact of newborn skin-cleansing with chlorhexidine on neonatal mortality in southern Nepal: a community-based, cluster-randomized trial.', 'date': '2007-01-11'}}",1.0,Pediatrics & Neonatology
163,"Is neonatal mortality higher, lower, or the same when comparing community-led chlorhexidine cord cleansing to community-led dry cord care?",lower,high,no,"['22322124', '16546539', '22322126']",25739381,2015,"{'22322124': {'article_id': '22322124', 'content': ""Up to half of neonatal deaths in high mortality settings are due to infections, many of which can originate through the freshly cut umbilical cord stump. We aimed to assess the effectiveness of two cord-cleansing regimens with the promotion of dry cord care in the prevention of neonatal mortality.\nWe did a community-based, parallel cluster-randomised trial in Sylhet, Bangladesh. We divided the study area into 133 clusters, which were randomly assigned to one of the two chlorhexidine cleansing regimens (single cleansing as soon as possible after birth; daily cleansing for 7 days after birth) or promotion of dry cord care. Randomisation was done by use of a computer-generated sequence, stratified by cluster-specific participation in a previous trial. All livebirths were eligible; those visited within 7 days by a local female village health worker trained to deliver the cord care intervention were enrolled. We did not mask study workers and participants to the study interventions. Our primary outcome was neonatal mortality (within 28 days of birth) per 1000 livebirths, which we analysed on an intention-to-treat basis. This trial is registered with ClinicalTrials.gov, number NCT00434408.\nBetween June, 2007, and September, 2009, we enrolled 29\u2008760 newborn babies (10\u2008329, 9423, and 10\u2008008 in the multiple-cleansing, single-cleansing, and dry cord care groups, respectively). Neonatal mortality was lower in the single-cleansing group (22·5 per 1000 livebirths) than it was in the dry cord care group (28·3 per 1000 livebirths; relative risk [RR] 0·80 [95% CI] 0·65-0·98). Neonatal mortality in the multiple-cleansing group (26·6 per 1000 livebirths) was not statistically significantly lower than it was in the dry cord care group (RR 0·94 [0·78-1·14]). Compared with the dry cord care group, we recorded a statistically significant reduction in the occurrence of severe cord infection (redness with pus) in the multiple-cleansing group (risk per 1000 livebirths=4·2 vs risk per 1000 livebirths=1·2; RR 0·35 [0·15-0·81]) but not in the single-cleansing group (risk per 1000 livebirths=3·3; RR 0·77 [0·40-1·48]).\nChlorhexidine cleansing of a neonate's umbilical cord can save lives, but further studies are needed to establish the best frequency with which to deliver the intervention.\nUnited States Agency for International Development and Save the Children's Saving Newborn Lives program, through a grant from the Bill & Melinda Gates Foundation."", 'title': 'The effect of cord cleansing with chlorhexidine on neonatal mortality in rural Bangladesh: a community-based, cluster-randomised trial.', 'date': '2012-02-11'}, '16546539': {'article_id': '16546539', 'content': 'Omphalitis contributes to neonatal morbidity and mortality in developing countries. Umbilical cord cleansing with antiseptics might reduce infection and mortality risk, but has not been rigorously investigated.\nIn our community-based, cluster-randomised trial, 413 communities in Sarlahi, Nepal, were randomly assigned to one of three cord-care regimens. 4934 infants were assigned to 4.0% chlorhexidine, 5107 to cleansing with soap and water, and 5082 to dry cord care. In intervention clusters, the newborn cord was cleansed in the home on days 1-4, 6, 8, and 10. In all clusters, the cord was examined for signs of infection (pus, redness, or swelling) on these visits and in follow-up visits on days 12, 14, 21, and 28. Incidence of omphalitis was defined under three sign-based algorithms, with increasing severity. Infant vital status was recorded for 28 completed days. The primary outcomes were incidence of neonatal omphalitis and neonatal mortality. Analysis was by intention-to-treat. This trial is registered with , number NCT00109616.\nFrequency of omphalitis by all three definitions was reduced significantly in the chlorhexidine group. Severe omphalitis in chlorhexidine clusters was reduced by 75% (incidence rate ratio 0.25, 95% CI 0.12-0.53; 13 infections/4839 neonatal periods) compared with dry cord-care clusters (52/4930). Neonatal mortality was 24% lower in the chlorhexidine group (relative risk 0.76 [95% CI 0.55-1.04]) than in the dry cord care group. In infants enrolled within the first 24 h, mortality was significantly reduced by 34% in the chlorhexidine group (0.66 [0.46-0.95]). Soap and water did not reduce infection or mortality risk.\nRecommendations for dry cord care should be reconsidered on the basis of these findings that early antisepsis with chlorhexidine of the umbilical cord reduces local cord infections and overall neonatal mortality.', 'title': 'Topical applications of chlorhexidine to the umbilical cord for prevention of omphalitis and neonatal mortality in southern Nepal: a community-based, cluster-randomised trial.', 'date': '2006-03-21'}, '22322126': {'article_id': '22322126', 'content': 'Umbilical cord infection (omphalitis) is a risk factor for neonatal sepsis and mortality in low-resource settings where home deliveries are common. We aimed to assess the effect of umbilical-cord cleansing with 4% chlorhexidine (CHX) solution, with or without handwashing with antiseptic soap, on the incidence of omphalitis and neonatal mortality.\nWe did a two-by-two factorial, cluster-randomised trial in Dadu, a rural area of Sindh province, Pakistan. Clusters were defined as the population covered by a functional traditional birth attendant (TBA), and were randomly allocated to one of four groups (groups A to D) with a computer-generated random number sequence. Implementation and data collection teams were masked to allocation. Liveborn infants delivered by participating TBAs who received birth kits were eligible for enrolment in the study. One intervention comprised birth kits containing 4% CHX solution for application to the cord at birth by TBAs and once daily by family members for up to 14 days along with soap and educational messages promoting handwashing. One intervention was CHX solution only and another was handwashing only. Standard dry cord care was promoted in the control group. The primary outcomes were incidence of neonatal omphalitis and neonatal mortality. The trial is registered with ClinicalTrials.gov, number NCT00682006.\n187 clusters were randomly allocated to one of the four study groups. Of 9741 newborn babies delivered by participating TBAs, factorial analysis indicated a reduction in risk of omphalitis with CHX application (risk ratio [RR]=0·58, 95% CI 0·41-0·82; p=0·002) but no evidence of an effect of handwashing (RR=0·83, 0·61-1·13; p=0·24). We recorded strong evidence of a reduction in neonatal mortality in neonates who received CHX cleansing (RR=0·62, 95 % CI 0·45-0·85; p=0·003) but no evidence of an effect of handwashing promotion on neonatal mortality (RR=1·08, 0·79-1·48; p=0·62). We recorded no serious adverse events.\nApplication of 4% CHX to the umbilical cord was effective in reducing the risk of omphalitis and neonatal mortality in rural Pakistan. Provision of CHX in birth kits might be a useful strategy for the prevention of neonatal mortality in high-mortality settings.\nThe United States Agency for International Development.', 'title': 'Topical application of chlorhexidine to neonatal umbilical cords for prevention of omphalitis and neonatal mortality in a rural district of Pakistan: a community-based, cluster-randomised trial.', 'date': '2012-02-11'}}",1.0,Pediatrics & Neonatology
164,"Is the risk of omphalitis/infections higher, lower, or the same when comparing community-led chlorhexidine cord cleansing to community-led dry cord care?",lower,high,no,"['22322124', '16546539', '22322126']",25739381,2015,"{'22322124': {'article_id': '22322124', 'content': ""Up to half of neonatal deaths in high mortality settings are due to infections, many of which can originate through the freshly cut umbilical cord stump. We aimed to assess the effectiveness of two cord-cleansing regimens with the promotion of dry cord care in the prevention of neonatal mortality.\nWe did a community-based, parallel cluster-randomised trial in Sylhet, Bangladesh. We divided the study area into 133 clusters, which were randomly assigned to one of the two chlorhexidine cleansing regimens (single cleansing as soon as possible after birth; daily cleansing for 7 days after birth) or promotion of dry cord care. Randomisation was done by use of a computer-generated sequence, stratified by cluster-specific participation in a previous trial. All livebirths were eligible; those visited within 7 days by a local female village health worker trained to deliver the cord care intervention were enrolled. We did not mask study workers and participants to the study interventions. Our primary outcome was neonatal mortality (within 28 days of birth) per 1000 livebirths, which we analysed on an intention-to-treat basis. This trial is registered with ClinicalTrials.gov, number NCT00434408.\nBetween June, 2007, and September, 2009, we enrolled 29\u2008760 newborn babies (10\u2008329, 9423, and 10\u2008008 in the multiple-cleansing, single-cleansing, and dry cord care groups, respectively). Neonatal mortality was lower in the single-cleansing group (22·5 per 1000 livebirths) than it was in the dry cord care group (28·3 per 1000 livebirths; relative risk [RR] 0·80 [95% CI] 0·65-0·98). Neonatal mortality in the multiple-cleansing group (26·6 per 1000 livebirths) was not statistically significantly lower than it was in the dry cord care group (RR 0·94 [0·78-1·14]). Compared with the dry cord care group, we recorded a statistically significant reduction in the occurrence of severe cord infection (redness with pus) in the multiple-cleansing group (risk per 1000 livebirths=4·2 vs risk per 1000 livebirths=1·2; RR 0·35 [0·15-0·81]) but not in the single-cleansing group (risk per 1000 livebirths=3·3; RR 0·77 [0·40-1·48]).\nChlorhexidine cleansing of a neonate's umbilical cord can save lives, but further studies are needed to establish the best frequency with which to deliver the intervention.\nUnited States Agency for International Development and Save the Children's Saving Newborn Lives program, through a grant from the Bill & Melinda Gates Foundation."", 'title': 'The effect of cord cleansing with chlorhexidine on neonatal mortality in rural Bangladesh: a community-based, cluster-randomised trial.', 'date': '2012-02-11'}, '16546539': {'article_id': '16546539', 'content': 'Omphalitis contributes to neonatal morbidity and mortality in developing countries. Umbilical cord cleansing with antiseptics might reduce infection and mortality risk, but has not been rigorously investigated.\nIn our community-based, cluster-randomised trial, 413 communities in Sarlahi, Nepal, were randomly assigned to one of three cord-care regimens. 4934 infants were assigned to 4.0% chlorhexidine, 5107 to cleansing with soap and water, and 5082 to dry cord care. In intervention clusters, the newborn cord was cleansed in the home on days 1-4, 6, 8, and 10. In all clusters, the cord was examined for signs of infection (pus, redness, or swelling) on these visits and in follow-up visits on days 12, 14, 21, and 28. Incidence of omphalitis was defined under three sign-based algorithms, with increasing severity. Infant vital status was recorded for 28 completed days. The primary outcomes were incidence of neonatal omphalitis and neonatal mortality. Analysis was by intention-to-treat. This trial is registered with , number NCT00109616.\nFrequency of omphalitis by all three definitions was reduced significantly in the chlorhexidine group. Severe omphalitis in chlorhexidine clusters was reduced by 75% (incidence rate ratio 0.25, 95% CI 0.12-0.53; 13 infections/4839 neonatal periods) compared with dry cord-care clusters (52/4930). Neonatal mortality was 24% lower in the chlorhexidine group (relative risk 0.76 [95% CI 0.55-1.04]) than in the dry cord care group. In infants enrolled within the first 24 h, mortality was significantly reduced by 34% in the chlorhexidine group (0.66 [0.46-0.95]). Soap and water did not reduce infection or mortality risk.\nRecommendations for dry cord care should be reconsidered on the basis of these findings that early antisepsis with chlorhexidine of the umbilical cord reduces local cord infections and overall neonatal mortality.', 'title': 'Topical applications of chlorhexidine to the umbilical cord for prevention of omphalitis and neonatal mortality in southern Nepal: a community-based, cluster-randomised trial.', 'date': '2006-03-21'}, '22322126': {'article_id': '22322126', 'content': 'Umbilical cord infection (omphalitis) is a risk factor for neonatal sepsis and mortality in low-resource settings where home deliveries are common. We aimed to assess the effect of umbilical-cord cleansing with 4% chlorhexidine (CHX) solution, with or without handwashing with antiseptic soap, on the incidence of omphalitis and neonatal mortality.\nWe did a two-by-two factorial, cluster-randomised trial in Dadu, a rural area of Sindh province, Pakistan. Clusters were defined as the population covered by a functional traditional birth attendant (TBA), and were randomly allocated to one of four groups (groups A to D) with a computer-generated random number sequence. Implementation and data collection teams were masked to allocation. Liveborn infants delivered by participating TBAs who received birth kits were eligible for enrolment in the study. One intervention comprised birth kits containing 4% CHX solution for application to the cord at birth by TBAs and once daily by family members for up to 14 days along with soap and educational messages promoting handwashing. One intervention was CHX solution only and another was handwashing only. Standard dry cord care was promoted in the control group. The primary outcomes were incidence of neonatal omphalitis and neonatal mortality. The trial is registered with ClinicalTrials.gov, number NCT00682006.\n187 clusters were randomly allocated to one of the four study groups. Of 9741 newborn babies delivered by participating TBAs, factorial analysis indicated a reduction in risk of omphalitis with CHX application (risk ratio [RR]=0·58, 95% CI 0·41-0·82; p=0·002) but no evidence of an effect of handwashing (RR=0·83, 0·61-1·13; p=0·24). We recorded strong evidence of a reduction in neonatal mortality in neonates who received CHX cleansing (RR=0·62, 95 % CI 0·45-0·85; p=0·003) but no evidence of an effect of handwashing promotion on neonatal mortality (RR=1·08, 0·79-1·48; p=0·62). We recorded no serious adverse events.\nApplication of 4% CHX to the umbilical cord was effective in reducing the risk of omphalitis and neonatal mortality in rural Pakistan. Provision of CHX in birth kits might be a useful strategy for the prevention of neonatal mortality in high-mortality settings.\nThe United States Agency for International Development.', 'title': 'Topical application of chlorhexidine to neonatal umbilical cords for prevention of omphalitis and neonatal mortality in a rural district of Pakistan: a community-based, cluster-randomised trial.', 'date': '2012-02-11'}}",0.666666667,Pediatrics & Neonatology
225,"Is the risk of omphalitis/infections higher, lower, or the same when comparing community-led chlorhexidine skin cleansing to community-led usual skin care?",insufficient data,,no,['17210728'],25739381,2015,"{'17210728': {'article_id': '17210728', 'content': 'Hospital-based data from Africa suggest that newborn skin-cleansing with chlorhexidine may reduce neonatal mortality. Evaluation of this intervention in the communities where most births occur in the home has not been done. Our objective was to assess the efficacy of a 1-time skin-cleansing of newborn infants with 0.25% chlorhexidine on neonatal mortality.\nThe design was a community-based, placebo-controlled, cluster-randomized trial in Sarlahi District in southern Nepal. Newborn infants were cleansed with infant wipes that contained 0.25% chlorhexidine or placebo solution as soon as possible after delivery in the home (median: 5.8 hours). The primary outcome was all-cause mortality by 28 days. After the completion of the randomized phase, all newborns in study clusters were converted to chlorhexidine treatment for the subsequent 9 months.\nA total of 17,530 live births occurred in the enrolled sectors, 8650 and 8880 in the chlorhexidine and placebo groups, respectively. Baseline characteristics were similar in the treatment groups. Intention-to-treat analysis among all live births showed no impact of the intervention on neonatal mortality. Among live-born infants who actually received their assigned treatment (98.7%), there was a nonsignificant 11% lower neonatal mortality rate among those who were treated with chlorhexidine compared with placebo. Low birth weight infants had a statistically significant 28% reduction in neonatal mortality; there was no significant difference among infants who were born weighing > or = 2500 g. After conversion to active treatment in the placebo clusters, there was a 37% reduction in mortality among low birth weight infants in the placebo clusters versus no change in the chlorhexidine clusters.\nNewborn skin-wiping with chlorhexidine solution once, soon after birth, reduced neonatal mortality only among low birth weight infants. Evidence from additional trials is needed to determine whether this inexpensive and simple intervention could improve survival significantly among low birth weight infants in settings where home delivery is common and hygiene practices are poor.', 'title': 'Impact of newborn skin-cleansing with chlorhexidine on neonatal mortality in southern Nepal: a community-based, cluster-randomized trial.', 'date': '2007-01-11'}}",1.0,Pediatrics & Neonatology
165,"Is distance visual acuity higher, lower, or the same when comparing vision screening alone to no vision screening?",no difference,high,no,['18614568'],29460275,2018,"{'18614568': {'article_id': '18614568', 'content': ""To assess the effects of vision screening, and subsequent management of visual impairment, on visual acuity and vision-related quality of life among frail older people.\nRandomised controlled trial.\nCommunity in Sydney, Australia.\n616 men and women aged 70 years and over (mean age 81 years) recruited mainly from people attending outpatient aged care services.\nNo vision assessment or intervention\nComprehensive vision and eye examinations conducted by an optometrist. Three hundred subjects were seen by the study optometrist, with 146 judged to need treatment for a vision or eye problem. The optometrist arranged new glasses for 92 subjects; 24 were referred for a home visit by an occupational therapist; 17 were referred for glaucoma management; and 15 were referred for cataract surgery.\nDistance and near visual acuity (logMAR) and composite scores on the 25-item version of the National Eye Institute Visual Function Questionnaire, both assessed at a 12-month follow-up home visit.\nAfter 12 months' follow-up, the mean (logMAR) distance visual acuity was 0.27 in the intervention group and 0.25 in the control group (p = 0.32). The mean (logMAR) near visual acuities were -0.01 in the intervention group and -0.03 in the control group (p = 0.26). The mean composite score on the National Eye Institute Visual Function Questionnaire was 84.3 in the intervention group and 86.4 in the control group (p = 0.49).\nVision screening by an optometrist for frail older people living in the community in Australia does not lead to improvements in vision or vision-related quality of life after 1 year's follow-up."", 'title': 'Vision screening for frail older people: a randomised trial.', 'date': '2008-07-11'}}",0.0,"Public Health, Epidemiology & Health Systems"
166,"Is near visual acuity higher, lower, or the same when comparing vision screening alone to no vision screening?",no difference,high,no,['18614568'],29460275,2018,"{'18614568': {'article_id': '18614568', 'content': ""To assess the effects of vision screening, and subsequent management of visual impairment, on visual acuity and vision-related quality of life among frail older people.\nRandomised controlled trial.\nCommunity in Sydney, Australia.\n616 men and women aged 70 years and over (mean age 81 years) recruited mainly from people attending outpatient aged care services.\nNo vision assessment or intervention\nComprehensive vision and eye examinations conducted by an optometrist. Three hundred subjects were seen by the study optometrist, with 146 judged to need treatment for a vision or eye problem. The optometrist arranged new glasses for 92 subjects; 24 were referred for a home visit by an occupational therapist; 17 were referred for glaucoma management; and 15 were referred for cataract surgery.\nDistance and near visual acuity (logMAR) and composite scores on the 25-item version of the National Eye Institute Visual Function Questionnaire, both assessed at a 12-month follow-up home visit.\nAfter 12 months' follow-up, the mean (logMAR) distance visual acuity was 0.27 in the intervention group and 0.25 in the control group (p = 0.32). The mean (logMAR) near visual acuities were -0.01 in the intervention group and -0.03 in the control group (p = 0.26). The mean composite score on the National Eye Institute Visual Function Questionnaire was 84.3 in the intervention group and 86.4 in the control group (p = 0.49).\nVision screening by an optometrist for frail older people living in the community in Australia does not lead to improvements in vision or vision-related quality of life after 1 year's follow-up."", 'title': 'Vision screening for frail older people: a randomised trial.', 'date': '2008-07-11'}}",0.0,"Public Health, Epidemiology & Health Systems"
167,"Is visually-related quality of life higher, lower, or the same when comparing vision screening alone to no vision screening?",no difference,high,no,['18614568'],29460275,2018,"{'18614568': {'article_id': '18614568', 'content': ""To assess the effects of vision screening, and subsequent management of visual impairment, on visual acuity and vision-related quality of life among frail older people.\nRandomised controlled trial.\nCommunity in Sydney, Australia.\n616 men and women aged 70 years and over (mean age 81 years) recruited mainly from people attending outpatient aged care services.\nNo vision assessment or intervention\nComprehensive vision and eye examinations conducted by an optometrist. Three hundred subjects were seen by the study optometrist, with 146 judged to need treatment for a vision or eye problem. The optometrist arranged new glasses for 92 subjects; 24 were referred for a home visit by an occupational therapist; 17 were referred for glaucoma management; and 15 were referred for cataract surgery.\nDistance and near visual acuity (logMAR) and composite scores on the 25-item version of the National Eye Institute Visual Function Questionnaire, both assessed at a 12-month follow-up home visit.\nAfter 12 months' follow-up, the mean (logMAR) distance visual acuity was 0.27 in the intervention group and 0.25 in the control group (p = 0.32). The mean (logMAR) near visual acuities were -0.01 in the intervention group and -0.03 in the control group (p = 0.26). The mean composite score on the National Eye Institute Visual Function Questionnaire was 84.3 in the intervention group and 86.4 in the control group (p = 0.49).\nVision screening by an optometrist for frail older people living in the community in Australia does not lead to improvements in vision or vision-related quality of life after 1 year's follow-up."", 'title': 'Vision screening for frail older people: a randomised trial.', 'date': '2008-07-11'}}",1.0,Family Medicine & Preventive Care
168,"Is the risk of visual impairment higher, lower, or the same when comparing a detailed health assessment including measurement of visual acuity to a brief health assessment including one question about vision (standard care)?",no difference,moderate,no,['14593039'],29460275,2018,"{'14593039': {'article_id': '14593039', 'content': 'To determine the effectiveness of screening for visual impairment in people aged 75 or over as part of a multidimensional screening programme.\nCluster randomised trial.\nGeneral practices in the United Kingdom participating in the MRC trial of assessment and management of older people in the community.\n4340 people aged 75 years or over randomly sampled from 20 general practices, excluding people resident in hospitals or nursing homes.\nVisual acuity testing and referral to eye services for people with visual impairment. Universal screening (assessment and visual acuity testing) was compared with targeted screening, in which only participants with a range of health related problems were offered an assessment that included acuity screening.\nProportion of people with visual acuity less than 6/18 in either eye; mean composite score of 25 item version of the National Eye Institute visual function questionnaire.\nThree to five years after screening, the relative risk of having visual acuity < 6/18 in either eye, comparing universal with targeted screening, was 1.07 (95% confidence interval 0.84 to 1.36; P = 0.58). The mean composite score of the visual function questionnaire was 85.6 in the targeted screening group and 86.0 in the universal group (difference 0.4, 95% confidence interval -1.7 to 2.5, P = 0.69).\nIncluding a vision screening component by a practice nurse in a pragmatic trial of multidimensional screening for older people did not lead to improved visual outcomes.', 'title': 'Screening older people for impaired vision in primary care: cluster randomised trial.', 'date': '2003-11-01'}}",0.0,"Public Health, Epidemiology & Health Systems"
169,"Is visually-related quality of life higher, lower, or the same when comparing a detailed health assessment including measurement of visual acuity to a brief health assessment including one question about vision (standard care)?",no difference,high,no,['14593039'],29460275,2018,"{'14593039': {'article_id': '14593039', 'content': 'To determine the effectiveness of screening for visual impairment in people aged 75 or over as part of a multidimensional screening programme.\nCluster randomised trial.\nGeneral practices in the United Kingdom participating in the MRC trial of assessment and management of older people in the community.\n4340 people aged 75 years or over randomly sampled from 20 general practices, excluding people resident in hospitals or nursing homes.\nVisual acuity testing and referral to eye services for people with visual impairment. Universal screening (assessment and visual acuity testing) was compared with targeted screening, in which only participants with a range of health related problems were offered an assessment that included acuity screening.\nProportion of people with visual acuity less than 6/18 in either eye; mean composite score of 25 item version of the National Eye Institute visual function questionnaire.\nThree to five years after screening, the relative risk of having visual acuity < 6/18 in either eye, comparing universal with targeted screening, was 1.07 (95% confidence interval 0.84 to 1.36; P = 0.58). The mean composite score of the visual function questionnaire was 85.6 in the targeted screening group and 86.0 in the universal group (difference 0.4, 95% confidence interval -1.7 to 2.5, P = 0.69).\nIncluding a vision screening component by a practice nurse in a pragmatic trial of multidimensional screening for older people did not lead to improved visual outcomes.', 'title': 'Screening older people for impaired vision in primary care: cluster randomised trial.', 'date': '2003-11-01'}}",1.0,"Public Health, Epidemiology & Health Systems"
170,"Is the risk of job loss higher, lower, or the same when comparing non-pharmacological job loss prevention interventions to usual care?",uncertain effect,very low,no,"['16208658', '14613285']",25375291,2014,"{'16208658': {'article_id': '16208658', 'content': 'Work disability is a major consequence of inflammatory rheumatic conditions. Evidence regarding the effectiveness of interventions aimed at the prevention or reduction of work disability in rheumatic diseases is limited. We conducted a randomized controlled trial to investigate the effectiveness of a multidisciplinary job-retention vocational rehabilitation (VR) program in patients with a rheumatic condition who were at risk for job loss.\nA total of 140 patients with a chronic rheumatic condition were randomly assigned to either a multidisciplinary job-retention VR program (n = 74) or usual outpatient care (UC) (n = 66). Patients in the VR group were assessed and guided by a multidisciplinary team, whereas patients in the UC group received care as initiated by their rheumatologist, supplemented with written information. The main outcome measure was the occurrence of job loss (complete work disability or unemployment); additional outcome measures included job satisfaction, pain, functional status, emotional status, and quality of life.\nThere was no difference between the 2 groups regarding the proportion of patients having lost their job at any time point, with 24% and 23% of the patients in the VR and UC groups, respectively, having lost their job after 24 months. Over the total period of 24 months, patients in the VR group had a significantly greater improvement of the fatigue visual analog scale and of emotional status (all P values < 0.05).\nA job-retention VR program did not reduce the risk of job loss but improved fatigue and mental health in patients with chronic rheumatic diseases at risk for job loss.', 'title': 'Randomized comparison of a multidisciplinary job-retention vocational rehabilitation program with usual outpatient care in patients with chronic arthritis at risk for job loss.', 'date': '2005-10-07'}, '14613285': {'article_id': '14613285', 'content': 'Job loss is a major consequence of rheumatic diseases, and clinicians may refer patients to vocational rehabilitation for help. When provided after job loss, the impact of vocational rehabilitation is short term. This randomized controlled trial with 48 months of followup was undertaken to determine the efficacy of vocational rehabilitation provided to persons with rheumatic diseases while they are still employed, but at risk for job loss.\nA total of 242 patients with rheumatic diseases residing in Massachusetts were recruited through their rheumatologists for study. Participants were randomly assigned to the experimental group (n = 122) or the control group (n = 120). Subjects in the experimental group received two 1.5-hour sessions of vocational rehabilitation; those in the control group received print materials about disability employment issues and resources by mail. The main outcome assessed was the time to first job loss. Job losses were defined as permanent disability, premature retirement, or a period of unemployment. All analyses were conducted on an intent-to-treat basis.\nJob loss was delayed in the experimental group compared with the control group (P = 0.03 by log rank test). After adjustment for confounders, participation in the experimental group was found to be protective against job loss (odds ratio 0.58 [95% confidence interval 0.34-0.99], P = 0.05 by pooled logistic regression).\nVocational rehabilitation delivered to patients at risk for job loss, but while they were still employed, delayed job loss. Such an intervention has the potential to reduce the high indirect costs, as well as the personal impact, of rheumatic diseases.', 'title': 'Reduction of job loss in persons with rheumatic diseases receiving vocational rehabilitation: a randomized controlled trial.', 'date': '2003-11-13'}}",0.0,Family Medicine & Preventive Care
171,"Is intermediate term work functioning higher, lower, or the same when comparing non-pharmacological job loss prevention interventions to usual care?",higher,very low,no,['19877106'],25375291,2014,"{'19877106': {'article_id': '19877106', 'content': 'Work disability is a serious consequence of rheumatoid arthritis (RA). We conducted a 6-month, prospective randomized controlled trial comparing assessments of function, work, coping, and disease activity in employed patients with RA receiving occupational therapy intervention versus usual care.\nEmployed patients with RA with increased perceived work disability risk were identified by the RA Work Instability Scale (WIS; score >or=10). Patients were stratified into medium- (score >or=10 and <17) and high-risk (>or=17) groups, then randomized into occupational therapy or usual care groups. Assessments were conducted at baseline and 6 months. The primary outcome was the Canadian Occupational Performance Measure (COPM), a standardized patient self-report of function. Other outcomes included the disability index (DI) of the Health Assessment Questionnaire (HAQ); Disease Activity Score in 28 joints (DAS28); RA WIS; EuroQol Index; visual analog scales (VAS) for pain, work satisfaction, and work performance; and days missed/month. Independent sample t-tests and Mann-Whitney U tests were used.\nWe recruited 32 employed patients with RA. At baseline the groups were well matched. At 6 months the improvement in the occupational therapy group was significantly greater than that in the usual care group for all functional outcomes (COPM performance P < 0.001, COPM satisfaction P < 0.001, HAQ DI P = 0.02) and most work outcomes (RA WIS [P = 0.04], VAS work satisfaction [P < 0.001], VAS work performance [P = 0.01]). Additionally, Arthritis Helplessness Index (P = 0.02), Arthritis Impact Measurement Scales II pain subscale (P = 0.03), VAS pain (P = 0.007), EuroQol Index (P = 0.02), EuroQol global (P = 0.02), and DAS28 (P = 0.03) scores significantly improved.\nTargeted, comprehensive occupational therapy intervention improves functional and work-related outcomes in employed RA patients at risk of work disability.', 'title': 'Functional and work outcomes improve in patients with rheumatoid arthritis who receive targeted, comprehensive occupational therapy.', 'date': '2009-10-31'}}",1.0,"Public Health, Epidemiology & Health Systems"
220,"Is the risk of adverse events higher, lower, or the same when comparing non-pharmacological job loss prevention interventions to usual care?",insufficient data,,no,"['16208658', '14613285', '19877106']",25375291,2014,"{'16208658': {'article_id': '16208658', 'content': 'Work disability is a major consequence of inflammatory rheumatic conditions. Evidence regarding the effectiveness of interventions aimed at the prevention or reduction of work disability in rheumatic diseases is limited. We conducted a randomized controlled trial to investigate the effectiveness of a multidisciplinary job-retention vocational rehabilitation (VR) program in patients with a rheumatic condition who were at risk for job loss.\nA total of 140 patients with a chronic rheumatic condition were randomly assigned to either a multidisciplinary job-retention VR program (n = 74) or usual outpatient care (UC) (n = 66). Patients in the VR group were assessed and guided by a multidisciplinary team, whereas patients in the UC group received care as initiated by their rheumatologist, supplemented with written information. The main outcome measure was the occurrence of job loss (complete work disability or unemployment); additional outcome measures included job satisfaction, pain, functional status, emotional status, and quality of life.\nThere was no difference between the 2 groups regarding the proportion of patients having lost their job at any time point, with 24% and 23% of the patients in the VR and UC groups, respectively, having lost their job after 24 months. Over the total period of 24 months, patients in the VR group had a significantly greater improvement of the fatigue visual analog scale and of emotional status (all P values < 0.05).\nA job-retention VR program did not reduce the risk of job loss but improved fatigue and mental health in patients with chronic rheumatic diseases at risk for job loss.', 'title': 'Randomized comparison of a multidisciplinary job-retention vocational rehabilitation program with usual outpatient care in patients with chronic arthritis at risk for job loss.', 'date': '2005-10-07'}, '14613285': {'article_id': '14613285', 'content': 'Job loss is a major consequence of rheumatic diseases, and clinicians may refer patients to vocational rehabilitation for help. When provided after job loss, the impact of vocational rehabilitation is short term. This randomized controlled trial with 48 months of followup was undertaken to determine the efficacy of vocational rehabilitation provided to persons with rheumatic diseases while they are still employed, but at risk for job loss.\nA total of 242 patients with rheumatic diseases residing in Massachusetts were recruited through their rheumatologists for study. Participants were randomly assigned to the experimental group (n = 122) or the control group (n = 120). Subjects in the experimental group received two 1.5-hour sessions of vocational rehabilitation; those in the control group received print materials about disability employment issues and resources by mail. The main outcome assessed was the time to first job loss. Job losses were defined as permanent disability, premature retirement, or a period of unemployment. All analyses were conducted on an intent-to-treat basis.\nJob loss was delayed in the experimental group compared with the control group (P = 0.03 by log rank test). After adjustment for confounders, participation in the experimental group was found to be protective against job loss (odds ratio 0.58 [95% confidence interval 0.34-0.99], P = 0.05 by pooled logistic regression).\nVocational rehabilitation delivered to patients at risk for job loss, but while they were still employed, delayed job loss. Such an intervention has the potential to reduce the high indirect costs, as well as the personal impact, of rheumatic diseases.', 'title': 'Reduction of job loss in persons with rheumatic diseases receiving vocational rehabilitation: a randomized controlled trial.', 'date': '2003-11-13'}, '19877106': {'article_id': '19877106', 'content': 'Work disability is a serious consequence of rheumatoid arthritis (RA). We conducted a 6-month, prospective randomized controlled trial comparing assessments of function, work, coping, and disease activity in employed patients with RA receiving occupational therapy intervention versus usual care.\nEmployed patients with RA with increased perceived work disability risk were identified by the RA Work Instability Scale (WIS; score >or=10). Patients were stratified into medium- (score >or=10 and <17) and high-risk (>or=17) groups, then randomized into occupational therapy or usual care groups. Assessments were conducted at baseline and 6 months. The primary outcome was the Canadian Occupational Performance Measure (COPM), a standardized patient self-report of function. Other outcomes included the disability index (DI) of the Health Assessment Questionnaire (HAQ); Disease Activity Score in 28 joints (DAS28); RA WIS; EuroQol Index; visual analog scales (VAS) for pain, work satisfaction, and work performance; and days missed/month. Independent sample t-tests and Mann-Whitney U tests were used.\nWe recruited 32 employed patients with RA. At baseline the groups were well matched. At 6 months the improvement in the occupational therapy group was significantly greater than that in the usual care group for all functional outcomes (COPM performance P < 0.001, COPM satisfaction P < 0.001, HAQ DI P = 0.02) and most work outcomes (RA WIS [P = 0.04], VAS work satisfaction [P < 0.001], VAS work performance [P = 0.01]). Additionally, Arthritis Helplessness Index (P = 0.02), Arthritis Impact Measurement Scales II pain subscale (P = 0.03), VAS pain (P = 0.007), EuroQol Index (P = 0.02), EuroQol global (P = 0.02), and DAS28 (P = 0.03) scores significantly improved.\nTargeted, comprehensive occupational therapy intervention improves functional and work-related outcomes in employed RA patients at risk of work disability.', 'title': 'Functional and work outcomes improve in patients with rheumatoid arthritis who receive targeted, comprehensive occupational therapy.', 'date': '2009-10-31'}}",1.0,"Public Health, Epidemiology & Health Systems"
172,"Is time to establish full enteral feeds higher, lower, or the same when comparing monitoring of gastric residual volume and quality to only monitoring of gastric residual quality?",uncertain effect,very low,no,['29866595'],31425604,2019,"{'29866595': {'article_id': '29866595', 'content': 'To evaluate the effect of not relying on prefeeding gastric residual volumes to guide feeding advancement on the time to reach full feeding volumes in preterm infants, compared with routine measurement of gastric residual volumes. We hypothesized that not measuring prefeeding gastric residual volumes can shorten the time to reach full feeds.\nIn this single-center, randomized, controlled trial, we included gavage fed preterm infants with birth weights (BW) 1500-2000\u2009g who were enrolled within 48\u2009hours of birth. Exclusion criteria were major congenital malformations, asphyxia, and BW below the third percentile. In the study group, the gastric residual volume was measured only in the presence of bloody aspirates, vomiting, or an abnormal abdominal examination. In the control group, gastric residual volume was assessed routinely, and feeding advancement was based on the gastric residual volume. The primary outcome was the time to reach feeding volumes of 120\u2009mL/kg per day. Secondary outcomes were time to regain BW, episodes of feeding interruptions, sepsis, and necrotizing enterocolitis.\nEighty-seven infants were enrolled. There were no differences between the study and control groups with respect to time to reach full feeds (6 days [95% CI, 5.5-6.5] vs 5 days [95% CI, 4.5-5.5]; P\u2009=\u2009.82), time to regain BW, episodes of feeding interruptions, or sepsis. Two infants in the control group developed necrotizing enterocolitis.\nAvoiding routine assessment of gastric residual volume before feeding advancement did not shorten the time to reach full feeds in preterm infants with BW between 1500 and 2000 g.\nClinicaltrials.gov: NCT01337622.', 'title': 'Gastric Residual Volume in Feeding Advancement in Preterm Infants (GRIP Study): A Randomized Trial.', 'date': '2018-06-06'}}",0.0,Surgery
173,"Is the risk of necrotizing enterocolitis higher, lower, or the same when comparing monitoring of gastric residual volume and quality to only monitoring of gastric residual quality?",uncertain effect,very low,no,['29866595'],31425604,2019,"{'29866595': {'article_id': '29866595', 'content': 'To evaluate the effect of not relying on prefeeding gastric residual volumes to guide feeding advancement on the time to reach full feeding volumes in preterm infants, compared with routine measurement of gastric residual volumes. We hypothesized that not measuring prefeeding gastric residual volumes can shorten the time to reach full feeds.\nIn this single-center, randomized, controlled trial, we included gavage fed preterm infants with birth weights (BW) 1500-2000\u2009g who were enrolled within 48\u2009hours of birth. Exclusion criteria were major congenital malformations, asphyxia, and BW below the third percentile. In the study group, the gastric residual volume was measured only in the presence of bloody aspirates, vomiting, or an abnormal abdominal examination. In the control group, gastric residual volume was assessed routinely, and feeding advancement was based on the gastric residual volume. The primary outcome was the time to reach feeding volumes of 120\u2009mL/kg per day. Secondary outcomes were time to regain BW, episodes of feeding interruptions, sepsis, and necrotizing enterocolitis.\nEighty-seven infants were enrolled. There were no differences between the study and control groups with respect to time to reach full feeds (6 days [95% CI, 5.5-6.5] vs 5 days [95% CI, 4.5-5.5]; P\u2009=\u2009.82), time to regain BW, episodes of feeding interruptions, or sepsis. Two infants in the control group developed necrotizing enterocolitis.\nAvoiding routine assessment of gastric residual volume before feeding advancement did not shorten the time to reach full feeds in preterm infants with BW between 1500 and 2000 g.\nClinicaltrials.gov: NCT01337622.', 'title': 'Gastric Residual Volume in Feeding Advancement in Preterm Infants (GRIP Study): A Randomized Trial.', 'date': '2018-06-06'}}",0.0,Pediatrics & Neonatology
174,"Is time taken to establish full enteral feeds higher, lower, or the same when comparing routine monitoring of gastric residual to no monitoring of gastric residual?",higher,low,no,"['25238118', '25166623']",31425604,2019,"{'25238118': {'article_id': '25238118', 'content': 'The aim of the study was to compare prefeed abdominal circumference (AC) and gastric residual volume (GRV) as a measure of feed intolerance in very-low-birth-weight infants (VLBW).\nEighty VLBW infants were randomized to 2 groups; feed intolerance was monitored by measuring either GRV group or prefeed AC group. The primary outcome was time to full enteral feeds (180 mL · kg · day). Other main outcome measures were feed interruption days, duration of parenteral nutrition, incidence of culture positive sepsis, necrotizing enterocolitis, mortality, and duration of hospital stay.\nThe median (interquartile range) time to achieve full feeds was 10 (9-13) versus 14 (12-17.5) days in AC and GRV groups, respectively (P\u200a<\u200a0.001). Infants in AC group had fewer feed interruption days (0 [0-2] vs 2.0 [1, 5], P\u200a<\u200a0.001) and shorter duration of parenteral nutrition (P\u200a<\u200a0.001). The incidence of culture-positive sepsis in AC and GRV groups was 17.5% and 30 %, respectively (P\u200a=\u200a0.18). Duration of hospital stay and mortality were comparable in both the groups.\nPrefeed AC as a measure of feed intolerance in VLBW infants may shorten the time taken to achieve full feeds.', 'title': 'Abdominal circumference or gastric residual volume as measure of feed intolerance in VLBW infants.', 'date': '2014-09-23'}, '25166623': {'article_id': '25166623', 'content': 'Little information exists regarding gastric residual (GR) evaluation prior to feedings in premature infants. The purpose of this study was to compare the amount of feedings at 2 and 3 weeks of age, number of days to full feedings, growth and incidence of complications between infants who underwent RGR (routine evaluation of GR) evaluation versus those who did not.\nSixty-one premature infants were randomized to one of two groups. Group 1 received RGR evaluation prior to feeds and Group 2 did not.\nThere was no difference in amount of feeding at 2 (P=0.66) or 3 (P=0.41) weeks of age, growth, days on parenteral nutrition or complications. Although not statistically significant, infants without RGR evaluation reached feeds of 150\u2009ml\u2009kg(-1) per day 6 days earlier and had 6 fewer days with central venous access.\nRESULTs suggest RGR evaluation may not improve nutritional outcomes in premature infants.', 'title': 'The value of routine evaluation of gastric residuals in very low birth weight infants.', 'date': '2014-08-29'}}",0.0,Internal Medicine & Subspecialties
175,"Is the number of total parenteral nutrition days higher, lower, or the same when comparing routine monitoring of gastric residual to no monitoring of gastric residual?",higher,low,no,"['25238118', '25166623']",31425604,2019,"{'25238118': {'article_id': '25238118', 'content': 'The aim of the study was to compare prefeed abdominal circumference (AC) and gastric residual volume (GRV) as a measure of feed intolerance in very-low-birth-weight infants (VLBW).\nEighty VLBW infants were randomized to 2 groups; feed intolerance was monitored by measuring either GRV group or prefeed AC group. The primary outcome was time to full enteral feeds (180 mL · kg · day). Other main outcome measures were feed interruption days, duration of parenteral nutrition, incidence of culture positive sepsis, necrotizing enterocolitis, mortality, and duration of hospital stay.\nThe median (interquartile range) time to achieve full feeds was 10 (9-13) versus 14 (12-17.5) days in AC and GRV groups, respectively (P\u200a<\u200a0.001). Infants in AC group had fewer feed interruption days (0 [0-2] vs 2.0 [1, 5], P\u200a<\u200a0.001) and shorter duration of parenteral nutrition (P\u200a<\u200a0.001). The incidence of culture-positive sepsis in AC and GRV groups was 17.5% and 30 %, respectively (P\u200a=\u200a0.18). Duration of hospital stay and mortality were comparable in both the groups.\nPrefeed AC as a measure of feed intolerance in VLBW infants may shorten the time taken to achieve full feeds.', 'title': 'Abdominal circumference or gastric residual volume as measure of feed intolerance in VLBW infants.', 'date': '2014-09-23'}, '25166623': {'article_id': '25166623', 'content': 'Little information exists regarding gastric residual (GR) evaluation prior to feedings in premature infants. The purpose of this study was to compare the amount of feedings at 2 and 3 weeks of age, number of days to full feedings, growth and incidence of complications between infants who underwent RGR (routine evaluation of GR) evaluation versus those who did not.\nSixty-one premature infants were randomized to one of two groups. Group 1 received RGR evaluation prior to feeds and Group 2 did not.\nThere was no difference in amount of feeding at 2 (P=0.66) or 3 (P=0.41) weeks of age, growth, days on parenteral nutrition or complications. Although not statistically significant, infants without RGR evaluation reached feeds of 150\u2009ml\u2009kg(-1) per day 6 days earlier and had 6 fewer days with central venous access.\nRESULTs suggest RGR evaluation may not improve nutritional outcomes in premature infants.', 'title': 'The value of routine evaluation of gastric residuals in very low birth weight infants.', 'date': '2014-08-29'}}",0.0,Surgery
176,"Is the risk of recurrence higher, lower, or the same when comparing absorbable tacks to nonabsorbable tacks?",uncertain effect,very low,no,"['29799075', '26885113']",34046884,2021,"{'29799075': {'article_id': '29799075', 'content': 'The method of anchoring the mesh in laparoscopic ventral hernia repair is claimed to cause postoperative pain, affecting the quality of life of the patients. The aim of this randomized study was to compare the effect of three types of fixation devices on postoperative pain, patient quality of life, and hernia recurrence.\nPatients with ventral hernias between 2 and 7\xa0cm were randomized into one of three mesh fixation groups: permanent tacks (Protack™), absorbable tacks (Securestrap™), and absorbable synthetic glue (Glubran™). The primary endpoint was pain on the second postoperative day, measured on a visual analogue scale. Quality of life and recurrence rate were secondary endpoints and investigated through questionnaires and clinical examination at follow-up visits 1, 6, and 12\xa0months after surgery.\nSeventy-five non-consecutive patients were included in the study, with 25 patients in each group. There was no significant difference between groups for unspecified pain on the second postoperative day (p\u2009=\u20090.250). The DoloTest™ values were 55.3\u2009±\u200928.9\xa0mm, 43.5\u2009±\u200928.5\xa0mm, and 55.9\u2009±\u200926.3\xa0mm for permanent tacks, absorbable tacks, and synthetic glue, respectively. No differences were observed between groups with respect to quality of life of the patients and hernia recurrence rate.\nIn patients with small- and medium-sized ventral hernias, the type of fixation device did not affect the immediate or long-term postoperative pain, quality of life, or recurrence rate when comparing permanent tacks, absorbable tacks, and synthetic glue for mesh fixation.\nNCT01534780.', 'title': 'Effect of fixation devices on postoperative pain after laparoscopic ventral hernia repair: a randomized clinical trial of permanent tacks, absorbable tacks, and synthetic glue.', 'date': '2018-05-26'}, '26885113': {'article_id': '26885113', 'content': 'The aim of this prospective randomized trial was to compare 2 main fixation devices in regard to pain and recurrence in laparoscopic ventral incisional hernia repair (LVIHR). A total of 51 patients were evaluated in this study (n = 25, nonabsorbable tack (NAT) and n = 26, absorbable tack (AT) groups). A visual analogue scale (VAS) was performed on both groups preoperatively and on the postoperative (PO) first day, second week, and sixth month. All patients were followed for recurrence by clinical examination, ultrasonography, and/or abdominal computed tomography. The median follow-up time was 31 months (15-45). The mean age and the mean body mass index (BMI) of the patients were 53.1 ± 11 years and 34 ± 5 kg/m(2), respectively. The median defect size was 60 cm(2) (35-150) and median operation time was 110 minutes (40-360). In 2 patients from AT group and 2 from NAT group (7.8%), recurrence occurred. The 2 groups had similar features regarding demographics, operation time, postoperative hospital stay, morbidity, and VAS scores. The 2 fixation methods were found similar for PO pain and recurrence. In our opinion, the choice of either of these fixation methods during surgery should not be based on the concerns of pain or recurrence. AT may be the preferable option in LVIHR due to the lower cost.', 'title': 'Prospective randomized trial of mesh fixation with absorbable versus nonabsorbable tacker in laparoscopic ventral incisional hernia repair.', 'date': '2016-02-18'}}",0.0,Surgery
177,"Is the risk of recurrence higher, lower, or the same when comparing nonabsorbable tacks to nonabsorbable sutures?",uncertain effect,very low,no,['20652715'],34046884,2021,"{'20652715': {'article_id': '20652715', 'content': 'Mesh fixation during laparoscopic ventral hernia repair can be performed using transfascial sutures or metal tacks. The aim of the present study is to compare mesh shrinkage and pain between two different techniques of mesh fixation in a prospective randomized trial.\nA randomized trial was performed. Patients with ventral hernia of maximal diameter 8\xa0cm were assigned to mesh fixation using either transfascial nonabsorbable sutures or metal tacks for fixation of a parietene composite mesh. The borders of the mesh were marked using clips, and radiological images in prone position were used for assessment of mesh size and location. The primary endpoint was mesh shrinkage; secondary endpoints included postoperative pain, mesh dislocation, and surgical morbidity.\nDemographic parameters were similar in both groups. A total of 40 patients were randomized, and 36 patients were available for follow-up. There was one hernia recurrence in each group. Pain was significantly higher following suture fixation after 6\xa0weeks, but no difference was found after 6\xa0months. Mesh shrinkage after 6\xa0months was significantly higher using tacks for mesh fixation.\nTransfascial sutures are associated with more pain within the first 6 postoperative weeks and less mesh shrinkage after 6\xa0months compared with mesh fixation using metal tacks.', 'title': 'Mesh shrinkage and pain in laparoscopic ventral hernia repair: a randomized clinical trial comparing suture versus tack mesh fixation.', 'date': '2010-07-24'}}",0.0,Surgery
178,"Is early postoperative pain higher, lower, or the same when comparing absorbable tacks to absorbable sutures?",uncertain effect,very low,no,['31236731'],34046884,2021,"{'31236731': {'article_id': '31236731', 'content': 'In open intra-peritoneal onlay mesh (IPOM) hernia repair, mesh fixation can be done by tacks, sutures or fibrin glue. There are randomized controlled trials (RCTs) on laparoscopic IPOM procedure, but no RCT so far has examined mesh fixation techniques in open IPOM repair.\nIn a single-center RCT, 48 patients undergoing open IPOM repair of an abdominal wall hernia were included. After randomization, surgery was performed in a standardized fashion. Hernia size, extent of mesh fixation, and duration of surgery were documented. The primary endpoint was postoperative pain intensity. Secondary endpoints were: complications, length of stay, quality of life, return to work, hernia recurrence. Follow-up was 1\xa0year in all 48 patients.\nAfter using tacks, mean pain intensity was 16.9, which is slightly lower than after suture fixation (19.6, p\u2009=\u20090.20). The duration of surgery was about the same (83 vs. 85\xa0min). When using tack fixation, significantly more fixation points were applied as compared to sutures (19 vs. 12; p\u2009=\u20090.02), although mesh size was similar. The complication rate was similar (tacks: 6/28 vs. sutures: 3/20). Cost of suture fixation was about 26 €, which is markedly lower than the 180 € associated with tacks. However, surgeons clearly preferred mesh fixation with tacks, because it is more comfortable especially in small hernias.\nThe present study failed to show an advantage of tacks over suture fixation and even there are more severe adverse events. Using tacks significantly increases the costs of hernia repair.', 'title': 'Mesh fixation in open IPOM procedure with tackers or sutures? A randomized clinical trial with preliminary results.', 'date': '2019-06-27'}}",0.0,Surgery
179,"Is the risk of recurrence higher, lower, or the same when comparing nonabsorbable tacks to fibrin sealant?",uncertain effect,very low,no,"['29799075', '23657861']",34046884,2021,"{'29799075': {'article_id': '29799075', 'content': 'The method of anchoring the mesh in laparoscopic ventral hernia repair is claimed to cause postoperative pain, affecting the quality of life of the patients. The aim of this randomized study was to compare the effect of three types of fixation devices on postoperative pain, patient quality of life, and hernia recurrence.\nPatients with ventral hernias between 2 and 7\xa0cm were randomized into one of three mesh fixation groups: permanent tacks (Protack™), absorbable tacks (Securestrap™), and absorbable synthetic glue (Glubran™). The primary endpoint was pain on the second postoperative day, measured on a visual analogue scale. Quality of life and recurrence rate were secondary endpoints and investigated through questionnaires and clinical examination at follow-up visits 1, 6, and 12\xa0months after surgery.\nSeventy-five non-consecutive patients were included in the study, with 25 patients in each group. There was no significant difference between groups for unspecified pain on the second postoperative day (p\u2009=\u20090.250). The DoloTest™ values were 55.3\u2009±\u200928.9\xa0mm, 43.5\u2009±\u200928.5\xa0mm, and 55.9\u2009±\u200926.3\xa0mm for permanent tacks, absorbable tacks, and synthetic glue, respectively. No differences were observed between groups with respect to quality of life of the patients and hernia recurrence rate.\nIn patients with small- and medium-sized ventral hernias, the type of fixation device did not affect the immediate or long-term postoperative pain, quality of life, or recurrence rate when comparing permanent tacks, absorbable tacks, and synthetic glue for mesh fixation.\nNCT01534780.', 'title': 'Effect of fixation devices on postoperative pain after laparoscopic ventral hernia repair: a randomized clinical trial of permanent tacks, absorbable tacks, and synthetic glue.', 'date': '2018-05-26'}, '23657861': {'article_id': '23657861', 'content': 'Fibrin sealant for mesh fixation has significant positive effects on early outcome after laparoscopic ventral hernia repair (LVHR) compared with titanium tacks. Whether fibrin sealant fixation also results in better long-term outcome is unknown.\nWe performed a randomized controlled trial including patients with umbilical hernia defects from 1.5 to 5 cm at three Danish hernia centres. We used a 12 cm circular mesh. Participants were randomized to fibrin sealant or titanium tack fixation. Patients were seen in the outpatient clinic at 1 and 12 months follow-up.\nForty patients were included of whom 34 were available for intention to treat analysis after 1 year. There were no significant differences in pain, discomfort, fatigue, satisfaction or quality of life between the two groups at the 1-year follow-up. Five patients (26 %) in the fibrin sealant group and one (6 %) in the tack group were diagnosed with a recurrence at the 1-year follow-up (p = 0.182) (overall recurrence rate 17 %). Hernia defects in patients with recurrence were significantly larger than in those without recurrence (median 4.0 vs. 2.8 cm, p = 0.009).\nPatients with larger hernia defects and fibrin sealant mesh fixation had higher recurrence rates than expected, although the study was not powered for assessment of recurrence. There was no significant difference between groups in any parameters after the 1-year follow-up. The beneficial effects of mesh fixation with fibrin sealant on early outcome warrant further studies on optimization of the surgical technique to prevent recurrence.', 'title': 'Fibrin sealant for mesh fixation in laparoscopic umbilical hernia repair: 1-year results of a randomized controlled double-blinded study.', 'date': '2013-05-10'}}",0.0,Surgery
180,"Is flexibility higher, lower, or the same when comparing exercise to usual care?",higher,very low,no,['17964881'],30320433,2018,"{'17964881': {'article_id': '17964881', 'content': 'To show fatigue prevention and quality of life (QOL) improvement from cardiovascular exercise during radiotherapy.\nProspective enrollment (n=21), randomized to exercise (n=11) and control groups (n=10), with pre- and post-radiotherapy between- and within-group comparisons.\nAcademic medical center.\nLocalized prostate cancer patients undergoing radiotherapy.\nThe interventional group received radiotherapy plus aerobic exercise 3 times a week for 8 weeks whereas the control group received radiotherapy without exercise.\nPre- and post-radiotherapy differences in cardiac fitness, fatigue, depression, functional status, physical, social, and functional well-being, leg strength, and flexibility were examined within and between 2 groups.\nNo significant differences existed between 2 groups at pre-radiotherapy assessment. At post-radiotherapy assessment, the exercise group showed significant within group improvements in: cardiac fitness (P<.001), fatigue (P=.02), Functional Assessment of Cancer Therapy-Prostate (FACT-P) (P=.04), physical well-being (P=.002), social well-being (P=.02), flexibility (P=.006), and leg strength (P=.000). Within the control group, there was a significant increase in fatigue score (P=.004) and a decline in social well-being (P<.05) at post-radiotherapy assessment. Between-group differences at post-radiotherapy assessment were significant in cardiac fitness (P=.006), strength (P=.000), flexibility (P<.01), fatigue (P<.001), FACT-P (P=.006), physical well-being (P<.001), social well-being (P=.002), and functional well-being (P=.04).\nAn 8-week cardiovascular exercise program in patients with localized prostate cancer undergoing radiotherapy improved cardiovascular fitness, flexibility, muscle strength, and overall QOL and prevented fatigue.', 'title': 'Exercise prevents fatigue and improves quality of life in prostate cancer patients undergoing radiotherapy.', 'date': '2007-10-30'}}",1.0,Family Medicine & Preventive Care
181,"Is the number of falls higher, lower, or the same when comparing exercise to usual care?",no difference,very low,no,['19335674'],30320433,2018,"{'19335674': {'article_id': '19335674', 'content': ""(a) to determine if 110 postmenopausal breast cancer survivors (BCS) with bone loss who participated in 24 months of strength and weight training (ST) exercises had improved muscle strength and balance and had fewer falls compared to BCS who did not exercise; and (b) to describe type and frequency of ST exercises; adverse effects of exercises; and participants' adherence to exercises at home, at fitness centers, and at 36-month follow up.\nFindings reported are from a federally funded multicomponent intervention study of 223 postmenopausal BCS with either osteopenia or osteoporosis who were randomly assigned to an exercise (n=110) or comparison (n=113) group.\nTime points for testing outcomes were baseline, 6, 12, and 24 months into intervention. Muscle strength was tested using Biodex Velocity Spectrum Evaluation, and dynamic balance using Timed Backward Tandem Walk. Adherence to exercises was measured using self-report of number of prescribed sessions attended and participants' reports of falls.\nMean adherence over 24 months was 69.4%. Using generalized estimating equation (GEE) analyses, compared to participants not exercising, participants who exercised for 24 months had significantly improved hip flexion (p=0.011), hip extension (p=0.0006), knee flexion (p<0.0001, knee extension (p=0.0018), wrist flexion (p=0.031), and balance (p=0.010). Gains in muscle strength were 9.5% and 28.5% for hip flexion and extension, 50.0% and 19.4% for wrist flexion and extension, and 21.1% and 11.6% for knee flexion and extension. Balance improved by 39.4%. Women who exercised had fewer falls, but difference in number of falls between the two groups was not significant.\nMany postmenopausal BCS with bone loss can adhere to a 24 month ST exercise intervention, and exercises can result in meaningful gains in muscle strength and balance.\nMore studies are needed for examining relationships between muscle strength and balance in postmenopausal BCS with bone loss and their incidence of falls and fractures."", 'title': 'An exercise intervention for breast cancer survivors with bone loss.', 'date': '2009-04-02'}}",1.0,Family Medicine & Preventive Care
182,"Is the rate of falls higher, lower, or the same when comparing exercise to usual care?",insufficient data,,no,['19335674'],30320433,2018,"{'19335674': {'article_id': '19335674', 'content': ""(a) to determine if 110 postmenopausal breast cancer survivors (BCS) with bone loss who participated in 24 months of strength and weight training (ST) exercises had improved muscle strength and balance and had fewer falls compared to BCS who did not exercise; and (b) to describe type and frequency of ST exercises; adverse effects of exercises; and participants' adherence to exercises at home, at fitness centers, and at 36-month follow up.\nFindings reported are from a federally funded multicomponent intervention study of 223 postmenopausal BCS with either osteopenia or osteoporosis who were randomly assigned to an exercise (n=110) or comparison (n=113) group.\nTime points for testing outcomes were baseline, 6, 12, and 24 months into intervention. Muscle strength was tested using Biodex Velocity Spectrum Evaluation, and dynamic balance using Timed Backward Tandem Walk. Adherence to exercises was measured using self-report of number of prescribed sessions attended and participants' reports of falls.\nMean adherence over 24 months was 69.4%. Using generalized estimating equation (GEE) analyses, compared to participants not exercising, participants who exercised for 24 months had significantly improved hip flexion (p=0.011), hip extension (p=0.0006), knee flexion (p<0.0001, knee extension (p=0.0018), wrist flexion (p=0.031), and balance (p=0.010). Gains in muscle strength were 9.5% and 28.5% for hip flexion and extension, 50.0% and 19.4% for wrist flexion and extension, and 21.1% and 11.6% for knee flexion and extension. Balance improved by 39.4%. Women who exercised had fewer falls, but difference in number of falls between the two groups was not significant.\nMany postmenopausal BCS with bone loss can adhere to a 24 month ST exercise intervention, and exercises can result in meaningful gains in muscle strength and balance.\nMore studies are needed for examining relationships between muscle strength and balance in postmenopausal BCS with bone loss and their incidence of falls and fractures."", 'title': 'An exercise intervention for breast cancer survivors with bone loss.', 'date': '2009-04-02'}}",0.0,Family Medicine & Preventive Care
183,"Is the number of fallers higher, lower, or the same when comparing exercise to usual care?",insufficient data,,no,['19335674'],30320433,2018,"{'19335674': {'article_id': '19335674', 'content': ""(a) to determine if 110 postmenopausal breast cancer survivors (BCS) with bone loss who participated in 24 months of strength and weight training (ST) exercises had improved muscle strength and balance and had fewer falls compared to BCS who did not exercise; and (b) to describe type and frequency of ST exercises; adverse effects of exercises; and participants' adherence to exercises at home, at fitness centers, and at 36-month follow up.\nFindings reported are from a federally funded multicomponent intervention study of 223 postmenopausal BCS with either osteopenia or osteoporosis who were randomly assigned to an exercise (n=110) or comparison (n=113) group.\nTime points for testing outcomes were baseline, 6, 12, and 24 months into intervention. Muscle strength was tested using Biodex Velocity Spectrum Evaluation, and dynamic balance using Timed Backward Tandem Walk. Adherence to exercises was measured using self-report of number of prescribed sessions attended and participants' reports of falls.\nMean adherence over 24 months was 69.4%. Using generalized estimating equation (GEE) analyses, compared to participants not exercising, participants who exercised for 24 months had significantly improved hip flexion (p=0.011), hip extension (p=0.0006), knee flexion (p<0.0001, knee extension (p=0.0018), wrist flexion (p=0.031), and balance (p=0.010). Gains in muscle strength were 9.5% and 28.5% for hip flexion and extension, 50.0% and 19.4% for wrist flexion and extension, and 21.1% and 11.6% for knee flexion and extension. Balance improved by 39.4%. Women who exercised had fewer falls, but difference in number of falls between the two groups was not significant.\nMany postmenopausal BCS with bone loss can adhere to a 24 month ST exercise intervention, and exercises can result in meaningful gains in muscle strength and balance.\nMore studies are needed for examining relationships between muscle strength and balance in postmenopausal BCS with bone loss and their incidence of falls and fractures."", 'title': 'An exercise intervention for breast cancer survivors with bone loss.', 'date': '2009-04-02'}}",0.0,Family Medicine & Preventive Care
184,"Is balance as measured by the backward walk test higher, lower, or the same when comparing exercise to usual care?",higher,low,no,"['19949016', '19335674']",30320433,2018,"{'19949016': {'article_id': '19949016', 'content': 'Androgen suppression therapy (AST) results in musculoskeletal toxicity that reduces physical function and quality of life. This study examined the impact of a combined resistance and aerobic exercise program as a countermeasure to these AST-related toxicities.\nBetween 2007 and 2008, 57 patients with prostate cancer undergoing AST (commenced > 2 months prior) were randomly assigned to a program of resistance and aerobic exercise (n = 29) or usual care (n = 28) for 12 weeks. Primary end points were whole body and regional lean mass. Secondary end points were muscle strength and function, cardiorespiratory capacity, blood biomarkers, and quality of life.\nAnalysis of covariance was used to compare outcomes for groups at 12 weeks adjusted for baseline values and potential confounders. Patients undergoing exercise showed an increase in lean mass compared with usual care (total body, P = .047; upper limb, P < .001; lower limb, P = .019) and similarly better muscle strength (P < .01), 6-meter walk time (P = .024), and 6-meter backward walk time (P = .039). Exercise also improved several aspects of quality of life including general health (P = .022) and reduced fatigue (P = .021) and decreased levels of C-reactive protein (P = .008). There were no adverse events during the testing or exercise intervention program.\nA relatively brief exposure to exercise significantly improved muscle mass, strength, physical function, and balance in hypogonadal men compared with normal care. The exercise regimen was well tolerated and could be recommended for patients undergoing AST as an effective countermeasure to these common treatment-related adverse effects.', 'title': 'Combined resistance and aerobic exercise program reverses muscle loss in men undergoing androgen suppression therapy for prostate cancer without bone metastases: a randomized controlled trial.', 'date': '2009-12-02'}, '19335674': {'article_id': '19335674', 'content': ""(a) to determine if 110 postmenopausal breast cancer survivors (BCS) with bone loss who participated in 24 months of strength and weight training (ST) exercises had improved muscle strength and balance and had fewer falls compared to BCS who did not exercise; and (b) to describe type and frequency of ST exercises; adverse effects of exercises; and participants' adherence to exercises at home, at fitness centers, and at 36-month follow up.\nFindings reported are from a federally funded multicomponent intervention study of 223 postmenopausal BCS with either osteopenia or osteoporosis who were randomly assigned to an exercise (n=110) or comparison (n=113) group.\nTime points for testing outcomes were baseline, 6, 12, and 24 months into intervention. Muscle strength was tested using Biodex Velocity Spectrum Evaluation, and dynamic balance using Timed Backward Tandem Walk. Adherence to exercises was measured using self-report of number of prescribed sessions attended and participants' reports of falls.\nMean adherence over 24 months was 69.4%. Using generalized estimating equation (GEE) analyses, compared to participants not exercising, participants who exercised for 24 months had significantly improved hip flexion (p=0.011), hip extension (p=0.0006), knee flexion (p<0.0001, knee extension (p=0.0018), wrist flexion (p=0.031), and balance (p=0.010). Gains in muscle strength were 9.5% and 28.5% for hip flexion and extension, 50.0% and 19.4% for wrist flexion and extension, and 21.1% and 11.6% for knee flexion and extension. Balance improved by 39.4%. Women who exercised had fewer falls, but difference in number of falls between the two groups was not significant.\nMany postmenopausal BCS with bone loss can adhere to a 24 month ST exercise intervention, and exercises can result in meaningful gains in muscle strength and balance.\nMore studies are needed for examining relationships between muscle strength and balance in postmenopausal BCS with bone loss and their incidence of falls and fractures."", 'title': 'An exercise intervention for breast cancer survivors with bone loss.', 'date': '2009-04-02'}}",1.0,Family Medicine & Preventive Care
185,"Is fatigue higher, lower, or the same when comparing exercise to usual care?",lower,very low,no,"['17964881', '19949016']",30320433,2018,"{'17964881': {'article_id': '17964881', 'content': 'To show fatigue prevention and quality of life (QOL) improvement from cardiovascular exercise during radiotherapy.\nProspective enrollment (n=21), randomized to exercise (n=11) and control groups (n=10), with pre- and post-radiotherapy between- and within-group comparisons.\nAcademic medical center.\nLocalized prostate cancer patients undergoing radiotherapy.\nThe interventional group received radiotherapy plus aerobic exercise 3 times a week for 8 weeks whereas the control group received radiotherapy without exercise.\nPre- and post-radiotherapy differences in cardiac fitness, fatigue, depression, functional status, physical, social, and functional well-being, leg strength, and flexibility were examined within and between 2 groups.\nNo significant differences existed between 2 groups at pre-radiotherapy assessment. At post-radiotherapy assessment, the exercise group showed significant within group improvements in: cardiac fitness (P<.001), fatigue (P=.02), Functional Assessment of Cancer Therapy-Prostate (FACT-P) (P=.04), physical well-being (P=.002), social well-being (P=.02), flexibility (P=.006), and leg strength (P=.000). Within the control group, there was a significant increase in fatigue score (P=.004) and a decline in social well-being (P<.05) at post-radiotherapy assessment. Between-group differences at post-radiotherapy assessment were significant in cardiac fitness (P=.006), strength (P=.000), flexibility (P<.01), fatigue (P<.001), FACT-P (P=.006), physical well-being (P<.001), social well-being (P=.002), and functional well-being (P=.04).\nAn 8-week cardiovascular exercise program in patients with localized prostate cancer undergoing radiotherapy improved cardiovascular fitness, flexibility, muscle strength, and overall QOL and prevented fatigue.', 'title': 'Exercise prevents fatigue and improves quality of life in prostate cancer patients undergoing radiotherapy.', 'date': '2007-10-30'}, '19949016': {'article_id': '19949016', 'content': 'Androgen suppression therapy (AST) results in musculoskeletal toxicity that reduces physical function and quality of life. This study examined the impact of a combined resistance and aerobic exercise program as a countermeasure to these AST-related toxicities.\nBetween 2007 and 2008, 57 patients with prostate cancer undergoing AST (commenced > 2 months prior) were randomly assigned to a program of resistance and aerobic exercise (n = 29) or usual care (n = 28) for 12 weeks. Primary end points were whole body and regional lean mass. Secondary end points were muscle strength and function, cardiorespiratory capacity, blood biomarkers, and quality of life.\nAnalysis of covariance was used to compare outcomes for groups at 12 weeks adjusted for baseline values and potential confounders. Patients undergoing exercise showed an increase in lean mass compared with usual care (total body, P = .047; upper limb, P < .001; lower limb, P = .019) and similarly better muscle strength (P < .01), 6-meter walk time (P = .024), and 6-meter backward walk time (P = .039). Exercise also improved several aspects of quality of life including general health (P = .022) and reduced fatigue (P = .021) and decreased levels of C-reactive protein (P = .008). There were no adverse events during the testing or exercise intervention program.\nA relatively brief exposure to exercise significantly improved muscle mass, strength, physical function, and balance in hypogonadal men compared with normal care. The exercise regimen was well tolerated and could be recommended for patients undergoing AST as an effective countermeasure to these common treatment-related adverse effects.', 'title': 'Combined resistance and aerobic exercise program reverses muscle loss in men undergoing androgen suppression therapy for prostate cancer without bone metastases: a randomized controlled trial.', 'date': '2009-12-02'}}",1.0,Family Medicine & Preventive Care
186,"Is the incidence of bronchopulmonary dysplasia (BPD) higher, lower, or the same when comparing prophylactic CPAP to very early CPAP?",uncertain effect,very low,yes,['23930249'],34661278,2021,"{'23930249': {'article_id': '23930249', 'content': 'This prospective study was performed to identify whether the early use of nasal continuous positive airway pressure (n CPAP) would reduce the rate of endotracheal intubation, mechanical ventilation and surfactant administration.\nThis study was conducted from June 2009 to September 2010 in the Shahid Beheshti University Hospital, Isfahan-Iran. A total of 72 preterm infants with 25-30 weeks gestation who needed respiratory support at 5 min after birth entered the study. Infants were randomly assigned to the very early CPAP (initiated 5 min after birth) or to the late CPAP (initiated 30 min after birth) treatment groups. The primary outcomes were need for intubation and mechanical ventilation during the first 48 h after birth and secondary outcomes were death, pneumothorax, intraventricular hemorrhage, duration of mechanical ventilation and bronchopulmonary dysplasia.\nThere were no significant differences between the two groups with regard to mortality rate, bronchopulmonary dysplasia and patent ductus arteriosus. The need for surfactant administration was significantly reduced in the early CPAP group (P = 0.04). Infants in the early CPAP group less frequently required intubation and mechanical ventilation.\nEarly n CPAP is more effective than late n CPAP for the treatment of respiratory distress syndrome. In addition, the early use of n CPAP would reduce the need for some invasive procedures such as intubation and mechanical ventilation.', 'title': 'Early versus delayed initiation of nasal continuous positive airway pressure for treatment of respiratory distress syndrome in premature newborns: A randomized clinical trial.', 'date': '2013-08-10'}}",0.0,Pediatrics & Neonatology
187,"Is the risk of death higher, lower, or the same when comparing prophylactic CPAP to very early CPAP?",no difference,very low,no,['23930249'],34661278,2021,"{'23930249': {'article_id': '23930249', 'content': 'This prospective study was performed to identify whether the early use of nasal continuous positive airway pressure (n CPAP) would reduce the rate of endotracheal intubation, mechanical ventilation and surfactant administration.\nThis study was conducted from June 2009 to September 2010 in the Shahid Beheshti University Hospital, Isfahan-Iran. A total of 72 preterm infants with 25-30 weeks gestation who needed respiratory support at 5 min after birth entered the study. Infants were randomly assigned to the very early CPAP (initiated 5 min after birth) or to the late CPAP (initiated 30 min after birth) treatment groups. The primary outcomes were need for intubation and mechanical ventilation during the first 48 h after birth and secondary outcomes were death, pneumothorax, intraventricular hemorrhage, duration of mechanical ventilation and bronchopulmonary dysplasia.\nThere were no significant differences between the two groups with regard to mortality rate, bronchopulmonary dysplasia and patent ductus arteriosus. The need for surfactant administration was significantly reduced in the early CPAP group (P = 0.04). Infants in the early CPAP group less frequently required intubation and mechanical ventilation.\nEarly n CPAP is more effective than late n CPAP for the treatment of respiratory distress syndrome. In addition, the early use of n CPAP would reduce the need for some invasive procedures such as intubation and mechanical ventilation.', 'title': 'Early versus delayed initiation of nasal continuous positive airway pressure for treatment of respiratory distress syndrome in premature newborns: A randomized clinical trial.', 'date': '2013-08-10'}}",0.0,Pediatrics & Neonatology
188,"Is the incidence of neurodevelopmental impairment higher, lower, or the same when comparing prophylactic CPAP to very early CPAP?",insufficient data,,no,['23930249'],34661278,2021,"{'23930249': {'article_id': '23930249', 'content': 'This prospective study was performed to identify whether the early use of nasal continuous positive airway pressure (n CPAP) would reduce the rate of endotracheal intubation, mechanical ventilation and surfactant administration.\nThis study was conducted from June 2009 to September 2010 in the Shahid Beheshti University Hospital, Isfahan-Iran. A total of 72 preterm infants with 25-30 weeks gestation who needed respiratory support at 5 min after birth entered the study. Infants were randomly assigned to the very early CPAP (initiated 5 min after birth) or to the late CPAP (initiated 30 min after birth) treatment groups. The primary outcomes were need for intubation and mechanical ventilation during the first 48 h after birth and secondary outcomes were death, pneumothorax, intraventricular hemorrhage, duration of mechanical ventilation and bronchopulmonary dysplasia.\nThere were no significant differences between the two groups with regard to mortality rate, bronchopulmonary dysplasia and patent ductus arteriosus. The need for surfactant administration was significantly reduced in the early CPAP group (P = 0.04). Infants in the early CPAP group less frequently required intubation and mechanical ventilation.\nEarly n CPAP is more effective than late n CPAP for the treatment of respiratory distress syndrome. In addition, the early use of n CPAP would reduce the need for some invasive procedures such as intubation and mechanical ventilation.', 'title': 'Early versus delayed initiation of nasal continuous positive airway pressure for treatment of respiratory distress syndrome in premature newborns: A randomized clinical trial.', 'date': '2013-08-10'}}",1.0,Pediatrics & Neonatology
189,"Is the risk of pneumothorax higher, lower, or the same when comparing prophylactic CPAP to very early CPAP?",insufficient data,,yes,['23930249'],34661278,2021,"{'23930249': {'article_id': '23930249', 'content': 'This prospective study was performed to identify whether the early use of nasal continuous positive airway pressure (n CPAP) would reduce the rate of endotracheal intubation, mechanical ventilation and surfactant administration.\nThis study was conducted from June 2009 to September 2010 in the Shahid Beheshti University Hospital, Isfahan-Iran. A total of 72 preterm infants with 25-30 weeks gestation who needed respiratory support at 5 min after birth entered the study. Infants were randomly assigned to the very early CPAP (initiated 5 min after birth) or to the late CPAP (initiated 30 min after birth) treatment groups. The primary outcomes were need for intubation and mechanical ventilation during the first 48 h after birth and secondary outcomes were death, pneumothorax, intraventricular hemorrhage, duration of mechanical ventilation and bronchopulmonary dysplasia.\nThere were no significant differences between the two groups with regard to mortality rate, bronchopulmonary dysplasia and patent ductus arteriosus. The need for surfactant administration was significantly reduced in the early CPAP group (P = 0.04). Infants in the early CPAP group less frequently required intubation and mechanical ventilation.\nEarly n CPAP is more effective than late n CPAP for the treatment of respiratory distress syndrome. In addition, the early use of n CPAP would reduce the need for some invasive procedures such as intubation and mechanical ventilation.', 'title': 'Early versus delayed initiation of nasal continuous positive airway pressure for treatment of respiratory distress syndrome in premature newborns: A randomized clinical trial.', 'date': '2013-08-10'}}",1.0,Pediatrics & Neonatology
190,"Is the risk of neurodevelopmental impairment higher, lower, or the same when comparing prophylactic or very early CPAP to supportive care?",insufficient data,,no,"['15321956', '22402568', '24554040', '3102211']",34661278,2021,"{'15321956': {'article_id': '15321956', 'content': 'The role of nasal continuous positive airways pressure (nCPAP) in the management of respiratory distress syndrome in preterm infants is not completely defined.\nTo evaluate the benefits and risks of prophylactic nCPAP in infants of 28-31 weeks gestation.\nMulticentre randomised controlled clinical trial.\nSeventeen Italian neonatal intensive care units.\nA total of 230 newborns of 28-31 weeks gestation, not intubated in the delivery room and without major malformations, were randomly assigned to prophylactic or rescue nCPAP.\nProphylactic nCPAP was started within 30 minutes of birth, irrespective of oxygen requirement and clinical status. Rescue nCPAP was started when Fio2 requirement was > 0.4, for more than 30 minutes, to maintain transcutaneous oxygen saturation between 93% and 96%. Exogenous surfactant was given when Fio2 requirement was > 0.4 in nCPAP in the presence of radiological signs of respiratory distress syndrome.\nPrimary end point: need for exogenous surfactant. Secondary end points: need for mechanical ventilation and incidence of air leaks.\nSurfactant was needed by 22.6% in the prophylaxis group and 21.7% in the rescue group. Mechanical ventilation was required by 12.2% in both the prophylaxis and rescue group. The incidence of air leaks was 2.6% in both groups. More than 80% of both groups had received prenatal steroids.\nIn newborns of 28-31 weeks gestation, there is no greater benefit in giving prophylactic nCPAP than in starting nCPAP when the oxygen requirement increases to a Fio2 > 0.4.', 'title': 'Prophylactic nasal continuous positive airways pressure in newborns of 28-31 weeks gestation: multicentre randomised controlled clinical trial.', 'date': '2004-08-24'}, '22402568': {'article_id': '22402568', 'content': 'To determine whether very low birth weight infants (VLBWIs), initially supported with continuous positive airway pressure (CPAP) and then selectively treated with the INSURE (intubation, surfactant, and extubation to CPAP; CPAP/INSURE) protocol, need less mechanical ventilation than those supported with supplemental oxygen, surfactant, and mechanical ventilation if required (Oxygen/mechanical ventilation [MV]).\nIn a multicenter randomized controlled trial, spontaneously breathing VLBWIs weighing 800-1500 g were allocated to receive either therapy. In the CPAP/INSURE group, if respiratory distress syndrome (RDS) did not occur, CPAP was discontinued after 3-6 hours. If RDS developed and the fraction of inspired oxygen (FiO(2)) was >0.35, the INSURE protocol was indicated. Failure criteria included FiO(2) >0.60, severe apnea or respiratory acidosis, and receipt of more than 2 doses of surfactant. In the Oxygen/MV group, in the presence of RDS, supplemental oxygen without CPAP was given, and if FiO(2) was >0.35, surfactant and mechanical ventilation were provided.\nA total of 256 patients were randomized to either the CPAP/INSURE group (n = 131) or the Oxygen/MV group (n = 125). The need for mechanical ventilation was lower in the CPAP/INSURE group (29.8% vs 50.4%; P = .001), as was the use of surfactant (27.5% vs 46.4%; P = .002). There were no differences in death, pneumothorax, bronchopulmonary dysplasia, and other complications of prematurity between the 2 groups.\nCPAP and early selective INSURE reduced the need for mechanical ventilation and surfactant in VLBWIs without increasing morbidity and death. These results may be particularly relevant for resource-limited regions.', 'title': 'Randomized trial of early bubble continuous positive airway pressure for very low birth weight infants.', 'date': '2012-03-10'}, '24554040': {'article_id': '24554040', 'content': 'This study evaluated whether the use of continuous positive airway pressure (CPAP) in the delivery room alters the need for mechanical ventilation and surfactant during the first 5 days of life and modifies the incidence of respiratory morbidity and mortality during the hospital stay. The study was a multicenter randomized clinical trial conducted in five public university hospitals in Brazil, from June 2008 to December 2009. Participants were 197 infants with birth weight of 1000-1500 g and without major birth defects. They were treated according to the guidelines of the American Academy of Pediatrics (APP). Infants not intubated or extubated less than 15 min after birth were randomized for two treatments, routine or CPAP, and were followed until hospital discharge. The routine (n=99) and CPAP (n=98) infants studied presented no statistically significant differences regarding birth characteristics, complications during the prenatal period, the need for mechanical ventilation during the first 5 days of life (19.2 vs 23.4%, P=0.50), use of surfactant (18.2 vs 17.3% P=0.92), or respiratory morbidity and mortality until discharge. The CPAP group required a greater number of doses of surfactant (1.5 vs 1.0, P=0.02). When CPAP was applied to the routine group, it was installed within a median time of 30 min. We found that CPAP applied less than 15 min after birth was not able to reduce the need for ventilator support and was associated with a higher number of doses of surfactant when compared to CPAP applied as clinically indicated within a median time of 30 min.', 'title': 'Application of continuous positive airway pressure in the delivery room: a multicenter randomized clinical trial.', 'date': '2014-02-21'}, '3102211': {'article_id': '3102211', 'content': 'Application of continuous distending pressure at birth (very early CDP) should stabilize the immature airways and reduce the severity of respiratory distress syndrome (RDS) in preterm infants. Eighty-two preterm infants of less than 32 weeks gestation were randomly assigned at birth to early treatment group (TG), in which CDP of 6 cm water pressure was applied at birth by the nasopharyngeal route (NP-CDP), or to control group (CG), in which CDP was applied when indicated for established criteria (pO2 less than 50 mmHg in FiO2 greater than 0.5). Characteristics of the infants in the two groups were comparable. No statistically significant difference between the two groups was found in the incidence of RDS. The course of RDS, and oxygen and ventilatory requirements also did not appear to be changed. In blood gas parameters of most of the time frames, no significant difference was found between the two groups when the results were analyzed according to the assigned group. When the results were analyzed separately for the infants who developed RDS, infants in TG appear to have fared worse from the therapy in terms of oxygenation, as indicated by significantly higher FiO2 (P less than 0.01) and lower a/A (P less than 0.01) values on the third day of the course of RDS, as compared to infants in CG. The incidence of complications was comparable in the two groups. Four infants from TG (9.3%) and one from CG (2.6%) died (P = NS). We conclude that VECDP by nasopharyngeal route does not reduce the incidence of RDS and does not appear to improve the outcome and may worsen the severity of RDS when compared to application of CDP for established criteria.', 'title': 'Randomized controlled trial of very early continuous distending pressure in the management of preterm infants.', 'date': '1987-01-01'}}",1.0,Pediatrics & Neonatology
191,"Is the risk of death higher, lower, or the same when comparing prophylactic or very early CPAP to supportive care?",no difference,moderate,no,"['22402568', '24554040', '3102211']",34661278,2021,"{'22402568': {'article_id': '22402568', 'content': 'To determine whether very low birth weight infants (VLBWIs), initially supported with continuous positive airway pressure (CPAP) and then selectively treated with the INSURE (intubation, surfactant, and extubation to CPAP; CPAP/INSURE) protocol, need less mechanical ventilation than those supported with supplemental oxygen, surfactant, and mechanical ventilation if required (Oxygen/mechanical ventilation [MV]).\nIn a multicenter randomized controlled trial, spontaneously breathing VLBWIs weighing 800-1500 g were allocated to receive either therapy. In the CPAP/INSURE group, if respiratory distress syndrome (RDS) did not occur, CPAP was discontinued after 3-6 hours. If RDS developed and the fraction of inspired oxygen (FiO(2)) was >0.35, the INSURE protocol was indicated. Failure criteria included FiO(2) >0.60, severe apnea or respiratory acidosis, and receipt of more than 2 doses of surfactant. In the Oxygen/MV group, in the presence of RDS, supplemental oxygen without CPAP was given, and if FiO(2) was >0.35, surfactant and mechanical ventilation were provided.\nA total of 256 patients were randomized to either the CPAP/INSURE group (n = 131) or the Oxygen/MV group (n = 125). The need for mechanical ventilation was lower in the CPAP/INSURE group (29.8% vs 50.4%; P = .001), as was the use of surfactant (27.5% vs 46.4%; P = .002). There were no differences in death, pneumothorax, bronchopulmonary dysplasia, and other complications of prematurity between the 2 groups.\nCPAP and early selective INSURE reduced the need for mechanical ventilation and surfactant in VLBWIs without increasing morbidity and death. These results may be particularly relevant for resource-limited regions.', 'title': 'Randomized trial of early bubble continuous positive airway pressure for very low birth weight infants.', 'date': '2012-03-10'}, '24554040': {'article_id': '24554040', 'content': 'This study evaluated whether the use of continuous positive airway pressure (CPAP) in the delivery room alters the need for mechanical ventilation and surfactant during the first 5 days of life and modifies the incidence of respiratory morbidity and mortality during the hospital stay. The study was a multicenter randomized clinical trial conducted in five public university hospitals in Brazil, from June 2008 to December 2009. Participants were 197 infants with birth weight of 1000-1500 g and without major birth defects. They were treated according to the guidelines of the American Academy of Pediatrics (APP). Infants not intubated or extubated less than 15 min after birth were randomized for two treatments, routine or CPAP, and were followed until hospital discharge. The routine (n=99) and CPAP (n=98) infants studied presented no statistically significant differences regarding birth characteristics, complications during the prenatal period, the need for mechanical ventilation during the first 5 days of life (19.2 vs 23.4%, P=0.50), use of surfactant (18.2 vs 17.3% P=0.92), or respiratory morbidity and mortality until discharge. The CPAP group required a greater number of doses of surfactant (1.5 vs 1.0, P=0.02). When CPAP was applied to the routine group, it was installed within a median time of 30 min. We found that CPAP applied less than 15 min after birth was not able to reduce the need for ventilator support and was associated with a higher number of doses of surfactant when compared to CPAP applied as clinically indicated within a median time of 30 min.', 'title': 'Application of continuous positive airway pressure in the delivery room: a multicenter randomized clinical trial.', 'date': '2014-02-21'}, '3102211': {'article_id': '3102211', 'content': 'Application of continuous distending pressure at birth (very early CDP) should stabilize the immature airways and reduce the severity of respiratory distress syndrome (RDS) in preterm infants. Eighty-two preterm infants of less than 32 weeks gestation were randomly assigned at birth to early treatment group (TG), in which CDP of 6 cm water pressure was applied at birth by the nasopharyngeal route (NP-CDP), or to control group (CG), in which CDP was applied when indicated for established criteria (pO2 less than 50 mmHg in FiO2 greater than 0.5). Characteristics of the infants in the two groups were comparable. No statistically significant difference between the two groups was found in the incidence of RDS. The course of RDS, and oxygen and ventilatory requirements also did not appear to be changed. In blood gas parameters of most of the time frames, no significant difference was found between the two groups when the results were analyzed according to the assigned group. When the results were analyzed separately for the infants who developed RDS, infants in TG appear to have fared worse from the therapy in terms of oxygenation, as indicated by significantly higher FiO2 (P less than 0.01) and lower a/A (P less than 0.01) values on the third day of the course of RDS, as compared to infants in CG. The incidence of complications was comparable in the two groups. Four infants from TG (9.3%) and one from CG (2.6%) died (P = NS). We conclude that VECDP by nasopharyngeal route does not reduce the incidence of RDS and does not appear to improve the outcome and may worsen the severity of RDS when compared to application of CDP for established criteria.', 'title': 'Randomized controlled trial of very early continuous distending pressure in the management of preterm infants.', 'date': '1987-01-01'}}",1.0,Pediatrics & Neonatology
192,"Is the risk of failed treatment higher, lower, or the same when comparing prophylactic or very early CPAP to supportive care?",lower,very low,no,"['15321956', '22402568', '24554040', '3102211']",34661278,2021,"{'15321956': {'article_id': '15321956', 'content': 'The role of nasal continuous positive airways pressure (nCPAP) in the management of respiratory distress syndrome in preterm infants is not completely defined.\nTo evaluate the benefits and risks of prophylactic nCPAP in infants of 28-31 weeks gestation.\nMulticentre randomised controlled clinical trial.\nSeventeen Italian neonatal intensive care units.\nA total of 230 newborns of 28-31 weeks gestation, not intubated in the delivery room and without major malformations, were randomly assigned to prophylactic or rescue nCPAP.\nProphylactic nCPAP was started within 30 minutes of birth, irrespective of oxygen requirement and clinical status. Rescue nCPAP was started when Fio2 requirement was > 0.4, for more than 30 minutes, to maintain transcutaneous oxygen saturation between 93% and 96%. Exogenous surfactant was given when Fio2 requirement was > 0.4 in nCPAP in the presence of radiological signs of respiratory distress syndrome.\nPrimary end point: need for exogenous surfactant. Secondary end points: need for mechanical ventilation and incidence of air leaks.\nSurfactant was needed by 22.6% in the prophylaxis group and 21.7% in the rescue group. Mechanical ventilation was required by 12.2% in both the prophylaxis and rescue group. The incidence of air leaks was 2.6% in both groups. More than 80% of both groups had received prenatal steroids.\nIn newborns of 28-31 weeks gestation, there is no greater benefit in giving prophylactic nCPAP than in starting nCPAP when the oxygen requirement increases to a Fio2 > 0.4.', 'title': 'Prophylactic nasal continuous positive airways pressure in newborns of 28-31 weeks gestation: multicentre randomised controlled clinical trial.', 'date': '2004-08-24'}, '22402568': {'article_id': '22402568', 'content': 'To determine whether very low birth weight infants (VLBWIs), initially supported with continuous positive airway pressure (CPAP) and then selectively treated with the INSURE (intubation, surfactant, and extubation to CPAP; CPAP/INSURE) protocol, need less mechanical ventilation than those supported with supplemental oxygen, surfactant, and mechanical ventilation if required (Oxygen/mechanical ventilation [MV]).\nIn a multicenter randomized controlled trial, spontaneously breathing VLBWIs weighing 800-1500 g were allocated to receive either therapy. In the CPAP/INSURE group, if respiratory distress syndrome (RDS) did not occur, CPAP was discontinued after 3-6 hours. If RDS developed and the fraction of inspired oxygen (FiO(2)) was >0.35, the INSURE protocol was indicated. Failure criteria included FiO(2) >0.60, severe apnea or respiratory acidosis, and receipt of more than 2 doses of surfactant. In the Oxygen/MV group, in the presence of RDS, supplemental oxygen without CPAP was given, and if FiO(2) was >0.35, surfactant and mechanical ventilation were provided.\nA total of 256 patients were randomized to either the CPAP/INSURE group (n = 131) or the Oxygen/MV group (n = 125). The need for mechanical ventilation was lower in the CPAP/INSURE group (29.8% vs 50.4%; P = .001), as was the use of surfactant (27.5% vs 46.4%; P = .002). There were no differences in death, pneumothorax, bronchopulmonary dysplasia, and other complications of prematurity between the 2 groups.\nCPAP and early selective INSURE reduced the need for mechanical ventilation and surfactant in VLBWIs without increasing morbidity and death. These results may be particularly relevant for resource-limited regions.', 'title': 'Randomized trial of early bubble continuous positive airway pressure for very low birth weight infants.', 'date': '2012-03-10'}, '24554040': {'article_id': '24554040', 'content': 'This study evaluated whether the use of continuous positive airway pressure (CPAP) in the delivery room alters the need for mechanical ventilation and surfactant during the first 5 days of life and modifies the incidence of respiratory morbidity and mortality during the hospital stay. The study was a multicenter randomized clinical trial conducted in five public university hospitals in Brazil, from June 2008 to December 2009. Participants were 197 infants with birth weight of 1000-1500 g and without major birth defects. They were treated according to the guidelines of the American Academy of Pediatrics (APP). Infants not intubated or extubated less than 15 min after birth were randomized for two treatments, routine or CPAP, and were followed until hospital discharge. The routine (n=99) and CPAP (n=98) infants studied presented no statistically significant differences regarding birth characteristics, complications during the prenatal period, the need for mechanical ventilation during the first 5 days of life (19.2 vs 23.4%, P=0.50), use of surfactant (18.2 vs 17.3% P=0.92), or respiratory morbidity and mortality until discharge. The CPAP group required a greater number of doses of surfactant (1.5 vs 1.0, P=0.02). When CPAP was applied to the routine group, it was installed within a median time of 30 min. We found that CPAP applied less than 15 min after birth was not able to reduce the need for ventilator support and was associated with a higher number of doses of surfactant when compared to CPAP applied as clinically indicated within a median time of 30 min.', 'title': 'Application of continuous positive airway pressure in the delivery room: a multicenter randomized clinical trial.', 'date': '2014-02-21'}, '3102211': {'article_id': '3102211', 'content': 'Application of continuous distending pressure at birth (very early CDP) should stabilize the immature airways and reduce the severity of respiratory distress syndrome (RDS) in preterm infants. Eighty-two preterm infants of less than 32 weeks gestation were randomly assigned at birth to early treatment group (TG), in which CDP of 6 cm water pressure was applied at birth by the nasopharyngeal route (NP-CDP), or to control group (CG), in which CDP was applied when indicated for established criteria (pO2 less than 50 mmHg in FiO2 greater than 0.5). Characteristics of the infants in the two groups were comparable. No statistically significant difference between the two groups was found in the incidence of RDS. The course of RDS, and oxygen and ventilatory requirements also did not appear to be changed. In blood gas parameters of most of the time frames, no significant difference was found between the two groups when the results were analyzed according to the assigned group. When the results were analyzed separately for the infants who developed RDS, infants in TG appear to have fared worse from the therapy in terms of oxygenation, as indicated by significantly higher FiO2 (P less than 0.01) and lower a/A (P less than 0.01) values on the third day of the course of RDS, as compared to infants in CG. The incidence of complications was comparable in the two groups. Four infants from TG (9.3%) and one from CG (2.6%) died (P = NS). We conclude that VECDP by nasopharyngeal route does not reduce the incidence of RDS and does not appear to improve the outcome and may worsen the severity of RDS when compared to application of CDP for established criteria.', 'title': 'Randomized controlled trial of very early continuous distending pressure in the management of preterm infants.', 'date': '1987-01-01'}}",0.0,Pediatrics & Neonatology
193,"Is the risk of death or bronchopulmonary dysplasia (BPD) higher, lower, or the same when comparing prophylactic or very early CPAP to mechanical ventilation?",lower,moderate,no,"['18272893', '20472939', '22025591']",34661278,2021,"{'18272893': {'article_id': '18272893', 'content': ""Bronchopulmonary dysplasia is associated with ventilation and oxygen treatment. This randomized trial investigated whether nasal continuous positive airway pressure (CPAP), rather than intubation and ventilation, shortly after birth would reduce the rate of death or bronchopulmonary dysplasia in very preterm infants.\nWe randomly assigned 610 infants who were born at 25-to-28-weeks' gestation to CPAP or intubation and ventilation at 5 minutes after birth. We assessed outcomes at 28 days of age, at 36 weeks' gestational age, and before discharge.\nAt 36 weeks' gestational age, 33.9% of 307 infants who were assigned to receive CPAP had died or had bronchopulmonary dysplasia, as compared with 38.9% of 303 infants who were assigned to receive intubation (odds ratio favoring CPAP, 0.80; 95% confidence interval [CI], 0.58 to 1.12; P=0.19). At 28 days, there was a lower risk of death or need for oxygen therapy in the CPAP group than in the intubation group (odds ratio, 0.63; 95% CI, 0.46 to 0.88; P=0.006). There was little difference in overall mortality. In the CPAP group, 46% of infants were intubated during the first 5 days, and the use of surfactant was halved. The incidence of pneumothorax was 9% in the CPAP group, as compared with 3% in the intubation group (P<0.001). There were no other serious adverse events. The CPAP group had fewer days of ventilation.\nIn infants born at 25-to-28-weeks' gestation, early nasal CPAP did not significantly reduce the rate of death or bronchopulmonary dysplasia, as compared with intubation. Even though the CPAP group had more incidences of pneumothorax, fewer infants received oxygen at 28 days, and they had fewer days of ventilation. (Australian New Zealand Clinical Trials Registry number, 12606000258550.)."", 'title': 'Nasal CPAP or intubation at birth for very preterm infants.', 'date': '2008-02-15'}, '20472939': {'article_id': '20472939', 'content': 'There are limited data to inform the choice between early treatment with continuous positive airway pressure (CPAP) and early surfactant treatment as the initial support for extremely-low-birth-weight infants.\nWe performed a randomized, multicenter trial, with a 2-by-2 factorial design, involving infants who were born between 24 weeks 0 days and 27 weeks 6 days of gestation. Infants were randomly assigned to intubation and surfactant treatment (within 1 hour after birth) or to CPAP treatment initiated in the delivery room, with subsequent use of a protocol-driven limited ventilation strategy. Infants were also randomly assigned to one of two target ranges of oxygen saturation. The primary outcome was death or bronchopulmonary dysplasia as defined by the requirement for supplemental oxygen at 36 weeks (with an attempt at withdrawal of supplemental oxygen in neonates who were receiving less than 30% oxygen).\nA total of 1316 infants were enrolled in the study. The rates of the primary outcome did not differ significantly between the CPAP group and the surfactant group (47.8% and 51.0%, respectively; relative risk with CPAP, 0.95; 95% confidence interval [CI], 0.85 to 1.05) after adjustment for gestational age, center, and familial clustering. The results were similar when bronchopulmonary dysplasia was defined according to the need for any supplemental oxygen at 36 weeks (rates of primary outcome, 48.7% and 54.1%, respectively; relative risk with CPAP, 0.91; 95% CI, 0.83 to 1.01). Infants who received CPAP treatment, as compared with infants who received surfactant treatment, less frequently required intubation or postnatal corticosteroids for bronchopulmonary dysplasia (P<0.001), required fewer days of mechanical ventilation (P=0.03), and were more likely to be alive and free from the need for mechanical ventilation by day 7 (P=0.01). The rates of other adverse neonatal outcomes did not differ significantly between the two groups.\nThe results of this study support consideration of CPAP as an alternative to intubation and surfactant in preterm infants. (ClinicalTrials.gov number, NCT00233324.)', 'title': 'Early CPAP versus surfactant in extremely preterm infants.', 'date': '2010-05-18'}, '22025591': {'article_id': '22025591', 'content': ""We designed a multicenter randomized trial to compare 3 approaches to the initial respiratory management of preterm neonates: prophylactic surfactant followed by a period of mechanical ventilation (prophylactic surfactant [PS]); prophylactic surfactant with rapid extubation to bubble nasal continuous positive airway pressure (intubate-surfactant-extubate [ISX]) or initial management with bubble continuous positive airway pressure and selective surfactant treatment (nCPAP).\nNeonates born at 26 0/7 to 29 6/7 weeks' gestation were enrolled at participating Vermont Oxford Network centers and randomly assigned to PS, ISX, or nCPAP groups before delivery. Primary outcome was the incidence of death or bronchopulmonary dysplasia (BPD) at 36 weeks' postmenstrual age.\n648 infants enrolled at 27 centers. The study was halted before the desired sample size was reached because of declining enrollment. When compared with the PS group, the relative risk of BPD or death was 0.78 (95% confidence interval: 0.59-1.03) for the ISX group and 0.83 (95% confidence interval: 0.64-1.09) for the nCPAP group. There were no statistically significant differences in mortality or other complications of prematurity. In the nCPAP group, 48% were managed without intubation and ventilation, and 54% without surfactant treatment.\nPreterm neonates were initially managed with either nCPAP or PS with rapid extubation to nCPAP had similar clinical outcomes to those treated with PS followed by a period of mechanical ventilation. An approach that uses early nCPAP leads to a reduction in the number of infants who are intubated and given surfactant."", 'title': 'Randomized trial comparing 3 approaches to the initial respiratory management of preterm neonates.', 'date': '2011-10-26'}}",0.0,Pediatrics & Neonatology
194,"Is the risk of wound infection in open fracture wounds higher, lower, or the same when comparing negative pressure wound therapy (NPWT) at 125 mmHg to standard care?",uncertain effect,very low,no,"['29896626', '19704269', '27857499', '27022347']",29969521,2018,"{'29896626': {'article_id': '29896626', 'content': ""Open fractures of the lower limb occur when a broken bone penetrates the skin. There can be major complications from these fractures, which can be life-changing.\nTo assess the disability, rate of deep infection, and quality of life in patients with severe open fracture of the lower limb treated with negative pressure wound therapy (NPWT) vs standard wound management after the first surgical debridement of the wound.\nMulticenter randomized trial performed in the UK Major Trauma Network, recruiting 460 patients aged 16 years or older with a severe open fracture of the lower limb from July 2012 through December 2015. Final outcome data were collected through November 2016. Exclusions were presentation more than 72 hours after injury and inability to complete questionnaires.\nNPWT (n\u2009=\u2009226) in which an open-cell solid foam or gauze was placed over the surface of the wound and connected to a suction pump, creating a partial vacuum over the dressing, vs standard dressings not involving application of negative pressure (n\u2009=\u2009234).\nDisability Rating Index score (range, 0 [no disability] to 100 [completely disabled]) at 12 months was the primary outcome measure, with a minimal clinically important difference of 8 points. Secondary outcomes were complications including deep infection and quality of life (score ranged from 1 [best possible] to -0.59 [worst possible]; minimal clinically important difference, 0.08) collected at 3, 6, 9, and 12 months.\nAmong 460 patients who were randomized (mean age, 45.3 years; 74% men), 88% (374/427) of available study participants completed the trial. There were no statistically significant differences in the patients' Disability Rating Index score at 12 months (mean score, 45.5 in the NPWT group vs 42.4 in the standard dressing group; mean difference, -3.9 [95% CI, -8.9 to 1.2]; P\u2009=\u2009.13), in the number of deep surgical site infections (16 [7.1%] in the NPWT group vs 19 [8.1%] in the standard dressing group; difference, 1.0% [95% CI, -4.2% to 6.3%]; P\u2009=\u2009.64), or in quality of life between groups (difference in EuroQol 5-dimensions questionnaire, 0.02 [95% CI, -0.05 to 0.08]; Short Form-12 Physical Component Score, 0.5 [95% CI, -3.1 to 4.1] and Mental Health Component Score, -0.4 [95% CI, -2.2 to 1.4]).\nAmong patients with severe open fracture of the lower limb, use of NPWT compared with standard wound dressing did not improve self-rated disability at 12 months. The findings do not support this treatment for severe open fractures.\nisrctn.org Identifier: ISRCTN33756652."", 'title': 'Effect of Negative Pressure Wound Therapy vs Standard Wound Management on 12-Month Disability Among Adults With Severe Open Fracture of the Lower Limb: The WOLLF Randomized Clinical Trial.', 'date': '2018-06-14'}, '19704269': {'article_id': '19704269', 'content': 'To evaluate the impact of negative pressure wound therapy (NPWT) after severe open fractures on deep infection.\nProspective randomized study.\nAcademic level I trauma center.\nFifty-nine patients with 63 severe high-energy open fractures were enrolled in this study, with data available on 58 patients with 62 open fractures.\nTwenty-three patients with 25 fractures randomized to the control group and underwent initial irrigation and debridement followed by standard fine mesh gauze dressing, with repeat irrigation and debridement every 48-72 hours until wound closure. Thirty-five patients randomized to the NPWT group and had identical treatment except that NPWT was applied to the wounds between irrigation and debridement procedures until closure.\nThe presence or absence of deep wound infection or osteomyelitis, wound dehiscence, and fracture union were primary outcome measures.\nControl patients developed 2 acute infections (8%) and 5 delayed infections (20%), for a total of 7 deep infections (28%), whereas NPWT patients developed 0 acute infections, 2 delayed infections (5.4%), for a total of 2 deep infections (5.4%). There is a significant difference between the groups for total infections (P = 0.024). The relative risk ratio is 0.199 (95% confidence interval: 0.045-0.874), suggesting that patients treated with NPWT were only one-fifth as likely to have an infection compared with patients randomized to the control group. NPWT represents a promising new therapy for severe open fractures after high-energy trauma.', 'title': 'Negative pressure wound therapy after severe open fractures: a prospective randomized study.', 'date': '2009-08-26'}, '27857499': {'article_id': '27857499', 'content': 'Open tibial fractures are associated with a high incidence of mainly osteomyelitis. Negative pressure wound therapy (NPWT) is a novel form of treatment that uses subatmospheric pressure to effect early wound healing.\nTo determine the effect of NPWT on incidence of deep infections/osteomyelitis after open tibial fractures using a prospective randomized study design.\nNinety-three open tibial fractures were randomized into two groups receiving NPWT and the second group undergoing periodic irrigation, cleaning and debridement respectively. The wounds were closed or covered on shrinkage in size and sufficient granulation. Evidence of infection was sought during the course of treatment and follow up. Also serial cultures were sent every time the wound was cleaned.\nPatients in the control group developed a total of 11 infections (22%) as opposed to only 2 (4.6%) in the NPWT group (', 'title': 'Impact of negative pressure wound therapy on open diaphyseal tibial fractures: A prospective randomized trial.', 'date': '2016-11-20'}, '27022347': {'article_id': '27022347', 'content': 'Successful closure is a primary step of treatment in open fracture wounds. Delayed healing or complications can lead to increased treatment duration, costs and disability rates. The aim of this study was to compare Negative Pressure Wound Therapy (NPWT) and conventional wound dressings in patients with open fracture wounds.\nIn a prospective randomized clinical trial study, 90 patients with open fractures that were referred for treatment were enrolled between February 2013 to March 2015. Patients were divided into two groups. Group I underwent NPWT and group II underwent conventional wound dressing. Then patients were followed up for one month. Within the one month, the number of dressing change varied based on the extent of the wound. Duration of wound healing, presence of infection and the number of hospitalization days in these patients were recorded and compared at the end of the study between the two groups. Questionnaires and check lists were used to collect data. Analysis was done with SPSS 20, paired sample T-test, and chi-square tests. P<0.05 was considered significant.\nThere was a significant difference between the rate of wound healing in the group one or NPWT group and group II (conventional wound dressings) P<0.05. There was no significant difference between two groups in incidence of infection (P=0.6).\nUsing NPWT expedites the healing process of extremity wounds. It is more economical and can be considered as a substitute for the treatment of extremity wounds.', 'title': 'Comparison of negative pressure wound therapy (NPWT) &conventional wound dressings in the open fracture wounds.', 'date': '2016-03-30'}}",0.0,Surgery
195,"Is the risk of wound infection in non-fracture traumatic open wounds higher, lower, or the same when comparing negative pressure wound therapy (NPWT) at 125 mmHg to NWPT at 75 mmHg?",uncertain effect,very low,no,['26964825'],29969521,2018,"{'26964825': {'article_id': '26964825', 'content': 'The objectives were to investigate the emergency treatment of serious dog bite lacerations on limbs and to identify whether negative pressure wound therapy (NPWT) was beneficial in these instances.\nA total of 580 cases with serious limb lacerations due to dog bites were randomly divided into 2 groups. After thorough debridement, the limb lacerations of group A (n = 329) were left open. The remaining cases (n = 251) were randomly divided into 2 subgroups, group B and group C, which were treated with 125 and 75 mm Hg of continuous negative pressure, respectively. Antibiotics were only used in cases where there were systemic signs of wound infection, and were not given prophylactically. The infection rate, infection time, and healing time were analyzed.\nThe wound infection rates of groups A, B, and C were 9.1%, 4.1%, and 3.9%, respectively. The infection times of the 3 groups were 26.3 ± 11.6, 159.8 ± 13.4, and 166.4 ± 16.2 hours, respectively. The recovery times of the infection patients in the 3 groups were 19.2 ± 4.6, 13.2 ± 2.1, and 12.7 ± 2.3 days, respectively, and in the noninfection patients, the recovery times were 15.6 ± 2.7, 10.1 ± 2.3, and 10.5 ± 1.9 days, respectively. In groups B (-125 mm Hg) and C (-75 mm Hg), the infection rate, infection time, and healing time showed no significant differences.\nPatients with serious dog bite laceration on limbs could benefit from NPWT. Compared with the traditional treatment of leaving the wounds open, NPWT reduced the infection rate and shortened recovery time. When NPWT was performed, low negative pressure (-75 mm Hg) had the same positive effects as high pressure (-125 mm Hg). Prophylactic antibiotics administration is not recommended for treating this kind of laceration.\nTherapeutic/care management, level II.', 'title': 'Negative pressure wound therapy for serious dog bites of extremities: a prospective randomized trial.', 'date': '2016-03-12'}}",0.0,Surgery
196,"Is the risk of wound infection in non-fracture traumatic open wounds higher, lower, or the same when comparing negative pressure wound therapy (NPWT) at 75 mmHg to standard care?",uncertain effect,very low,no,['26964825'],29969521,2018,"{'26964825': {'article_id': '26964825', 'content': 'The objectives were to investigate the emergency treatment of serious dog bite lacerations on limbs and to identify whether negative pressure wound therapy (NPWT) was beneficial in these instances.\nA total of 580 cases with serious limb lacerations due to dog bites were randomly divided into 2 groups. After thorough debridement, the limb lacerations of group A (n = 329) were left open. The remaining cases (n = 251) were randomly divided into 2 subgroups, group B and group C, which were treated with 125 and 75 mm Hg of continuous negative pressure, respectively. Antibiotics were only used in cases where there were systemic signs of wound infection, and were not given prophylactically. The infection rate, infection time, and healing time were analyzed.\nThe wound infection rates of groups A, B, and C were 9.1%, 4.1%, and 3.9%, respectively. The infection times of the 3 groups were 26.3 ± 11.6, 159.8 ± 13.4, and 166.4 ± 16.2 hours, respectively. The recovery times of the infection patients in the 3 groups were 19.2 ± 4.6, 13.2 ± 2.1, and 12.7 ± 2.3 days, respectively, and in the noninfection patients, the recovery times were 15.6 ± 2.7, 10.1 ± 2.3, and 10.5 ± 1.9 days, respectively. In groups B (-125 mm Hg) and C (-75 mm Hg), the infection rate, infection time, and healing time showed no significant differences.\nPatients with serious dog bite laceration on limbs could benefit from NPWT. Compared with the traditional treatment of leaving the wounds open, NPWT reduced the infection rate and shortened recovery time. When NPWT was performed, low negative pressure (-75 mm Hg) had the same positive effects as high pressure (-125 mm Hg). Prophylactic antibiotics administration is not recommended for treating this kind of laceration.\nTherapeutic/care management, level II.', 'title': 'Negative pressure wound therapy for serious dog bites of extremities: a prospective randomized trial.', 'date': '2016-03-12'}}",0.0,Surgery
197,"Is the risk of mortality higher, lower, or the same when comparing 1g paracetamol IV to saline IV?",uncertain effect,very low,yes,['26678710'],33126293,2020,"{'26678710': {'article_id': '26678710', 'content': 'Strategies to prevent pyrexia in patients with acute neurological injury may reduce secondary neuronal damage. The aim of this study was to determine the safety and efficacy of the routine administration of 6 grams/day of intravenous paracetamol in reducing body temperature following severe traumatic brain injury, compared to placebo.\nA multicentre, randomised, blind, placebo-controlled clinical trial in adult patients with traumatic brain injury (TBI). Patients were randomised to receive an intravenous infusion of either 1g of paracetamol or 0.9% sodium chloride (saline) every 4 hours for 72 hours. The primary outcome was the mean difference in core temperature during the study intervention period.\nForty-one patients were included in this study: 21 were allocated to paracetamol and 20 to saline. The median (interquartile range) number of doses of study drug was 18 (17-18) in the paracetamol group and 18 (16-18) in the saline group (P = 0.85). From randomisation until 4 hours after the last dose of study treatment, there were 2798 temperature measurements (median 73 [67-76] per patient). The mean ± standard deviation temperature was 37.4±0.5°C in the paracetamol group and 37.7±0.4°C in the saline group (absolute difference -0.3°C; 95% confidence interval -0.6 to 0.0; P = 0.09). There were no significant differences in the use of physical cooling, or episodes of hypotension or hepatic abnormalities, between the two groups.\nThe routine administration of 6g/day of intravenous paracetamol did not significantly reduce core body temperature in patients with TBI.\nAustralian New Zealand Clinical Trials Registry ACTRN12609000444280.', 'title': 'The Effect of Paracetamol on Core Body Temperature in Acute Traumatic Brain Injury: A Randomised, Controlled Clinical Trial.', 'date': '2015-12-19'}}",0.0,Emergency Medicine & Critical Care
198,"Is the rate of any PCR‐positive SARS‐COV‐2 infection higher, lower, or the same when comparing test‐based attendance to standard 10‐day self‐isolation?",uncertain effect,very low,yes,['34534517'],35514111,2022,"{'34534517': {'article_id': '34534517', 'content': 'School-based COVID-19 contacts in England have been asked to self-isolate at home, missing key educational opportunities. We trialled daily testing of contacts as an alternative to assess whether this resulted in similar control of transmission, while allowing more school attendance.\nWe did an open-label, cluster-randomised, controlled trial in secondary schools and further education colleges in England. Schools were randomly assigned (1:1) to self-isolation of school-based COVID-19 contacts for 10 days (control) or to voluntary daily lateral flow device (LFD) testing for 7 days with LFD-negative contacts remaining at school (intervention). Randomisation was stratified according to school type and size, presence of a sixth form, presence of residential students, and proportion of students eligible for free school meals. Group assignment was not masked during procedures or analysis. Coprimary outcomes in all students and staff were COVID-19-related school absence and symptomatic PCR-confirmed COVID-19, adjusted for community case rates, to estimate within-school transmission (non-inferiority margin <50% relative increase). Analyses were done on an intention-to-treat basis using quasi-Poisson regression, also estimating complier average causal effects (CACE). This trial is registered with the ISRCTN registry, ISRCTN18100261.\nBetween March 18 and May 4, 2021, 204 schools were taken through the consent process, during which three decided not to participate further. 201 schools were randomly assigned (control group n=99, intervention group n=102) in the 10-week study (April 19-May 10, 2021), which continued until the pre-appointed stop date (June 27, 2021). 76 control group schools and 86 intervention group schools actively participated; additional national data allowed most non-participating schools to be included in analysis of coprimary outcomes. 2432 (42·4%) of 5763 intervention group contacts participated in daily contact testing. There were 657 symptomatic PCR-confirmed infections during 7\u2009782\u2009537 days-at-risk (59·1 per 100\u2009000 per week) in the control group and 740 during 8\u2009379\u2009749 days-at-risk (61·8 per 100\u2009000 per week) in the intervention group (intention-to-treat adjusted incidence rate ratio [aIRR] 0·96 [95% CI 0·75-1·22]; p=0·72; CACE aIRR 0·86 [0·55-1·34]). Among students and staff, there were 59\u2009422 (1·62%) COVID-19-related absences during 3\u2009659\u2009017 person-school-days in the control group and 51\u2009541 (1·34%) during 3\u2009845\u2009208 person-school-days in the intervention group (intention-to-treat aIRR 0·80 [95% CI 0·54-1·19]; p=0·27; CACE aIRR 0·61 [0·30-1·23]).\nDaily contact testing of school-based contacts was non-inferior to self-isolation for control of COVID-19 transmission, with similar rates of symptomatic infections among students and staff with both approaches. Infection rates in school-based contacts were low, with very few school contacts testing positive. Daily contact testing should be considered for implementation as a safe alternative to home isolation following school-based exposures.\nUK Government Department of Health and Social Care.', 'title': 'Daily testing for contacts of individuals with SARS-CoV-2 infection and attendance and SARS-CoV-2 transmission in English secondary schools and colleges: an open-label, cluster-randomised trial.', 'date': '2021-09-18'}}",0.0,"Public Health, Epidemiology & Health Systems"
199,"Is the rate of COVID‐related absenteeism higher, lower, or the same when comparing test‐based attendance to standard 10‐day self‐isolation?",no difference,low,no,['34534517'],35514111,2022,"{'34534517': {'article_id': '34534517', 'content': 'School-based COVID-19 contacts in England have been asked to self-isolate at home, missing key educational opportunities. We trialled daily testing of contacts as an alternative to assess whether this resulted in similar control of transmission, while allowing more school attendance.\nWe did an open-label, cluster-randomised, controlled trial in secondary schools and further education colleges in England. Schools were randomly assigned (1:1) to self-isolation of school-based COVID-19 contacts for 10 days (control) or to voluntary daily lateral flow device (LFD) testing for 7 days with LFD-negative contacts remaining at school (intervention). Randomisation was stratified according to school type and size, presence of a sixth form, presence of residential students, and proportion of students eligible for free school meals. Group assignment was not masked during procedures or analysis. Coprimary outcomes in all students and staff were COVID-19-related school absence and symptomatic PCR-confirmed COVID-19, adjusted for community case rates, to estimate within-school transmission (non-inferiority margin <50% relative increase). Analyses were done on an intention-to-treat basis using quasi-Poisson regression, also estimating complier average causal effects (CACE). This trial is registered with the ISRCTN registry, ISRCTN18100261.\nBetween March 18 and May 4, 2021, 204 schools were taken through the consent process, during which three decided not to participate further. 201 schools were randomly assigned (control group n=99, intervention group n=102) in the 10-week study (April 19-May 10, 2021), which continued until the pre-appointed stop date (June 27, 2021). 76 control group schools and 86 intervention group schools actively participated; additional national data allowed most non-participating schools to be included in analysis of coprimary outcomes. 2432 (42·4%) of 5763 intervention group contacts participated in daily contact testing. There were 657 symptomatic PCR-confirmed infections during 7\u2009782\u2009537 days-at-risk (59·1 per 100\u2009000 per week) in the control group and 740 during 8\u2009379\u2009749 days-at-risk (61·8 per 100\u2009000 per week) in the intervention group (intention-to-treat adjusted incidence rate ratio [aIRR] 0·96 [95% CI 0·75-1·22]; p=0·72; CACE aIRR 0·86 [0·55-1·34]). Among students and staff, there were 59\u2009422 (1·62%) COVID-19-related absences during 3\u2009659\u2009017 person-school-days in the control group and 51\u2009541 (1·34%) during 3\u2009845\u2009208 person-school-days in the intervention group (intention-to-treat aIRR 0·80 [95% CI 0·54-1·19]; p=0·27; CACE aIRR 0·61 [0·30-1·23]).\nDaily contact testing of school-based contacts was non-inferior to self-isolation for control of COVID-19 transmission, with similar rates of symptomatic infections among students and staff with both approaches. Infection rates in school-based contacts were low, with very few school contacts testing positive. Daily contact testing should be considered for implementation as a safe alternative to home isolation following school-based exposures.\nUK Government Department of Health and Social Care.', 'title': 'Daily testing for contacts of individuals with SARS-CoV-2 infection and attendance and SARS-CoV-2 transmission in English secondary schools and colleges: an open-label, cluster-randomised trial.', 'date': '2021-09-18'}}",1.0,"Public Health, Epidemiology & Health Systems"
200,"Is the rate of symptomatic PCR‐positive SARS‐COV‐2 infection higher, lower, or the same when comparing test‐based attendance to standard 10‐day self‐isolation?",uncertain effect,very low,yes,['34534517'],35514111,2022,"{'34534517': {'article_id': '34534517', 'content': 'School-based COVID-19 contacts in England have been asked to self-isolate at home, missing key educational opportunities. We trialled daily testing of contacts as an alternative to assess whether this resulted in similar control of transmission, while allowing more school attendance.\nWe did an open-label, cluster-randomised, controlled trial in secondary schools and further education colleges in England. Schools were randomly assigned (1:1) to self-isolation of school-based COVID-19 contacts for 10 days (control) or to voluntary daily lateral flow device (LFD) testing for 7 days with LFD-negative contacts remaining at school (intervention). Randomisation was stratified according to school type and size, presence of a sixth form, presence of residential students, and proportion of students eligible for free school meals. Group assignment was not masked during procedures or analysis. Coprimary outcomes in all students and staff were COVID-19-related school absence and symptomatic PCR-confirmed COVID-19, adjusted for community case rates, to estimate within-school transmission (non-inferiority margin <50% relative increase). Analyses were done on an intention-to-treat basis using quasi-Poisson regression, also estimating complier average causal effects (CACE). This trial is registered with the ISRCTN registry, ISRCTN18100261.\nBetween March 18 and May 4, 2021, 204 schools were taken through the consent process, during which three decided not to participate further. 201 schools were randomly assigned (control group n=99, intervention group n=102) in the 10-week study (April 19-May 10, 2021), which continued until the pre-appointed stop date (June 27, 2021). 76 control group schools and 86 intervention group schools actively participated; additional national data allowed most non-participating schools to be included in analysis of coprimary outcomes. 2432 (42·4%) of 5763 intervention group contacts participated in daily contact testing. There were 657 symptomatic PCR-confirmed infections during 7\u2009782\u2009537 days-at-risk (59·1 per 100\u2009000 per week) in the control group and 740 during 8\u2009379\u2009749 days-at-risk (61·8 per 100\u2009000 per week) in the intervention group (intention-to-treat adjusted incidence rate ratio [aIRR] 0·96 [95% CI 0·75-1·22]; p=0·72; CACE aIRR 0·86 [0·55-1·34]). Among students and staff, there were 59\u2009422 (1·62%) COVID-19-related absences during 3\u2009659\u2009017 person-school-days in the control group and 51\u2009541 (1·34%) during 3\u2009845\u2009208 person-school-days in the intervention group (intention-to-treat aIRR 0·80 [95% CI 0·54-1·19]; p=0·27; CACE aIRR 0·61 [0·30-1·23]).\nDaily contact testing of school-based contacts was non-inferior to self-isolation for control of COVID-19 transmission, with similar rates of symptomatic infections among students and staff with both approaches. Infection rates in school-based contacts were low, with very few school contacts testing positive. Daily contact testing should be considered for implementation as a safe alternative to home isolation following school-based exposures.\nUK Government Department of Health and Social Care.', 'title': 'Daily testing for contacts of individuals with SARS-CoV-2 infection and attendance and SARS-CoV-2 transmission in English secondary schools and colleges: an open-label, cluster-randomised trial.', 'date': '2021-09-18'}}",0.0,"Public Health, Epidemiology & Health Systems"
201,"Is the quality of life higher, lower, or the same when comparing test‐based attendance to standard 10‐day self‐isolation?",insufficient data,,yes,['34534517'],35514111,2022,"{'34534517': {'article_id': '34534517', 'content': 'School-based COVID-19 contacts in England have been asked to self-isolate at home, missing key educational opportunities. We trialled daily testing of contacts as an alternative to assess whether this resulted in similar control of transmission, while allowing more school attendance.\nWe did an open-label, cluster-randomised, controlled trial in secondary schools and further education colleges in England. Schools were randomly assigned (1:1) to self-isolation of school-based COVID-19 contacts for 10 days (control) or to voluntary daily lateral flow device (LFD) testing for 7 days with LFD-negative contacts remaining at school (intervention). Randomisation was stratified according to school type and size, presence of a sixth form, presence of residential students, and proportion of students eligible for free school meals. Group assignment was not masked during procedures or analysis. Coprimary outcomes in all students and staff were COVID-19-related school absence and symptomatic PCR-confirmed COVID-19, adjusted for community case rates, to estimate within-school transmission (non-inferiority margin <50% relative increase). Analyses were done on an intention-to-treat basis using quasi-Poisson regression, also estimating complier average causal effects (CACE). This trial is registered with the ISRCTN registry, ISRCTN18100261.\nBetween March 18 and May 4, 2021, 204 schools were taken through the consent process, during which three decided not to participate further. 201 schools were randomly assigned (control group n=99, intervention group n=102) in the 10-week study (April 19-May 10, 2021), which continued until the pre-appointed stop date (June 27, 2021). 76 control group schools and 86 intervention group schools actively participated; additional national data allowed most non-participating schools to be included in analysis of coprimary outcomes. 2432 (42·4%) of 5763 intervention group contacts participated in daily contact testing. There were 657 symptomatic PCR-confirmed infections during 7\u2009782\u2009537 days-at-risk (59·1 per 100\u2009000 per week) in the control group and 740 during 8\u2009379\u2009749 days-at-risk (61·8 per 100\u2009000 per week) in the intervention group (intention-to-treat adjusted incidence rate ratio [aIRR] 0·96 [95% CI 0·75-1·22]; p=0·72; CACE aIRR 0·86 [0·55-1·34]). Among students and staff, there were 59\u2009422 (1·62%) COVID-19-related absences during 3\u2009659\u2009017 person-school-days in the control group and 51\u2009541 (1·34%) during 3\u2009845\u2009208 person-school-days in the intervention group (intention-to-treat aIRR 0·80 [95% CI 0·54-1·19]; p=0·27; CACE aIRR 0·61 [0·30-1·23]).\nDaily contact testing of school-based contacts was non-inferior to self-isolation for control of COVID-19 transmission, with similar rates of symptomatic infections among students and staff with both approaches. Infection rates in school-based contacts were low, with very few school contacts testing positive. Daily contact testing should be considered for implementation as a safe alternative to home isolation following school-based exposures.\nUK Government Department of Health and Social Care.', 'title': 'Daily testing for contacts of individuals with SARS-CoV-2 infection and attendance and SARS-CoV-2 transmission in English secondary schools and colleges: an open-label, cluster-randomised trial.', 'date': '2021-09-18'}}",1.0,"Public Health, Epidemiology & Health Systems"
202,"Is SARS‐CoV‐2‐related mortality higher, lower, or the same when comparing test‐based attendance to standard 10‐day self‐isolation?",insufficient data,,yes,['34534517'],35514111,2022,"{'34534517': {'article_id': '34534517', 'content': 'School-based COVID-19 contacts in England have been asked to self-isolate at home, missing key educational opportunities. We trialled daily testing of contacts as an alternative to assess whether this resulted in similar control of transmission, while allowing more school attendance.\nWe did an open-label, cluster-randomised, controlled trial in secondary schools and further education colleges in England. Schools were randomly assigned (1:1) to self-isolation of school-based COVID-19 contacts for 10 days (control) or to voluntary daily lateral flow device (LFD) testing for 7 days with LFD-negative contacts remaining at school (intervention). Randomisation was stratified according to school type and size, presence of a sixth form, presence of residential students, and proportion of students eligible for free school meals. Group assignment was not masked during procedures or analysis. Coprimary outcomes in all students and staff were COVID-19-related school absence and symptomatic PCR-confirmed COVID-19, adjusted for community case rates, to estimate within-school transmission (non-inferiority margin <50% relative increase). Analyses were done on an intention-to-treat basis using quasi-Poisson regression, also estimating complier average causal effects (CACE). This trial is registered with the ISRCTN registry, ISRCTN18100261.\nBetween March 18 and May 4, 2021, 204 schools were taken through the consent process, during which three decided not to participate further. 201 schools were randomly assigned (control group n=99, intervention group n=102) in the 10-week study (April 19-May 10, 2021), which continued until the pre-appointed stop date (June 27, 2021). 76 control group schools and 86 intervention group schools actively participated; additional national data allowed most non-participating schools to be included in analysis of coprimary outcomes. 2432 (42·4%) of 5763 intervention group contacts participated in daily contact testing. There were 657 symptomatic PCR-confirmed infections during 7\u2009782\u2009537 days-at-risk (59·1 per 100\u2009000 per week) in the control group and 740 during 8\u2009379\u2009749 days-at-risk (61·8 per 100\u2009000 per week) in the intervention group (intention-to-treat adjusted incidence rate ratio [aIRR] 0·96 [95% CI 0·75-1·22]; p=0·72; CACE aIRR 0·86 [0·55-1·34]). Among students and staff, there were 59\u2009422 (1·62%) COVID-19-related absences during 3\u2009659\u2009017 person-school-days in the control group and 51\u2009541 (1·34%) during 3\u2009845\u2009208 person-school-days in the intervention group (intention-to-treat aIRR 0·80 [95% CI 0·54-1·19]; p=0·27; CACE aIRR 0·61 [0·30-1·23]).\nDaily contact testing of school-based contacts was non-inferior to self-isolation for control of COVID-19 transmission, with similar rates of symptomatic infections among students and staff with both approaches. Infection rates in school-based contacts were low, with very few school contacts testing positive. Daily contact testing should be considered for implementation as a safe alternative to home isolation following school-based exposures.\nUK Government Department of Health and Social Care.', 'title': 'Daily testing for contacts of individuals with SARS-CoV-2 infection and attendance and SARS-CoV-2 transmission in English secondary schools and colleges: an open-label, cluster-randomised trial.', 'date': '2021-09-18'}}",1.0,"Public Health, Epidemiology & Health Systems"
203,"Is the rate of hospitalization higher, lower, or the same when comparing test‐based attendance to standard 10‐day self‐isolation?",insufficient data,,yes,['34534517'],35514111,2022,"{'34534517': {'article_id': '34534517', 'content': 'School-based COVID-19 contacts in England have been asked to self-isolate at home, missing key educational opportunities. We trialled daily testing of contacts as an alternative to assess whether this resulted in similar control of transmission, while allowing more school attendance.\nWe did an open-label, cluster-randomised, controlled trial in secondary schools and further education colleges in England. Schools were randomly assigned (1:1) to self-isolation of school-based COVID-19 contacts for 10 days (control) or to voluntary daily lateral flow device (LFD) testing for 7 days with LFD-negative contacts remaining at school (intervention). Randomisation was stratified according to school type and size, presence of a sixth form, presence of residential students, and proportion of students eligible for free school meals. Group assignment was not masked during procedures or analysis. Coprimary outcomes in all students and staff were COVID-19-related school absence and symptomatic PCR-confirmed COVID-19, adjusted for community case rates, to estimate within-school transmission (non-inferiority margin <50% relative increase). Analyses were done on an intention-to-treat basis using quasi-Poisson regression, also estimating complier average causal effects (CACE). This trial is registered with the ISRCTN registry, ISRCTN18100261.\nBetween March 18 and May 4, 2021, 204 schools were taken through the consent process, during which three decided not to participate further. 201 schools were randomly assigned (control group n=99, intervention group n=102) in the 10-week study (April 19-May 10, 2021), which continued until the pre-appointed stop date (June 27, 2021). 76 control group schools and 86 intervention group schools actively participated; additional national data allowed most non-participating schools to be included in analysis of coprimary outcomes. 2432 (42·4%) of 5763 intervention group contacts participated in daily contact testing. There were 657 symptomatic PCR-confirmed infections during 7\u2009782\u2009537 days-at-risk (59·1 per 100\u2009000 per week) in the control group and 740 during 8\u2009379\u2009749 days-at-risk (61·8 per 100\u2009000 per week) in the intervention group (intention-to-treat adjusted incidence rate ratio [aIRR] 0·96 [95% CI 0·75-1·22]; p=0·72; CACE aIRR 0·86 [0·55-1·34]). Among students and staff, there were 59\u2009422 (1·62%) COVID-19-related absences during 3\u2009659\u2009017 person-school-days in the control group and 51\u2009541 (1·34%) during 3\u2009845\u2009208 person-school-days in the intervention group (intention-to-treat aIRR 0·80 [95% CI 0·54-1·19]; p=0·27; CACE aIRR 0·61 [0·30-1·23]).\nDaily contact testing of school-based contacts was non-inferior to self-isolation for control of COVID-19 transmission, with similar rates of symptomatic infections among students and staff with both approaches. Infection rates in school-based contacts were low, with very few school contacts testing positive. Daily contact testing should be considered for implementation as a safe alternative to home isolation following school-based exposures.\nUK Government Department of Health and Social Care.', 'title': 'Daily testing for contacts of individuals with SARS-CoV-2 infection and attendance and SARS-CoV-2 transmission in English secondary schools and colleges: an open-label, cluster-randomised trial.', 'date': '2021-09-18'}}",1.0,"Public Health, Epidemiology & Health Systems"
204,"Is loneliness at 6 months higher, lower, or the same when comparing video calls to usual care?",uncertain effect,very low,yes,"['31992217', '22086660']",32441330,2020,"{'31992217': {'article_id': '31992217', 'content': 'Smartphones can optimize the opportunities for interactions between nursing home residents and their families. However, the effectiveness of smartphone-based videoconferencing programs in enhancing emotional status and quality of life has not been explored. The purpose of this study was to evaluate of the effect of a smartphone-based videoconferencing program on nursing home residents\' feelings of loneliness, depressive symptoms and quality of life.\nThis study used a quasi-experimental research design. Older residents from seven nursing homes in Taiwan participated in this study. Nursing homes (NH) were randomly selected as sites for either the intervention group (5 NH) or the control group (2 NH); NH residents who met the inclusion criteria were invited to participate. The intervention group was comprised of 32 participants; the control group was comprised of 30 participants. The intervention group interacted with their family members once a week for 6\u2009months using a smartphone and a ""LINE"" application (app). Data were collected with self-report instruments: subjective feelings of loneliness, using the University of California Los Angeles Loneliness Scale; depressive symptoms, using the Geriatric Depression Scale; and quality of life using the SF-36. Data were collected at four time points (baseline, and at 1-month, 3-months and 6-months from baseline). Data were analysed using the generalized estimating equation approach.\nAfter the intervention, as compared to those in the control group, participants in interventional group had significant decreases in baseline loneliness scores at 1\u2009months (β\u2009=\u2009-\u20093.41, p\u2009<\u20090.001), 3\u2009months (β\u2009=\u2009-\u20095.96, p\u2009<\u20090.001), and 6\u2009months (β\u2009=\u2009-\u20097.50, p\u2009<\u20090.001), and improvements in physical role (β\u2009=\u200936.49, p\u2009=\u20090.01), vitality (β\u2009=\u200913.11, p\u2009<\u20090.001) and pain scores (β\u2009=\u200916.71, p\u2009=\u20090.01) at 6\u2009months. However, changes in mean depression scores did not significantly differ between groups.\nSmartphone-based videoconferencing effectively improved residents\' feelings of loneliness, and physiological health, vitality and pain, but not depressive symptoms. Future investigations might evaluate the effectiveness of other media-based technologies in nursing homes as well as their effectiveness within and between different age cohorts.', 'title': 'Effects of a smartphone-based videoconferencing program for older nursing home residents on depression, loneliness, and quality of life: a quasi-experimental study.', 'date': '2020-01-30'}, '22086660': {'article_id': '22086660', 'content': ""A 3-month videoconference interaction program with family members has been shown to decrease depression and loneliness in nursing home residents. However, little is known about the long-term effects on residents' depressive symptoms, social support, and loneliness.\nThe purpose of this longitudinal quasi-experimental study was to evaluate the long-term effectiveness of a videoconference intervention in improving nursing home residents' social support, loneliness, and depressive status over 1 year.\nWe purposively sampled 16 nursing homes in various areas of Taiwan. Elderly residents (N = 90) of these nursing homes meeting our inclusion criteria were divided into an experimental (n = 40) and a comparison (n = 50) group. The experimental group received at least 5 minutes/week for 3 months of videoconference interaction with their family members in addition to usual family visits, and the comparison group received regular family visits only. Data were collected in face-to face interviews on social support, loneliness, and depressive status using the Social Support Behaviors Scale, University of California Los Angeles Loneliness Scale, and Geriatric Depression Scale, respectively, at four times (baseline, 3 months, 6 months, and 12 months after baseline). Data were analyzed using the generalized estimating equation approach.\nAfter the videoconferencing program, participants in the experimental group had significantly lower mean change in instrumental social support scores at 6 months (-0.42, P = .03) and 12 months (-0.41, P = .03), and higher mean change in emotional social support at 3 (0.74, P < .001) and 12 months (0.61, P = .02), and in appraisal support at 3 months (0.74, P = .001) after adjusting for confounding variables. Participants in the experimental group also had significantly lower mean loneliness and depressive status scores at 3 months (-5.40, P < .001; -2.64, P < .001, respectively), 6 months (-6.47, P < .001; -4.33, P < .001), and 12 months (-6.27, P = .001; -4.40, P < .001) compared with baseline than those in the comparison group.\nOur videoconference program had a long-term effect in alleviating depressive symptoms and loneliness for elderly residents in nursing homes. This intervention also improved long-term emotional social support and short-term appraisal support, and decreased residents' instrumental social support. However, this intervention had no effect on informational social support."", 'title': 'Changes in depressive symptoms, social support, and loneliness over 1 year after a minimum 3-month videoconference program for older nursing home residents.', 'date': '2011-11-17'}}",0.0,Psychiatry & Neurology
205,"Is loneliness at 1 year higher, lower, or the same when comparing video calls to usual care?",uncertain effect,very low,yes,['22086660'],32441330,2020,"{'22086660': {'article_id': '22086660', 'content': ""A 3-month videoconference interaction program with family members has been shown to decrease depression and loneliness in nursing home residents. However, little is known about the long-term effects on residents' depressive symptoms, social support, and loneliness.\nThe purpose of this longitudinal quasi-experimental study was to evaluate the long-term effectiveness of a videoconference intervention in improving nursing home residents' social support, loneliness, and depressive status over 1 year.\nWe purposively sampled 16 nursing homes in various areas of Taiwan. Elderly residents (N = 90) of these nursing homes meeting our inclusion criteria were divided into an experimental (n = 40) and a comparison (n = 50) group. The experimental group received at least 5 minutes/week for 3 months of videoconference interaction with their family members in addition to usual family visits, and the comparison group received regular family visits only. Data were collected in face-to face interviews on social support, loneliness, and depressive status using the Social Support Behaviors Scale, University of California Los Angeles Loneliness Scale, and Geriatric Depression Scale, respectively, at four times (baseline, 3 months, 6 months, and 12 months after baseline). Data were analyzed using the generalized estimating equation approach.\nAfter the videoconferencing program, participants in the experimental group had significantly lower mean change in instrumental social support scores at 6 months (-0.42, P = .03) and 12 months (-0.41, P = .03), and higher mean change in emotional social support at 3 (0.74, P < .001) and 12 months (0.61, P = .02), and in appraisal support at 3 months (0.74, P = .001) after adjusting for confounding variables. Participants in the experimental group also had significantly lower mean loneliness and depressive status scores at 3 months (-5.40, P < .001; -2.64, P < .001, respectively), 6 months (-6.47, P < .001; -4.33, P < .001), and 12 months (-6.27, P = .001; -4.40, P < .001) compared with baseline than those in the comparison group.\nOur videoconference program had a long-term effect in alleviating depressive symptoms and loneliness for elderly residents in nursing homes. This intervention also improved long-term emotional social support and short-term appraisal support, and decreased residents' instrumental social support. However, this intervention had no effect on informational social support."", 'title': 'Changes in depressive symptoms, social support, and loneliness over 1 year after a minimum 3-month videoconference program for older nursing home residents.', 'date': '2011-11-17'}}",0.0,"Public Health, Epidemiology & Health Systems"
206,"Is depression at 6 months higher, lower, or the same when comparing video calls to usual care?",uncertain effect,very low,yes,"['31992217', '22086660']",32441330,2020,"{'31992217': {'article_id': '31992217', 'content': 'Smartphones can optimize the opportunities for interactions between nursing home residents and their families. However, the effectiveness of smartphone-based videoconferencing programs in enhancing emotional status and quality of life has not been explored. The purpose of this study was to evaluate of the effect of a smartphone-based videoconferencing program on nursing home residents\' feelings of loneliness, depressive symptoms and quality of life.\nThis study used a quasi-experimental research design. Older residents from seven nursing homes in Taiwan participated in this study. Nursing homes (NH) were randomly selected as sites for either the intervention group (5 NH) or the control group (2 NH); NH residents who met the inclusion criteria were invited to participate. The intervention group was comprised of 32 participants; the control group was comprised of 30 participants. The intervention group interacted with their family members once a week for 6\u2009months using a smartphone and a ""LINE"" application (app). Data were collected with self-report instruments: subjective feelings of loneliness, using the University of California Los Angeles Loneliness Scale; depressive symptoms, using the Geriatric Depression Scale; and quality of life using the SF-36. Data were collected at four time points (baseline, and at 1-month, 3-months and 6-months from baseline). Data were analysed using the generalized estimating equation approach.\nAfter the intervention, as compared to those in the control group, participants in interventional group had significant decreases in baseline loneliness scores at 1\u2009months (β\u2009=\u2009-\u20093.41, p\u2009<\u20090.001), 3\u2009months (β\u2009=\u2009-\u20095.96, p\u2009<\u20090.001), and 6\u2009months (β\u2009=\u2009-\u20097.50, p\u2009<\u20090.001), and improvements in physical role (β\u2009=\u200936.49, p\u2009=\u20090.01), vitality (β\u2009=\u200913.11, p\u2009<\u20090.001) and pain scores (β\u2009=\u200916.71, p\u2009=\u20090.01) at 6\u2009months. However, changes in mean depression scores did not significantly differ between groups.\nSmartphone-based videoconferencing effectively improved residents\' feelings of loneliness, and physiological health, vitality and pain, but not depressive symptoms. Future investigations might evaluate the effectiveness of other media-based technologies in nursing homes as well as their effectiveness within and between different age cohorts.', 'title': 'Effects of a smartphone-based videoconferencing program for older nursing home residents on depression, loneliness, and quality of life: a quasi-experimental study.', 'date': '2020-01-30'}, '22086660': {'article_id': '22086660', 'content': ""A 3-month videoconference interaction program with family members has been shown to decrease depression and loneliness in nursing home residents. However, little is known about the long-term effects on residents' depressive symptoms, social support, and loneliness.\nThe purpose of this longitudinal quasi-experimental study was to evaluate the long-term effectiveness of a videoconference intervention in improving nursing home residents' social support, loneliness, and depressive status over 1 year.\nWe purposively sampled 16 nursing homes in various areas of Taiwan. Elderly residents (N = 90) of these nursing homes meeting our inclusion criteria were divided into an experimental (n = 40) and a comparison (n = 50) group. The experimental group received at least 5 minutes/week for 3 months of videoconference interaction with their family members in addition to usual family visits, and the comparison group received regular family visits only. Data were collected in face-to face interviews on social support, loneliness, and depressive status using the Social Support Behaviors Scale, University of California Los Angeles Loneliness Scale, and Geriatric Depression Scale, respectively, at four times (baseline, 3 months, 6 months, and 12 months after baseline). Data were analyzed using the generalized estimating equation approach.\nAfter the videoconferencing program, participants in the experimental group had significantly lower mean change in instrumental social support scores at 6 months (-0.42, P = .03) and 12 months (-0.41, P = .03), and higher mean change in emotional social support at 3 (0.74, P < .001) and 12 months (0.61, P = .02), and in appraisal support at 3 months (0.74, P = .001) after adjusting for confounding variables. Participants in the experimental group also had significantly lower mean loneliness and depressive status scores at 3 months (-5.40, P < .001; -2.64, P < .001, respectively), 6 months (-6.47, P < .001; -4.33, P < .001), and 12 months (-6.27, P = .001; -4.40, P < .001) compared with baseline than those in the comparison group.\nOur videoconference program had a long-term effect in alleviating depressive symptoms and loneliness for elderly residents in nursing homes. This intervention also improved long-term emotional social support and short-term appraisal support, and decreased residents' instrumental social support. However, this intervention had no effect on informational social support."", 'title': 'Changes in depressive symptoms, social support, and loneliness over 1 year after a minimum 3-month videoconference program for older nursing home residents.', 'date': '2011-11-17'}}",0.0,Psychiatry & Neurology
207,"Is depression at 1 year higher, lower, or the same when comparing video calls to usual care?",uncertain effect,very low,yes,['22086660'],32441330,2020,"{'22086660': {'article_id': '22086660', 'content': ""A 3-month videoconference interaction program with family members has been shown to decrease depression and loneliness in nursing home residents. However, little is known about the long-term effects on residents' depressive symptoms, social support, and loneliness.\nThe purpose of this longitudinal quasi-experimental study was to evaluate the long-term effectiveness of a videoconference intervention in improving nursing home residents' social support, loneliness, and depressive status over 1 year.\nWe purposively sampled 16 nursing homes in various areas of Taiwan. Elderly residents (N = 90) of these nursing homes meeting our inclusion criteria were divided into an experimental (n = 40) and a comparison (n = 50) group. The experimental group received at least 5 minutes/week for 3 months of videoconference interaction with their family members in addition to usual family visits, and the comparison group received regular family visits only. Data were collected in face-to face interviews on social support, loneliness, and depressive status using the Social Support Behaviors Scale, University of California Los Angeles Loneliness Scale, and Geriatric Depression Scale, respectively, at four times (baseline, 3 months, 6 months, and 12 months after baseline). Data were analyzed using the generalized estimating equation approach.\nAfter the videoconferencing program, participants in the experimental group had significantly lower mean change in instrumental social support scores at 6 months (-0.42, P = .03) and 12 months (-0.41, P = .03), and higher mean change in emotional social support at 3 (0.74, P < .001) and 12 months (0.61, P = .02), and in appraisal support at 3 months (0.74, P = .001) after adjusting for confounding variables. Participants in the experimental group also had significantly lower mean loneliness and depressive status scores at 3 months (-5.40, P < .001; -2.64, P < .001, respectively), 6 months (-6.47, P < .001; -4.33, P < .001), and 12 months (-6.27, P = .001; -4.40, P < .001) compared with baseline than those in the comparison group.\nOur videoconference program had a long-term effect in alleviating depressive symptoms and loneliness for elderly residents in nursing homes. This intervention also improved long-term emotional social support and short-term appraisal support, and decreased residents' instrumental social support. However, this intervention had no effect on informational social support."", 'title': 'Changes in depressive symptoms, social support, and loneliness over 1 year after a minimum 3-month videoconference program for older nursing home residents.', 'date': '2011-11-17'}}",0.0,Psychiatry & Neurology
208,"Is physical-role quality of life higher, lower, or the same when comparing video calls to usual care?",uncertain effect,very low,yes,['31992217'],32441330,2020,"{'31992217': {'article_id': '31992217', 'content': 'Smartphones can optimize the opportunities for interactions between nursing home residents and their families. However, the effectiveness of smartphone-based videoconferencing programs in enhancing emotional status and quality of life has not been explored. The purpose of this study was to evaluate of the effect of a smartphone-based videoconferencing program on nursing home residents\' feelings of loneliness, depressive symptoms and quality of life.\nThis study used a quasi-experimental research design. Older residents from seven nursing homes in Taiwan participated in this study. Nursing homes (NH) were randomly selected as sites for either the intervention group (5 NH) or the control group (2 NH); NH residents who met the inclusion criteria were invited to participate. The intervention group was comprised of 32 participants; the control group was comprised of 30 participants. The intervention group interacted with their family members once a week for 6\u2009months using a smartphone and a ""LINE"" application (app). Data were collected with self-report instruments: subjective feelings of loneliness, using the University of California Los Angeles Loneliness Scale; depressive symptoms, using the Geriatric Depression Scale; and quality of life using the SF-36. Data were collected at four time points (baseline, and at 1-month, 3-months and 6-months from baseline). Data were analysed using the generalized estimating equation approach.\nAfter the intervention, as compared to those in the control group, participants in interventional group had significant decreases in baseline loneliness scores at 1\u2009months (β\u2009=\u2009-\u20093.41, p\u2009<\u20090.001), 3\u2009months (β\u2009=\u2009-\u20095.96, p\u2009<\u20090.001), and 6\u2009months (β\u2009=\u2009-\u20097.50, p\u2009<\u20090.001), and improvements in physical role (β\u2009=\u200936.49, p\u2009=\u20090.01), vitality (β\u2009=\u200913.11, p\u2009<\u20090.001) and pain scores (β\u2009=\u200916.71, p\u2009=\u20090.01) at 6\u2009months. However, changes in mean depression scores did not significantly differ between groups.\nSmartphone-based videoconferencing effectively improved residents\' feelings of loneliness, and physiological health, vitality and pain, but not depressive symptoms. Future investigations might evaluate the effectiveness of other media-based technologies in nursing homes as well as their effectiveness within and between different age cohorts.', 'title': 'Effects of a smartphone-based videoconferencing program for older nursing home residents on depression, loneliness, and quality of life: a quasi-experimental study.', 'date': '2020-01-30'}}",0.0,"Public Health, Epidemiology & Health Systems"
209,"Is anxiety higher, lower, or the same when comparing antidepressants to placebo?",uncertain effect,low,yes,"['27664274', '21936344', '26600836']",30977111,2019,"{'27664274': {'article_id': '27664274', 'content': ""Previous studies have shown that antidepressants reduce inflammation in animal models of colitis. The present trial aimed to examine whether fluoxetine added to standard therapy for Crohn's disease [CD] maintained remission, improved quality of life [QoL] and/or mental health in people with CD as compared to placebo.\nA parallel randomized double-blind placebo controlled trial was conducted. Participants with clinically established CD, with quiescent or only mild disease, were randomly assigned to receive either fluoxetine 20 mg daily or placebo, and followed for 12 months. Participants provided blood and stool samples and completed mental health and QoL questionnaires. Immune functions were assessed by stimulated cytokine secretion [CD3/CD28 stimulation] and flow cytometry for cell type. Linear mixed-effects models were used to compare groups.\nOf the 26 participants, 14 were randomized to receive fluoxetine and 12 to placebo. Overall, 14 [54%] participants were male. The mean age was 37.4 [SD=13.2] years. Fluoxetine had no effect on inflammatory bowel disease activity measured using either the Crohn's Disease Activity Index [F(3, 27.5)=0.064, p=0.978] or faecal calprotectin [F(3, 32.5)=1.08, p=0.371], but did have modest effects on immune function. There was no effect of fluoxetine on physical, psychological, social or environmental QoL, anxiety or depressive symptoms as compared to placebo [all p>0.05].\nIn this small pilot clinical trial, fluoxetine was not superior to placebo in maintaining remission or improving QoL. [ID: ACTRN12612001067864.]."", 'title': ""Fluoxetine for Maintenance of Remission and to Improve Quality of Life in Patients with Crohn's Disease: a Pilot Randomized Placebo-Controlled Trial."", 'date': '2016-11-01'}, '21936344': {'article_id': '21936344', 'content': 'In order to maintain ulcerative colitis (UC) in remission, chronic use of aminosalicylates is recommended. All patients have a fear of the recurrence of symptoms, which makes their mental state and quality of life worse. Because of this a number of patients are recommended to use different sorts of anxiolytic drugs and antidepressants.\nEvaluation of the influence of tianeptine (selective serotonin reuptake enhancer) on the mental and somatic status in the group of patients.\nThe research was conducted in two groups of thirty patients, with benign form of ulcerative colitis in remission, aged 24-46 years. Patients, during a period of 12 months, were given aminosalicylates in a daily doses 2 x 1.0 g and tianeptine in a doses 3 x 12.5 mg (group I) or placebo (group II). During the treatment every three months anxiety (Hamilton Anxiety Rating Scale-HARS), depression (Back Depression Inventory-BDI), The Mayo Clinic Disease Activity Index (MCDAI), hemoglobin and C-reactive protein (CRP) level were evaluated.\nAfter 12 months in a group of patients who took tianeptine decrease in anxiety (from 20.35 +/- 4.03 to 12.65 +/- 3.78 points) and depression (from 19.95 +/- 4.49 points to 9.60 +/- 2.76 points) was obtained; difference compared with placebo was statistically significant (p < 0.01). At the same time significant decrease compared with placebo (p < 0.05) of disease activity index (respectively 3.05 +/- 1.36 and 4.65 +/- 1.69), insignificantly lower level of CRP (7.00 5.65 and 9.41 +/- 10.12) and higher level of hemoglobin (11.93 +/- 0.83 and 11.0 +/- 0.70) was observed.\nTianeptine has a positive influence on mental and somatic status of patients with UC. Results give the support for tianeptine apllication in UC as adjuvant drug.', 'title': '[Evaluation of the influence of tianeptine on the psychosomatic status of patients with ulcerative colitis in remission].', 'date': '2011-09-23'}, '26600836': {'article_id': '26600836', 'content': ""Treating inflammatory bowel disease (IBD) with antidepressants might be of utility to improve patient's condition. The aim of this study was to assess the efficacy of Duloxetine on depression, anxiety, severity of symptoms, and quality of life (QOL) in IBD patients.\nIn a randomized, double-blind, controlled clinical trial on 2013-2014, in Alzahra Hospital (Isfahan, Iran), 44 IBD patients were chosen to receive either duloxetine (60 mg/day) or placebo. They were treated in a 12 weeks program, and all of the participants also received mesalazine, 2-4 g daily. We assessed anxiety and depression with Hospital Anxiety and Depression Scale, the severity of symptoms with Lichtiger Colitis Activity Index and QOL with World Health Organization Quality of Life Instruments, before and just after the treatment. The data were analyzed using Paired sample t-test and ANCOVA.\nIn 35 subjects who completed the study, the mean (standard error [SE]) scores of depression and anxiety were reduced in duloxetine more than placebo group, significantly (P = 0.041 and P = 0.049, respectively). The mean (SE) scores of severity of symptom were also reduced in duloxetine more than the placebo group, significantly (P = 0.02). The mean (SE) scores of physical, psychological, and social dimensions of QOL were increased after treatment with duloxetine more than placebo group, significantly (P = 0.001, P = 0.038, and P = 0.015, respectively). The environmental QOL was not increased significantly (P = 0.260).\nDuloxetine is probably effective and safe for reducing depression, anxiety and severity of physical symptoms. It also could increase physical, psychological, and social QOL in patients."", 'title': 'Efficacy of duloxetine add on in treatment of inflammatory bowel disease patients: A double-blind controlled study.', 'date': '2015-11-26'}}",0.0,Psychiatry & Neurology
210,"Is depression higher, lower, or the same when comparing antidepressants to placebo?",uncertain effect,low,yes,"['27664274', '21936344', '26600836']",30977111,2019,"{'27664274': {'article_id': '27664274', 'content': ""Previous studies have shown that antidepressants reduce inflammation in animal models of colitis. The present trial aimed to examine whether fluoxetine added to standard therapy for Crohn's disease [CD] maintained remission, improved quality of life [QoL] and/or mental health in people with CD as compared to placebo.\nA parallel randomized double-blind placebo controlled trial was conducted. Participants with clinically established CD, with quiescent or only mild disease, were randomly assigned to receive either fluoxetine 20 mg daily or placebo, and followed for 12 months. Participants provided blood and stool samples and completed mental health and QoL questionnaires. Immune functions were assessed by stimulated cytokine secretion [CD3/CD28 stimulation] and flow cytometry for cell type. Linear mixed-effects models were used to compare groups.\nOf the 26 participants, 14 were randomized to receive fluoxetine and 12 to placebo. Overall, 14 [54%] participants were male. The mean age was 37.4 [SD=13.2] years. Fluoxetine had no effect on inflammatory bowel disease activity measured using either the Crohn's Disease Activity Index [F(3, 27.5)=0.064, p=0.978] or faecal calprotectin [F(3, 32.5)=1.08, p=0.371], but did have modest effects on immune function. There was no effect of fluoxetine on physical, psychological, social or environmental QoL, anxiety or depressive symptoms as compared to placebo [all p>0.05].\nIn this small pilot clinical trial, fluoxetine was not superior to placebo in maintaining remission or improving QoL. [ID: ACTRN12612001067864.]."", 'title': ""Fluoxetine for Maintenance of Remission and to Improve Quality of Life in Patients with Crohn's Disease: a Pilot Randomized Placebo-Controlled Trial."", 'date': '2016-11-01'}, '21936344': {'article_id': '21936344', 'content': 'In order to maintain ulcerative colitis (UC) in remission, chronic use of aminosalicylates is recommended. All patients have a fear of the recurrence of symptoms, which makes their mental state and quality of life worse. Because of this a number of patients are recommended to use different sorts of anxiolytic drugs and antidepressants.\nEvaluation of the influence of tianeptine (selective serotonin reuptake enhancer) on the mental and somatic status in the group of patients.\nThe research was conducted in two groups of thirty patients, with benign form of ulcerative colitis in remission, aged 24-46 years. Patients, during a period of 12 months, were given aminosalicylates in a daily doses 2 x 1.0 g and tianeptine in a doses 3 x 12.5 mg (group I) or placebo (group II). During the treatment every three months anxiety (Hamilton Anxiety Rating Scale-HARS), depression (Back Depression Inventory-BDI), The Mayo Clinic Disease Activity Index (MCDAI), hemoglobin and C-reactive protein (CRP) level were evaluated.\nAfter 12 months in a group of patients who took tianeptine decrease in anxiety (from 20.35 +/- 4.03 to 12.65 +/- 3.78 points) and depression (from 19.95 +/- 4.49 points to 9.60 +/- 2.76 points) was obtained; difference compared with placebo was statistically significant (p < 0.01). At the same time significant decrease compared with placebo (p < 0.05) of disease activity index (respectively 3.05 +/- 1.36 and 4.65 +/- 1.69), insignificantly lower level of CRP (7.00 5.65 and 9.41 +/- 10.12) and higher level of hemoglobin (11.93 +/- 0.83 and 11.0 +/- 0.70) was observed.\nTianeptine has a positive influence on mental and somatic status of patients with UC. Results give the support for tianeptine apllication in UC as adjuvant drug.', 'title': '[Evaluation of the influence of tianeptine on the psychosomatic status of patients with ulcerative colitis in remission].', 'date': '2011-09-23'}, '26600836': {'article_id': '26600836', 'content': ""Treating inflammatory bowel disease (IBD) with antidepressants might be of utility to improve patient's condition. The aim of this study was to assess the efficacy of Duloxetine on depression, anxiety, severity of symptoms, and quality of life (QOL) in IBD patients.\nIn a randomized, double-blind, controlled clinical trial on 2013-2014, in Alzahra Hospital (Isfahan, Iran), 44 IBD patients were chosen to receive either duloxetine (60 mg/day) or placebo. They were treated in a 12 weeks program, and all of the participants also received mesalazine, 2-4 g daily. We assessed anxiety and depression with Hospital Anxiety and Depression Scale, the severity of symptoms with Lichtiger Colitis Activity Index and QOL with World Health Organization Quality of Life Instruments, before and just after the treatment. The data were analyzed using Paired sample t-test and ANCOVA.\nIn 35 subjects who completed the study, the mean (standard error [SE]) scores of depression and anxiety were reduced in duloxetine more than placebo group, significantly (P = 0.041 and P = 0.049, respectively). The mean (SE) scores of severity of symptom were also reduced in duloxetine more than the placebo group, significantly (P = 0.02). The mean (SE) scores of physical, psychological, and social dimensions of QOL were increased after treatment with duloxetine more than placebo group, significantly (P = 0.001, P = 0.038, and P = 0.015, respectively). The environmental QOL was not increased significantly (P = 0.260).\nDuloxetine is probably effective and safe for reducing depression, anxiety and severity of physical symptoms. It also could increase physical, psychological, and social QOL in patients."", 'title': 'Efficacy of duloxetine add on in treatment of inflammatory bowel disease patients: A double-blind controlled study.', 'date': '2015-11-26'}}",0.0,Psychiatry & Neurology
211,"Is the risk of adverse events higher, lower, or the same when comparing antidepressants to placebo?",uncertain effect,low,yes,['27664274'],30977111,2019,"{'27664274': {'article_id': '27664274', 'content': ""Previous studies have shown that antidepressants reduce inflammation in animal models of colitis. The present trial aimed to examine whether fluoxetine added to standard therapy for Crohn's disease [CD] maintained remission, improved quality of life [QoL] and/or mental health in people with CD as compared to placebo.\nA parallel randomized double-blind placebo controlled trial was conducted. Participants with clinically established CD, with quiescent or only mild disease, were randomly assigned to receive either fluoxetine 20 mg daily or placebo, and followed for 12 months. Participants provided blood and stool samples and completed mental health and QoL questionnaires. Immune functions were assessed by stimulated cytokine secretion [CD3/CD28 stimulation] and flow cytometry for cell type. Linear mixed-effects models were used to compare groups.\nOf the 26 participants, 14 were randomized to receive fluoxetine and 12 to placebo. Overall, 14 [54%] participants were male. The mean age was 37.4 [SD=13.2] years. Fluoxetine had no effect on inflammatory bowel disease activity measured using either the Crohn's Disease Activity Index [F(3, 27.5)=0.064, p=0.978] or faecal calprotectin [F(3, 32.5)=1.08, p=0.371], but did have modest effects on immune function. There was no effect of fluoxetine on physical, psychological, social or environmental QoL, anxiety or depressive symptoms as compared to placebo [all p>0.05].\nIn this small pilot clinical trial, fluoxetine was not superior to placebo in maintaining remission or improving QoL. [ID: ACTRN12612001067864.]."", 'title': ""Fluoxetine for Maintenance of Remission and to Improve Quality of Life in Patients with Crohn's Disease: a Pilot Randomized Placebo-Controlled Trial."", 'date': '2016-11-01'}}",0.0,Psychiatry & Neurology
212,"Is maintenance of clinical remission higher, lower, or the same when comparing antidepressants to placebo?",uncertain effect,low,yes,['27664274'],30977111,2019,"{'27664274': {'article_id': '27664274', 'content': ""Previous studies have shown that antidepressants reduce inflammation in animal models of colitis. The present trial aimed to examine whether fluoxetine added to standard therapy for Crohn's disease [CD] maintained remission, improved quality of life [QoL] and/or mental health in people with CD as compared to placebo.\nA parallel randomized double-blind placebo controlled trial was conducted. Participants with clinically established CD, with quiescent or only mild disease, were randomly assigned to receive either fluoxetine 20 mg daily or placebo, and followed for 12 months. Participants provided blood and stool samples and completed mental health and QoL questionnaires. Immune functions were assessed by stimulated cytokine secretion [CD3/CD28 stimulation] and flow cytometry for cell type. Linear mixed-effects models were used to compare groups.\nOf the 26 participants, 14 were randomized to receive fluoxetine and 12 to placebo. Overall, 14 [54%] participants were male. The mean age was 37.4 [SD=13.2] years. Fluoxetine had no effect on inflammatory bowel disease activity measured using either the Crohn's Disease Activity Index [F(3, 27.5)=0.064, p=0.978] or faecal calprotectin [F(3, 32.5)=1.08, p=0.371], but did have modest effects on immune function. There was no effect of fluoxetine on physical, psychological, social or environmental QoL, anxiety or depressive symptoms as compared to placebo [all p>0.05].\nIn this small pilot clinical trial, fluoxetine was not superior to placebo in maintaining remission or improving QoL. [ID: ACTRN12612001067864.]."", 'title': ""Fluoxetine for Maintenance of Remission and to Improve Quality of Life in Patients with Crohn's Disease: a Pilot Randomized Placebo-Controlled Trial."", 'date': '2016-11-01'}}",0.0,Psychiatry & Neurology
213,"Is pain relief at 48 hours higher, lower, or the same when comparing paracetamol to placebo?",higher,low,no,['8871138'],37594020,2023,"{'8871138': {'article_id': '8871138', 'content': 'Two hundred and nineteen children (boys: 56%, girls: 44%) were included in a randomized, double-blind, multicentre (4 centres) controlled trial designed to assess the efficacy and safety of ibuprofen (IBU) in the treatment of 1 to 6 year-old children with otoscopically proven acute otitis media (AOM), either unilateral or bilateral. They randomly received 10 mg/kg IBU (n = 71), or acetaminophen (PARA) (n = 73) or placebo (PLA) (n = 75), orally, tid, for 48 hours. All received oral cefaclor (Alfatil, Lilly, France) for seven days. They were evaluated before (D0) and at the end of treatment (D2). The main criterion of response was the aspect (landmarks and color) of the tympanic membrane assessed on a semi-quantitative scale from 0 to 6. Other criteria, assessed on semi-quantitative scales, included relief of pain (0 or 1), rectal temperature (0 to 2), and overall evaluation by parents of the improvement of quality of life on three items: appetite (0 to 2), sleep (0 to 2), and playing activity (0 to 2). The results at D2 were as follows: there was no significant difference between treatment groups as to the main criterion, but only a trend for IBU and PARA to do better than PLA but not for IBU to do better than PARA. From these data there is no argument to emphasize the utility of non-steroidal anti-inflammatory drugs (NSAIDs) in treating the inflammatory signs of the tympanic membrane in otitis. There was a statistically significant difference between treatment groups at D2 for pain, IBU being superior to PLA (P < 0.01): 7%, 10% and 25% of the children were still suffering at D2 in the IBU, PARA and PLA treatment groups, respectively. The difference between PARA and PLA for pain was not statistically significant. There was no significant difference between treatment groups for the other criteria. All treatments were well and equally tolerated. Although no significant difference was found between the treatment groups on the aspect of the tympanic membrane, the efficacy of IBU was evidenced on the relief of pain, the symptom that most disturbs the child.', 'title': 'A randomized, double-blind, multicentre controlled trial of ibuprofen versus acetaminophen and placebo for symptoms of acute otitis media in children.', 'date': '1996-01-01'}}",0.0,Family Medicine & Preventive Care
214,"Is pain relief at 48 hours higher, lower, or the same when comparing NSAIDs to placebo?",higher,low,no,['8871138'],37594020,2023,"{'8871138': {'article_id': '8871138', 'content': 'Two hundred and nineteen children (boys: 56%, girls: 44%) were included in a randomized, double-blind, multicentre (4 centres) controlled trial designed to assess the efficacy and safety of ibuprofen (IBU) in the treatment of 1 to 6 year-old children with otoscopically proven acute otitis media (AOM), either unilateral or bilateral. They randomly received 10 mg/kg IBU (n = 71), or acetaminophen (PARA) (n = 73) or placebo (PLA) (n = 75), orally, tid, for 48 hours. All received oral cefaclor (Alfatil, Lilly, France) for seven days. They were evaluated before (D0) and at the end of treatment (D2). The main criterion of response was the aspect (landmarks and color) of the tympanic membrane assessed on a semi-quantitative scale from 0 to 6. Other criteria, assessed on semi-quantitative scales, included relief of pain (0 or 1), rectal temperature (0 to 2), and overall evaluation by parents of the improvement of quality of life on three items: appetite (0 to 2), sleep (0 to 2), and playing activity (0 to 2). The results at D2 were as follows: there was no significant difference between treatment groups as to the main criterion, but only a trend for IBU and PARA to do better than PLA but not for IBU to do better than PARA. From these data there is no argument to emphasize the utility of non-steroidal anti-inflammatory drugs (NSAIDs) in treating the inflammatory signs of the tympanic membrane in otitis. There was a statistically significant difference between treatment groups at D2 for pain, IBU being superior to PLA (P < 0.01): 7%, 10% and 25% of the children were still suffering at D2 in the IBU, PARA and PLA treatment groups, respectively. The difference between PARA and PLA for pain was not statistically significant. There was no significant difference between treatment groups for the other criteria. All treatments were well and equally tolerated. Although no significant difference was found between the treatment groups on the aspect of the tympanic membrane, the efficacy of IBU was evidenced on the relief of pain, the symptom that most disturbs the child.', 'title': 'A randomized, double-blind, multicentre controlled trial of ibuprofen versus acetaminophen and placebo for symptoms of acute otitis media in children.', 'date': '1996-01-01'}}",1.0,Surgery
215,"Is the risk of cryptococcal IRIS events higher, lower, or the same when comparing early ART initiation to delayed ART initiation?",uncertain effect,very low,yes,"['24963568', '23362285']",30039850,2018,"{'24963568': {'article_id': '24963568', 'content': 'Cryptococcal meningitis accounts for 20 to 25% of acquired immunodeficiency syndrome-related deaths in Africa. Antiretroviral therapy (ART) is essential for survival; however, the question of when ART should be initiated after diagnosis of cryptococcal meningitis remains unanswered.\nWe assessed survival at 26 weeks among 177 human immunodeficiency virus-infected adults in Uganda and South Africa who had cryptococcal meningitis and had not previously received ART. We randomly assigned study participants to undergo either earlier ART initiation (1 to 2 weeks after diagnosis) or deferred ART initiation (5 weeks after diagnosis). Participants received amphotericin B (0.7 to 1.0 mg per kilogram of body weight per day) and fluconazole (800 mg per day) for 14 days, followed by consolidation therapy with fluconazole.\nThe 26-week mortality with earlier ART initiation was significantly higher than with deferred ART initiation (45% [40 of 88 patients] vs. 30% [27 of 89 patients]; hazard ratio for death, 1.73; 95% confidence interval [CI], 1.06 to 2.82; P=0.03). The excess deaths associated with earlier ART initiation occurred 2 to 5 weeks after diagnosis (P=0.007 for the comparison between groups); mortality was similar in the two groups thereafter. Among patients with few white cells in their cerebrospinal fluid (<5 per cubic millimeter) at randomization, mortality was particularly elevated with earlier ART as compared with deferred ART (hazard ratio, 3.87; 95% CI, 1.41 to 10.58; P=0.008). The incidence of recognized cryptococcal immune reconstitution inflammatory syndrome did not differ significantly between the earlier-ART group and the deferred-ART group (20% and 13%, respectively; P=0.32). All other clinical, immunologic, virologic, and microbiologic outcomes, as well as adverse events, were similar between the groups.\nDeferring ART for 5 weeks after the diagnosis of cryptococcal meningitis was associated with significantly improved survival, as compared with initiating ART at 1 to 2 weeks, especially among patients with a paucity of white cells in cerebrospinal fluid. (Funded by the National Institute of Allergy and Infectious Diseases and others; COAT ClinicalTrials.gov number, NCT01075152.).', 'title': 'Timing of antiretroviral therapy after diagnosis of cryptococcal meningitis.', 'date': '2014-06-26'}, '23362285': {'article_id': '23362285', 'content': 'The burden of Cryptococcus neoformans in cerebrospinal fluid (CSF) predicts clinical outcomes in human immunodeficiency virus (HIV)-associated cryptococcal meningitis (CM) and is lower in patients on antiretroviral therapy (ART). This study tested the hypothesis that initiation of ART during initial treatment of HIV/CM would improve CSF clearance of C. neoformans.\nA randomized treatment-strategy trial was conducted in Botswana. HIV-infected, ART-naive adults aged≥21 years initiating amphotericin B treatment for CM were randomized to ART initiation within 7 (intervention) vs after 28 days (control) of randomization, and the primary outcome of the rate of CSF clearance of C. neoformans over the subsequent 4 weeks was compared. Adverse events, including CM immune reconstitution inflammatory syndrome (CM-IRIS), and immunologic and virologic responses were compared over 24 weeks.\nAmong 27 subjects enrolled (13 intervention and 14 control), [corrected] the median times to ART initiation were 7 (interquartile range [IQR], 5–10) and 32days (IQR, 28–36), respectively. The estimated rate of CSF clearance did not differ significantly by treatment strategy (-0.32 log10 colony-forming units [CFU]/mL/day±0.20 intervention and -0.52 log10 CFUs/mL/day (±0.48) control, P=.4). Two of 13 (15%) and 5 of 14 (36%) subjects died in the intervention and control arms, respectively (P=0.39). Seven of 13 subjects (54%) in the intervention arm vs 0 of 14 in the control arm experienced CM-IRIS (P=.002).\nEarly ART was not associated with improved CSF fungal clearance, but resulted in a high risk of CM-IRIS. Further research on optimal incorporation of ART into CM care is needed.\nNCT00976040.', 'title': 'Early versus delayed antiretroviral therapy and cerebrospinal fluid fungal clearance in adults with HIV and cryptococcal meningitis.', 'date': '2013-01-31'}}",0.0,Internal Medicine & Subspecialties
216,"Is virological suppression at 6 months higher, lower, or the same when comparing early ART initiation to delayed ART initiation?",uncertain effect,very low,yes,['24963568'],30039850,2018,"{'24963568': {'article_id': '24963568', 'content': 'Cryptococcal meningitis accounts for 20 to 25% of acquired immunodeficiency syndrome-related deaths in Africa. Antiretroviral therapy (ART) is essential for survival; however, the question of when ART should be initiated after diagnosis of cryptococcal meningitis remains unanswered.\nWe assessed survival at 26 weeks among 177 human immunodeficiency virus-infected adults in Uganda and South Africa who had cryptococcal meningitis and had not previously received ART. We randomly assigned study participants to undergo either earlier ART initiation (1 to 2 weeks after diagnosis) or deferred ART initiation (5 weeks after diagnosis). Participants received amphotericin B (0.7 to 1.0 mg per kilogram of body weight per day) and fluconazole (800 mg per day) for 14 days, followed by consolidation therapy with fluconazole.\nThe 26-week mortality with earlier ART initiation was significantly higher than with deferred ART initiation (45% [40 of 88 patients] vs. 30% [27 of 89 patients]; hazard ratio for death, 1.73; 95% confidence interval [CI], 1.06 to 2.82; P=0.03). The excess deaths associated with earlier ART initiation occurred 2 to 5 weeks after diagnosis (P=0.007 for the comparison between groups); mortality was similar in the two groups thereafter. Among patients with few white cells in their cerebrospinal fluid (<5 per cubic millimeter) at randomization, mortality was particularly elevated with earlier ART as compared with deferred ART (hazard ratio, 3.87; 95% CI, 1.41 to 10.58; P=0.008). The incidence of recognized cryptococcal immune reconstitution inflammatory syndrome did not differ significantly between the earlier-ART group and the deferred-ART group (20% and 13%, respectively; P=0.32). All other clinical, immunologic, virologic, and microbiologic outcomes, as well as adverse events, were similar between the groups.\nDeferring ART for 5 weeks after the diagnosis of cryptococcal meningitis was associated with significantly improved survival, as compared with initiating ART at 1 to 2 weeks, especially among patients with a paucity of white cells in cerebrospinal fluid. (Funded by the National Institute of Allergy and Infectious Diseases and others; COAT ClinicalTrials.gov number, NCT01075152.).', 'title': 'Timing of antiretroviral therapy after diagnosis of cryptococcal meningitis.', 'date': '2014-06-26'}}",0.0,Internal Medicine & Subspecialties
217,"Is all-cause mortality at 6-12 months higher, lower, or the same when comparing early ART initiation to delayed ART initiation?",higher,low,yes,"['23362285', '24963568', '20415574', '19440326']",30039850,2018,"{'23362285': {'article_id': '23362285', 'content': 'The burden of Cryptococcus neoformans in cerebrospinal fluid (CSF) predicts clinical outcomes in human immunodeficiency virus (HIV)-associated cryptococcal meningitis (CM) and is lower in patients on antiretroviral therapy (ART). This study tested the hypothesis that initiation of ART during initial treatment of HIV/CM would improve CSF clearance of C. neoformans.\nA randomized treatment-strategy trial was conducted in Botswana. HIV-infected, ART-naive adults aged≥21 years initiating amphotericin B treatment for CM were randomized to ART initiation within 7 (intervention) vs after 28 days (control) of randomization, and the primary outcome of the rate of CSF clearance of C. neoformans over the subsequent 4 weeks was compared. Adverse events, including CM immune reconstitution inflammatory syndrome (CM-IRIS), and immunologic and virologic responses were compared over 24 weeks.\nAmong 27 subjects enrolled (13 intervention and 14 control), [corrected] the median times to ART initiation were 7 (interquartile range [IQR], 5–10) and 32days (IQR, 28–36), respectively. The estimated rate of CSF clearance did not differ significantly by treatment strategy (-0.32 log10 colony-forming units [CFU]/mL/day±0.20 intervention and -0.52 log10 CFUs/mL/day (±0.48) control, P=.4). Two of 13 (15%) and 5 of 14 (36%) subjects died in the intervention and control arms, respectively (P=0.39). Seven of 13 subjects (54%) in the intervention arm vs 0 of 14 in the control arm experienced CM-IRIS (P=.002).\nEarly ART was not associated with improved CSF fungal clearance, but resulted in a high risk of CM-IRIS. Further research on optimal incorporation of ART into CM care is needed.\nNCT00976040.', 'title': 'Early versus delayed antiretroviral therapy and cerebrospinal fluid fungal clearance in adults with HIV and cryptococcal meningitis.', 'date': '2013-01-31'}, '24963568': {'article_id': '24963568', 'content': 'Cryptococcal meningitis accounts for 20 to 25% of acquired immunodeficiency syndrome-related deaths in Africa. Antiretroviral therapy (ART) is essential for survival; however, the question of when ART should be initiated after diagnosis of cryptococcal meningitis remains unanswered.\nWe assessed survival at 26 weeks among 177 human immunodeficiency virus-infected adults in Uganda and South Africa who had cryptococcal meningitis and had not previously received ART. We randomly assigned study participants to undergo either earlier ART initiation (1 to 2 weeks after diagnosis) or deferred ART initiation (5 weeks after diagnosis). Participants received amphotericin B (0.7 to 1.0 mg per kilogram of body weight per day) and fluconazole (800 mg per day) for 14 days, followed by consolidation therapy with fluconazole.\nThe 26-week mortality with earlier ART initiation was significantly higher than with deferred ART initiation (45% [40 of 88 patients] vs. 30% [27 of 89 patients]; hazard ratio for death, 1.73; 95% confidence interval [CI], 1.06 to 2.82; P=0.03). The excess deaths associated with earlier ART initiation occurred 2 to 5 weeks after diagnosis (P=0.007 for the comparison between groups); mortality was similar in the two groups thereafter. Among patients with few white cells in their cerebrospinal fluid (<5 per cubic millimeter) at randomization, mortality was particularly elevated with earlier ART as compared with deferred ART (hazard ratio, 3.87; 95% CI, 1.41 to 10.58; P=0.008). The incidence of recognized cryptococcal immune reconstitution inflammatory syndrome did not differ significantly between the earlier-ART group and the deferred-ART group (20% and 13%, respectively; P=0.32). All other clinical, immunologic, virologic, and microbiologic outcomes, as well as adverse events, were similar between the groups.\nDeferring ART for 5 weeks after the diagnosis of cryptococcal meningitis was associated with significantly improved survival, as compared with initiating ART at 1 to 2 weeks, especially among patients with a paucity of white cells in cerebrospinal fluid. (Funded by the National Institute of Allergy and Infectious Diseases and others; COAT ClinicalTrials.gov number, NCT01075152.).', 'title': 'Timing of antiretroviral therapy after diagnosis of cryptococcal meningitis.', 'date': '2014-06-26'}, '20415574': {'article_id': '20415574', 'content': 'BACKGROUND. Cryptococcal meningitis (CM) remains a leading cause of acquired immunodeficiency syndrome-related death in sub-Saharan Africa. The timing of the initiation of antiretroviral therapy (ART) for human immunodeficiency virus (HIV)-associated CM remains uncertain. The study aimed to determine the optimal timing for initiation of ART in HIV-positive individuals with CM. METHODS. A prospective, open-label, randomized clinical trial was conducted at a tertiary teaching hospital in Zimbabwe. Participants were aged > or = 18 years, were ART naive, had received a first CM diagnosis, and were randomized to receive early ART (within 72 h after CM diagnosis) or delayed ART (after 10 weeks of treatment with fluconazole alone). Participants received 800 mg of fluconazole per day. The ART regimen used was stavudine, lamivudine, and nevirapine given twice daily. The duration of follow-up was up to 3 years. The primary end point was all-cause mortality. RESULTS. Fifty-four participants were enrolled in the study (28 in the early ART arm and 26 in the delayed ART arm). The median CD4 cell count at enrollment was 37 cells/mm(3) (interquartile range, 17-69 cells/mm(3)). The 3-year mortality rate differed significantly between the early and delayed ART groups (88% vs 54%; P < .006); the overall 3-year mortality rate was 73%. The median durations of survival were 28 days and 637 days in the early and delayed ART groups, respectively (P = .031, by log-rank test). The risk of mortality was almost 3 times as great in the early ART group versus the delayed ART group (adjusted hazard ratio, 2.85; 95% confidence interval, 1.1-7.23). The study was terminated early by the data safety monitoring committee. CONCLUSIONS. In resource-limited settings where CM management may be suboptimal, when compared with a delay of 10 weeks after a CM diagnosis, early initiation of ART results in increased mortality. Trial registration. ClinicalTrials.gov identifier: NCT00830856.', 'title': 'Early versus delayed initiation of antiretroviral therapy for concurrent HIV infection and cryptococcal meningitis in sub-saharan Africa.', 'date': '2010-04-27'}, '19440326': {'article_id': '19440326', 'content': 'Optimal timing of ART initiation for individuals presenting with AIDS-related OIs has not been defined.\nA5164 was a randomized strategy trial of ""early ART""--given within 14 days of starting acute OI treatment versus ""deferred ART""--given after acute OI treatment is completed. Randomization was stratified by presenting OI and entry CD4 count. The primary week 48 endpoint was 3-level ordered categorical variable: 1. Death/AIDS progression; 2. No progression with incomplete viral suppression (ie HIV viral load (VL) >or=50 copies/ml); 3. No progression with optimal viral suppression (ie HIV VL <50 copies/ml). Secondary endpoints included: AIDS progression/death; plasma HIV RNA and CD4 responses and safety parameters including IRIS. 282 subjects were evaluable; 141 per arm. Entry OIs included Pneumocytis jirovecii pneumonia 63%, cryptococcal meningitis 12%, and bacterial infections 12%. The early and deferred arms started ART a median of 12 and 45 days after start of OI treatment, respectively. THE DIFFERENCE IN THE PRIMARY ENDPOINT DID NOT REACH STATISTICAL SIGNIFICANCE: AIDS progression/death was seen in 20 (14%) vs. 34 (24%); whereas no progression but with incomplete viral suppression was seen in 54 (38%) vs. 44 (31%); and no progression with optimal viral suppression in 67 (48%) vs 63 (45%) in the early vs. deferred arm, respectively (p = 0.22). However, the early ART arm had fewer AIDS progression/deaths (OR = 0.51; 95% CI = 0.27-0.94) and a longer time to AIDS progression/death (stratified HR = 0.53; 95% CI = 0.30-0.92). The early ART had shorter time to achieving a CD4 count above 50 cells/mL (p<0.001) and no increase in adverse events.\nEarly ART resulted in less AIDS progression/death with no increase in adverse events or loss of virologic response compared to deferred ART. These results support the early initiation of ART in patients presenting with acute AIDS-related OIs, absent major contraindications.\nClinicalTrials.gov NCT00055120.', 'title': 'Early antiretroviral therapy reduces AIDS progression/death in individuals with acute opportunistic infections: a multicenter randomized strategy trial.', 'date': '2009-05-15'}}",0.5,"Public Health, Epidemiology & Health Systems"
218,"Is the risk of cryptococcal meningitis relapse higher, lower, or the same when comparing early ART initiation to delayed ART initiation?",lower,low,yes,['24963568'],30039850,2018,"{'24963568': {'article_id': '24963568', 'content': 'Cryptococcal meningitis accounts for 20 to 25% of acquired immunodeficiency syndrome-related deaths in Africa. Antiretroviral therapy (ART) is essential for survival; however, the question of when ART should be initiated after diagnosis of cryptococcal meningitis remains unanswered.\nWe assessed survival at 26 weeks among 177 human immunodeficiency virus-infected adults in Uganda and South Africa who had cryptococcal meningitis and had not previously received ART. We randomly assigned study participants to undergo either earlier ART initiation (1 to 2 weeks after diagnosis) or deferred ART initiation (5 weeks after diagnosis). Participants received amphotericin B (0.7 to 1.0 mg per kilogram of body weight per day) and fluconazole (800 mg per day) for 14 days, followed by consolidation therapy with fluconazole.\nThe 26-week mortality with earlier ART initiation was significantly higher than with deferred ART initiation (45% [40 of 88 patients] vs. 30% [27 of 89 patients]; hazard ratio for death, 1.73; 95% confidence interval [CI], 1.06 to 2.82; P=0.03). The excess deaths associated with earlier ART initiation occurred 2 to 5 weeks after diagnosis (P=0.007 for the comparison between groups); mortality was similar in the two groups thereafter. Among patients with few white cells in their cerebrospinal fluid (<5 per cubic millimeter) at randomization, mortality was particularly elevated with earlier ART as compared with deferred ART (hazard ratio, 3.87; 95% CI, 1.41 to 10.58; P=0.008). The incidence of recognized cryptococcal immune reconstitution inflammatory syndrome did not differ significantly between the earlier-ART group and the deferred-ART group (20% and 13%, respectively; P=0.32). All other clinical, immunologic, virologic, and microbiologic outcomes, as well as adverse events, were similar between the groups.\nDeferring ART for 5 weeks after the diagnosis of cryptococcal meningitis was associated with significantly improved survival, as compared with initiating ART at 1 to 2 weeks, especially among patients with a paucity of white cells in cerebrospinal fluid. (Funded by the National Institute of Allergy and Infectious Diseases and others; COAT ClinicalTrials.gov number, NCT01075152.).', 'title': 'Timing of antiretroviral therapy after diagnosis of cryptococcal meningitis.', 'date': '2014-06-26'}}",0.0,Internal Medicine & Subspecialties
243,"Is rheumatoid arthritis remission rate higher, lower, or the same when comparing tofacitinib monotherapy to placebo?",insufficient data,,no,"['16162882', '12115176', '24972708', '23294500']",28282491,2017,"{'16162882': {'article_id': '16162882', 'content': 'A substantial number of patients with rheumatoid arthritis have an inadequate or unsustained response to tumor necrosis factor alpha (TNF-alpha) inhibitors. We conducted a randomized, double-blind, phase 3 trial to evaluate the efficacy and safety of abatacept, a selective costimulation modulator, in patients with active rheumatoid arthritis and an inadequate response to at least three months of anti-TNF-alpha therapy.\nPatients with active rheumatoid arthritis and an inadequate response to anti-TNF-alpha therapy were randomly assigned in a 2:1 ratio to receive abatacept or placebo on days 1, 15, and 29 and every 28 days thereafter for 6 months, in addition to at least one disease-modifying antirheumatic drug. Patients discontinued anti-TNF-alpha therapy before randomization. The rates of American College of Rheumatology (ACR) 20 responses (indicating a clinical improvement of 20 percent or greater) and improvement in functional disability, as reflected by scores for the Health Assessment Questionnaire (HAQ) disability index, were assessed.\nAfter six months, the rates of ACR 20 responses were 50.4 percent in the abatacept group and 19.5 percent in the placebo group (P<0.001); the respective rates of ACR 50 and ACR 70 responses were also significantly higher in the abatacept group than in the placebo group (20.3 percent vs. 3.8 percent, P<0.001; and 10.2 percent vs. 1.5 percent, P=0.003). At six months, significantly more patients in the abatacept group than in the placebo group had a clinically meaningful improvement in physical function, as reflected by an improvement from baseline of at least 0.3 in the HAQ disability index (47.3 percent vs. 23.3 percent, P<0.001). The incidence of adverse events and peri-infusional adverse events was 79.5 percent and 5.0 percent, respectively, in the abatacept group and 71.4 percent and 3.0 percent, respectively, in the placebo group. The incidence of serious infections was 2.3 percent in each group.\nAbatacept produced significant clinical and functional benefits in patients who had had an inadequate response to anti-TNF-alpha therapy.', 'title': 'Abatacept for rheumatoid arthritis refractory to tumor necrosis factor alpha inhibition.', 'date': '2005-09-16'}, '12115176': {'article_id': '12115176', 'content': 'T cells are involved in the pathogenesis of rheumatoid arthritis (RA). In animal models of autoimmune diseases, blockade of costimulatory molecules on antigen-presenting cells has been demonstrated to be effective in preventing or treating this disease by preventing T cell activation. To date, the effect of costimulatory blockade in patients with RA is unknown. The goal of this multicenter, multinational study was to determine the safety and preliminary efficacy of costimulatory blockade using CTLA-4Ig and LEA29Y in RA patients who have been treated unsuccessfully with at least 1 disease-modifying agent.\nCTLA-4Ig, LEA29Y (0.5, 2, or 10 mg/kg), or placebo was administered intravenously to 214 patients with RA. Patients received 4 infusions of study medication, on days 1, 15, 29, and 57, and were evaluated on day 85. The primary end point was the proportion of patients meeting the American College of Rheumatology 20% improvement criteria (ACR20). All patients were monitored for treatment safety and tolerability.\nCTLA-4Ig and LEA29Y infusions were well tolerated at all dose levels. Peri-infusional adverse events were carefully monitored, and showed similar incidence across all dose groups with the exception of headaches, which were slightly more frequent in the 2 treatment groups. The incidence of discontinuations due to worsening of RA was 19%, 12%, and 9% at 0.5, 2, and 10 mg/kg, respectively, in the CTLA-4Ig-treated patients and 3%, 3%, and 6% at 0.5, 2, and 10 mg/kg, respectively, in the LEA29Y-treated patients (versus 31% in the placebo group). ACR20 responses on day 85 had increased in a dose-dependent manner (23%, 44%, and 53% of CTLA-4Ig-treated patients and 34%, 45%, and 61% of LEA29Y-treated patients at 0.5, 2.0, and 10 mg/kg, respectively, versus 31% of placebo-treated patients).\nBoth of the costimulatory blocking molecules studied were generally safe and well tolerated. As compared with placebo, both CTLA-4Ig and LEA29Y demonstrated efficacy in the treatment of RA.', 'title': 'Costimulatory blockade in patients with rheumatoid arthritis: a pilot, dose-finding, double-blind, placebo-controlled clinical trial evaluating CTLA-4Ig and LEA29Y eighty-five days after the first infusion.', 'date': '2002-07-13'}, '24972708': {'article_id': '24972708', 'content': 'To study the efficacy and safety of certolizumab pegol (CZP) in patients with active rheumatoid arthritis (RA) who had discontinued an initially effective TNF inhibitor (TNF-IR).\nA randomised 12-week double-blind trial with CZP (n=27) or placebo (n=10) followed by an open-label 12 week extension period with CZP.\nBaseline characteristics of the 2 groups were similar. ACR20 response (primary end point) at week 12 was achieved in 61.5%, and none of CZP and placebo-treated patients, respectively. Weeks 12-24 showed a maximum effect for CZP at 12 weeks, and that placebo patients switched blindly to CZP attained similar results seen with CZP in weeks 0-12. Since this result was highly significant, study inclusion was terminated after entry of 33.6% of the originally planned 102 patients. Adverse events occurred in 16/27 (59.3%) CZP subjects and 4/10 (40%) placebo subjects. There were no serious adverse events, neoplasms, opportunistic, or serious infections.\nThis first, prospective, blinded trial of CZP in secondary TNF-IR shows that the ACR20 response rate observed with CZP was higher than that reported in most previous studies of TNF-IR. Additionally, CZP demonstrated good safety and tolerability. This study supports the use of CZP in RA patients who are secondary non-responders to anti-TNF therapies.', 'title': 'Rheumatoid arthritis secondary non-responders to TNF can attain an efficacious and safe response by switching to certolizumab pegol: a phase IV, randomised, multicentre, double-blind, 12-week study, followed by a 12-week open-label phase.', 'date': '2014-06-29'}, '23294500': {'article_id': '23294500', 'content': 'Rheumatoid arthritis is a heterogeneous chronic disease, and no therapeutic agent has been identified which is universally and persistently effective in all patients. We investigated the effectiveness of tofacitinib (CP-690,550), a novel oral Janus kinase inhibitor, as a targeted immunomodulator and disease-modifying therapy for rheumatoid arthritis.\nWe did a 6-month, double-blind, parallel-group phase 3 study at 82 centres in 13 countries, including North America, Europe, and Latin America. 399 patients aged 18 years or older with moderate-to-severe rheumatoid arthritis and inadequate response to tumour necrosis factor inhibitors (TNFi) were randomly assigned in a 2:2:1:1 ratio with an automated internet or telephone system to receive twice a day treatment with: tofacitinib 5 mg (n=133); tofacitinib 10 mg (n=134); or placebo (n=132), all with methotrexate. At month 3, patients given placebo advanced to either tofacitinib 5 mg twice a day (n=66) or 10 mg twice a day (n=66). Primary endpoints included American College of Rheumatology (ACR)20 response rate, mean change from baseline in Health Assessment Questionnaire-Disability Index (HAQ-DI), and rates of disease activity score (DAS)28-4(ESR) less than 2·6 (referred to as DAS28<2·6), all at month 3. The full analysis set for the primary analysis included all randomised patients who received at least one dose of study medication and had at least one post-baseline assessment. This trial is registered with www.ClinicalTrials.gov, number NCT00960440.\nAt month 3, ACR20 response rates were 41·7% (55 of 132 [95% CI vs placebo 6·06-28·41]; p=0·0024) for tofacitinib 5 mg twice a day and 48·1% (64 of 133; [12·45-34·92]; p<0·0001) for tofacitinib 10 mg twice a day versus 24·4% (32 of 131) for placebo. Improvements from baseline in HAQ-DI were -0·43 ([-0·36 to -0·15]; p<0·0001) for 5 mg twice a day and -0·46 ([-0·38 to -0·17]; p<0·0001) for 10 mg twice a day tofacitinib versus -0·18 for placebo; DAS28<2·6 rates were 6·7% (eight of 119; [0-10·10]; p=0·0496) for 5 mg twice a day tofacitinib and 8·8% (11 of 125 [1·66-12·60]; p=0·0105) for 10 mg twice a day tofacitinib versus 1·7% (two of 120) for placebo. Safety was consistent with phase 2 and 3 studies. The most common adverse events in months 0-3 were diarrhoea (13 of 267; 4·9%), nasopharyngitis (11 of 267; 4·1%), headache (11 of 267; 4·1%), and urinary tract infection (eight of 267; 3·0%) across tofacitinib groups, and nausea (nine of 132; 6·8%) in the placebo group.\nIn this treatment-refractory population, tofacitinib with methotrexate had rapid and clinically meaningful improvements in signs and symptoms of rheumatoid arthritis and physical function over 6 months with manageable safety. Tofacitinib could provide an effective treatment option in patients with an inadequate response to TNFi.\nPfizer.', 'title': 'Tofacitinib (CP-690,550) in combination with methotrexate in patients with active rheumatoid arthritis with an inadequate response to tumour necrosis factor inhibitors: a randomised phase 3 trial.', 'date': '2013-01-09'}}",1.0,Internal Medicine & Subspecialties
244,"Is radiographic progression higher, lower, or the same when comparing tofacitinib and MTX to MTX and placebo?",insufficient data,,no,['23294500'],28282491,2017,"{'23294500': {'article_id': '23294500', 'content': 'Rheumatoid arthritis is a heterogeneous chronic disease, and no therapeutic agent has been identified which is universally and persistently effective in all patients. We investigated the effectiveness of tofacitinib (CP-690,550), a novel oral Janus kinase inhibitor, as a targeted immunomodulator and disease-modifying therapy for rheumatoid arthritis.\nWe did a 6-month, double-blind, parallel-group phase 3 study at 82 centres in 13 countries, including North America, Europe, and Latin America. 399 patients aged 18 years or older with moderate-to-severe rheumatoid arthritis and inadequate response to tumour necrosis factor inhibitors (TNFi) were randomly assigned in a 2:2:1:1 ratio with an automated internet or telephone system to receive twice a day treatment with: tofacitinib 5 mg (n=133); tofacitinib 10 mg (n=134); or placebo (n=132), all with methotrexate. At month 3, patients given placebo advanced to either tofacitinib 5 mg twice a day (n=66) or 10 mg twice a day (n=66). Primary endpoints included American College of Rheumatology (ACR)20 response rate, mean change from baseline in Health Assessment Questionnaire-Disability Index (HAQ-DI), and rates of disease activity score (DAS)28-4(ESR) less than 2·6 (referred to as DAS28<2·6), all at month 3. The full analysis set for the primary analysis included all randomised patients who received at least one dose of study medication and had at least one post-baseline assessment. This trial is registered with www.ClinicalTrials.gov, number NCT00960440.\nAt month 3, ACR20 response rates were 41·7% (55 of 132 [95% CI vs placebo 6·06-28·41]; p=0·0024) for tofacitinib 5 mg twice a day and 48·1% (64 of 133; [12·45-34·92]; p<0·0001) for tofacitinib 10 mg twice a day versus 24·4% (32 of 131) for placebo. Improvements from baseline in HAQ-DI were -0·43 ([-0·36 to -0·15]; p<0·0001) for 5 mg twice a day and -0·46 ([-0·38 to -0·17]; p<0·0001) for 10 mg twice a day tofacitinib versus -0·18 for placebo; DAS28<2·6 rates were 6·7% (eight of 119; [0-10·10]; p=0·0496) for 5 mg twice a day tofacitinib and 8·8% (11 of 125 [1·66-12·60]; p=0·0105) for 10 mg twice a day tofacitinib versus 1·7% (two of 120) for placebo. Safety was consistent with phase 2 and 3 studies. The most common adverse events in months 0-3 were diarrhoea (13 of 267; 4·9%), nasopharyngitis (11 of 267; 4·1%), headache (11 of 267; 4·1%), and urinary tract infection (eight of 267; 3·0%) across tofacitinib groups, and nausea (nine of 132; 6·8%) in the placebo group.\nIn this treatment-refractory population, tofacitinib with methotrexate had rapid and clinically meaningful improvements in signs and symptoms of rheumatoid arthritis and physical function over 6 months with manageable safety. Tofacitinib could provide an effective treatment option in patients with an inadequate response to TNFi.\nPfizer.', 'title': 'Tofacitinib (CP-690,550) in combination with methotrexate in patients with active rheumatoid arthritis with an inadequate response to tumour necrosis factor inhibitors: a randomised phase 3 trial.', 'date': '2013-01-09'}}",1.0,Internal Medicine & Subspecialties
245,"Is radiographic progression higher, lower, or the same when comparing biologic + MTX to only MTX/other traditional DMARDs?",insufficient data,,no,"['16947627', '18625622', '19560810']",28282491,2017,"{'16947627': {'article_id': '16947627', 'content': 'To determine the efficacy and safety of treatment with rituximab plus methotrexate (MTX) in patients with active rheumatoid arthritis (RA) who had an inadequate response to anti-tumor necrosis factor (anti-TNF) therapies and to explore the pharmacokinetics and pharmacodynamics of rituximab in this population.\nWe evaluated primary efficacy and safety at 24 weeks in patients enrolled in the Randomized Evaluation of Long-Term Efficacy of Rituximab in RA (REFLEX) Trial, a 2-year, multicenter, randomized, double-blind, placebo-controlled, phase III study of rituximab therapy. Patients with active RA and an inadequate response to 1 or more anti-TNF agents were randomized to receive intravenous rituximab (1 course, consisting of 2 infusions of 1,000 mg each) or placebo, both with background MTX. The primary efficacy end point was a response on the American College of Rheumatology 20% improvement criteria (ACR20) at 24 weeks. Secondary end points were responses on the ACR50 and ACR70 improvement criteria, the Disease Activity Score in 28 joints, and the European League against Rheumatism (EULAR) response criteria at 24 weeks. Additional end points included scores on the Functional Assessment of Chronic Illness Therapy-Fatigue (FACIT-F), Health Assessment Questionnaire (HAQ) Disability Index (DI), and Short Form 36 (SF-36) instruments, as well as Genant-modified Sharp radiographic scores at 24 weeks.\nPatients assigned to placebo (n = 209) and rituximab (n = 311) had active, longstanding RA. At week 24, significantly more (P < 0.0001) rituximab-treated patients than placebo-treated patients demonstrated ACR20 (51% versus 18%), ACR50 (27% versus 5%), and ACR70 (12% versus 1%) responses and moderate-to-good EULAR responses (65% versus 22%). All ACR response parameters were significantly improved in rituximab-treated patients, who also had clinically meaningful improvements in fatigue, disability, and health-related quality of life (demonstrated by FACIT-F, HAQ DI, and SF-36 scores, respectively) and showed a trend toward less progression in radiographic end points. Rituximab depleted peripheral CD20+ B cells, but the mean immunoglobulin levels (IgG, IgM, and IgA) remained within normal ranges. Most adverse events occurred with the first rituximab infusion and were of mild-to-moderate severity. The rate of serious infections was 5.2 per 100 patient-years in the rituximab group and 3.7 per 100 patient-years in the placebo group.\nAt 24 weeks, a single course of rituximab with concomitant MTX therapy provided significant and clinically meaningful improvements in disease activity in patients with active, longstanding RA who had an inadequate response to 1 or more anti-TNF therapies.', 'title': 'Rituximab for rheumatoid arthritis refractory to anti-tumor necrosis factor therapy: Results of a multicenter, randomized, double-blind, placebo-controlled, phase III trial evaluating primary efficacy and safety at twenty-four weeks.', 'date': '2006-09-02'}, '18625622': {'article_id': '18625622', 'content': 'The phase III RADIATE study examined the efficacy and safety of tocilizumab, an anti-IL-6 receptor monoclonal antibody in patients with rheumatoid arthritis (RA) refractory to tumour necrosis factor (TNF) antagonist therapy.\n499 patients with inadequate response to one or more TNF antagonists were randomly assigned to receive 8 mg/kg or 4 mg/kg tocilizumab or placebo (control) intravenously every 4 weeks with stable methotrexate for 24 weeks. ACR20 responses, secondary efficacy and safety endpoints were assessed.\nACR20 was achieved at 24 weeks by 50.0%, 30.4% and 10.1% of patients in the 8 mg/kg, 4 mg/kg and control groups, respectively (less than p<0.001 both tocilizumab groups versus control). At week 4 more patients achieved ACR20 in 8 mg/kg tocilizumab versus controls (less than p = 0.001). Patients responded regardless of most recently failed anti-TNF or the number of failed treatments. DAS28 remission (DAS28 <2.6) rates at week 24 were clearly dose related, being achieved by 30.1%, 7.6% and 1.6% of 8 mg/kg, 4 mg/kg and control groups (less than p = 0.001 for 8 mg/kg and p = 0.053 for 4 mg/kg versus control). Most adverse events were mild or moderate with overall incidences of 84.0%, 87.1% and 80.6%, respectively. The most common adverse events with higher incidence in tocilizumab groups were infections, gastrointestinal symptoms, rash and headache. The incidence of serious adverse events was higher in controls (11.3%) than in the 8 mg/kg (6.3%) and 4 mg/kg (7.4%) groups.\nTocilizumab plus methotrexate is effective in achieving rapid and sustained improvements in signs and symptoms of RA in patients with inadequate response to TNF antagonists and has a manageable safety profile.\nNCT00106522.', 'title': 'IL-6 receptor inhibition with tocilizumab improves treatment outcomes in patients with rheumatoid arthritis refractory to anti-tumour necrosis factor biologicals: results from a 24-week multicentre randomised placebo-controlled trial.', 'date': '2008-07-16'}, '19560810': {'article_id': '19560810', 'content': 'Tumour necrosis factor alpha (TNFalpha) inhibitors are frequently used to treat rheumatoid arthritis, but whether use of a different TNFalpha inhibitor can improve patient response is unknown. We assess the efficacy and safety of the TNFalpha inhibitor golimumab in patients with active rheumatoid arthritis who had previously received one or more TNFalpha inhibitors.\n461 patients with active rheumatoid arthritis from 82 sites in 10 countries were randomly allocated by interactive voice response system, stratified by study site and methotrexate use, to receive subcutaneous injections of placebo (n=155), 50 mg golimumab (n=153), or 100 mg golimumab (n=153) every 4 weeks between Feb 21, 2006, and Sept 26, 2007. Allocation was double-blind. Eligible patients had been treated with at least one dose of a TNFalpha inhibitor previously. Patients continued stable doses of methotrexate, sulfasalazine, hydroxychloroquine, oral corticosteroids, and non-steroidal anti-inflammatory drugs. The primary endpoint was achievement at week 14 of 20% or higher improvement in American College of Rheumatology criteria for assessment of rheumatoid arthritis (ACR20). At week 16, patients who had less than 20% improvement in tender and swollen joint counts were given rescue therapy and changed treatment from placebo to 50 mg golimumab, or from 50 mg to 100 mg golimumab. Drug efficacy was assessed by intention to treat and safety was assessed according to the study drug given. This study is registered with ClinicalTrials.gov, number NCT00299546.\nPatients had discontinued previous TNFalpha inhibitors because of lack of effectiveness (269 [58%] patients) or reasons unrelated to effectiveness (246 [53%] patients), such as intolerance and accessibility issues. Patients had active disease, which was indicated by a median of 14.0 (IQR 9.0-22.0) swollen and 26.0 (16.0-41.0) tender joints for the whole group. 28 (18%) patients on placebo, 54 (35%) patients on 50 mg golimumab (odds ratio 2.5 [95% CI 1.5-4.2], p=0.0006), and 58 (38%) patients on 100 mg golimumab (2.8 [1.6-4.7], p=0.0001) achieved ACR20 at week 14. Two patients were never treated, and 57 patients did not complete the study because of adverse events, unsatisfactory treatment effect, loss to follow-up, death, or other reasons. 155 patients on placebo, 153 on 50 mg golimumab, and 153 on 100 mg golimumab were assessed for drug efficacy. For weeks 1-16, serious adverse events were recorded in 11 (7%) patients on placebo, 8 (5%) on 50 mg golimumab, and 4 (3%) on 100 mg golimumab. For weeks 1-24, after some patients were given rescue therapy, serious adverse events were recorded in 15 (10%) patients on placebo, 14 (5%) on 50 mg golimumab, and 8 (4%) on 100 mg golimumab.\nGolimumab reduced the signs and symptoms of rheumatoid arthritis in patients with active disease who had previously received one or more TNFalpha inhibitors.\nCentocor Research and Development and Schering-Plough Research Institute.', 'title': 'Golimumab in patients with active rheumatoid arthritis after treatment with tumour necrosis factor alpha inhibitors (GO-AFTER study): a multicentre, randomised, double-blind, placebo-controlled, phase III trial.', 'date': '2009-06-30'}}",1.0,Internal Medicine & Subspecialties
246,"Is patient function measured by HAQ higher, lower, or the same when comparing biologic monotherapy to placebo?",insufficient data,,no,"['16162882', '12115176', '24972708']",28282491,2017,"{'16162882': {'article_id': '16162882', 'content': 'A substantial number of patients with rheumatoid arthritis have an inadequate or unsustained response to tumor necrosis factor alpha (TNF-alpha) inhibitors. We conducted a randomized, double-blind, phase 3 trial to evaluate the efficacy and safety of abatacept, a selective costimulation modulator, in patients with active rheumatoid arthritis and an inadequate response to at least three months of anti-TNF-alpha therapy.\nPatients with active rheumatoid arthritis and an inadequate response to anti-TNF-alpha therapy were randomly assigned in a 2:1 ratio to receive abatacept or placebo on days 1, 15, and 29 and every 28 days thereafter for 6 months, in addition to at least one disease-modifying antirheumatic drug. Patients discontinued anti-TNF-alpha therapy before randomization. The rates of American College of Rheumatology (ACR) 20 responses (indicating a clinical improvement of 20 percent or greater) and improvement in functional disability, as reflected by scores for the Health Assessment Questionnaire (HAQ) disability index, were assessed.\nAfter six months, the rates of ACR 20 responses were 50.4 percent in the abatacept group and 19.5 percent in the placebo group (P<0.001); the respective rates of ACR 50 and ACR 70 responses were also significantly higher in the abatacept group than in the placebo group (20.3 percent vs. 3.8 percent, P<0.001; and 10.2 percent vs. 1.5 percent, P=0.003). At six months, significantly more patients in the abatacept group than in the placebo group had a clinically meaningful improvement in physical function, as reflected by an improvement from baseline of at least 0.3 in the HAQ disability index (47.3 percent vs. 23.3 percent, P<0.001). The incidence of adverse events and peri-infusional adverse events was 79.5 percent and 5.0 percent, respectively, in the abatacept group and 71.4 percent and 3.0 percent, respectively, in the placebo group. The incidence of serious infections was 2.3 percent in each group.\nAbatacept produced significant clinical and functional benefits in patients who had had an inadequate response to anti-TNF-alpha therapy.', 'title': 'Abatacept for rheumatoid arthritis refractory to tumor necrosis factor alpha inhibition.', 'date': '2005-09-16'}, '12115176': {'article_id': '12115176', 'content': 'T cells are involved in the pathogenesis of rheumatoid arthritis (RA). In animal models of autoimmune diseases, blockade of costimulatory molecules on antigen-presenting cells has been demonstrated to be effective in preventing or treating this disease by preventing T cell activation. To date, the effect of costimulatory blockade in patients with RA is unknown. The goal of this multicenter, multinational study was to determine the safety and preliminary efficacy of costimulatory blockade using CTLA-4Ig and LEA29Y in RA patients who have been treated unsuccessfully with at least 1 disease-modifying agent.\nCTLA-4Ig, LEA29Y (0.5, 2, or 10 mg/kg), or placebo was administered intravenously to 214 patients with RA. Patients received 4 infusions of study medication, on days 1, 15, 29, and 57, and were evaluated on day 85. The primary end point was the proportion of patients meeting the American College of Rheumatology 20% improvement criteria (ACR20). All patients were monitored for treatment safety and tolerability.\nCTLA-4Ig and LEA29Y infusions were well tolerated at all dose levels. Peri-infusional adverse events were carefully monitored, and showed similar incidence across all dose groups with the exception of headaches, which were slightly more frequent in the 2 treatment groups. The incidence of discontinuations due to worsening of RA was 19%, 12%, and 9% at 0.5, 2, and 10 mg/kg, respectively, in the CTLA-4Ig-treated patients and 3%, 3%, and 6% at 0.5, 2, and 10 mg/kg, respectively, in the LEA29Y-treated patients (versus 31% in the placebo group). ACR20 responses on day 85 had increased in a dose-dependent manner (23%, 44%, and 53% of CTLA-4Ig-treated patients and 34%, 45%, and 61% of LEA29Y-treated patients at 0.5, 2.0, and 10 mg/kg, respectively, versus 31% of placebo-treated patients).\nBoth of the costimulatory blocking molecules studied were generally safe and well tolerated. As compared with placebo, both CTLA-4Ig and LEA29Y demonstrated efficacy in the treatment of RA.', 'title': 'Costimulatory blockade in patients with rheumatoid arthritis: a pilot, dose-finding, double-blind, placebo-controlled clinical trial evaluating CTLA-4Ig and LEA29Y eighty-five days after the first infusion.', 'date': '2002-07-13'}, '24972708': {'article_id': '24972708', 'content': 'To study the efficacy and safety of certolizumab pegol (CZP) in patients with active rheumatoid arthritis (RA) who had discontinued an initially effective TNF inhibitor (TNF-IR).\nA randomised 12-week double-blind trial with CZP (n=27) or placebo (n=10) followed by an open-label 12 week extension period with CZP.\nBaseline characteristics of the 2 groups were similar. ACR20 response (primary end point) at week 12 was achieved in 61.5%, and none of CZP and placebo-treated patients, respectively. Weeks 12-24 showed a maximum effect for CZP at 12 weeks, and that placebo patients switched blindly to CZP attained similar results seen with CZP in weeks 0-12. Since this result was highly significant, study inclusion was terminated after entry of 33.6% of the originally planned 102 patients. Adverse events occurred in 16/27 (59.3%) CZP subjects and 4/10 (40%) placebo subjects. There were no serious adverse events, neoplasms, opportunistic, or serious infections.\nThis first, prospective, blinded trial of CZP in secondary TNF-IR shows that the ACR20 response rate observed with CZP was higher than that reported in most previous studies of TNF-IR. Additionally, CZP demonstrated good safety and tolerability. This study supports the use of CZP in RA patients who are secondary non-responders to anti-TNF therapies.', 'title': 'Rheumatoid arthritis secondary non-responders to TNF can attain an efficacious and safe response by switching to certolizumab pegol: a phase IV, randomised, multicentre, double-blind, 12-week study, followed by a 12-week open-label phase.', 'date': '2014-06-29'}}",0.666666667,Internal Medicine & Subspecialties
247,"Is radiographic progression higher, lower, or the same when comparing biologic monotherapy to placebo?",insufficient data,,no,"['16162882', '12115176', '24972708']",28282491,2017,"{'16162882': {'article_id': '16162882', 'content': 'A substantial number of patients with rheumatoid arthritis have an inadequate or unsustained response to tumor necrosis factor alpha (TNF-alpha) inhibitors. We conducted a randomized, double-blind, phase 3 trial to evaluate the efficacy and safety of abatacept, a selective costimulation modulator, in patients with active rheumatoid arthritis and an inadequate response to at least three months of anti-TNF-alpha therapy.\nPatients with active rheumatoid arthritis and an inadequate response to anti-TNF-alpha therapy were randomly assigned in a 2:1 ratio to receive abatacept or placebo on days 1, 15, and 29 and every 28 days thereafter for 6 months, in addition to at least one disease-modifying antirheumatic drug. Patients discontinued anti-TNF-alpha therapy before randomization. The rates of American College of Rheumatology (ACR) 20 responses (indicating a clinical improvement of 20 percent or greater) and improvement in functional disability, as reflected by scores for the Health Assessment Questionnaire (HAQ) disability index, were assessed.\nAfter six months, the rates of ACR 20 responses were 50.4 percent in the abatacept group and 19.5 percent in the placebo group (P<0.001); the respective rates of ACR 50 and ACR 70 responses were also significantly higher in the abatacept group than in the placebo group (20.3 percent vs. 3.8 percent, P<0.001; and 10.2 percent vs. 1.5 percent, P=0.003). At six months, significantly more patients in the abatacept group than in the placebo group had a clinically meaningful improvement in physical function, as reflected by an improvement from baseline of at least 0.3 in the HAQ disability index (47.3 percent vs. 23.3 percent, P<0.001). The incidence of adverse events and peri-infusional adverse events was 79.5 percent and 5.0 percent, respectively, in the abatacept group and 71.4 percent and 3.0 percent, respectively, in the placebo group. The incidence of serious infections was 2.3 percent in each group.\nAbatacept produced significant clinical and functional benefits in patients who had had an inadequate response to anti-TNF-alpha therapy.', 'title': 'Abatacept for rheumatoid arthritis refractory to tumor necrosis factor alpha inhibition.', 'date': '2005-09-16'}, '12115176': {'article_id': '12115176', 'content': 'T cells are involved in the pathogenesis of rheumatoid arthritis (RA). In animal models of autoimmune diseases, blockade of costimulatory molecules on antigen-presenting cells has been demonstrated to be effective in preventing or treating this disease by preventing T cell activation. To date, the effect of costimulatory blockade in patients with RA is unknown. The goal of this multicenter, multinational study was to determine the safety and preliminary efficacy of costimulatory blockade using CTLA-4Ig and LEA29Y in RA patients who have been treated unsuccessfully with at least 1 disease-modifying agent.\nCTLA-4Ig, LEA29Y (0.5, 2, or 10 mg/kg), or placebo was administered intravenously to 214 patients with RA. Patients received 4 infusions of study medication, on days 1, 15, 29, and 57, and were evaluated on day 85. The primary end point was the proportion of patients meeting the American College of Rheumatology 20% improvement criteria (ACR20). All patients were monitored for treatment safety and tolerability.\nCTLA-4Ig and LEA29Y infusions were well tolerated at all dose levels. Peri-infusional adverse events were carefully monitored, and showed similar incidence across all dose groups with the exception of headaches, which were slightly more frequent in the 2 treatment groups. The incidence of discontinuations due to worsening of RA was 19%, 12%, and 9% at 0.5, 2, and 10 mg/kg, respectively, in the CTLA-4Ig-treated patients and 3%, 3%, and 6% at 0.5, 2, and 10 mg/kg, respectively, in the LEA29Y-treated patients (versus 31% in the placebo group). ACR20 responses on day 85 had increased in a dose-dependent manner (23%, 44%, and 53% of CTLA-4Ig-treated patients and 34%, 45%, and 61% of LEA29Y-treated patients at 0.5, 2.0, and 10 mg/kg, respectively, versus 31% of placebo-treated patients).\nBoth of the costimulatory blocking molecules studied were generally safe and well tolerated. As compared with placebo, both CTLA-4Ig and LEA29Y demonstrated efficacy in the treatment of RA.', 'title': 'Costimulatory blockade in patients with rheumatoid arthritis: a pilot, dose-finding, double-blind, placebo-controlled clinical trial evaluating CTLA-4Ig and LEA29Y eighty-five days after the first infusion.', 'date': '2002-07-13'}, '24972708': {'article_id': '24972708', 'content': 'To study the efficacy and safety of certolizumab pegol (CZP) in patients with active rheumatoid arthritis (RA) who had discontinued an initially effective TNF inhibitor (TNF-IR).\nA randomised 12-week double-blind trial with CZP (n=27) or placebo (n=10) followed by an open-label 12 week extension period with CZP.\nBaseline characteristics of the 2 groups were similar. ACR20 response (primary end point) at week 12 was achieved in 61.5%, and none of CZP and placebo-treated patients, respectively. Weeks 12-24 showed a maximum effect for CZP at 12 weeks, and that placebo patients switched blindly to CZP attained similar results seen with CZP in weeks 0-12. Since this result was highly significant, study inclusion was terminated after entry of 33.6% of the originally planned 102 patients. Adverse events occurred in 16/27 (59.3%) CZP subjects and 4/10 (40%) placebo subjects. There were no serious adverse events, neoplasms, opportunistic, or serious infections.\nThis first, prospective, blinded trial of CZP in secondary TNF-IR shows that the ACR20 response rate observed with CZP was higher than that reported in most previous studies of TNF-IR. Additionally, CZP demonstrated good safety and tolerability. This study supports the use of CZP in RA patients who are secondary non-responders to anti-TNF therapies.', 'title': 'Rheumatoid arthritis secondary non-responders to TNF can attain an efficacious and safe response by switching to certolizumab pegol: a phase IV, randomised, multicentre, double-blind, 12-week study, followed by a 12-week open-label phase.', 'date': '2014-06-29'}}",1.0,Internal Medicine & Subspecialties
248,"Is cancer risk higher, lower, or the same when comparing biologic monotherapy to placebo?",insufficient data,,no,"['16162882', '12115176', '24972708']",28282491,2017,"{'16162882': {'article_id': '16162882', 'content': 'A substantial number of patients with rheumatoid arthritis have an inadequate or unsustained response to tumor necrosis factor alpha (TNF-alpha) inhibitors. We conducted a randomized, double-blind, phase 3 trial to evaluate the efficacy and safety of abatacept, a selective costimulation modulator, in patients with active rheumatoid arthritis and an inadequate response to at least three months of anti-TNF-alpha therapy.\nPatients with active rheumatoid arthritis and an inadequate response to anti-TNF-alpha therapy were randomly assigned in a 2:1 ratio to receive abatacept or placebo on days 1, 15, and 29 and every 28 days thereafter for 6 months, in addition to at least one disease-modifying antirheumatic drug. Patients discontinued anti-TNF-alpha therapy before randomization. The rates of American College of Rheumatology (ACR) 20 responses (indicating a clinical improvement of 20 percent or greater) and improvement in functional disability, as reflected by scores for the Health Assessment Questionnaire (HAQ) disability index, were assessed.\nAfter six months, the rates of ACR 20 responses were 50.4 percent in the abatacept group and 19.5 percent in the placebo group (P<0.001); the respective rates of ACR 50 and ACR 70 responses were also significantly higher in the abatacept group than in the placebo group (20.3 percent vs. 3.8 percent, P<0.001; and 10.2 percent vs. 1.5 percent, P=0.003). At six months, significantly more patients in the abatacept group than in the placebo group had a clinically meaningful improvement in physical function, as reflected by an improvement from baseline of at least 0.3 in the HAQ disability index (47.3 percent vs. 23.3 percent, P<0.001). The incidence of adverse events and peri-infusional adverse events was 79.5 percent and 5.0 percent, respectively, in the abatacept group and 71.4 percent and 3.0 percent, respectively, in the placebo group. The incidence of serious infections was 2.3 percent in each group.\nAbatacept produced significant clinical and functional benefits in patients who had had an inadequate response to anti-TNF-alpha therapy.', 'title': 'Abatacept for rheumatoid arthritis refractory to tumor necrosis factor alpha inhibition.', 'date': '2005-09-16'}, '12115176': {'article_id': '12115176', 'content': 'T cells are involved in the pathogenesis of rheumatoid arthritis (RA). In animal models of autoimmune diseases, blockade of costimulatory molecules on antigen-presenting cells has been demonstrated to be effective in preventing or treating this disease by preventing T cell activation. To date, the effect of costimulatory blockade in patients with RA is unknown. The goal of this multicenter, multinational study was to determine the safety and preliminary efficacy of costimulatory blockade using CTLA-4Ig and LEA29Y in RA patients who have been treated unsuccessfully with at least 1 disease-modifying agent.\nCTLA-4Ig, LEA29Y (0.5, 2, or 10 mg/kg), or placebo was administered intravenously to 214 patients with RA. Patients received 4 infusions of study medication, on days 1, 15, 29, and 57, and were evaluated on day 85. The primary end point was the proportion of patients meeting the American College of Rheumatology 20% improvement criteria (ACR20). All patients were monitored for treatment safety and tolerability.\nCTLA-4Ig and LEA29Y infusions were well tolerated at all dose levels. Peri-infusional adverse events were carefully monitored, and showed similar incidence across all dose groups with the exception of headaches, which were slightly more frequent in the 2 treatment groups. The incidence of discontinuations due to worsening of RA was 19%, 12%, and 9% at 0.5, 2, and 10 mg/kg, respectively, in the CTLA-4Ig-treated patients and 3%, 3%, and 6% at 0.5, 2, and 10 mg/kg, respectively, in the LEA29Y-treated patients (versus 31% in the placebo group). ACR20 responses on day 85 had increased in a dose-dependent manner (23%, 44%, and 53% of CTLA-4Ig-treated patients and 34%, 45%, and 61% of LEA29Y-treated patients at 0.5, 2.0, and 10 mg/kg, respectively, versus 31% of placebo-treated patients).\nBoth of the costimulatory blocking molecules studied were generally safe and well tolerated. As compared with placebo, both CTLA-4Ig and LEA29Y demonstrated efficacy in the treatment of RA.', 'title': 'Costimulatory blockade in patients with rheumatoid arthritis: a pilot, dose-finding, double-blind, placebo-controlled clinical trial evaluating CTLA-4Ig and LEA29Y eighty-five days after the first infusion.', 'date': '2002-07-13'}, '24972708': {'article_id': '24972708', 'content': 'To study the efficacy and safety of certolizumab pegol (CZP) in patients with active rheumatoid arthritis (RA) who had discontinued an initially effective TNF inhibitor (TNF-IR).\nA randomised 12-week double-blind trial with CZP (n=27) or placebo (n=10) followed by an open-label 12 week extension period with CZP.\nBaseline characteristics of the 2 groups were similar. ACR20 response (primary end point) at week 12 was achieved in 61.5%, and none of CZP and placebo-treated patients, respectively. Weeks 12-24 showed a maximum effect for CZP at 12 weeks, and that placebo patients switched blindly to CZP attained similar results seen with CZP in weeks 0-12. Since this result was highly significant, study inclusion was terminated after entry of 33.6% of the originally planned 102 patients. Adverse events occurred in 16/27 (59.3%) CZP subjects and 4/10 (40%) placebo subjects. There were no serious adverse events, neoplasms, opportunistic, or serious infections.\nThis first, prospective, blinded trial of CZP in secondary TNF-IR shows that the ACR20 response rate observed with CZP was higher than that reported in most previous studies of TNF-IR. Additionally, CZP demonstrated good safety and tolerability. This study supports the use of CZP in RA patients who are secondary non-responders to anti-TNF therapies.', 'title': 'Rheumatoid arthritis secondary non-responders to TNF can attain an efficacious and safe response by switching to certolizumab pegol: a phase IV, randomised, multicentre, double-blind, 12-week study, followed by a 12-week open-label phase.', 'date': '2014-06-29'}}",1.0,Oncology & Hematology
283,"Is improvement in ACR50 higher, lower, or the same when comparing tofacitinib monotherapy to placebo?",insufficient data,,no,"['16162882', '12115176', '24972708', '23294500']",28282491,2017,"{'16162882': {'article_id': '16162882', 'content': 'A substantial number of patients with rheumatoid arthritis have an inadequate or unsustained response to tumor necrosis factor alpha (TNF-alpha) inhibitors. We conducted a randomized, double-blind, phase 3 trial to evaluate the efficacy and safety of abatacept, a selective costimulation modulator, in patients with active rheumatoid arthritis and an inadequate response to at least three months of anti-TNF-alpha therapy.\nPatients with active rheumatoid arthritis and an inadequate response to anti-TNF-alpha therapy were randomly assigned in a 2:1 ratio to receive abatacept or placebo on days 1, 15, and 29 and every 28 days thereafter for 6 months, in addition to at least one disease-modifying antirheumatic drug. Patients discontinued anti-TNF-alpha therapy before randomization. The rates of American College of Rheumatology (ACR) 20 responses (indicating a clinical improvement of 20 percent or greater) and improvement in functional disability, as reflected by scores for the Health Assessment Questionnaire (HAQ) disability index, were assessed.\nAfter six months, the rates of ACR 20 responses were 50.4 percent in the abatacept group and 19.5 percent in the placebo group (P<0.001); the respective rates of ACR 50 and ACR 70 responses were also significantly higher in the abatacept group than in the placebo group (20.3 percent vs. 3.8 percent, P<0.001; and 10.2 percent vs. 1.5 percent, P=0.003). At six months, significantly more patients in the abatacept group than in the placebo group had a clinically meaningful improvement in physical function, as reflected by an improvement from baseline of at least 0.3 in the HAQ disability index (47.3 percent vs. 23.3 percent, P<0.001). The incidence of adverse events and peri-infusional adverse events was 79.5 percent and 5.0 percent, respectively, in the abatacept group and 71.4 percent and 3.0 percent, respectively, in the placebo group. The incidence of serious infections was 2.3 percent in each group.\nAbatacept produced significant clinical and functional benefits in patients who had had an inadequate response to anti-TNF-alpha therapy.', 'title': 'Abatacept for rheumatoid arthritis refractory to tumor necrosis factor alpha inhibition.', 'date': '2005-09-16'}, '12115176': {'article_id': '12115176', 'content': 'T cells are involved in the pathogenesis of rheumatoid arthritis (RA). In animal models of autoimmune diseases, blockade of costimulatory molecules on antigen-presenting cells has been demonstrated to be effective in preventing or treating this disease by preventing T cell activation. To date, the effect of costimulatory blockade in patients with RA is unknown. The goal of this multicenter, multinational study was to determine the safety and preliminary efficacy of costimulatory blockade using CTLA-4Ig and LEA29Y in RA patients who have been treated unsuccessfully with at least 1 disease-modifying agent.\nCTLA-4Ig, LEA29Y (0.5, 2, or 10 mg/kg), or placebo was administered intravenously to 214 patients with RA. Patients received 4 infusions of study medication, on days 1, 15, 29, and 57, and were evaluated on day 85. The primary end point was the proportion of patients meeting the American College of Rheumatology 20% improvement criteria (ACR20). All patients were monitored for treatment safety and tolerability.\nCTLA-4Ig and LEA29Y infusions were well tolerated at all dose levels. Peri-infusional adverse events were carefully monitored, and showed similar incidence across all dose groups with the exception of headaches, which were slightly more frequent in the 2 treatment groups. The incidence of discontinuations due to worsening of RA was 19%, 12%, and 9% at 0.5, 2, and 10 mg/kg, respectively, in the CTLA-4Ig-treated patients and 3%, 3%, and 6% at 0.5, 2, and 10 mg/kg, respectively, in the LEA29Y-treated patients (versus 31% in the placebo group). ACR20 responses on day 85 had increased in a dose-dependent manner (23%, 44%, and 53% of CTLA-4Ig-treated patients and 34%, 45%, and 61% of LEA29Y-treated patients at 0.5, 2.0, and 10 mg/kg, respectively, versus 31% of placebo-treated patients).\nBoth of the costimulatory blocking molecules studied were generally safe and well tolerated. As compared with placebo, both CTLA-4Ig and LEA29Y demonstrated efficacy in the treatment of RA.', 'title': 'Costimulatory blockade in patients with rheumatoid arthritis: a pilot, dose-finding, double-blind, placebo-controlled clinical trial evaluating CTLA-4Ig and LEA29Y eighty-five days after the first infusion.', 'date': '2002-07-13'}, '24972708': {'article_id': '24972708', 'content': 'To study the efficacy and safety of certolizumab pegol (CZP) in patients with active rheumatoid arthritis (RA) who had discontinued an initially effective TNF inhibitor (TNF-IR).\nA randomised 12-week double-blind trial with CZP (n=27) or placebo (n=10) followed by an open-label 12 week extension period with CZP.\nBaseline characteristics of the 2 groups were similar. ACR20 response (primary end point) at week 12 was achieved in 61.5%, and none of CZP and placebo-treated patients, respectively. Weeks 12-24 showed a maximum effect for CZP at 12 weeks, and that placebo patients switched blindly to CZP attained similar results seen with CZP in weeks 0-12. Since this result was highly significant, study inclusion was terminated after entry of 33.6% of the originally planned 102 patients. Adverse events occurred in 16/27 (59.3%) CZP subjects and 4/10 (40%) placebo subjects. There were no serious adverse events, neoplasms, opportunistic, or serious infections.\nThis first, prospective, blinded trial of CZP in secondary TNF-IR shows that the ACR20 response rate observed with CZP was higher than that reported in most previous studies of TNF-IR. Additionally, CZP demonstrated good safety and tolerability. This study supports the use of CZP in RA patients who are secondary non-responders to anti-TNF therapies.', 'title': 'Rheumatoid arthritis secondary non-responders to TNF can attain an efficacious and safe response by switching to certolizumab pegol: a phase IV, randomised, multicentre, double-blind, 12-week study, followed by a 12-week open-label phase.', 'date': '2014-06-29'}, '23294500': {'article_id': '23294500', 'content': 'Rheumatoid arthritis is a heterogeneous chronic disease, and no therapeutic agent has been identified which is universally and persistently effective in all patients. We investigated the effectiveness of tofacitinib (CP-690,550), a novel oral Janus kinase inhibitor, as a targeted immunomodulator and disease-modifying therapy for rheumatoid arthritis.\nWe did a 6-month, double-blind, parallel-group phase 3 study at 82 centres in 13 countries, including North America, Europe, and Latin America. 399 patients aged 18 years or older with moderate-to-severe rheumatoid arthritis and inadequate response to tumour necrosis factor inhibitors (TNFi) were randomly assigned in a 2:2:1:1 ratio with an automated internet or telephone system to receive twice a day treatment with: tofacitinib 5 mg (n=133); tofacitinib 10 mg (n=134); or placebo (n=132), all with methotrexate. At month 3, patients given placebo advanced to either tofacitinib 5 mg twice a day (n=66) or 10 mg twice a day (n=66). Primary endpoints included American College of Rheumatology (ACR)20 response rate, mean change from baseline in Health Assessment Questionnaire-Disability Index (HAQ-DI), and rates of disease activity score (DAS)28-4(ESR) less than 2·6 (referred to as DAS28<2·6), all at month 3. The full analysis set for the primary analysis included all randomised patients who received at least one dose of study medication and had at least one post-baseline assessment. This trial is registered with www.ClinicalTrials.gov, number NCT00960440.\nAt month 3, ACR20 response rates were 41·7% (55 of 132 [95% CI vs placebo 6·06-28·41]; p=0·0024) for tofacitinib 5 mg twice a day and 48·1% (64 of 133; [12·45-34·92]; p<0·0001) for tofacitinib 10 mg twice a day versus 24·4% (32 of 131) for placebo. Improvements from baseline in HAQ-DI were -0·43 ([-0·36 to -0·15]; p<0·0001) for 5 mg twice a day and -0·46 ([-0·38 to -0·17]; p<0·0001) for 10 mg twice a day tofacitinib versus -0·18 for placebo; DAS28<2·6 rates were 6·7% (eight of 119; [0-10·10]; p=0·0496) for 5 mg twice a day tofacitinib and 8·8% (11 of 125 [1·66-12·60]; p=0·0105) for 10 mg twice a day tofacitinib versus 1·7% (two of 120) for placebo. Safety was consistent with phase 2 and 3 studies. The most common adverse events in months 0-3 were diarrhoea (13 of 267; 4·9%), nasopharyngitis (11 of 267; 4·1%), headache (11 of 267; 4·1%), and urinary tract infection (eight of 267; 3·0%) across tofacitinib groups, and nausea (nine of 132; 6·8%) in the placebo group.\nIn this treatment-refractory population, tofacitinib with methotrexate had rapid and clinically meaningful improvements in signs and symptoms of rheumatoid arthritis and physical function over 6 months with manageable safety. Tofacitinib could provide an effective treatment option in patients with an inadequate response to TNFi.\nPfizer.', 'title': 'Tofacitinib (CP-690,550) in combination with methotrexate in patients with active rheumatoid arthritis with an inadequate response to tumour necrosis factor inhibitors: a randomised phase 3 trial.', 'date': '2013-01-09'}}",1.0,Internal Medicine & Subspecialties
249,"Is pain relief for children with acute otitis media at 48 hour higher, lower, or the same comparing paracetamol to placebo? ",lower,very low,no,['8871138'],27977844,2016,"{'8871138': {'article_id': '8871138', 'content': 'Two hundred and nineteen children (boys: 56%, girls: 44%) were included in a randomized, double-blind, multicentre (4 centres) controlled trial designed to assess the efficacy and safety of ibuprofen (IBU) in the treatment of 1 to 6 year-old children with otoscopically proven acute otitis media (AOM), either unilateral or bilateral. They randomly received 10 mg/kg IBU (n = 71), or acetaminophen (PARA) (n = 73) or placebo (PLA) (n = 75), orally, tid, for 48 hours. All received oral cefaclor (Alfatil, Lilly, France) for seven days. They were evaluated before (D0) and at the end of treatment (D2). The main criterion of response was the aspect (landmarks and color) of the tympanic membrane assessed on a semi-quantitative scale from 0 to 6. Other criteria, assessed on semi-quantitative scales, included relief of pain (0 or 1), rectal temperature (0 to 2), and overall evaluation by parents of the improvement of quality of life on three items: appetite (0 to 2), sleep (0 to 2), and playing activity (0 to 2). The results at D2 were as follows: there was no significant difference between treatment groups as to the main criterion, but only a trend for IBU and PARA to do better than PLA but not for IBU to do better than PARA. From these data there is no argument to emphasize the utility of non-steroidal anti-inflammatory drugs (NSAIDs) in treating the inflammatory signs of the tympanic membrane in otitis. There was a statistically significant difference between treatment groups at D2 for pain, IBU being superior to PLA (P < 0.01): 7%, 10% and 25% of the children were still suffering at D2 in the IBU, PARA and PLA treatment groups, respectively. The difference between PARA and PLA for pain was not statistically significant. There was no significant difference between treatment groups for the other criteria. All treatments were well and equally tolerated. Although no significant difference was found between the treatment groups on the aspect of the tympanic membrane, the efficacy of IBU was evidenced on the relief of pain, the symptom that most disturbs the child.', 'title': 'A randomized, double-blind, multicentre controlled trial of ibuprofen versus acetaminophen and placebo for symptoms of acute otitis media in children.', 'date': '1996-01-01'}}",1.0,Internal Medicine & Subspecialties
250,"Is visual acuity at 5 years higher, lower, or the same when comparing children with pseudophakia and aphakia?",no difference,very low,no,['29906430'],36107778,2022,"{'29906430': {'article_id': '29906430', 'content': 'Comparative evaluation of complications and visual outcomes following bilateral congenital cataract surgery in children up to 2 years of age with and without primary intraocular lens (IOL) implantation at 5 years follow-up.\nRandomized controlled clinical trial.\nSixty children (120 eyes) up to 2 years of age undergoing bilateral congenital cataract surgery were randomized to Group 1, primary aphakia (n\xa0=\xa030), or Group 2, primary IOL implantation (pseudophakia) (n\xa0= 30). A single surgeon performed surgeries with identical surgical technique. All patients were followed up regularly until 5 years postoperatively. At each follow-up, glaucoma, visual axis obscuration (VAO) requiring surgery, and inflammation (cell deposits, posterior synechiae) were assessed. Visual acuity was assessed until 5 years follow-up. The first operated eye was selected for statistical analysis.\nMedian age of the patients at time of surgery was 5.11\xa0months (aphakia group) and 6.01\xa0months (pseudophakia group) (P\xa0= .56). Five years postoperatively, incidence of glaucoma was 16% and 13.8% in Groups 1 and 2 (P\xa0= .82). Incidence of posterior synechiae was significantly higher in the pseudophakia group (27.6%) compared to the aphakia group (8%) (P\xa0= .004). VAO requiring surgery was seen in 8% and 10.3% of eyes in Groups 1 and 2 (P\xa0= .76). Mean logMAR visual acuity at 5 years follow-up was 0.59 ± 0.33 and 0.5 ± 0.23 in Groups 1 and 2, respectively (P\xa0= .79). However, more eyes in the pseudophakic group started giving documentable vision earlier in their postoperative follow-ups.\nIncidence of postoperative complications was comparable between the groups, except for a higher incidence of posterior synechiae in pseudophakic eyes. Visual rehabilitation was faster in the pseudophakic group.', 'title': 'Five-Year Postoperative Outcomes of Bilateral Aphakia and Pseudophakia in Children up to 2 Years of Age: A Randomized Clinical Trial.', 'date': '2018-06-16'}}",1.0,Surgery
251,"Is the mean intraocular pressure (IOP) higher, lower, or the same when comparing argon LPIp as a primary procedure with peripheral iridotomy to peripheral iridotomy alone?",no difference,low,yes,"['21860572', '20472226']",33755197,2021,"{'21860572': {'article_id': '21860572', 'content': 'To compare conventional laser peripheral iridotomy (LPI) and LPI combined with laser peripheral iridoplasty in eyes with primary angle closure suspect (PACS) by assessment of anterior chamber dimensional changes using a Pentacam.\nForty-eight eyes of 24 subjects with bilateral PACS were recruited consecutively. Each eye was randomly allocated to treatment with conventional LPI, argon LPI only, or LPI plus iridoplasty, which consisted of simultaneous argon LPI and peripheral iridoplasty. Anterior chamber measurements were performed on each eye using a Pentacam, both before and after treatment. Mean anterior chamber depth (ACD), anterior chamber volume (ACV), and anterior chamber angle were measured, and topographic ACD analysis was performed. Results were compared between the two treatment groups.\nAfter treatment with either conventional LPI or LPI plus iridoplasty, the mean ACD and ACV increased significantly. Topographic ACD analysis revealed that the mid-to-peripheral ACD increase was significantly greater in the LPI plus iridoplasty group than in eyes treated with conventional LPI. Intraocular pressure changes and post-LPI complications did not differ between the groups.\nCompared with conventional LPI, our study showed that LPI plus iridoplasty improved the mid-to-peripheral ACD increase. This procedure may have a role as an adjunct for reducing angle closure by simultaneously eliminating pupillary and non-pupillary block components.', 'title': 'Laser peripheral iridotomy with iridoplasty in primary angle closure suspect: anterior chamber analysis by pentacam.', 'date': '2011-08-24'}, '20472226': {'article_id': '20472226', 'content': 'To compare the efficacy and safety of laser peripheral iridotomy with or without laser peripheral iridoplasty in the treatment of eyes with synechial primary angle-closure or primary angle-closure glaucoma.\nRandomized, controlled clinical trial.\nConsecutive patients older than 40 years with synechial primary angle-closure or primary angle closure glaucoma were recruited. Eligible patients were randomized to 1 of 2 treatment options, iridotomy or iridotomy plus iridoplasty, and were followed up for 1 year. Main outcome measures were intraocular pressure (IOP), peripheral anterior synechiae, corneal endothelial cell count, and complications.\nSeventy-seven eyes (77 patients) were randomized to the iridotomy group, and 81 eyes (81 patients) were randomized to the iridotomy plus iridoplasty group. Sixty-one patients (79.2%) in the iridotomy and 65 patients (80.2%) from the iridotomy plus iridoplasty groups completed 1 year of follow-up. There were no significant differences between the groups in the baseline data. IOP was reduced from 24.66 +/- 13.76 mm Hg to 19.03 +/- 6.21 mm Hg in the iridotomy group (P < .001) and from 27.96 +/- 13.06 mm Hg to 20.45 +/- 7.26 mm Hg in the iridotomy plus iridoplasty group (P < .001). Extent of peripheral anterior synechiae was decreased by 1 more clock-hour after iridoplasty compared with that after iridotomy in the iridotomy plus iridoplasty group (P < .001). There was no significant difference in IOP, medications, need for surgery, or visual function between groups at the 1-year visit.\nIn eyes with synechial primary angle-closure or primary angle-closure glaucoma, both iridotomy alone or combined with iridoplasty provide a significant and equivalent reduction in IOP. There is also a possible reduction in peripheral anterior synechiae, more so in the iridoplasty group.', 'title': 'Laser peripheral iridotomy with and without iridoplasty for primary angle-closure glaucoma: 1-year results of a randomized pilot study.', 'date': '2010-05-18'}}",1.0,Surgery
252,"Is the effect on neuropathy symptoms measured by Total Symptom Score (TSS) after six months lower, higher, or the same when comparing Alpha-lipoic acid with placebo?",no difference,low,no,['10480774'],38205823,2024,"{'10480774': {'article_id': '10480774', 'content': 'To evaluate the efficacy and safety of alpha-lipoic acid given intravenously, followed by oral treatment in type 2 diabetic patients with symptomatic polyneuropathy.\nIn a multicenter randomized double-blind placebo-controlled trial (Alpha-Lipoic Acid in Diabetic Neuropathy [ALADIN] III Study), 509 outpatients were randomly assigned to sequential treatment with 600 mg alpha-lipoic acid once daily intravenously for 3 weeks, followed by 600 mg alpha-lipoic acid three times a day orally for 6 months (A-A; n = 167); 600 mg alpha-lipoic acid once daily intravenously for 3 weeks, followed by placebo three times a day orally for 6 months (A-P; n = 174); and placebo once daily intravenously for 3 weeks, followed by placebo three times a day orally for 6 months (P-P; n = 168). Outcome measures included the Total Symptom Score (TSS) for neuropathic symptoms (pain, burning, paresthesias, and numbness) in the feet, and the Neuropathy Impairment Score (NIS). Data analysis was based on the intention to treat.\nNo significant differences between the groups were noted for the demographic variables and the nerve function parameters at baseline. The TSS in the feet decreased from baseline to day 19 (median [range]) by -3.7 (-12.6 to 5.0) points in the group given alpha-lipoic acid intravenously and by -3.0 (-12.3 to 8.0) points in the placebo group (P = 0.447), but the area under curve on a daily basis was significantly smaller in the active as compared with the placebo group (85.6 [0-219] vs. 95.9 [5.5-220]); P = 0.033). After 7 months, the changes in the TSS from baseline were not significantly different between the three groups studied, which could be due to increasing intercenter variability in the TSS during the trial. The NIS decreased after 19 days by -4.34+/-0.35 points (mean +/- SEM) in A-A and A-P and -3.49+/-0.58 points in P-P (P = 0.02 for alpha-lipoic acid versus placebo) and after 7 months by -5.82+/-0.73 points in A-A, -5.76+/-0.69 points in A-P, and -4.37+/-0.83 points in P-P (P = 0.09 for A-A vs. P-P). The rates of adverse events were not different between the groups throughout the study.\nThese findings indicate that a 3-week intravenous treatment with alpha-lipoic acid, followed by a 6-month oral treatment, had no effect on neuropathic symptoms distinguishable from placebo to a clinically meaningful degree, possibly due to increasing intercenter variability in symptom scoring during the study. However, this treatment was associated with a favorable effect on neuropathic deficits without causing significant adverse reactions. Long-term trials that focus on neuropathic deficits rather than symptoms as the primary criterion of efficacy are needed to see whether oral treatment with alpha-lipoic acid over several years may slow or reverse the progression of diabetic neuropathy.', 'title': 'Treatment of symptomatic diabetic polyneuropathy with the antioxidant alpha-lipoic acid: a 7-month multicenter randomized controlled trial (ALADIN III Study). ALADIN III Study Group. Alpha-Lipoic Acid in Diabetic Neuropathy.', 'date': '1999-09-10'}}",1.0,Psychiatry & Neurology
253,"Is mortality in people living with HIV at eight weeks higher, lower, or the same when comparing the use of lipoarabinomannan assay (LAM) testing to routine tuberculosis diagnostic testing?",lower,moderate,no,"['30032978', '26970721']",34416013,2021,"{'30032978': {'article_id': '30032978', 'content': 'Current diagnostics for HIV-associated tuberculosis are suboptimal, with missed diagnoses contributing to high hospital mortality and approximately 374\u2008000 annual HIV-positive deaths globally. Urine-based assays have a good diagnostic yield; therefore, we aimed to assess whether urine-based screening in HIV-positive inpatients for tuberculosis improved outcomes.\nWe did a pragmatic, multicentre, double-blind, randomised controlled trial in two hospitals in Malawi and South Africa. We included HIV-positive medical inpatients aged 18 years or more who were not taking tuberculosis treatment. We randomly assigned patients (1:1), using a computer-generated list of random block size stratified by site, to either the standard-of-care or the intervention screening group, irrespective of symptoms or clinical presentation. Attending clinicians made decisions about care; and patients, clinicians, and the study team were masked to the group allocation. In both groups, sputum was tested using the Xpert MTB/RIF assay (Xpert; Cepheid, Sunnyvale, CA, USA). In the standard-of-care group, urine samples were not tested for tuberculosis. In the intervention group, urine was tested with the Alere Determine TB-LAM Ag (TB-LAM; Alere, Waltham, MA, USA), and Xpert assays. The primary outcome was all-cause 56-day mortality. Subgroup analyses for the primary outcome were prespecified based on baseline CD4 count, haemoglobin, clinical suspicion for tuberculosis; and by study site and calendar time. We used an intention-to-treat principle for our analyses. This trial is registered with the ISRCTN registry, number ISRCTN71603869.\nBetween Oct 26, 2015, and Sept 19, 2017, we screened 4788 HIV-positive adults, of which 2600 (54%) were randomly assigned to the study groups (n=1300 for each group). 13 patients were excluded after randomisation from analysis in each group, leaving 2574 in the final intention-to-treat analysis (n=1287 in each group). At admission, 1861 patients were taking antiretroviral therapy and median CD4 count was 227 cells per μL (IQR 79-436). Mortality at 56 days was reported for 272 (21%) of 1287 patients in the standard-of-care group and 235 (18%) of 1287 in the intervention group (adjusted risk reduction [aRD] -2·8%, 95% CI -5·8 to 0·3; p=0·074). In three of the 12 prespecified, but underpowered subgroups, mortality was lower in the intervention group than in the standard-of-care group for CD4 counts less than 100 cells per μL (aRD -7·1%, 95% CI -13·7 to -0·4; p=0.036), severe anaemia (-9·0%, -16·6 to -1·3; p=0·021), and patients with clinically suspected tuberculosis (-5·7%, -10·9 to -0·5; p=0·033); with no difference by site or calendar period. Adverse events were similar in both groups.\nUrine-based tuberculosis screening did not reduce overall mortality in all HIV-positive inpatients, but might benefit some high-risk subgroups. Implementation could contribute towards global targets to reduce tuberculosis mortality.\nJoint Global Health Trials Scheme of the Medical Research Council, the UK Department for International Development, and the Wellcome Trust.', 'title': 'Rapid urine-based screening for tuberculosis in HIV-positive patients admitted to hospital in Africa (STAMP): a pragmatic, multicentre, parallel-group, double-blind, randomised controlled trial.', 'date': '2018-07-24'}, '26970721': {'article_id': '26970721', 'content': 'HIV-associated tuberculosis is difficult to diagnose and results in high mortality. Frequent extra-pulmonary presentation, inability to obtain sputum, and paucibacillary samples limits the usefulness of nucleic-acid amplification tests and smear microscopy. We therefore assessed a urine-based, lateral flow, point-of-care, lipoarabinomannan assay (LAM) and the effect of a LAM-guided anti-tuberculosis treatment initiation strategy on mortality.\nWe did a pragmatic, randomised, parallel-group, multicentre trial in ten hospitals in Africa--four in South Africa, two in Tanzania, two in Zambia, and two in Zimbabwe. Eligible patients were HIV-positive adults aged at least 18 years with at least one of the following symptoms of tuberculosis (fever, cough, night sweats, or self-reported weightloss) and illness severity necessitating admission to hospital. Exclusion criteria included receipt of any anti-tuberculosis medicine in the 60 days before enrolment. We randomly assigned patients (1:1) to either LAM plus routine diagnostic tests for tuberculosis (smear microscopy, Xpert-MTB/RIF, and culture; LAM group) or routine diagnostic tests alone (no LAM group) using computer-generated allocation lists in blocks of ten. All patients were asked to provide a urine sample of at least 30 mL at enrolment, and trained research nurses did the LAM test in patients allocated to this group using the Alere Determine tuberculosis LAM Ag lateral flow strip test (Alere, USA) at the bedside on enrolment. On the basis of a positive test result, the nurses made a recommendation for initiating anti-tuberculosis treatment. The attending physician made an independent decision about whether to start treatment or not. Neither patients nor health-care workers were masked to group allocation and test results. The primary endpoint was 8-week all-cause mortality assessed in the modified intention-to-treat population (those who received their allocated intervention). This trial is registered with ClinicalTrials.gov, number NCT01770730.\nBetween Jan 1, 2013, and Oct 2, 2014, we screened 8728 patients and randomly assigned 2659 to treatment (1336 to LAM, 1323 to no LAM). 108 patients did not receive their allocated treatment, mainly because they did not meet the inclusion criteria, and 23 were excluded from analysis, leaving 2528 in the final modified intention-to-treat analysis (1257 in the LAM group, 1271 in the no LAM group). Overall all-cause 8-week mortality occurred in 578 (23%) patients, 261 (21%) in LAM and 317 (25%) in no LAM, an absolute reduction of 4% (95% CI 1-7). The risk ratio adjusted for country was 0·83 (95% CI 0·73-0·96), p=0·012, with a relative risk reduction of 17% (95% CI 4-28). With the time-to-event analysis, there were 159 deaths per 100 person-years in LAM and 196 per 100 person-years in no LAM (hazard ratio adjusted for country 0·82 [95% CI 0·70-0·96], p=0·015). No adverse events were associated with LAM testing.\nBedside LAM-guided initiation of anti-tuberculosis treatment in HIV-positive hospital inpatients with suspected tuberculosis was associated with reduced 8-week mortality. The implementation of LAM testing is likely to offer the greatest benefit in hospitals where diagnostic resources are most scarce and where patients present with severe illness, advanced immunosuppression, and an inability to self-expectorate sputum.\nEuropean Developing Clinical Trials Partnership, the South African Medical Research Council, and the South African National Research Foundation.', 'title': 'Effect on mortality of point-of-care, urine-based lipoarabinomannan testing to guide tuberculosis treatment initiation in HIV-positive hospital inpatients: a pragmatic, parallel-group, multicountry, open-label, randomised controlled trial.', 'date': '2016-03-14'}}",0.5,"Public Health, Epidemiology & Health Systems"
254,"Is the proportion of wounds which are infection-free for dog bites higher, lower, or the same when comparing primary closure to no closure?",uncertain effect,,no,"['23916901', '23902527']",31805611,2019,"{'23916901': {'article_id': '23916901', 'content': 'Dog bite wounds represent a major health problem. Despite their importance, their management and especially the role of primary closure remain controversial. In this randomised controlled trial, the outcome between primary suturing and non-closure was compared.\n168 consecutive patients with dog bite injuries were included in this study. The wounds were allocated randomly in two treatment approaches: Group 1, consisting of eighty-two patients, had their wound sutured, whilst Group 2, consisting of eighty-six patients, did not have their wounds sutured. All wounds were cleansed using high-pressure irrigation and povidone iodine. All patients received the same type of antibiotic treatment. Our measured outcomes included presence of infection and cosmetic appearance. Cosmetic outcome was evaluated using the Vancouver Scar Scale (VSS). Wound and patient characteristics, such as time of management, wound location and size, and patient age, were recorded and analysed for their potential role in the resulting outcome.\nThe overall infection rate was 8.3%. No difference in the infection rate between primary suturing and non-suturing group was detected in the present study. The cosmetic appearance of the sutured wounds was significantly better (mean score 1.74) compared to the wounds that were left open (mean score 3.05) (p=0.0001). The infection rate was comparable among all age groups. Wounds treated within 8h of injury demonstrated an infection rate of 4.5%, which is lower compared to the 22.2% rate observed in wounds treated later than 8h. The wounds located at the head and neck exhibited better results in both infection rate and cosmetic outcome. Additionally, wounds >3 cm negatively affected the cosmetic appearance of the outcome.\nPrimary suturing of wounds caused by dog bites resulted in similar infection rate compared to non-suturing. However, primary suturing exhibited improved cosmetic appearance. Time of management appeared to be critical, as early treatment resulted in lower infection rate and improved cosmetic appearance regardless suturing or not. Furthermore, wounds located at the head and face demonstrated better results.', 'title': 'Primary closure versus non-closure of dog bite wounds. a randomised controlled trial.', 'date': '2013-08-07'}, '23902527': {'article_id': '23902527', 'content': 'To investigate the emergency treatment on facial laceration of dog bite wounds and identify whether immediate primary closure is feasible.\nSix hundred cases with facial laceration attacked by dog were divided into two groups randomly and evenly. After thorough debridement, the facial lacerations of group A were left open, while the lacerations of group B were undertaken immediate primary closure. Antibiotics use was administrated only after wound infected, not prophylactically given. The infection rate, infection time and healing time were analyzed.\nThe infection rate of group A and B was 8.3% and 6.3% respectively (P>0.05); the infection time was 26.3 ± 11.6h and 24.9 ± 13.8h respectively (P>0.05), the healing time was 9.12 ± 1.30 d and 6.57 ± 0.49 d respectively (P<0.05) in taintless cases, 14.24 ± 2.63 d and 10.65 ± 1.69 d respectively (P<0.05) in infected cases.Compared with group A, there was no evident tendency in increasing infection rate (8.3% in group A and 6.3% in group B respectively) and infection period (26.3 ± 11.6h in group A and 24.9 ± 13.8h in group B respectively) in group B. Meanwhile, in group B, the wound healing time was shorter than group A statistically in both taintless cases (9.12 ± 1.30 d in group A and 6.57 ± 0.49 d in group B respectively) and infected cases (14.24 ± 2.63 d in group A and 10.65 ± 1.69 d in group B respectively).\nThe facial laceration of dog bite wounds should be primary closed immediately after formal and thoroughly debridement. And the primary closure would shorten the healing time of the dog bite wounds without increasing the rate and period of infection. There is no potentiality of increasing infection incidence and infection speed, compared immediate primary closure with the wounds left open. On the contrary, primary closure the wounds can promote its primary healing. Prophylactic antibiotics administration was not recommended. and the important facial organ or tissue injuries should be secondary reconditioned.', 'title': 'Emergency treatment on facial laceration of dog bite wounds with immediate primary closure: a prospective randomized trial study.', 'date': '2013-08-09'}}",0.0,Surgery
255,"Is the need for mechanical ventilation in children with acute bronchiolitis higher, lower, or the same when comparing continuous positive airway pressure (CPAP) to control?",lower,low,no,"['28952459', '22431446', '17344251']",35377462,2022,"{'28952459': {'article_id': '28952459', 'content': 'To evaluate the efficacy of nasal continuous positive airway pressure (nCPAP) in decreasing respiratory distress in bronchiolitis.\nRandomized controlled trial.\nTertiary-care hospital in New Delhi, India. Participants: 72 infants (age <1y) hospitalized with a clinical diagnosis of bronchiolitis were randomized to receive standard care, or nCPAP in addition to standard care, in the first hour after admission. 23 parents refused to give consent for participation. 2 infants did not tolerate nCPAP.\n72 infants (age <1y) hospitalized with a clinical diagnosis of bronchiolitis were randomized to receive standard care, or nCPAP in addition to standard care, in the first hour after admission. 23 parents refused to give consent for participation. 2 infants did not tolerate nCPAP.\nThe outcome was assessed after 60 minutes. If nCPAP was not tolerated or the distress increased, the infant was switched to standard care. Analysis was done on intention-to-treat basis.\nChange in respiratory rate, Silverman-Anderson score and a Modified Pediatric Society of New Zealand Severity Score.\n14 out of 32 in nCPAP group and 5 out of 35 in standard care group had change in respiratory rate ≥10 (P=0.008). The mean (SD) change in respiratory rate [8.0 (5.8) vs 5.1 (4.0), P=0.02] in Silverman-Anderson score [0.78 (0.87) vs 0.39 (0.73), P=0.029] and in Modified Pediatric Society of New Zealand Severity Score [2.5 (3.01) vs. 1.08 (1.3), P=0.012] were significantly different in the nCPAP and standard care groups, respectively.\nnCPAP helped reduce respiratory distress significantly compared to standard care.', 'title': 'Nasal Continuous Positive Airway Pressure in Bronchiolitis: A Randomized Controlled Trial.', 'date': '2017-09-28'}, '22431446': {'article_id': '22431446', 'content': ""To compare the effects of nasal continuous positive airway pressure (nCPAP) and conventional oxygen therapy on the clinical signs of respiratory distress and the respiratory muscle workload in acute viral bronchiolitis.\nProspective, randomized, monocentric study carried out in the pediatric intensive care unit (PICU) of a university hospital.\nInfants <6 months old, admitted to the PICU with severe respiratory syncytial virus bronchiolitis.\nThe patients were randomized into two groups for 6 hr. The nCPAP group (n = 10) received 6 cmH(2)O pressure support delivered by a jet flow generator and the control group (n = 9) received an air/oxygen mixture from a heated humidifier. Respiratory distress was assessed by the modified Wood's clinical asthma score (m-WCAS), and inspiratory muscle work was evaluated by calculating the pressure-time product per breath (PTP(insp) /breath) and per minute (PTP(insp) /min) from the esophageal pressure (Pes) recordings.\nCompared with control condition, nCPAP decreased m-WCAS [-2.4 (1.05) vs. -0.5 (1.3), P = 0.03], PTPes(insp)/breath [-9.7 (5.7) vs. -1.4 (8.2), P = 0.04], PTPes(insp) /min [-666 (402) vs. -116 (352), P = 0.015], and FiO(2) [-7 (10) vs. +5 (15), P = 0.05]. Significant worsening of m-WCAS was only observed in the control group (4/9 vs. 0/10, P = 0.03).\nnCPAP rapidly decreased inspiratory work in young infants with acute bronchiolitis. Improvement in the respiratory distress score at 6 hr was proportional to the initial clinical severity, suggesting the importance of rapid nCPAP initiation in the more severe forms of the disease."", 'title': '6 cmH2O continuous positive airway pressure versus conventional oxygen therapy in severe viral bronchiolitis: a randomized trial.', 'date': '2012-03-21'}, '17344251': {'article_id': '17344251', 'content': 'To compare continuous positive airways pressure (CPAP) with standard treatment (ST) in the management of bronchiolitis.\nChildren <1 year of age with bronchiolitis and capillary PCO2 >6 kPa were recruited and randomised to CPAP or ST and then crossed over to the alternative treatment after 12 h. ST was intravenous fluids and supplemental oxygen by nasal prongs or face mask. The change in PCO2 was compared between the groups after 12 and 24 h. Secondary outcomes were change in capillary pH, respiratory rate, pulse rate and the need for invasive ventilatory support.\n29 of 31 children completed the study. PCO2 after 12 h fell by 0.92 kPa in children treated with CPAP compared with a rise of 0.04 kPa in those on ST (p<0.015). If CPAP was used first, there was a significantly better reduction in PCO2 than if it was used second. There were no differences in secondary outcome measures. CPAP was well tolerated with no complications identified.\nThis study suggests that CPAP compared with ST improves ventilation in children with bronchiolitis and hypercapnoea.', 'title': 'Randomised controlled trial of nasal continuous positive airways pressure (CPAP) in bronchiolitis.', 'date': '2007-03-09'}}",0.0,Pediatrics & Neonatology
256,"Is the rate of modified Rankin Scale (mRS) at three months higher, lower, or the same when comparing thrombo-aspiration to stent-retrieval thrombectomy?",no difference,,no,"['28763550', '30860055']",37249304,2023,"{'28763550': {'article_id': '28763550', 'content': 'The benefits of endovascular revascularization using the contact aspiration technique vs the stent retriever technique in patients with acute ischemic stroke remain uncertain because of lack of evidence from randomized trials.\nTo compare efficacy and adverse events using the contact aspiration technique vs the standard stent retriever technique as a first-line endovascular treatment for successful revascularization among patients with acute ischemic stroke and large vessel occlusion.\nThe Contact Aspiration vs Stent Retriever for Successful Revascularization (ASTER) study was a randomized, open-label, blinded end-point clinical trial conducted in 8 comprehensive stroke centers in France (October 2015-October 2016). Patients who presented with acute ischemic stroke and a large vessel occlusion in the anterior circulation within 6 hours of symptom onset were included.\nPatients were randomly assigned to first-line contact aspiration (n\u2009=\u2009192) or first-line stent retriever (n\u2009=\u2009189) immediately prior to mechanical thrombectomy.\nThe primary outcome was the proportion of patients with successful revascularization defined as a modified Thrombolysis in Cerebral Infarction score of 2b or 3 at the end of all endovascular procedures. Secondary outcomes included degree of disability assessed by overall distribution of the modified Rankin Scale (mRS) score at 90 days, change in National Institutes of Health Stroke Scale (NIHSS) score at 24 hours, all-cause mortality at 90 days, and procedure-related serious adverse events.\nAmong 381 patients randomized (mean age, 69.9 years; 174 women [45.7%]), 363 (95.3%) completed the trial. Median time from symptom onset to arterial puncture was 227 minutes (interquartile range, 180-280 minutes). For the primary outcome, the proportion of patients with successful revascularization was 85.4% (n\u2009=\u2009164) in the contact aspiration group vs 83.1% (n\u2009=\u2009157) in the stent retriever group (odds ratio, 1.20 [95% CI, 0.68-2.10]; P\u2009=\u2009.53; difference, 2.4% [95% CI, -5.4% to 9.7%]). For the clinical efficacy outcomes (change in NIHSS score at 24 hours, mRS score at 90 days) and adverse events, there were no significant differences between groups.\nAmong patients with ischemic stroke in the anterior circulation undergoing thrombectomy, first-line thrombectomy with contact aspiration compared with stent retriever did not result in an increased successful revascularization rate at the end of the procedure.\nclinicaltrials.gov Identifier: NCT02523261.', 'title': 'Effect of Endovascular Contact Aspiration vs Stent Retriever on Revascularization in Patients With Acute Ischemic Stroke and Large Vessel Occlusion: The ASTER Randomized Clinical Trial.', 'date': '2017-08-02'}, '30860055': {'article_id': '30860055', 'content': 'Stent retriever thrombectomy of large-vessel occlusion results in better outcomes than medical therapy alone. Alternative thrombectomy strategies, particularly a direct aspiration as first pass technique, while promising, have not been rigorously assessed for clinical efficacy in randomised trials. We designed COMPASS to assess whether patients treated with aspiration as first pass have non-inferior functional outcomes to those treated with a stent retriever as first line.\nWe did a multicentre, randomised, open label, blinded outcome, core lab adjudicated non-inferiority trial at 15 sites (ten hospitals and four specialty clinics in the USA and one hospital in Canada). Eligible participants were patients presenting with acute ischaemic stroke from anterior circulation large-vessel occlusion within 6 h of onset and an Alberta Stroke Program Early CT Score of greater than 6. We randomly assigned participants (1:1) via a central web-based system without stratification to either direct aspiration first pass or stent retriever first line thrombectomy. Those assessing primary outcomes via clinical examinations were masked to group assignment as they were not involved in the procedures. Physicians were allowed to use adjunctive technology as was consistent with their standard of care. The null hypothesis for this study was that patients treated with aspiration as first pass achieve inferior outcomes compared with those treated with a stent retriever first line approach. The primary outcome was non-inferiority of clinical functional outcome at 90 days as measured by the percentage of patients achieving a modified Rankin Scale score of 0-2, analysed by intent to treat; non-inferiority was established with a margin of 0·15. All randomly assigned patients were included in the safety analyses. This trial is registered at ClinicalTrials.gov, number: NCT02466893.\nBetween June 1, 2015, and July 5, 2017, we assigned 270 patients to treatment: 134 to aspiration first pass and 136 to stent retriever first line. A modified Rankin score of 0-2 at 90 days was achieved by 69 patients (52%; 95% CI 43·8-60·3) in the aspiration group and 67 patients (50%; 41·6-57·4) in the stent retriever group, showing that aspiration as first pass was non-inferior to stent retriever first line (p\nA direct aspiration as first pass thrombectomy conferred non-inferior functional outcome at 90 days compared with stent retriever first line thrombectomy. This study supports the use of direct aspiration as an alternative to stent retriever as first-line therapy for stroke thrombectomy.\nPenumbra.', 'title': 'Aspiration thrombectomy versus stent retriever thrombectomy as first-line approach for large vessel occlusion (COMPASS): a multicentre, randomised, open label, blinded outcome, non-inferiority trial.', 'date': '2019-03-13'}}",1.0,Surgery
257,"Is hospital mortality higher, lower, or the same when comparing the study group with omentoplasty to the control?",no difference,,no,"['17062260', '22648923']",25274134,2014,"{'17062260': {'article_id': '17062260', 'content': 'Esophagogastrectomy for carcinoma of the esophagus is the standard surgical treatment for cure or palliation. Esophagogastric anastomotic leakage is a life-threatening postoperative complication, more so if the leakage occurs in the chest.\nA prospective, randomized study was conducted on 238 patients treated for carcinoma of the esophagus between January 2000 and January 2006. The study excluded 44 patients (18.49%) who were inoperable. The patients were assigned to two treatment groups of 97 each (A and B) according to a restricted permuted block randomization plan. Group A patients underwent esophagogastrectomy with wrapping of the pedicled omentum around the esophagogastric anastomosis. Group B patients underwent esophagogastrectomy without using the omental graft. An Ivor-Lewis type esophagogastrectomy (TTE) was done in 122 patients (62.89%) and a transhiatal esophagogastrectomy (THE) was done in 72 (37.11%).\nAnastomotic leaks occurred in 3 group A patients (3.09%) and in 14 (14.43%) group B patients. In group A, 54 patients underwent THE and 43 had TTE, with anastomotic leakage in 2 (3.70%) and 1 (2.33%) patients, respectively. In group B, 48 patients had THE and 49 had TTE, with anastomotic leakage in 8 (16.26%) and 6 (12.24%), respectively. The difference in the incidence of leakage was statistically significant (p = 0.005). There was no complication related to the omental graft technique nor was there a significant difference in the mortality between the two groups.\nThe pedicled omental transposition for reinforcing the anastomotic suture line significantly reduces the incidence of leakage after esophagogastrectomy for carcinoma of the esophagus, thus decreasing the morbidity and mortality of the procedure.', 'title': 'Use of pedicled omentum in esophagogastric anastomosis for prevention of anastomotic leak.', 'date': '2006-10-26'}, '22648923': {'article_id': '22648923', 'content': 'Anastomotic leakage is a major cause of mortality in oesophageal surgery. Whether omentoplasty after oesophagogastrostomy could reduce anastomotic leakage is still controversial. The aim of this study is to investigate the function of omentoplasty to reinforce cervical oesophagogastrostomy after radical oesophagectomy with three-field lymphadenectomy.\nA total of 184 patients who underwent radical oesophagectomy with three-field lymphadenectomy took part in this prospective study. Patients were randomized to receive either the omentoplasty or non-omentoplasty. In the omentoplasty group, the omentum was wrapped around the oesophagogastric anastomosis after oesophagogastrostomy. Age, gender, location of carcinoma, stage, body mass index, diabetes, coronary artery disease, peripheral vascular disease and performance of omentoplasty were recorded. The anastomotic leakage and stricture and recurrence site were followed up for three years after the operation.\nThe two groups were comparable in terms of age, gender, location of carcinoma, stage, body mass index, diabetes, coronary artery disease and peripheral vascular disease (P > 0.05). In contrast to the non-omentoplasty group with a postoperative anastomotic leakage rate of 9.8%, the omentoplasty subjects demonstrated a significantly lower rate of 3.3% (P < 0.05). No lethal leakage was found in the omentoplasty group, while two non-omentoplasty patients developed incurable empyema and mediastinitis due to leakage and ultimately died. The rate of incidence of anastomotic stricture in the omentoplasty and non-omentoplasty groups were 4.3% and 2.2% respectively. Of the five cases of death during the hospital stay, two were found in the omentoplasty group and three in non-omentoplasty. There was no significant difference of lethal leakage, stricture and death rate between the two groups (P > 0.05). The hospital stay was significantly longer for non-omentoplasty patients, compared with that for the omentoplasty subjects (P < 0.05). Tumour recurrence in lymphatic- or haematogenous metastasis was similar in both groups (P > 0.05).\nOmentoplasty may prevent anastomotic leakage of oesophagogastrostomy following radical oesophagectomy with three-field lymphadenectomy.', 'title': 'Omentoplasty in preventing anastomotic leakage of oesophagogastrostomy following radical oesophagectomy with three-field lymphadenectomy.', 'date': '2012-06-01'}}",1.0,Surgery
258,"Is postextubation stridor higher, lower, or the same when comparing cuffed to uncuffed endotracheal tubes (ETTs)?",no difference,very low,no,['19887533'],29149469,2017,"{'19887533': {'article_id': '19887533', 'content': 'The use of cuffed tracheal tubes (TTs) in small children is still controversial. The aim of this study was to compare post-extubation morbidity and TT exchange rates when using cuffed vs uncuffed tubes in small children.\nPatients aged from birth to 5 yr requiring general anaesthesia with TT intubation were included in 24 European paediatric anaesthesia centres. Patients were prospectively randomized into a cuffed TT group (Microcuff PET) and an uncuffed TT group (Mallinckrodt, Portex, Rüsch, Sheridan). Endpoints were incidence of post-extubation stridor and the number of TT exchanges to find an appropriate-sized tube. For cuffed TTs, minimal cuff pressure required to seal the airway was noted; maximal cuff pressure was limited at 20 cm H(2)O with a pressure release valve. Data are mean (SD).\nA total of 2246 children were studied (1119/1127 cuffed/uncuffed). The age was 1.93 (1.48) yr in the cuffed and 1.87 (1.45) yr in the uncuffed groups. Post-extubation stridor was noted in 4.4% of patients with cuffed and in 4.7% with uncuffed TTs (P=0.543). TT exchange rate was 2.1% in the cuffed and 30.8% in the uncuffed groups (P<0.0001). Minimal cuff pressure required to seal the trachea was 10.6 (4.3) cm H(2)O.\nThe use of cuffed TTs in small children provides a reliably sealed airway at cuff pressures of <or=20 cm H(2)O, reduces the need for TT exchanges, and does not increase the risk for post-extubation stridor compared with uncuffed TTs.', 'title': 'Prospective randomized controlled multi-centre trial of cuffed or uncuffed endotracheal tubes in small children.', 'date': '2009-11-06'}}",1.0,Surgery
259,"Is the overall survival higher, lower, or the same when comparing hormone replacement therapy (HRT) to control?",higher,low,no,"['26417001', '10491528', '22740889']",31989588,2020,"{'26417001': {'article_id': '26417001', 'content': 'To assess the effects of adjuvant hormone therapy (AHT) on survival and disease outcome in women with epithelial ovarian cancer.\nParticipants were premenopausal and postmenopausal women who had been diagnosed with epithelial ovarian cancer (any International Federation of Gynecology and Obstetrics stage) 9 or fewer months previously. Ineligible patients included those with deliberately preserved ovarian function, with a history of a hormone-dependent malignancy, or with any contraindications to hormone-replacement therapy. Patients were centrally randomly assigned in a 1:1 ratio to either AHT for 5 years after random assignment or no AHT (control). Main outcome measures were overall survival (OS), defined as time from random assignment to death (any cause), and relapse-free survival, defined as time from random assignment to relapse or death (any cause). Patients who continued, alive and relapse free, were censored at their last known follow-up.\nA total of 150 patients (n = 75, AHT; n = 75, control) were randomly assigned from 1990 to 1995 from 19 centers in the United Kingdom, Spain, and Hungary; all patients were included in intention-to-treat analyses. The median follow-up in alive patients is currently 19.1 years. Of the 75 patients with AHT, 53 (71%) have died compared with 68 (91%) of 75 patients in the control group. OS was significantly improved in patients who were receiving AHT (hazard ratio, 0.63; 95% CI, 0.44 to 0.90; P = .011). A similar effect was seen for relapse-free survival (hazard ratio, 0.67; 95% CI, 0.47 to 0.97; P = .032). Effects remained after adjustment for known prognostic factors.\nThese results show that women who have severe menopausal symptoms after ovarian cancer treatment can safely take hormone-replacement therapy, and this may, in fact, infer benefits in terms of OS in addition to known advantages in terms of quality of life.', 'title': 'Adjuvant Hormone Therapy May Improve Survival in Epithelial Ovarian Cancer: Results of the AHT Randomized Trial.', 'date': '2015-09-30'}, '10491528': {'article_id': '10491528', 'content': 'Surgery for premenopausal and perimenopausal patients with epithelial ovarian carcinoma may often result in significant menopausal symptoms. Physicians may well be reluctant to prescribe, and patients loathe to take, postoperative estrogen replacement therapy because they fear that supplementation may lead to ovarian carcinoma relapse. This randomized prospective study was undertaken to determine whether postoperative estrogen replacement therapy had a negative influence on the disease free interval and overall survival of ovarian carcinoma survivors.\nBetween January 1987 and June 1994, at the time of a routine consultation held 6-8 weeks postoperatively, 130 patients younger than 59 years with invasive epithelial ovarian carcinoma were randomized to continuous oral conjugated equine estrogen (ERT) or to no supplementation (non-ERT). All patients were followed prospectively for a minimum of 48 months.\nThree patients in the ERT group and 2 in the non-ERT group were lost to follow-up, so 59 and 66 were finally analyzed in their respective groups. Nine patients originally randomized to ERT refused or stopped their supplementation, whereas 5 in the non-ERT commenced taking estrogens. A total of 32 recurrences occurred in the ERT group and 41 in the non-ERT group. The median disease free interval was 34 versus 27 months, respectively, whereas overall survival was 44 versus 34 months, respectively, for the two groups. The differences in disease free interval (P = 0.785) and overall survival (P = 0.354) between the two groups were not statistically significant.\nPostoperative estrogen replacement therapy did not have a negative influence on the disease free interval and overall survival of ovarian carcinoma survivors.', 'title': 'Estrogen replacement therapy for ovarian carcinoma survivors: A randomized controlled trial.', 'date': '1999-09-24'}, '22740889': {'article_id': '22740889', 'content': 'The present study aimed to assess the impact of post-surgical hormone replacement therapy (HRT) on life quality and prognosis in women with ovarian malignancy. HRT (Premarin, Nilestriol and medroxyprogesterone) was administered following surgery in 31 patients with ovarian cancer. A total of 44 ovarian cancer patients of similar age, clinical stage and pathological features did not receive HRT following surgery. The expression of estrogen receptor (ER)-α, ERβ and progesterone receptor (PR) in cancer tissues was detected by immunohistochemical staining. Serum levels of calcitonin (CT) and transforming growth factor (TGF)-α were determined by radioimmunoassay and enzyme-linked immunosorbent assay, respectively. Data were analyzed using Kaplan-Meier survival curves, a log-rank test and a Cox scale risk model. Quality of life was assessed in the patient groups and in healthy post-menopausal women (control) based on a questionnaire developed by the European Organization of Research and Treatment of Cancer (EORTC-C30), as well as our own specific questionnaire. A log-rank test revealed no difference in survival between the patients with and without HRT (p>0.05), and a Cox model showed that HRT was not an independent prognostic factor. The accumulated survival rate did not differ significantly based on the expression of ERα, ERβ or PR in patients with or without HRT (p>0.05). The serum TGFα levels prior to and following surgery were not significantly different in either of the two patient groups (p>0.05). Serum CT levels were higher in patients without HRT at 1.5 years following surgery (p<0.05), but no significant difference was found in the serum CT levels of patients receiving HRT. The HRT and non-HRT groups differed significantly with regard to the body and emotional functional sub-scales of the EORTC-C30 (p<0.05) and the sex quality and autonomic nerve maladjustment categories of our specific questionnaire (p<0.05). Findings of this study showed that HRT administered following surgery exhibited no apparent negative effect on prognosis in patients with ovarian cancer, regardless of ERα, ERβ or PR expression in cancer tissues, and had no effect on serum transforming growth factor (TGF)-α levels. Post-surgical HRT aided in the stabilization of serum CT levels and improved the quality of life in these patients.', 'title': 'Impact of post-operative hormone replacement therapy on life quality and prognosis in patients with ovarian malignancy.', 'date': '2012-06-29'}}",0.3333333333333333,Obstetrics & Gynecology
260,"Is the incidence of congenital syphilis higher, lower, or the same when comparing on-site screening to control? ",lower,,no,['19773681'],25352226,2014,"{'19773681': {'article_id': '19773681', 'content': 'This cluster randomized trial was performed to test whether one-stop service could better prevent congenital syphilis than the conventional antenatal screening service in Mongolia.\nOut of 14 antenatal clinics in 6 districts of Ulaanbaatar, 7 were randomly selected for the one-stop service and the remaining for the conventional service. Intervention clinics provided on-site rapid syphilis testing and immediate treatment for positive cases and their partners. In control clinics, syphilis screening services with routine off-site rapid plasma regain testing and case management were followed. Analysis was intention to treat.\nOf 3850 antenatal women recruited in each group, the proportion of syphilis testing at the first visit and third trimester was over 99% in the intervention group and 79.6% and 61.5% in the control group, respectively (P <0.001 for both periods). Correspondingly, syphilis cases detected in the intervention group were 73 (1.9%) and 20 (0.5%) for the first visit and third trimester, respectively, and 27 (0.9%) and 2 (0.08%) in the control group; and 98.9% (92/93) of the detected cases in the intervention group and 89.6% (26/29) in the control group were adequately treated (P = 0.02). The corresponding treatment rates for sexual partners were 94.6% and 55.2% (P <0.001). One congenital syphilis case out of 3632 deliveries in the intervention group, compared to 15 of 3552 in the control group, was diagnosed, a reduction of 93.5% (95% confidence interval, 66.0%-98.6%).\nOne-stop services increased the detection rate of syphilis, treated more positive women and their partners, and effectively reduced the rate of congenital syphilis.', 'title': 'One-stop service for antenatal syphilis screening and prevention of congenital syphilis in Ulaanbaatar, Mongolia: a cluster randomized trial.', 'date': '2009-09-24'}}",1.0,Obstetrics & Gynecology
261,"Is the risk of symptomatic UTI higher, lower, or the same when comparing bladder inoculation using E. coli to control?",uncertain effect,very low,no,"['16231269', '21683991', '20483149']",28884476,2017,"{'16231269': {'article_id': '16231269', 'content': 'This prospective, randomized, placebo-controlled, double-blind pilot trial examined the efficacy of bacterial interference in preventing urinary tract infection (UTI) in 27 patients with spinal cord injury. Patients whose bladders became colonized with Escherichia coli 83972 were half as likely (P=.01) than noncolonized patients to develop UTI during the subsequent year.', 'title': 'Bacterial interference for prevention of urinary tract infection: a prospective, randomized, placebo-controlled, double-blind pilot trial.', 'date': '2005-10-19'}, '21683991': {'article_id': '21683991', 'content': ""To compare the effectiveness of bacterial interference versus placebo in preventing urinary tract infection (UTI).\nThe main outcome measure was the numbers of episodes of UTI/patient-year. Randomization was computer generated, with allocation concealment by visibly indistinguishable products distributed from a core facility. The healthcare providers and those assessing the outcomes were unaware of the group allocation. Adult patients (n = 65) with neurogenic bladder after spinal cord injury and a history of recurrent UTI were randomized in a 3:1 ratio to receive either Escherichia coli HU2117 or sterile saline. Urine cultures were obtained weekly during the first month and then monthly for 1 year. The patients were evaluable if they remained colonized with E. coli HU2117 for >4 weeks (experimental group). The trial is closed to follow-up.\nOf the 59 patients who received bladder inoculations, 27 were evaluable (17 in the experimental group and 10 in the placebo group). The 2 study groups had comparable clinical characteristics. Of 17 patients colonized with E. coli HU2117 and the 10 control patients, 5 (29%, 95% confidence interval 0.11-0.56) and 7 (70%, 95% confidence interval 0.35-0.92) developed >1 episode of UTI (P = .049; 1-sided Fisher's exact test), respectively. The average number of episodes of UTI/patient-year was also lower (P = .02, Wilcoxon rank sum test) in the experimental (0.50) than in the control group (1.68). E. coli HU2117 did not cause symptomatic UTI.\nBladder colonization with E. coli HU2117 safely reduces the risk of symptomatic UTI in patients with spinal cord injury. Effective, but less complex, methods for achieving bladder colonization with E. coli HU2117 are under investigation."", 'title': 'Multicenter randomized controlled trial of bacterial interference for prevention of urinary tract infection in patients with neurogenic bladder.', 'date': '2011-06-21'}, '20483149': {'article_id': '20483149', 'content': 'We determined if the deliberate establishment of asymptomatic bacteriuria with Escherichia coli 83972 in patients with incomplete bladder emptying and recurrent urinary tract infection protects against recurrence.\nIn phase 1 of the study the patients were randomized to blinded inoculations with E. coli 83972 or saline. Crossover occurred after monitoring for 12 months or after a urinary tract infection. The outcome was the time to the first urinary tract infection in patients with and without E. coli 83972 bacteriuria. In phase 2 patients were subjected to additional blinded inoculations to extend periods with and without E. coli 83972 bacteriuria. The outcome was the number of urinary tract infections during 12 months with and 12 months without E. coli 83972 bacteriuria.\nA total of 20 patients completed the study. In phase 1 the time to the first urinary tract infection was longer with than without E. coli 83972 bacteriuria (median 11.3 vs 5.7 months, sign test p = 0.0129). Phase 2 was analyzed after patients had spent a total of 202 months with and 168 months without E. coli 83972 bacteriuria. There were fewer reported urinary tract infection episodes with vs without E. coli 83972 bacteriuria (13 vs 35 episodes, paired t test p = 0.009, CI 0.31-1.89). There was no febrile urinary tract infection episode in either of the study arms and no significant side effects of intravesical bacterial inoculation were reported.\nDeliberately induced E. coli 83972 bacteriuria protected patients with incomplete bladder emptying who are prone to urinary tract infection from recurrent urinary tract infection as demonstrated by the delay in time to urinary tract infection and the decrease in number of urinary tract infection episodes.', 'title': 'Escherichia coli 83972 bacteriuria protects against recurrent lower urinary tract infections in patients with incomplete bladder emptying.', 'date': '2010-05-21'}}",0.0,"Public Health, Epidemiology & Health Systems"
262,"Is urinary calcium higher, lower, or the same when comparing low salt, normal calcium diet with a broad diet?",lower,,no,['20042524'],24519664,2014,"{'20042524': {'article_id': '20042524', 'content': 'A direct relation exists between sodium and calcium excretion, but randomized studies evaluating the sustained effect of a low-salt diet on idiopathic hypercalciuria, one of the main risk factors for calcium-oxalate stone formation, are still lacking.\nOur goal was to evaluate the effect of a low-salt diet on urinary calcium excretion in patients affected by idiopathic calcium nephrolithiasis.\nPatients affected by idiopathic calcium stone disease and hypercalciuria (>300 mg Ca/d in men and >250 mg Ca/d in women) were randomly assigned to receive either water therapy alone (control diet) or water therapy and a low-salt diet (low-sodium diet) for 3 mo. Twenty-four-hour urine samples were obtained twice from all patients: one sample at baseline on a free diet and one sample after 3 mo of treatment.\nA total of 210 patients were randomly assigned to receive a control diet (n = 102) or a low-sodium diet (n = 108); 13 patients (2 on the control diet, 11 on the low-sodium diet) withdrew from the trial. At the follow-up visit, patients on the low-sodium diet had lower urinary sodium (mean +/- SD: 68 +/- 43 mmol/d at 3 mo compared with 228 +/- 57 mmol/d at baseline; P < 0.001). Concomitant with this change, they showed lower urinary calcium (271 +/- 86 mg/d at 3 mo compared with 361 +/- 129 mg/d on the control diet, P < 0.001) and lower oxalate excretion (28 +/- 8 mg/d at 3 mo compared with 32 +/- 10 mg/d on the control diet, P = 0.001). Urinary calcium was within the normal range in 61.9% of the patients on the low-salt diet and in 34.0% of those on the control diet (difference: +27.9%; 95% CI: +14.4%, +41.3%; P < 0.001).\nA low-salt diet can reduce calcium excretion in hypercalciuric stone formers. This trial was registered at clinicaltrials.gov as NCT01005082.', 'title': 'Effects of a low-salt diet on idiopathic hypercalciuria in calcium-oxalate stone formers: a 3-mo randomized controlled trial.', 'date': '2010-01-01'}}",0.0,Internal Medicine & Subspecialties
263,"Is the number of deaths higher, lower, or the same when comparing maintenance-fluid to restricted-fluid? ",no difference,low,no,['12070950'],27813057,2016,"{'12070950': {'article_id': '12070950', 'content': 'A multi-centre randomised open trial was done to determine whether moderate oral fluid restriction or intravenous fluid at full maintenance volumes would result in a better outcome for children with bacterial meningitis in Papua New Guinea, and what clinical signs could guide fluid management. Children with clinical signs and cerebrospinal fluid suggestive of bacterial meningitis received either breast milk by nasogastric tube at 60% of normal maintenance volumes (n = 172) or intravenous half-normal saline and 5% dextrose at 100% of normal maintenance volumes (n = 174) for the 1st 48 hrs of treatment. An adverse outcome was death or severe neurological sequelae, and a good outcome was defined as intact survival or survival with at worst mild-to-moderate neurological sequelae. The probability of an adverse outcome was 24.7% in the intravenous group and 33.1% in the oral-restricted group, but the difference was not statistically significant (RR 0.75, 0.53-1.04, p = 0.08). Sunken eyes or reduced skin turgor at presentation were risk factors for an adverse outcome (OR 5.70, 95% CI 2.87-11.29) and were most strongly associated with adverse outcome in the fluid-restricted group. Eyelid oedema during treatment was also a risk factor for an adverse outcome (OR 2.54, 95% CI 1.36-4.75) and eyelid oedema was much more common in the intravenous group (26%) than in the restricted group (5%). For many children with bacterial meningitis in less developed countries, moderate fluid restriction is unnecessary and will be harmful; a normal state of hydration should be achieved but over-hydration should be avoided. Giving 100% of normal maintenance fluids, especially with intravenous hypotonic fluid, will lead to oedema in up to one quarter of children with bacterial meningitis. If additional intravenous fluids are required for children with meningitis, an isotonic solution should be used.', 'title': 'Management of meningitis in children with oral fluid restriction or intravenous fluid at maintenance volumes: a randomised trial.', 'date': '2002-06-20'}}",0.0,Surgery
264,"Is initiation rate of contraceptive implants at the first postpartum check-up higher, lower, or the same when comparing the immediate insertion group to the delayed insertion group? ",higher,moderate,no,"['7847530', '21508750', '27561981']",28432791,2017,"{'7847530': {'article_id': '7847530', 'content': 'Our purpose was to determine the safety and tolerance of levonorgestrel contraceptive implants (Norplant, Wyeth-Ayerst, Philadelphia) when inserted immediately post partum, to document the effects on weight and blood pressure, and to determine the side effects.\nAfter vaginal delivery, 250 women were randomized to receive Norplant within 48 hours of delivery (study group) or at the 4- to 6-week postpartum visit (control group). Baseline measurements were recorded and compared with those obtained at the 4- to 6-week follow-up visit. A diary was maintained by patients who recorded bleeding and side effects. Statistical analysis was performed with t test and chi 2 analysis.\nThere were no episodes of acute postpartum hemorrhage or clinically significant bleeding. Compared with the control group, the immediate group reported significantly more bleeding days (p < 0.01). There was no significant difference between the two groups in the hemoglobin values obtained at 4 to 6 weeks post partum. The immediate insertion group reported significantly more headaches (p < 0.01) and acne (p = 0.01).\nNorplant is well tolerated and should be available for interested patients immediately post partum.\nDuring June 1992-February 1993, physicians in Charlotte, North Carolina, randomly assigned 250 women who had delivered vaginally at the Carolinas Medical Center to receive the contraceptive implant Norplant either before being discharged from the hospital on postpartum day 1, 2, or 3 or 4-6 weeks later at the scheduled postpartum follow-up visit. They wanted to ascertain whether insertion of Norplant in the immediate postpartum period was safe and well-tolerated. 26 women were lost to follow-up. 11 of these women were in the delayed insertion group, so they did not receive Norplant or an alternative contraceptive method, placing them at risk of pregnancy. Another 34 women in the same group returned for their follow-up visit but did not receive Norplant. The mean interval from delivery to insertion stood at 1.7 days for the immediate insertion group and 34.3 days for the delayed insertion group. No one in either group experienced acute postpartum hemorrhage. The immediate insertion group had much more bleeding and spotting than did the delayed insertion group (17 vs. 13.6 days, p 0.01, and 11.1 vs. 8.8 days, p = 0.03, respectively; 28.2 vs. 22.4 days for both spotting and bleeding, p 0.01). Since hemoglobin values of the two groups did not differ (12.9 vs. 12.7), the increased bleeding was not clinically significant. Women in the immediate insertion group were more likely than those in the delayed insertion group to have headaches on at least seven days between insertion and study follow-up (15.1% vs. 2.8%; p 0.01). They also were more likely to have acne during at least three days (18.9% vs. 6.4%; p 0.01). They were just as likely as the delayed insertion group to report nausea, hair loss, and hirsutism. 40% of the women in both groups had sexual intercourse before their 4-6 week follow-up visit. These findings show that Norplant can be safe and well-tolerated if inserted in the immediate postpartum period.', 'title': 'Use of Norplant contraceptive implants in the immediate postpartum period: safety and tolerance.', 'date': '1995-01-01'}, '21508750': {'article_id': '21508750', 'content': 'To evaluate lactogenesis after early postpartum insertion of the etonogestrel contraceptive implant.\nHealthy peripartum women with healthy, term newborns who desired the etonogestrel implant for contraception were randomly assigned to early (1-3 days) or standard (4-8 weeks) postpartum insertion. The primary outcomes, time to lactogenesis stage II and lactation failure, were documented by a validated measure. The noninferiority margin for the mean difference in time to lactogenesis stage II was defined as 8 additional hours. Secondary data (device continuation and contraceptive use, breast milk analysis, supplementation rates, side effects, and bleeding patterns) were collected at periodic intervals for 6 months.\nSixty-nine women were enrolled. Thirty-five were randomly assigned to early insertion and 34 to standard insertion. There were no statistically significant differences between the groups in age, race, parity, mode of delivery, use of anesthesia, or prior breastfeeding experience. Early insertion was demonstrated to be noninferior to standard insertion in time to lactogenesis stage II (early: [mean±standard deviation] 64.3±19.6 hours; standard: 65.2±18.5 hours, mean difference, -1.4 hours, 95% confidence interval [CI] -10.6 to 7.7 hours). Early insertion was also demonstrated to be noninferior to standard insertion in incidence of lactation failure (1/34 [3%] in the early insertion group, 0/35 [0%] in the standard insertion group [risk difference, 0.03, 95% CI -0.02 to 0.08]). Use of formula supplementation was not significantly different between the groups. Milk composition at 6 weeks was not significantly different between the groups.\nBreastfeeding outcomes were similar in women who underwent early compared with standard postpartum insertion of the etonogestrel implant.\nClinicalTrials.gov, www.clinicaltrials.gov, NCT00847587.', 'title': 'Lactogenesis after early postpartum use of the contraceptive implant: a randomized controlled trial.', 'date': '2011-04-22'}, '27561981': {'article_id': '27561981', 'content': 'To compare immediate postpartum insertion of the contraceptive implant to placement at the 6-week postpartum visit among adolescent and young women.\nNon-blinded, randomized controlled trial.\nPostpartum adolescents and young women ages 14-24\xa0years who delivered at an academic tertiary care hospital serving rural and urban populations in North Carolina.\nPlacement of an etonogestrel-releasing contraceptive implant before leaving the hospital postpartum, or at the 4-6\xa0week postpartum visit.\nContraceptive implant use at 12\xa0months postpartum.\nNinety-six participants were randomized into the trial. Data regarding use at 12\xa0months were available for 64 participants, 37 in the immediate group and 27 in the 6-week group. There was no difference in use at 12\xa0months between the immediate group and the 6-week group (30 of 37, 81% vs 21 of 27, 78%; P\xa0=\xa0.75). At 3\xa0months, the immediate group was more likely to have the implant in place (34 of 37, 92% vs 19 of 27, 70%; P\xa0=\xa0.02).\nPlacing the contraceptive implant in the immediate postpartum period results in a higher rate of use at 3\xa0months postpartum and appears to have similar use rates at 12\xa0months compared with 6-week postpartum placement. Providing contraceptive implants to adolescents before hospital discharge takes advantage of access to care, increases the likelihood of effective contraception in the early postpartum period, appears to have no adverse effects on breastfeeding, and might lead to increased utilization at 1\xa0year postpartum.', 'title': 'Etonogestrel-Releasing Contraceptive Implant for Postpartum Adolescents: A Randomized Controlled Trial.', 'date': '2016-08-27'}}",0.0,Obstetrics & Gynecology
265,"Is six-minute walk distance (6MWD) higher, lower, or the same when comparing following exercise training and control? ",higher,,no,"['25407957', '18245143']",26544672,2015,"{'25407957': {'article_id': '25407957', 'content': ""The study aimed to determine the short and long-term effects of exercise training on exercise capacity and health-related quality of life (HRQoL) compared to usual care in people with dust-related pleural and interstitial respiratory diseases. No previous studies have specifically evaluated exercise training in this patient population.\nParticipants with a diagnosis of a dust-related respiratory disease including asbestosis and asbestos related pleural disease were recruited and randomised to an eight-week exercise training group (EG) or a control group (CG) of usual care. Six-minute walk distance (6MWD), St George's Respiratory Questionnaire (SGRQ) and Chronic Respiratory Disease Questionnaire (CRQ) were measured at baseline, eight weeks and 26 weeks by an assessor blinded to group allocation.\nThirty-three of 35 male participants completed the study. Sixty-nine percent of participants had asbestos related pleural disease. At eight weeks, compared to the CG, the EG showed a significantly increased 6MWD (mean difference (95%CI)) 53 metres (32 to 74), improved SGRQ total score, -7 points (-13 to -1) and increased CRQ total score, 6.4 points (2.1 to 10.7). At 26 weeks significant between-group differences were maintained in 6MWD, 45 metres (17 to 73) and CRQ total score, 13.1 points (5.2 to 20.9).\nExercise training improved short and long-term exercise capacity and HRQoL in people with dust-related pleural and interstitial respiratory diseases.\nANZCTR12608000147381. Date trial registered: 27.03.2008."", 'title': 'Exercise training for asbestos-related and other dust-related respiratory diseases: a randomised controlled trial.', 'date': '2014-11-20'}, '18245143': {'article_id': '18245143', 'content': 'Interstitial lung disease (ILD) is characterised by exertional dyspnoea, exercise limitation and reduced quality of life. The role of exercise training in this diverse patient group is unclear. The aims of this study were to establish the safety of exercise training in ILD; its effects on exercise capacity, dyspnoea and quality of life; and whether patients with idiopathic pulmonary fibrosis (IPF) had similar responses to those with other types of ILD.\n57 subjects with ILD (34 IPF) were randomised to receive 8 weeks of supervised exercise training or weekly telephone support. The 6 min walk distance (6MWD), incremental exercise test, modified Medical Research Council (MRC) dyspnoea score and Chronic Respiratory Disease Questionnaire (CRDQ) were performed at baseline, following intervention and at 6 months.\n80% of subjects completed the exercise programme and no adverse events were recorded. The 6MWD increased following training (mean difference to control 35 m, 95% CI 6 to 64 m). A significant reduction in MRC score was observed (0.7 points, 95% CI 0.1 to 1.3) along with improvements in dyspnoea (p = 0.04) and fatigue (p<0.01) on the CRDQ. There was no change in peak oxygen uptake; however, exercise training reduced heart rate at maximum isoworkload (p = 0.01). There were no significant differences in response between those with and without IPF. After 6 months there were no differences between the training and control group for any outcome variable.\nExercise training improves exercise capacity and symptoms in patients with ILD, but these benefits are not sustained 6 months following intervention.', 'title': 'Short term improvement in exercise capacity and symptoms following exercise training in interstitial lung disease.', 'date': '2008-02-05'}}",1.0,Internal Medicine & Subspecialties
266,"Is functional status at 9 to 12 months higher, lower, or the same when comparing reablement and usual care? ",higher,,no,"['23009672', '26537789']",27726122,2016,"{'23009672': {'article_id': '23009672', 'content': ""A randomised controlled trial was conducted to test the effectiveness of the Home Independence Program (HIP), a restorative home-care programme for older adults, in reducing the need for ongoing services. Between June 2005 and August 2007, 750 older adults referred to a home-care service for assistance with their personal care participated in the study and received HIP or 'usual' home-care services. Service outcomes were compared at 3 and 12 months. Subgroups of 150 from each group were also compared on functional and quality of life measures. Data were analysed by 'intention-to-treat' and 'as-treated'. The intention-to-treat analysis showed at 3 and 12 months that the HIP group was significantly less likely to need ongoing personal care [Odds ratio (OR) = 0.18, 95% CI = 0.13-0.26, P < 0.001; OR = 0.22, 95% CI = 0.15-0.32, P < 0.001]. Both subgroups showed improvements on the individual outcome measures over time with the only significant differences being found at 12 months for Instrumental Activities of Daily Living (IADL) in the as-treated analysis. Contamination of the control group by an increased emphasis on independence across the home-care agency involved, together with other methodological problems encountered, is thought to account for the few differences between groups in individual outcomes. Despite no difference between the groups over time in their overall ADL scores, a significantly smaller proportion of the HIP group required assistance with bathing/showering, the most common reason for referral, at 3 and 12 months. The results support earlier findings that participating in a short-term restorative programme appears to reduce the need for ongoing home care. The implementation of such programmes more broadly throughout Australia could substantially offset the projected increase in demand for home care associated with the five-fold projected increase in numbers of the oldest old expected over the next 40 years."", 'title': 'A randomised controlled trial of the Home Independence Program, an Australian restorative home-care programme for older adults.', 'date': '2012-09-27'}, '26537789': {'article_id': '26537789', 'content': 'There has been an increasing interest in reablement in Norway recently and many municipalities have implemented this form of rehabilitation despite a lack of robust evidence of its effectiveness. The aim of this study was to investigate the effectiveness of reablement in home-dwelling older adults compared with usual care in relation to daily activities, physical functioning, and health-related quality of life.\nThis is a parallel-group randomised controlled trial conducted in a rural municipality in Norway. Sixty-one home-dwelling older adults with functional decline were randomised to an intervention group (n = 31) or a control group (n = 30). The intervention group received ten weeks of multicomponent home-based rehabilitation. The Canadian Occupational Performance Measure (COPM) was used to measure self-perceived activity performance and satisfaction with performance. In addition, physical capacity and health-related quality of life were measured. The participants were assessed at baseline and at 3- and 9-month follow-ups.\nThere were significant improvements in mean scores favouring reablement in COPM performance at 3 months with a score of 1.5 points (p = 0.02), at 9 months 1.4 points (p = 0.03) and overall treatment 1.5 points (p = 0.01), and for COPM satisfaction at 9 months 1.4 points (p = 0.03) and overall treatment 1.2 points (p = 0.04). No significant group differences were found concerning COPM satisfaction at 3 months, physical capacity or health-related quality of life.\nA 10-week reablement program resulted in better activity performance and satisfaction with performance on a long-term basis, but not the other outcomes measured.\nThe trial was registered in ClinicalTrials.gov November 20, 2012, identifier NCT02043262 .', 'title': 'Reablement in community-dwelling older adults: a randomised controlled trial.', 'date': '2015-11-06'}}",0.5,Family Medicine & Preventive Care
267,"Is freedom from atrial arrhythmias higher, lower, or the same when comparing catheter ablation to anti-arrhythmic drugs? ",higher,low,no,"['18775050', '24135832', '16214831']",27871122,2016,"{'18775050': {'article_id': '18775050', 'content': 'Atrial fibrillation (AF) and diabetes mellitus type 2 (DM2) often coexist; however, a small number of patients with DM2 undergoing catheter ablation (CA) of AF have been included in previous studies. The aim of this study was to evaluate safety and efficacy of ablation therapy in DM2 patients with drug refractory AF.\nFrom January 2005 to September 2006, 70 patients with a diagnosis of DM2 and paroxysmal (n = 29) or persistent (n = 41) AF were randomized to receive either pulmonary vein isolation or a new antiarrhythmic drug treatment (ADT) with a 1-year follow-up. The primary endpoint was the time to first AF recurrence. By Kaplan-Meier analysis, at the end of follow-up, 42.9% of patients in the ADT group and 80% of patients who received a single ablation procedure and were without medications were free of AF (P = 0.001). In the ablation group, a significant improvement in quality-of-life (QoL) scores as compared with ADT group was observed. Six patients in the ADT group (17.1%) developed significant adverse drug effects. Hospitalization rate during follow-up was higher in the ADT group (P = 0.01). The only complication attributable to ablation was one significant access-site hematoma.\nIn patients with DM2, CA of AF provides significant clinical benefits over the ADT and appears to be a reasonable approach regarding feasibility, effectiveness, and low procedural risk.', 'title': 'Catheter ablation of atrial fibrillation in patients with diabetes mellitus type 2: results from a randomized study comparing pulmonary vein isolation versus antiarrhythmic drug therapy.', 'date': '2008-09-09'}, '24135832': {'article_id': '24135832', 'content': 'A large multicentre study on the short and long term clinical and developmental outcome of infants randomised to different diets is being undertaken. This report represents an interim analysis of the early postnatal growth performance of an unselected population of 194 preterm infants (gestation, mean (SD) 31 . 0 (2 . 9) weeks; birthweight, mean (SD) 1364 (294) g), both ill and well, examined in two (of four) parallel trials. One trial compared banked breast milk with a new preterm formula (primary trial); the other compared these diets as supplements to maternal milk (supplement trial). A major dietary effect on the number of days taken to regain birthweight and subsequent gains in weight, length, and head circumference was observed in the primary trial. Infants fed banked breast milk and weighing less than 1200 g at birth took a calculated additional three weeks to reach 2000 g compared with those fed on the preterm formula. A significant influence of diet on body proportions was seen in the relation between body weight, head circumference, and length. Similar though smaller differences in growth patterns were seen in the supplement trial. By the time they reach 2000 g, infants of birthweights 1200 to 1849 g fed on banked breast milk and infants below 1200 g fed on either banked breast milk or maternal milk supplemented (as necessary) with banked breast milk, fulfilled stringent criteria for failure to thrive (weight less than 2 SD below the mean for age). Only infants fed the preterm formula as their sole diet had maintained their birth centile by discharge from hospital. The misleading nature of comparisons between extrauterine and intrauterine steady state weight gains is emphasised.', 'title': 'Multicentre trial on feeding low birthweight infants: effects of diet on early growth.', 'date': '1984-08-01'}, '16214831': {'article_id': '16214831', 'content': 'We conducted a multi-centre, prospective, controlled, randomized trial to investigate the adjunctive role of ablation therapy to antiarrhythmic drug therapy in preventing atrial fibrillation (AF) relapses in patients with paroxysmal or persistent AF in whom antiarrhythmic drug therapy had already failed.\nOne hundred and thirty seven patients were randomized to ablation and antiarrhythmic drug therapy (ablation group) or antiarrhythmic drug therapy alone (control group). In the ablation group, patients underwent cavo-tricuspid and left inferior pulmonary vein (PV)-mitral isthmus ablation plus circumferential PV ablation. The primary end-point of the study was the absence of any recurrence of atrial arrhythmia lasting >30 s in the 1-year follow-up period, after 1-month blanking period. Three (4.4%) major complications were related to ablation: one patient had a stroke during left atrium ablation, another suffered transient phrenic paralysis, and the third had a pericardial effusion which required pericardiocentesis. After 12 months of follow-up, 63/69 (91.3%) control group patients had at least one AF recurrence, whereas 30/68 (44.1%) (P<0.001) ablation group patients had atrial arrhythmia recurrence (four patients had atrial flutter, 26 patients AF).\nAblation therapy combined with antiarrhythmic drug therapy is superior to antiarrhythmic drug therapy alone in preventing atrial arrhythmia recurrences in patients with paroxysmal or persistent AF in whom antiarrhythmic drug therapy has already failed.', 'title': 'Catheter ablation treatment in patients with drug-refractory atrial fibrillation: a prospective, multi-centre, randomized, controlled study (Catheter Ablation For The Cure Of Atrial Fibrillation Study).', 'date': '2005-10-11'}}",1.0,Surgery
268,"Is points on disability scale mprovement higher, lower, or the same when comparing plasma exchange with sham exchange? ",higher,,no,['8813270'],26305459,2015,"{'8813270': {'article_id': '8813270', 'content': ""Eighteen patients with definite, untreated chronic inflammatory demyelinating polyradiculoneuropathy (CIDP) of chronic progressive (nine patients) or relapsing course (nine patients) were randomized prospectively to receive 10 plasma-exchange (PE) or sham plasma-exchange (SPE) treatments over 4 weeks in a double-blind trial. After a wash-out period of 5 weeks or when they returned to baseline scores, patients were crossed over to the alternate treatments. Neurological function was assessed serially using a quantitative neurological disability score (NDS), a functional clinical grade (CG) and grip strength (GS) measurements. Electrophysiological studies were done at the beginning and end of each treatment. A primary 'intention to treat' analysis showed significant improvement with PE in all clinical outcome measures: NDS by 38 points, P < 0.001; CG by 1.6 points, P < 0.001; GS by +13 kg, P < 0.003 and in selected electrophysiological measurements, sigma proximal CMAP, P < 0.01; sigma motor conduction velocities, P < 0.006; sigma distal motor latencies, P < 0.01. Fifteen patients completed the trial and of those, 12 patients (80%) improved substantially with PE; i.e. five out of seven patients with chronic progressive course and seven out of eight patients with relapsing CIDP improved. There were three drop-outs; one patient lost venous access; one patient suffered a stroke and one patient left the trial to receive open treatment elsewhere. The improvement in motor functions correlated with the electrophysiological data, i.e. with improved motor conduction velocities and reversal of conduction block. Eight of 12 PE responders (66%) relapsed within 7-14 days after stopping PE. All improved with subsequent open label PE; all but two patients required long-term immunosuppressive drug therapy for stabilization. The PE non-responders improved with prednisone. We conclude that PE is a very effective adjuvant therapy for CIDP of both chronic progressive and relapsing course; concurrent immunosuppressive drug treatment is required. Exchange treatments should be given two to three times per week until improvement is established; the treatment frequency should then be tapered over several months."", 'title': 'Plasma-exchange therapy in chronic inflammatory demyelinating polyneuropathy. A double-blind, sham-controlled, cross-over study.', 'date': '1996-08-01'}}",1.0,Internal Medicine & Subspecialties
269,"Is the rate of caesarean delivery higher, lower, or the same when comparing Bishop score to transvaginal ultrasound (TVUS)?",no difference,moderate,no,"['15660437', '21484904']",26068943,2015,"{'15660437': {'article_id': '15660437', 'content': 'To compare transvaginal ultrasound with the Bishop score in assessment of cervical ripening for choice of induction agent.\nEighty women were randomized to have preinduction cervical assessment for choice of induction agent based on either Bishop score or transvaginal ultrasound. The primary outcome measure was the percentage of women who were administered prostaglandin as a preinduction agent. The criteria for considering the cervix as unripe and thus for using prostaglandin were either a Bishop score < 6 or a cervical length > 30 mm with cervical wedging of < 30% of the total cervical length. Secondary outcome measures included interval to active phase, interval to delivery and rate of Cesarean section.\nWhile 85% of women received prostaglandin in the Bishop score group, only 50% of them did in the transvaginal ultrasound group (P = 0.001). The interval to active phase, interval to delivery and rate of Cesarean section were similar in both groups.\nWith the suggested cut-off values of a Bishop score < 6 or a cervical length > 30 mm and wedging < 30%, the use of transvaginal ultrasound instead of Bishop score for preinduction cervical assessment to choose induction agent significantly reduces the need for intracervical prostaglandin treatment without adversely affecting the success of induction.', 'title': 'Bishop score and transvaginal ultrasound for preinduction cervical assessment: a randomized clinical trial.', 'date': '2005-01-22'}, '21484904': {'article_id': '21484904', 'content': 'To compare sonographically measured cervical length with the Bishop score in determining the requirement for prostaglandin administration for preinduction cervical ripening in nulliparae at term.\nOne hundred and fifty-four women with singleton pregnancies at term who were scheduled for induction of labor were randomly assigned to receive prostaglandin for preinduction cervical ripening based on the Bishop score or sonographic cervical length. A cervix unfavorable for treatment with prostaglandin for preinduction cervical ripening was defined as having either a Bishop score of ≤ 4 or a cervical length of ≥ 28 mm. The primary outcome measures were induction success (defined as an ability to achieve the active phase of labor) and the percentage of patients treated with prostaglandin for preinduction cervical ripening.\nThe two groups were similar with respect to maternal demographics, gestational age, cervical length, and Bishop score. The rates of induction success and Cesarean delivery, the interval to active phase of labor, and the interval to delivery were also similar in the two groups. However, in the transvaginal ultrasound group (n = 77), prostaglandin was administered to only 36% of the nulliparae compared with 75% of those in the Bishop score group (n = 77) (P < 0.0001).\nIn comparison with the Bishop score, the use of sonographic cervical length for assessing the cervix prior to induction of labor can reduce the need for prostaglandin administration by approximately 50% without adversely affecting the outcome of induction in nulliparae at term if the cut-off values used are a Bishop score of ≤ 4 and a cervical length of ≥ 28 mm.', 'title': 'Comparison between sonographic cervical length and Bishop score in preinduction cervical assessment: a randomized trial.', 'date': '2011-04-13'}}",0.5,Obstetrics & Gynecology
270,"Is birth weight higher, lower, or the same when comparing caffeinated instant coffee and decaffeinated instant coffee? ",no difference,low,no,['17259189'],26058966,2015,"{'17259189': {'article_id': '17259189', 'content': ""To estimate the effect of reducing caffeine intake during pregnancy on birth weight and length of gestation.\nRandomised double blind controlled trial.\nDenmark.\n1207 pregnant women drinking at least three cups of coffee a day, recruited before 20 weeks' gestation.\nCaffeinated instant coffee (568 women) or decaffeinated instant coffee (629 women).\nBirth weight and length of gestation.\nData on birth weight were obtained for 1150 liveborn singletons and on length of gestation for 1153 liveborn singletons. No significant differences were found for mean birth weight or mean length of gestation between women in the decaffeinated coffee group (whose mean caffeine intake was 182 mg lower than that of the other group) and women in the caffeinated coffee group. After adjustment for length of gestation, parity, prepregnancy body mass index, and smoking at entry to the study the mean birth weight of babies born to women in the decaffeinated group was 16 g (95% confidence interval -40 to 73) higher than those born to women in the caffeinated group. The adjusted difference (decaffeinated group-caffeinated group) of length of gestation was -1.31 days (-2.87 to 0.25).\nA moderate reduction in caffeine intake in the second half of pregnancy has no effect on birth weight or length of gestation.\nClinical Trials NCT00131690 [ClinicalTrials.gov]."", 'title': 'Effect of reducing caffeine intake on birth weight and length of gestation: randomised controlled trial.', 'date': '2007-01-30'}}",1.0,Obstetrics & Gynecology
271,"Is all-cause mortality higher, lower, or the same when comparing the placement of transjugular intrahepatic portosystemic shunts (TIPS) placement with conventional treatment?",uncertain effect,low,no,"['12454841', '10841872']",38235907,2024,"{'12454841': {'article_id': '12454841', 'content': 'The transjugular intrahepatic portosystemic shunt (TIPS) has been shown to be more effective than repeated paracentesis plus albumin in the control of refractory ascites. However, its effect on survival and healthcare costs is still uncertain.\nSeventy patients with cirrhosis and refractory ascites were randomly assigned to TIPS (35 patients) or repeated paracentesis plus intravenous albumin (35 patients). The primary endpoint was survival without liver transplantation. Secondary endpoints were complications of cirrhosis and costs.\nTwenty patients treated with TIPS and 18 treated with paracentesis died during the study period, whereas 7 patients in each group underwent liver transplantation (mean follow-up 282 +/- 43 vs. 325 +/- 61 days, respectively). The probability of survival without liver transplantation was 41% at 1 year and 26% at 2 years in the TIPS group, as compared with 35% and 30% in the paracentesis group (P = 0.51). In a multivariate analysis, only baseline blood urea nitrogen levels and Child-Pugh score were independently associated with survival. Recurrence of ascites and development of hepatorenal syndrome were lower in the TIPS group compared with the paracentesis group, whereas the frequency of severe hepatic encephalopathy was greater in the TIPS group. The calculated costs were higher in the TIPS group than in the paracentesis group.\nIn patients with refractory ascites, TIPS lowers the rate of ascites recurrence and the risk of developing hepatorenal syndrome. However, TIPS does not improve survival and is associated with an increased frequency of severe encephalopathy and higher costs compared with repeated paracentesis plus albumin.', 'title': 'Transjugular intrahepatic portosystemic shunting versus paracentesis plus albumin for refractory ascites in cirrhosis.', 'date': '2002-11-28'}, '10841872': {'article_id': '10841872', 'content': 'In patients with cirrhosis and ascites, creation of a transjugular intrahepatic portosystemic shunt may reduce the ascites and improve renal function. However, the benefit of this procedure as compared with that of large-volume paracentesis is uncertain.\nWe randomly assigned 60 patients with cirrhosis and refractory or recurrent ascites (Child-Pugh class B in 42 patients and class C in 18 patients) to treatment with a transjugular shunt (29 patients) or large-volume paracentesis (31 patients). The mean (+/-SD) duration of follow-up was 45+/-16 months among those assigned to shunting and 44+/-18 months among those assigned to paracentesis. The primary outcome was survival without liver transplantation.\nAmong the patients in the shunt group, 15 died and 1 underwent liver transplantation during the study period, as compared with 23 patients and 2 patients, respectively, in the paracentesis group. The probability of survival without liver transplantation was 69 percent at one year and 58 percent at two years in the shunt group, as compared with 52 percent and 32 percent in the paracentesis group (P=0.11 for the overall comparison, by the log-rank test). In a multivariate analysis, treatment with transjugular shunting was independently associated with survival without the need for transplantation (P=0.02). At three months, 61 percent of the patients in the shunt group and 18 percent of those in the paracentesis group had no ascites (P=0.006). The frequency of hepatic encephalopathy was similar in the two groups. Of the patients assigned to paracentesis in whom this procedure was unsuccessful, 10 received a transjugular shunt a mean of 5.5+/-4 months after randomization; 4 had a response to this rescue treatment.\nIn comparison with large-volume paracentesis, the creation of a transjugular intrahepatic portosystemic shunt can improve the chance of survival without liver transplantation in patients with refractory or recurrent ascites.', 'title': 'A comparison of paracentesis and transjugular intrahepatic portosystemic shunting in patients with ascites.', 'date': '2000-06-08'}}",0.0,Internal Medicine & Subspecialties
272,"Is visual acuity at 6 to 12 months higher, lower, or the same when comparing macular hole and vitrectomy?",higher,moderate,no,"['14769600', '9006420', '8644802']",25965055,2015,"{'14769600': {'article_id': '14769600', 'content': 'To determine the benefits of idiopathic full-thickness macular hole (FTMH) surgery compared with observation and to evaluate the use of autologous serum as an intraoperative adjunct.\nA randomized clinical trial was performed to evaluate the anatomic and visual benefits of FTMH surgery for lesions of 9 months or less symptom duration and visual acuity of 20/60 or less. We compared surgery with natural history and determined whether use of intraoperative adjunctive autologous serum improves the surgical outcome. Eyes were randomized to (1). observation, (2). vitrectomy, or (3). vitrectomy plus serum and were followed for 24 months to assess anatomic status and visual function.\nIn total, 185 eyes of 174 patients were enrolled. In the observation group, spontaneous closure of the FTMH occurred in 7 (11.5%) of 61 patients, with little or no change in overall acuity levels in 24 months. In contrast, the surgical groups had an overall closure rate of 80.6% (100/124) at 24 months, with 45% of eyes achieving Snellen acuity of 20/40 or greater. Surgical eyes had better median near acuity than observation eyes by 6 lines (N5 vs N14). Use of autologous serum did not seem to affect anatomic or visual results. At 24 months, 72 (58.1%) of 124 surgical eyes had undergone cataract extraction.\nSurgery for FTMH is safe and effective and is associated with significant visual improvement compared with the natural history. Autologous serum application does not enhance the results of surgery.', 'title': 'Surgery for idiopathic full-thickness macular hole: two-year results of a randomized clinical trial comparing natural history, vitrectomy, and vitrectomy plus autologous serum: Morfields Macular Hole Study Group RAeport no. 1.', 'date': '2004-02-11'}, '9006420': {'article_id': '9006420', 'content': 'To prospectively assess the risks and benefits of vitrectomy surgery for eyes with stage 3 or 4 macular holes.\nA multicentered, controlled, randomized clinical trial.\nCommunity- and university-based ophthalmology clinics.\nOne hundred twenty patients (129 eyes) with stage 3 or 4 macular holes.\nStandardized macular hole surgery vs observation alone.\nFour measures of best-corrected visual function, standardized photographic evaluation of the extent of hole closure, evaluation of lens opacification, and determination of adverse events. Outcomes were determined at 6 months after randomization.\nCompared with observation alone, a significant benefit due to surgery was found in the rate of hole closure (4% vs 69%, P < .001). After adjusting for baseline visual acuity, hole duration, and maximum hole diameter, a significant benefit due to surgery was found in visual acuity for the Bailey-Lovie Word Reading (P = .02) and the Potential Acuity Meter (P < .01) tests; a marginally significant benefit due to surgery was found in visual acuity for the Early Treatment Diabetic Retinopathy Study chart (P = .05). Although the proportion of eyes achieving a change in visual acuity of 2 or more lines on the Early Treatment Diabetic Retinopathy Study chart was significantly greater for the surgery group vs the observed group (11 [19%] of 59 eyes vs 3 [5%] of 58 eyes, adjusted P = .05), 20 (34%) of 59 eyes randomized to surgery had a loss in visual acuity of 1 or more lines. Compared with the observation group, eyes randomized to surgery had higher nuclear sclerosis scores (2.4 vs 1.3, P < .001). Fourteen adverse events were noted in the surgery group; none were noted in the observed group.\nSome visual benefit of vitrectomy surgery for macular holes exists, despite a notable incidence of adverse events. The large variability in visual acuity outcome in the surgical group may be because of complications or progressive cataract. A study of the long-term outcome after macular hole surgery is needed.', 'title': 'Vitrectomy for the treatment of full-thickness stage 3 or 4 macular holes. Results of a multicentered randomized clinical trial. The Vitrectomy for Treatment of Macular Hole Study Group.', 'date': '1997-01-01'}, '8644802': {'article_id': '8644802', 'content': 'To determine the risks and benefits of vitrectomy surgery in eyes with stage 2 macular holes.\nA multicentered, controlled, randomized clinical trial was performed with participation of 16 community and university-based ophthalmology clinics. Thirty-six eyes with stage 2 macular holes and 12 months of follow-up were studied. Pars plana vitrectomy with separation of the posterior hyaloid membrane and intraocular injection of perfluoropropane (C3F8) was followed by postoperative face-down positioning for two weeks. This protocol was compared with observation alone. Outcome variables included anatomic closure of the macular hole, macular hole size, and four standardized measures of vision.\nAt 12 months, 15 (71%) of 21 eyes randomly assigned to observation progressed to stages 3 or 4, compared with three (20%) of 15 eyes randomly assigned to surgery (P < .006). Compared with eyes randomly assigned to observation, eyes randomly assigned to surgery had significantly smaller hole diameters (P < .01) and significantly better visual acuity outcomes, as measured by the Word Reading (P = .02) and Potential Acuity Meter (P = .002) charts. No significant differences were found for the Early Treatment Diabetic Retinopathy Study chart and Contrast Sensitivity test.\nCompared with observation alone, surgical intervention in stage 2 macular holes resulted in a significantly lower incidence of hole enlargement and appeared to be associated with better outcome in some measures of visual acuity.', 'title': 'Prospective randomized trial of vitrectomy or observation for stage 2 macular holes. Vitrectomy for Macular Hole Study Group.', 'date': '1996-06-01'}}",0.0,Surgery
273,"Is the number of antibiotics used by patients higher, lower, or the same when comparing the use of written information to usual care?",lower,moderate,no,['19640941'],27886368,2016,"{'19640941': {'article_id': '19640941', 'content': 'To establish whether an interactive booklet on respiratory tract infections in children reduces reconsultation for the same illness episode, reduces antibiotic use, and affects future consulting intentions, while maintaining parental satisfaction with care.\nPragmatic cluster randomised controlled trial.\n61 general practices in Wales and England.\n558 children (6 months to 14 years) presenting to primary care with an acute respiratory tract infection (7 days or less). Children with suspected pneumonia, asthma or a serious concomitant illness, or needing immediate hospital admission were excluded. Three withdrew and 27 were lost to follow-up, leaving 528 (94.6%) with main outcome data.\nClinicians in the intervention group were trained in the use of an interactive booklet on respiratory tract infections and asked to use the booklet during consultations with recruited patients (and provide it as a take home resource). Clinicians in the control group conducted their consultations as usual.\nThe proportion of children who attended a face-to-face consultation about the same illness during the two week follow-up period. Secondary outcomes included antibiotic prescribing, antibiotic consumption, future consulting intentions, and parental satisfaction, reassurance, and enablement.\nReconsultation occurred in 12.9% of children in the intervention group and 16.2% in the control group (absolute risk reduction 3.3%, 95% confidence interval -2.7% to 9.3%, P=0.29). Using multilevel modelling (at the practice and individual level) to account for clustering, no significant difference in reconsulting was noted (odds ratio 0.75; 0.41 to 1.38). Antibiotics were prescribed at the index consultation to 19.5% of children in the intervention group and 40.8% of children in the control group (absolute risk reduction 21.3%, 95% confidence interval 13.7 to 28.9), P<0.001). A significant difference was still present after adjusting for clustering (odds ratio 0.29; 0.14 to 0.60). There was also a significant difference in the proportion of parents who said they would consult in the future if their child developed a similar illness (odds ratio 0.34; 0.20 to 0.57). Satisfaction, reassurance, and parental enablement scores were not significantly different between the two groups.\nUse of a booklet on respiratory tract infections in children within primary care consultations led to important reductions in antibiotic prescribing and reduced intention to consult without reducing satisfaction with care.\nCurrent Controlled Trials ISRCTN46104365.', 'title': 'Effect of using an interactive booklet about childhood respiratory tract infections in primary care consultations on reconsulting and antibiotic prescribing: a cluster randomised controlled trial.', 'date': '2009-07-31'}}",1.0,"Public Health, Epidemiology & Health Systems"
274,"Is primary patency rate higher, lower, or the same when comparing endovascular group and the surgery group?",no difference,moderate,no,['16102611'],31868929,2019,"{'16102611': {'article_id': '16102611', 'content': 'The aim of this prospective randomized study was to evaluate the relative risks and advantages of using the Hemobahn graft for popliteal artery aneurysm (PAA) treatment compared with open repair (OR). The primary end point was patency rate; secondary end points were hospital stay and length of surgical procedure.\nThe study was a prospective, randomized clinical trial carried out at a single center from January 1999 to December 2003. Inclusion criteria were an aneurysmal lesion in the popliteal artery with a diameter > or = 2 cm at the angio-computed tomography (CT) scan, and proximal and distal neck of the aneurysm with a length of > 1 cm to offer a secure site of fixation of the stent graft. Exclusion criteria were age < 50 years old, poor distal runoff, contraindication to antiplatelet, anticoagulant, or thrombolytic therapy, and symptoms of nerve and vein compression. The enrolled patients were thereafter prospectively randomized in a 1-to-1 ratio between OR (group A) or endovascular therapy (ET) (group B). The follow-up protocol consisted of duplex ultrasound scan and ankle-brachial index (ABI) measured during a force leg flexion at 1, 3, and 6 months. Group B patients underwent an angio-CT scan and plain radiography of the knee with leg flexion (> 120 degrees) at 6 and 12 months, and then yearly.\nBetween January 1999 and December 2003, 30 PAAs were performed: 15 OR (group A) and 15 ET (group B). Bypass and exclusion of the PAA was the preferred method of OR; no perioperative graft failure was observed. Twenty stent grafts were placed in 15 PAAs. Endograft thrombosis occurred in one patient (6.7%) in the postoperative period. The mean follow-up period was 46.1 months (range, 12 to 72 months) for group A and 45.9 months (range, 12 to 65 months) for group B. Kaplan-Meier analysis showed a primary patency rate of 100% at 12 months for OR and 86.7% at 12 months with a secondary patency rate of 100% at 12 and 36 months for ET. No statistical differences were observed at the log-rank test. The mean operation time (OR, 155.3 minutes; ET, 75.4 minutes) and hospital stay (OR, 7.7 days; ET, 4.3 days) were statistically longer for OR compared with ET (P < .01).\nWe can conclude, with the power limitation of the study, that PAA treatment can be safely performed by using either OR or ET. ET has several advantages, such as quicker recovery and shorter hospital stay.', 'title': 'Open repair versus endovascular treatment for asymptomatic popliteal artery aneurysm: results of a prospective randomized study.', 'date': '2005-08-17'}}",1.0,Surgery
275,"Is visual acuity higher, lower, or the same when comparing part-time occlusion (with any necessary glasses) to glasses alone?",higher,,no,"['16751033', '15838439']",25051925,2014,"{'16751033': {'article_id': '16751033', 'content': 'To compare 2 hours of daily patching (combined with 1 hour of concurrent near visual activities) with a control group of spectacle wear alone (if needed) for treatment of moderate to severe amblyopia in children 3 to 7 years old.\nProspective randomized multicenter clinical trial (46 sites).\nOne hundred eighty children 3 to 7 years old with best-corrected amblyopic-eye visual acuity (VA) of 20/40 to 20/400 associated with strabismus, anisometropia, or both who had worn optimal refractive correction (if needed) for at least 16 weeks or for 2 consecutive visits without improvement.\nRandomization either to 2 hours of daily patching with 1 hour of near visual activities or to spectacles alone (if needed). Patients were continued on the randomized treatment (or no treatment) until no further improvement was noted.\nBest-corrected VA in the amblyopic eye after 5 weeks.\nImprovement in VA of the amblyopic eye from baseline to 5 weeks averaged 1.1 lines in the patching group and 0.5 lines in the control group (P = 0.006), and improvement from baseline to best measured VA at any visit averaged 2.2 lines in the patching group and 1.3 lines in the control group (P<0.001).\nAfter a period of treatment with spectacles, 2 hours of daily patching combined with 1 hour of near visual activities modestly improves moderate to severe amblyopia in children 3 to 7 years old.', 'title': 'A randomized trial to evaluate 2 hours of daily patching for strabismic and anisometropic amblyopia in children.', 'date': '2006-06-06'}, '15838439': {'article_id': '15838439', 'content': 'To plan a future randomized clinical trial, we conducted a pilot study to determine whether children randomized to near or non-near activities would perform prescribed activities. A secondary aim was to obtain a preliminary estimate of the effect of near versus non-near activities on amblyopic eye visual acuity, when combined with 2 hours of daily patching.\nSixty-four children, 3 to less than 7 years of age, with anisometropic, strabismic, or combined amblyopia (20/40 to 20/400) were randomly assigned to receive either 2 hours of daily patching with near activities or 2 hours of daily patching without near activities. Parents completed daily calendars for 4 weeks recording the activities performed while patched and received a weekly telephone call in which they were asked to describe the activities performed during the previous 2 hours of patching. Visual acuity was assessed at 4 weeks.\nThe children assigned to near visual activities performed more near activities than those assigned to non-near activities (by calendars, mean 1.6 +/- 0.5 hours versus 0.2 +/- 0.2 hours daily, P < 0.001; by telephone interviews, 1.6 +/- 0.4 hours versus 0.4 +/- 0.5 hours daily, P < 0.001). After 4 weeks of treatment, there was a suggestion of greater improvement in amblyopic eye visual acuity in those assigned to near visual activities (mean 2.6 lines versus 1.6 lines, P = 0.07). The treatment group difference in visual acuity was present for patients with severe amblyopia but not moderate amblyopia.\nChildren patched and instructed to perform near activities for amblyopia spent more time performing those near activities than children who were instructed to perform non-near activities. Our results suggest that performing near activities while patched may be beneficial in treating amblyopia. Based on our data, a formal randomized amblyopia treatment trial of patching with and without near activities is both feasible and desirable.', 'title': 'A randomized pilot study of near activities versus non-near activities during patching therapy for amblyopia.', 'date': '2005-04-20'}}",0.5,Other
276,"Is overall survival higher, lower, or the same when comparing Ppost-mastectomy radiotherapy (PMRT) to control?",higher,moderate,no,['17306393'],37327075,2023,"{'17306393': {'article_id': '17306393', 'content': 'Numerous consensus reports recommend that postmastectomy radiotherapy (RT) in addition to systemic therapy is indicated in high-risk patients with 4+ positive nodes, but not in patients with 1-3 positive nodes. A subgroup analysis of the DBCG 82 b&c trials was performed to evaluate the loco-regional recurrence rate and survival in relation to number of positive nodes.\nIn the DBCG 82 b&c trials 3083 pre- and postmenopausal high-risk women were randomized to postoperative RT in addition to adjuvant systemic therapy. Since many patients had relatively few lymph nodes removed (median 7), the present analysis was limited to 1152 node positive patients with 8 or more nodes removed.\nThe overall 15-year survival rate in the subgroup was 39% and 29% (p=0.015) after RT and no RT, respectively. RT reduced the 15-year loco-regional failure rate from 51% to 10% (p<0.001) in 4+ positive node patients and from 27% to 4% (p<0.001) in patients with 1-3 positive nodes. Similarly, the 15-year survival benefit after RT was significantly improved in both patients with 1-3 positive nodes (57% vs 48%, p=0.03) and in patients with 4+ positive nodes (21% vs 12%, p=0.03).\nThe survival benefit after postmastectomy RT was substantial and similar in patients with 1-3 and 4+ positive lymph nodes. Furthermore, it was not strictly associated with the risk of loco-regional recurrence, which was most pronounced in patients with 4+ positive nodes. The indication for RT seems therefore to be at least equally beneficial in patients with 1-3 positive nodes, and future consensus should be modified accordingly.', 'title': 'Is the benefit of postmastectomy irradiation limited to patients with four or more positive nodes, as recommended in international consensus reports? A subgroup analysis of the DBCG 82 b&c randomized trials.', 'date': '2007-02-20'}}",1.0,Oncology & Hematology
277,"Is the enrollment of children into health insurance higher, lower, or the same when comparing people offered health insurance information to control?",higher,moderate,no,['16322168'],25425010,2014,"{'16322168': {'article_id': '16322168', 'content': ""Lack of health insurance adversely affects children's health. Eight million US children are uninsured, with Latinos being the racial/ethnic group at greatest risk for being uninsured. A randomized, controlled trial comparing the effectiveness of various public insurance strategies for insuring uninsured children has never been conducted.\nTo evaluate whether case managers are more effective than traditional methods in insuring uninsured Latino children.\nRandomized, controlled trial conducted from May 2002 to August 2004.\nA total of 275 uninsured Latino children and their parents were recruited from urban community sites in Boston.\nUninsured children were assigned randomly to an intervention group with trained case managers or a control group that received traditional Medicaid and State Children's Health Insurance Program (SCHIP) outreach and enrollment. Case managers provided information on program eligibility, helped families complete insurance applications, acted as a family liaison with Medicaid/SCHIP, and assisted in maintaining coverage.\nObtaining health insurance, coverage continuity, the time to obtain coverage, and parental satisfaction with the process of obtaining insurance for children were assessed. Subjects were contacted monthly for 1 year to monitor outcomes by a researcher blinded with respect to group assignment.\nOne hundred thirty-nine subjects were assigned randomly to the intervention group and 136 to the control group. Intervention group children were significantly more likely to obtain health insurance (96% vs 57%) and had approximately 8 times the adjusted odds (odds ratio: 7.78; 95% confidence interval: 5.20-11.64) of obtaining insurance. Seventy-eight percent of intervention group children were insured continuously, compared with 30% of control group children. Intervention group children obtained insurance significantly faster (mean: 87.5 vs 134.8 days), and their parents were significantly more satisfied with the process of obtaining insurance.\nCommunity-based case managers are more effective than traditional Medicaid/SCHIP outreach and enrollment in insuring uninsured Latino children. Case management may be a useful mechanism to reduce the number of uninsured children, especially among high-risk populations."", 'title': 'A randomized, controlled trial of the effectiveness of community-based case management in insuring uninsured Latino children.', 'date': '2005-12-03'}}",1.0,"Public Health, Epidemiology & Health Systems"
278,"Is clinical pregancy rate higher, lower, or the same when comparing the group who underwent ovarian cyst aspiration to the conservatively managed group?",no difference,low,no,"['2120087', '16253965']",25502626,2014,"{'2120087': {'article_id': '2120087', 'content': 'This study was designed to ascertain whether any benefit would be derived from aspirating ovarian cysts identified before ovarian stimulation in patients undergoing in vitro fertilization. Thirty-seven patients who had ovarian cysts were categorized into two groups: group A (n = 14) with baseline ovarian cysts and group B (n = 23) with ovarian cysts that developed during pituitary suppression with the gonadotropin-releasing hormone analog. Each group was prospectively randomized into two subgroups depending on whether the ovarian cysts were aspirated or not. In group A, there was a significantly greater number of follicles and oocytes in the ovaries in which cysts were aspirated. However, there was no significant difference in the total number of follicles, oocytes retrieved and fertilized, or in the final outcome. In group B, there was no significant difference in folliculogenesis between the aspirated and nonaspirated subgroups. These observations suggest that the presence of a baseline ovarian cyst may reduce folliculogenesis but do not support routine cyst aspiration if the patient has two functional ovaries.', 'title': 'Ovarian cyst aspiration and the outcome of in vitro fertilization.', 'date': '1990-10-01'}, '16253965': {'article_id': '16253965', 'content': 'The formation of functional ovarian cysts has been recognized as one of the side effects of GnRH agonist administration. The formation of cysts during IVF treatment may be of no clinical significance or may negatively influence its outcome. The objective of this study was to determine the incidence of ovarian cyst formation following GnRH agonist administration and to examine their effect on IVF outcome.\nA prospective study of 1317 IVF patients who developed one or more functional ovarian cysts of >or=15 mm following GnRH agonist treatment was performed. Transvaginal ultrasonographic-guided cyst aspiration was carried out in 76 randomly allocated patients out of 122 patients who were found to have functional ovarian cysts before starting ovarian stimulation with gonadotropins.\nThe incidence of follicular cyst formation was 9.3%. Cyst cycles in comparison with non-cyst cycles had significantly elevated day 3 basal FSH (mean+/-SD of 8.3+/-3.2 versus 5.3+/-2.6 mIU/ml, P<0.05) and required more ampoules of gonadotropins (46.3+/-16.5 versus 35+/-14.6, P<0.01). Furthermore, they showed a statistically significant decrease in the quality and number of oocytes retrieved, fertilization rate, number and quality of embryos, implantation and pregnancy rates, with a significant increase in cancellation and abortion rates. Patients with bilateral cysts had a significantly lower number of oocytes and embryos retrieved, with a lower proportion of metaphase II oocytes. They also had a higher proportion of poor quality embryos. Cyst aspiration was not associated with a significant difference in the above parameters.\nThe incidence of cyst formation during GnRH agonist treatment is lower than previously reported. In such cases, the quality of oocytes and embryos were significantly compromised, with a significant increase in the cycle cancellation rate and a decrease in the implantation and pregnancy rates. Neither conservative management nor cyst aspiration improved the IVF outcome.', 'title': 'Ovarian cyst formation following GnRH agonist administration in IVF cycles: incidence and impact.', 'date': '2005-10-29'}}",0.5,Obstetrics & Gynecology
279,"Is the incidence rate of Frey's syndrome higher, lower, or the same when comparing sternocleidomastoid muscle flap procedure to control?",no difference,low,no,"['12464202', '15871587']",31578708,2019,"{'12464202': {'article_id': '12464202', 'content': ""Parotid surgery can cause postoperative facial nerve dysfunction, cosmetic impairment, and Frey's syndrome. Thirty-six patients listed for superficial parotidectomy were entered into a prospective randomised trial to find out if the use of a sternocleidomastoid flap could reduce the incidence of these complications. Partial facial nerve paresis was seen at 3 months in five patients in whom flaps were raised compared with six among those who did not have flaps (P=0.025). There was no difference between the two groups at 1 year. The flap was not associated with an improvement in either subjective (P=0.13) or objective (P=0.12) appearance measured on visual analogue scales. Eight patients in whom flaps were raised described symptoms suggestive of Frey's syndrome, compared with nine patients in whom a flap was not raised (P=0.31). Overall 19 of those who had a flap and 11 of those who did not had a positive starch-iodine test (P=0.21)."", 'title': 'Prospective randomised trial of the benefits of a sternocleidomastoid flap after superficial parotidectomy.', 'date': '2002-12-05'}, '15871587': {'article_id': '15871587', 'content': ""We studied the incidence of Frey's syndrome and facial contour deformity in two groups of patients who had undergone superficial parotidectomy. One group was made up of 12 patients who were randomized to undergo reconstruction of the surgical defect with a sternocleidomastoid muscle flap; the other 12 patients did not receive a flap. All 24 patients were evaluated via a short questionnaire, the starch-iodine test, and a visual examination. On the questionnaire, none of the 24 patients said they experienced abnormal facial sweating, flushing, or warmth while eating, although 6 of the 12 patients in the nonflap group had a mildly positive starch-iodine test. No patient in the flap group had a positive test. The difference between the two groups was statistically significant (p < 0.05). No statistically significant difference was seen between the two groups with respect to cosmetic results."", 'title': ""Sternocleidomastoid muscle flap reconstruction during parotidectomy to prevent Frey's syndrome and facial contour deformity."", 'date': '2005-05-06'}}",0.5,Surgery
280,"Is measures of pain at any of the time points higher, lower, or the same when comparing  systemic antibiotics and placebo?",no difference,very low,no,"['8734709', '11491635']",30259968,2018,"{'8734709': {'article_id': '8734709', 'content': 'Antibiotics are often prescribed indiscriminately to treat endodontic emergencies.\nThis study examined (1) the effect of penicillin supplementation on reduction of symptoms and (2) the course of recovery of localized acute apical abscess after emergency treatment.\nPatients with pulp necrosis and periapical pain and/or localized swelling were considered. Those eligible did not have any signs of spreading infections. Patients received appropriate local treatment, and a double-blind protocol was used to randomly assign them to one of three groups: penicillin VK group, placebo group, or neither medication group. All received ibuprofen 600 mg four times daily for 24 hours. Patients entered their pre- and postoperative pain and swelling experience on a visual analog scale for up to 72 hours.\nResolution was fairly rapid in most patients. Statistical analysis of the scores of 32 respondents revealed no significant differences (at p < 0.05) between the three groups in course of recovery or symptoms at any time period.\nPatients with localized periapical pain or swelling generally recovered quickly with local treatment. The data did not show a demonstrable benefit from penicillin supplementation.', 'title': 'Penicillin as a supplement in resolving the localized acute apical abscess.', 'date': '1996-05-01'}, '11491635': {'article_id': '11491635', 'content': 'The purpose of this prospective, randomized, double-blind, placebo-controlled study was to determine the effect of penicillin on postoperative pain and swelling in symptomatic necrotic teeth. Forty-one emergency patients participated and each had a clinical diagnosis of a symptomatic necrotic tooth with associated periapical radiolucency. After endodontic treatment patients randomly received a 7-day oral dose (twenty-eight 500 mg capsules to be taken every 6 h) of either penicillin or a placebo control in a double-blind manner. Patients also received ibuprofen; acetaminophen with codeine (30 mg); and a 7-day diary to record pain, percussion pain, swelling, and number and type of pain medication taken. The majority of patients with symptomatic necrotic teeth had significant postoperative pain and require analgesic medication to manage this pain. The administration of penicillin postoperatively did not significantly (p > 0.05) reduce pain, percussion pain, swelling, or the number of analgesic medications taken for symptomatic necrotic teeth with periapical radiolucencies.', 'title': 'Effect of penicillin on postoperative endodontic pain and swelling in symptomatic necrotic teeth.', 'date': '2001-08-09'}}",1.0,"Public Health, Epidemiology & Health Systems"
281,"Is cardiac iron assessment, as measured by heart T2* at 12 months higher, lower, or the same when comparing amlodipine to control?",no difference,low,no,"['23830536', '27412888']",29998494,2018,"{'23830536': {'article_id': '23830536', 'content': 'Iron chelation therapy in patients with thalassemia major may not prevent iron overload in all organs, especially those in which iron enters cells through specific calcium channels. We designed a controlled pilot study to assess the potential of the calcium channel blocker amlodipine in strengthening the efficacy of iron chelation.\nFifteen patients with thalassemia major undergoing chelation therapy were randomized to receive amlodipine added to standard treatment in a 1:2 allocation for 12 months. T2* values for assessment of iron overload in the liver and heart using magnetic resonance imaging were obtained at baseline and at 6 and 12 months.\nIn the amlodipine-treated group, heart T2* increased significantly in comparison to baseline at 6 and 12 months (21.7 ± 7.2 ms to 28.2 ± 7.9 ms and 28.3 ± 8.0 ms, with P = .007 and .03, respectively), while no differences were observed in the control group (25.1 ± 8.8 ms to 24.7 ± 7.8 ms and 26.2 ± 11.4 ms; P = .99 and 0.95, respectively); significant differences between groups were observed at 6 months (28.2 ± 7.9 ms vs 24.7 ± 7.8 ms in the control group, P = .03). A significant reduction in ferritin levels also was observed in the treated group at 12 months.\nThe use of amlodipine in conjunction with standard chelation therapy may suggest a new strategy in preventing and treating iron overload in patients with thalassemia major, especially in organs where iron absorption depends on active uptake by calcium channels like the heart.', 'title': 'Amlodipine reduces cardiac iron overload in patients with thalassemia major: a pilot trial.', 'date': '2013-07-09'}, '27412888': {'article_id': '27412888', 'content': 'Cardiovascular disease resulting from iron accumulation is still a major cause of death in patients with thalassemia major (TM). Voltage-gated calcium-channel blockade prevents iron entry into cardiomyocytes and may provide an adjuvant treatment to chelation, reducing myocardial iron uptake. We evaluated whether addition of amlodipine to chelation strategies would reduce myocardial iron overload in TM patients compared with placebo. In a multicenter, double-blind, randomized, placebo-controlled trial, 62 patients were allocated to receive oral amlodipine 5 mg/day or placebo in addition to their current chelation regimen. The main outcome was change in myocardial iron concentration (MIC) determined by magnetic resonance imaging at 12 months, with patients stratified into reduction or prevention groups according to their initial T2* below or above the normal human threshold of 35 ms (MIC, 0.59 mg/g dry weight). At 12 months, patients in the reduction group receiving amlodipine (n = 15) had a significant decrease in MIC compared with patients receiving placebo (n = 15) with a median of -0.26 mg/g (95% confidence interval, -1.02 to -0.01) vs 0.01 mg/g (95% confidence interval, -0.13 to 0.23), P = .02. No significant changes were observed in the prevention group (treatment-effect interaction with P = .005). The same findings were observed in the subgroup of patients with T2* <20 ms. Amlodipine treatment did not cause any serious adverse events. Thus, in TM patients with cardiac siderosis, amlodipine combined with chelation therapy reduced cardiac iron more effectively than chelation therapy alone. Because this conclusion is based on subgroup analyses, it needs to be confirmed in ad hoc clinical trials. This trial was registered at www.clinicaltrials.gov identifier as #NCT01395199.', 'title': 'A randomized trial of amlodipine in addition to standard chelation therapy in patients with thalassemia major.', 'date': '2016-07-15'}}",0.0,Internal Medicine & Subspecialties
282,"Is the time to establish enteral feeds higher, lower, or the same when comparing re-feeding to discarding gastric residuals?",no difference,low,no,['25552280'],37387544,2023,"{'25552280': {'article_id': '25552280', 'content': 'To determine whether re-feeding of gastric residual volumes reduces the time needed to achieve full enteral feeding in preterm infants.\nParallel-group randomised controlled trial with a 1:1 allocation ratio.\nRegional referral neonatal intensive care unit.\n72 infants of gestational age 23(0/7) to 28(6/7) weeks receiving minimal enteral nutrition (<24 mL/kg/day) during the first week after birth.\nInfants were randomised to either be re-fed with gastric residual volumes (Re-feeding group) or receive fresh formula/human milk (Fresh-feeding group) whenever large gastric residual volumes were noted.\nThe primary efficacy end point was time to achieve full enteral feeding (≥120 mL/kg/day) after randomisation.\nThe mean time to full enteral feeding was 10.0 days in the Re-feeding group and 11.3 days in the Fresh-feeding group (mean difference favouring re-feeding: -1.3 days; 95% CI -2.9 to 0.3; p=0.11). The composite safety end point of spontaneous intestinal perforation, surgical necrotising enterocolitis, or death occurred in 6 of 36 infants (17%) in the Re-feeding group versus 10 of 36 infants (28%) in the Fresh-feeding group (p=0.26).\nRe-feeding gastric residual volumes in extremely preterm infants does not reduce time to achieve full enteral feeding. This trial suggests that re-feeding might be as safe as fresh feeding, but further research is needed, due to lack of sufficient statistical power in this study for safety analysis.\nNCT01420263NCT01420263.', 'title': 'A randomised trial of re-feeding gastric residuals in preterm infants.', 'date': '2015-01-02'}}",1.0,Emergency Medicine & Critical Care
