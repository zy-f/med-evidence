{"original_review": "28660687", "question_data": [{"question_id": 0, "question": "Is the long-term rate of overall lymphocyst formation higher, lower, or the same when comparing retroperitoneal drainage to no drainage?", "answer": "no difference", "evidence_quality": "high", "fulltext_required": "no", "comment": "", "relevant_sources": ["17466514"]}, {"question_id": 1, "question": "Is the short-term rate of overall lymphocyst formation higher, lower, or the same when comparing retroperitoneal drainage to no drainage?", "answer": "no difference", "evidence_quality": "moderate", "fulltext_required": "no", "comment": "", "relevant_sources": ["9190979", "12214830"]}], "sources": [{"article_id": "17466514", "content": "Drainage, following radical hysterectomy and pelvic lymph node dissection to prevent postoperative lymphocyst formation and surgical morbidity, is controversial. To study the clinical significance of drainage, 253 patients were registered and 234 patients were randomised into two arms. In one arm (n=117) postoperative drainage was performed, in the other arm (n=117) no drains were inserted. In both arms closure of the peritoneum of the operating field was omitted. The main exclusion criteria were blood loss of more than 3000 ml during surgery or persistent oozing at the end of the operation. Clinical and ultrasound or CT-scan evaluation was done at one and 12 months postoperatively. The median follow-up amounted to 13.3 months. No difference in the incidence of postoperative lymphocyst formation or postoperative complications was found between the two study arms. The late (12 months) incidence of symptomatic lymphocysts was 3.4% (drains: 5.9%; no drains: 0.9%). The difference showed a p-value of 0.06 in Fisher's Exact test. The operating time was related to the occurrence of postoperative lymphocyst formation. It was concluded that drains can be safely omitted following radical hysterectomy and pelvic node dissection without pelvic reperitonisation in patients without excessive bleeding during or oozing at the end of surgery.", "title": "Randomised trial of drains versus no drains following radical hysterectomy and pelvic lymph node dissection: a European Organisation for Research and Treatment of Cancer-Gynaecological Cancer Group (EORTC-GCG) study in 234 patients.", "date": "2007-05-01"}, {"article_id": "12214830", "content": "To evaluate the postoperative morbidity and lymphocyst formation in invasive cervical cancer patients undergoing radical hysterectomy and pelvic lymphadenectomy (RHPL) with no drainage and no peritonization compared with retroperitoneal drainage and peritonization.\nBetween July 1999 and May 2000, 100 patients with stage IA-IIA cervical cancer undergoing RHPL in Chiang Mai University Hospital were prospectively randomized to receive either no peritonization and no drainage (Group A = 48 cases) or retroperitoneal drainage and peritonization (Group B = 52 cases). Perioperative data and morbidity were recorded. Transabdominal and transvaginal sonography were performed at 4, 8 and 12 weeks postoperatively to detect lymphocyst formation.\nBoth groups were similar regarding age, size and gross appearance of tumor, tumor histology and stage. There was no difference between groups in respect of operative time, need for blood transfusion, intraoperative complications, hospital stay, number of nodes removed, nodal metastases, and need for adjuvant radiation and chemotherapy. Asymptomatic lymphocysts were sonographically detected at 4, 8 and 12 weeks postoperatively in 3 (6.8%), 2 (4.6%), and 3 (7.7%) of 44, 43, and 39 patients, respectively in Group A, whereas none was found in Group B (P = 0.2). No significant difference was found in term of postoperative morbidity in the two groups.\nRoutine retroperitoneal drainage and peritonization after RHPL for invasive cervical cancer can be safely omitted.", "title": "A prospective randomized study comparing retroperitoneal drainage with no drainage and no peritonization following radical hysterectomy and pelvic lymphadenectomy for invasive cervical cancer.", "date": "2002-09-07"}, {"article_id": "7501348", "content": "To compare the incidence of lymphocyst formation and postoperative morbidity in patients drained or not drained following radical hysterectomy and pelvic lymph node dissection for cervical or endometrial malignancy.\nA prospective study was undertaken of consecutive patients undergoing radical hysterectomy and pelvic lymphadenectomy at the Regional Department of Gynaecological Oncology, Gateshead, United Kingdom, between February 1992 and September 1994. A Piver type II procedure was performed with nonclosure of the vaginal cuff and pelvic peritoneum. Patients were randomized at the end of surgery to have either two suction drains inserted along the pelvic sidewalls or to have no drains inserted. The detection of lymphocysts was made by clinical examination and abdominal ultrasound scan performed 8 weeks postoperatively.\nEight patients were excluded from the study when drains were deemed necessary to assess postoperative blood loss. Fifty-one were randomized to drains, and 49 to no drains. The detection of lymphocysts by ultrasound and clinical examination in the drained group (15.6 and 5.9%, respectively) was not significantly different from the group not drained (17.4 and 6.1%, respectively). There was no difference in postoperative morbidity in the two groups.\nThere appears to be no advantage to the routine use of pelvic suction drainage following radical hysterectomy and pelvic lymphadenectomy.", "title": "Drainage following radical hysterectomy and pelvic lymphadenectomy: dogma or need?", "date": "1995-12-01"}, {"article_id": "9190979", "content": "To evaluate the clinical effectiveness of retroperitoneal drainage following lymphadenectomy in gynecologic surgery.\nOne hundred thirty-seven consecutive patients undergoing systematic lymphadenectomy for gynecologic malignancies were randomized to receive (Group A, 68) or not (Group B, 69) retroperitoneal drainage. The pelvic peritoneum and the paracolic gutters were not sutured after node dissection. Perioperative data and complications were recorded.\nClinical and surgical parameters were comparable in the two groups. Postoperative hospital stay was significantly shorter in Group B (P < 0.001), whereas the complication rate was significantly higher in Group A (P = 0.01). This was mainly due to a significant increase in lymphocyst and lymphocyst-related morbidity. Sonographic monitoring for lymphocyst showed free abdominal fluid in 18% of drained and 36% of not-drained patients (P = 0.03). Symptomatic ascites developed in 2 drained (3%) and 3 not-drained (4%) patients (NS), respectively.\nProphylactic drainage of the retroperitoneum seems to increase lymphadenectomy-related morbidity and postoperative stay. Therefore, routine drainage following lymphadenectomy seems to be no longer indicated when the retroperitoneum is left open.", "title": "A randomized study comparing retroperitoneal drainage with no drainage after lymphadenectomy in gynecologic malignancies.", "date": "1997-06-01"}]}
{"original_review": "25734590", "question_data": [{"question_id": 2, "question": "Is the rate of clinician impression of cognitive change higher, lower, or the same when comparing cholinase inhibitors to placebo in patients with MS?", "answer": "higher", "evidence_quality": "high", "fulltext_required": "no", "comment": "", "relevant_sources": ["15534239", "21519001"]}, {"question_id": 3, "question": "Is the rate of patient self-reported impression of memory change higher, lower, or the same when comparing cholinase inhibitors to placebo in patients with MS?", "answer": "uncertain effect", "evidence_quality": "high", "fulltext_required": "no", "comment": "", "relevant_sources": ["15534239", "21519001"]}], "sources": [{"article_id": "18296124", "content": "Cholinergic deficits might contribute to vascular cognitive impairment. Trials of cholinesterase inhibitors in patients with vascular dementia are difficult because of heterogeneous disease mechanisms and overlap between vascular and Alzheimer's disease (AD) pathology in the age-group recruited. Cerebral autosomal dominant arteriopathy with subcortical infarcts and leucoencephalopathy (CADASIL) is a genetic form of subcortical ischaemic vascular dementia. It represents a homogeneous disease process, and because of CADASIL's early onset, comorbid AD pathology is rare. We did a multicentre, 18-week, placebo-controlled, double-blind, randomised parallel-group trial to determine whether the cholinesterase inhibitor donepezil improves cognition in patients with CADASIL.\n168 patients with CADASIL (mean age 54.8 years) were assigned to 10 mg donepezil per day (n=86) or placebo (n=82) by a computer-generated randomisation protocol. Inclusion criteria included a mini-mental state examination (MMSE) score of 10-27 or a trail making test (TMT) B time score at least 1.5 SD below the mean, after adjustment for age and education. The primary endpoint was change from baseline in the score on the vascular AD assessment scale cognitive subscale (V-ADAS-cog) at 18 weeks. Secondary endpoints included scores on the ADAS-cog, MMSE, TMT A time and B time, Stroop, executive interview-25 (EXIT25), CLOX, disability assessment for dementia, and sum of boxes of the clinical dementia rating scale. Analysis was done by intention to treat. This trial is registered with ClinicalTrials.gov, number NCT00103948.\n161 patients were analysed. There was no significant difference between donepezil (n=84) and placebo (n=77) in the primary endpoint. The least-squares mean change from baseline score was -0.81 (SE 0.59) in the placebo group and -0.85 (SE 0.57) in the donepezil group (p=0.956). There was a significant treatment effect favouring donepezil on the following secondary outcomes: TMT B time (p=0.023), TMT A time (p=0.015), and EXIT25 (p=0.022). Ten donepezil-treated patients discontinued treatment due to adverse events compared to seven placebo-treated patients.\nDonepezil had no effect on the primary endpoint, the V-ADAS-cog score in CADASIL patients with cognitive impairment. Improvements were noted on several measures of executive function, but the clinical relevance of these findings is not clear. Our findings may have implications for future trial design in subcortical vascular cognitive impairment.", "title": "Donepezil in patients with subcortical vascular cognitive impairment: a randomised double-blind trial in CADASIL.", "date": "2008-02-26"}, {"article_id": "25191771", "content": "In Huntington disease (HD) patients receiving rivastigmine treatment improvement of behavioral symptoms and of cognitive function (assessed with screening diagnostic instruments) has been reported. The aim of the present study was to verify such improvement in cognitive function by cognitive function assessment with a detailed neuropsychological battery covering all relevant cognitive systems expected to be impaired in early phase HD.\nEighteen (18) HD patients entered the study and were randomly allocated to the rivastigmine and placebo group. All subjects underwent neuropsychological assessment at baseline. Follow-up neuropsychological assessment was applied after 6 months of rivastigmine or placebo treatment. Eighteen (18) healthy controls entered the study to control for practice effect and underwent neuropsychological assessment at baseline and after 6 months, without treatment. The neuropsychological battery consisted of assessment tools that are sensitive to cognitive impairment seen in early phase HD: CTMT, SDMT, Stroop (attention and information control), RFFT, TOL, Verbal fluency (executive functioning), CVLT-II, RCFT (learning and memory). Effect of rivastigmine and possible effect of practice was assessed using the mixed ANOVA model.\nNo statistically significant effect of rivastigmine treatment on cognitive function in HD patients was detected. There was no evidence for practice or placebo effect.\nDetailed neuropsychological assessment did not confirm previously reported effect of rivastigmine treatment on cognitive function in HD patients. The limitations of our study are, in particular, small sample size and the lack of a single measure of relevant cognitive functioning in HD patients. Instead of focusing solely on statistical significance, a clinical relevance study is proposed to clarify the issue of rivastigmine effects in HD.", "title": "Cognitive function in early clinical phase huntington disease after rivastigmine treatment.", "date": "2014-09-06"}, {"article_id": "23069874", "content": "Cognitive decline has been recognised as a frequent symptom in multiple sclerosis (MS). Cholinesterase inhibitors (ChEIs) are employed for the treatment of Alzheimer's disease, but there is some evidence that ChEIs might also be effective in MS patients with cognitive deficits, particularly deficits of memory function.\nThe aim of this study was to evaluate efficacy on memory function and safety of the ChEI rivastigmine in MS patients with cognitive deficits as measured by the change from baseline of the total recall score of the selective reminding test (SRT) after 16 weeks of treatment.\nEfficacy and safety of rivastigmine were analysed in a 16-week, multicentre, double-blind, randomised, placebo-controlled study, followed by an optional one-year open-label treatment phase. Effects of rivastigmine and placebo were compared by an analysis of covariance.\nIn total, 86 patients were enrolled. Patients who received rivastigmine (n = 43) showed a non-significant increase in total recall score (sum of all words immediately recalled over all six trials) over placebo (n = 38) after 16 weeks of treatment (p = 0.2576). Other outcome measures provided no evidence supporting benefits of rivastigmine. Treatment with rivastigmine was well tolerated.\nWith the results of this study, the need for an effective therapy in cognitively impaired MS patients is still required. Thus, intensive and continued clinical research is required to explore therapeutic options for cognitive deficits in MS patients.", "title": "Randomised multicentre trial on safety and efficacy of rivastigmine in cognitively impaired multiple sclerosis patients.", "date": "2012-10-17"}, {"article_id": "18973065", "content": "Cognitive dysfunction is one of the common clinical symptoms in multiple sclerosis (MS), but there is no effective treatment for it.\nThe aim of this study was to evaluate the effect of rivastigmine in treating memory and cognitive dysfunction in MS.\nA single-center double-blind placebo-controlled randomized clinical trial conducted from October 2005 to February 2007. Sixty definite MS patients with cognitive impairment age 16 to 54 years were randomly allocated to receive a 12-week treatment course of either rivastigmine (1.5 mg once a day increment over 4 weeks to 3 mg twice daily) or placebo. Response to treatment was assessed by the Wechsler Memory Scale (WMS) at baseline and 12 weeks after start of therapy.\nA slight, but significant memory improvement occurred in both groups. Of the 30 patients treated with rivastigmine, the mean (SD) WMS general memory score increased from 60.3 (4.2) at baseline to 64.9 (5.3) at the end of study period (P<0.001). Correspondingly, in the 30 patients treated with placebo, the mean (SD) WMS general memory score increased from 60.5 (4.9) to 64.5 (3.7) (P < 0.001). The average WMS general memory score at the end of trial did not changed between rivastigmine and placebo group (mean difference, 0.4; 95% CI, -2.0, 2.8).\nNo significant differences were seen between rivastigmine and placebo on the mean (SD) WMS general memory score. A larger multicenter study of rivastigmine in MS is warranted in order to more definitely assess the efficacy of this intervention.", "title": "Effects of rivastigmine on memory and cognition in multiple sclerosis.", "date": "2008-11-01"}, {"article_id": "18196898", "content": "The treatment of frontotemporal dementia (FTD) has been mainly symptomatic. Small randomized or open-label case control studies of neurotransmitters have been inconclusive. We tried galantamine in the 2 most common varieties of FTD.\nThirty-six behavioral variety FTD and primary progressive aphasia (PPA) patients were treated in an open-label period of 18 weeks and a randomized, placebo-controlled phase for 8 weeks with galantamine. The primary efficacy measures were the Frontal Behavioral Inventory, the Aphasia Quotient of the Western Aphasia Battery, the Clinical Global Impression of Severity and the Clinical Global Impression of Improvement.\nNo significant differences in behavior or language were found for the total group. A treatment effect (p = 0.009), in a subgroup of subjects with PPA in the global severity score, in favor of galantamine was detected in the placebo-controlled withdrawal phase but was not considered significant after correction for multiple comparisons. The language scores for the treated PPA group also remained stable compared to the placebo group, which showed deterioration.\nGalantamine is not effective in the behavioral variety of FTD, but a trend of efficacy is shown in the aphasic subgroup, which may be clinically significant. Galantamine appeared safe in FTD/PPA.", "title": "Galantamine in frontotemporal dementia and primary progressive aphasia.", "date": "2008-01-17"}, {"article_id": "21519001", "content": "The goal of this study was to determine if memory would be improved by donepezil as compared to placebo in a multicenter, double-blind, randomized clinical trial (RCT).\nDonepezil 10 mg daily was compared to placebo to treat memory impairment. Eligibility criteria included the following: age 18-59 years, clinically definite multiple sclerosis (MS), and performance \u2264 \u00bd SD below published norms on the Rey Auditory Verbal Learning Test (RAVLT). Neuropsychological assessments were performed at baseline and 24 weeks. Primary outcomes were change on the Selective Reminding Test (SRT) of verbal memory and the participant's impression of memory change. Secondary outcomes included changes on other neuropsychological tests and the evaluating clinician's impression of memory change.\nA total of 120 participants were enrolled and randomized to either donepezil or placebo. No significant treatment effects were found between groups on either primary outcome of memory or any secondary cognitive outcomes. A trend was noted for the clinician's impression of memory change in favor of donepezil (37.7%) vs placebo (23.7%) (p = 0.097). No serious or unanticipated adverse events attributed to study medication developed.\nDonepezil did not improve memory as compared to placebo on either of the primary outcomes in this study.\nThis study provides Class I evidence which does not support the hypothesis that 10 mg of donepezil daily for 24 weeks is superior to placebo in improving cognition as measured by the SRT in people with MS whose baseline RAVLT score was 0.5 SD or more below average.", "title": "Multicenter randomized clinical trial of donepezil for memory impairment in multiple sclerosis.", "date": "2011-04-27"}, {"article_id": "17030764", "content": "Striatal cholinergic dysfunction may be important in Huntington disease (HD). We studied whether donepezil improves chorea, cognition, and quality of life (QoL) in HD. Thirty patients were randomly assigned to treatment with donepezil or placebo. At the doses studied, donepezil did not improve chorea, cognition, or QoL. Adverse events were similar between both groups. Based on this small sample study, donepezil was not an effective treatment for HD.", "title": "Effect of donepezil on motor and cognitive function in Huntington disease.", "date": "2006-10-13"}, {"article_id": "15534239", "content": "To determine the effect of donepezil in treating memory and cognitive dysfunction in multiple sclerosis (MS).\nThis single-center double-blind placebo-controlled clinical trial evaluated 69 MS patients with cognitive impairment who were randomly assigned to receive a 24-week treatment course of either donepezil (10 mg daily) or placebo. Patients underwent neuropsychological assessment at baseline and after 24 weeks of treatment. The primary outcome was change in verbal learning and memory on the Selective Reminding Test (SRT). Secondary outcomes included other tests of cognitive function, patient-reported change in memory, and clinician-reported impression of cognitive change.\nDonepezil-treated patients showed significant improvement in memory performance on the SRT compared to placebo (p = 0.043). The benefit of donepezil remained significant after controlling for various covariates including age, Expanded Disability Status Scale, baseline SRT score, reading ability, MS subtype, and sex. Donepezil-treated patients did not show significant improvements on other cognitive tests, but were more than twice as likely to report memory improvement than those in the placebo group (p = 0.006). The clinician also reported cognitive improvement in almost twice as many donepezil vs placebo patients (p = 0.036). No serious adverse events related to study medication occurred, although more donepezil (34.3%) than placebo (8.8%) subjects reported unusual/abnormal dreams (p = 0.010).\nDonepezil improved memory in MS patients with initial cognitive impairment in a single center clinical trial. A larger multicenter investigation of donepezil in MS is warranted in order to more definitively assess the efficacy of this intervention.", "title": "Donepezil improved memory in multiple sclerosis in a randomized clinical trial.", "date": "2004-11-10"}]}
{"original_review": "27056645", "question_data": [{"question_id": 4, "question": "Is the number of people with at least one neoplastic lesion detected higher, lower, or the same when comparing chromoscopy to conventional endoscopy?", "answer": "higher", "evidence_quality": "", "fulltext_required": "no", "comment": "", "relevant_sources": ["12196768", "16527699", "21159889", "16767577"]}, {"question_id": 5, "question": "Is total polyp detection higher, lower, or the same when comparing chromoscopy to conventional endoscopy?", "answer": "higher", "evidence_quality": "", "fulltext_required": "no", "comment": "", "relevant_sources": ["12196768", "14960519", "20179689", "16527699", "21159889", "16767577", "19139000"]}, {"question_id": 219, "question": "Is the risk of adverse events higher, lower, or the same when comparing chromoscopy to conventional endoscopy?", "answer": "insufficient data", "evidence_quality": "", "fulltext_required": "no", "comment": "", "relevant_sources": ["12196768", "14960519", "20179689", "16527699", "21159889", "16767577", "19139000"]}], "sources": [{"article_id": "19139000", "content": "Conventional colonoscopy misses some neoplastic lesions. We compared the sensitivity of chromoendoscopy and colonoscopy with intensive inspection for detecting adenomatous polyps missed by conventional colonoscopy. Fifty subjects with a history of colorectal cancer or adenomas underwent tandem colonoscopies at one of five centers of the Great Lakes New England Clinical Epidemiology and Validation Center of the Early Detection Research Network. The first exam was a conventional colonoscopy with removal of all visualized polyps. The second exam was randomly assigned as either pan-colonic indigocarmine chromoendoscopy or standard colonoscopy with intensive inspection lasting >20 minutes. Size, histology, and numbers of polyps detected on each exam were recorded. Twenty-seven subjects were randomized to a second exam with chromoendoscopy and 23 underwent intensive inspection. Forty adenomas were identified on the first standard colonoscopies. The second colonoscopies detected 24 additional adenomas: 19 were found using chromoendoscopy and 5 were found using intensive inspection. Chromoendoscopy found additional adenomas in more subjects than did intensive inspection (44% versus 17%) and identified significantly more missed adenomas per subject (0.7 versus 0.2, P < 0.01). Adenomas detected with chromoendoscopy were significantly smaller (mean size 2.66 +/- 0.97 mm) and were more often right-sided. Chromoendoscopy was associated with more normal tissue biopsies and longer procedure times than intensive inspection. After controlling for procedure time, chromoendoscopy detected more adenomas and hyperplastic polyps compared with colonoscopy using intensive inspection alone. Chromoendoscopy detected more polyps missed by standard colonoscopy than did intensive inspection. The clinical significance of these small missed lesions warrants further study.", "title": "Chromoendoscopy detects more adenomas than colonoscopy using intensive inspection without dye spraying.", "date": "2009-01-14"}, {"article_id": "16527699", "content": "High-resolution colonoscopy with chromoscopy (HRC) is a technique designed to improve the detection of colonic neoplasias. We prospectively compared standard colonoscopy (SC) and HRC in a randomized multicenter trial.\nPatients (n = 203; age, 58 +/- 10 years; sex ratio, 1) were recruited according to the following criteria: (1) a history of either familial or personal colonic neoplasia or (2) alarm symptoms after the age of 60 years. After randomization, an SC was performed in 100 patients (resolution, < or = 410,000 pixels) and a HRC in 103 patients (Fujinon EC485ZW, 850,000 pixels). In the HRC group, each colonic segment was examined before and after spraying with indigo carmine 0.4%.\nTwo hundred seventy-six polyps were detected in 198 patients. One hundred sixty of them were hyperplastic polyps, 116 were adenomas, and 2 were carcinomas. The numbers of hyperplastic polyps and purely flat adenomas were significantly higher in the HRC group than in the SC group (1.1 +/- 1.6 vs 0.5 +/- 1.4 and 0.22 +/- 0.68 vs 0.07 +/- 0.29, respectively; P = .01 and P = .04), but there was no significant difference in the total number of adenomas per patient (primary end point) detected between the HRC and the SC groups (0.6 +/- 1.0 vs 0.5 +/- 0.9, respectively).\nAlthough HRC improves detection of purely flat adenomas and hyperplastic polyps, the overall detection of colonic adenomas in a population at increased risk of neoplasia is not significantly improved. These findings do not support the routine use of HRC in clinical practice.", "title": "High resolution colonoscopy with chromoscopy versus standard colonoscopy for the detection of colonic neoplasia: a randomized study.", "date": "2006-03-11"}, {"article_id": "14960519", "content": "Diminutive and flat colorectal lesions can be difficult to detect using conventional colonoscopic techniques. Previous data have suggested that pan-chromoscopy may improve detection rates. No randomised control trial has been performed examining detection rates of such lesions while controlling for extubation time and lavage effect.\nWe conducted a randomised controlled trial of pan-colonic chromoscopic colonoscopy for the detection of diminutive and flat colorectal lesions while controlling for extubation time and lavage effect.\nConsecutive patients attending for routine colonoscopy were randomised to either pan-chromoscopy using 0.5% indigo carmine (IC) or targeted chromoscopy (control group). A minimum diagnostic extubation time was set at eight minutes with controls undergoing a matched volume of saline wash.\nA total of 260 patients were randomised; 132 controls and 128 to pan-colonic chromoscopy. Extubation times did not differ significantly between the control (median 15 minutes (range 8-41)) and chromoscopy (median 17 minutes (range 8-39)) groups. The volume of IC used in the pan-chromoscopy group (median 68 ml (range 65-90)) and normal saline used in the control group (69 ml (range 60-93)) did not differ significantly. There was a statistically significant difference between the groups regarding the total number of adenomas detected (p<0.05) with significantly more diminutive (<4 mm) adenomas detected in the pan-chromoscopy group (p = 0.03). Pan-chromoscopy diagnosed more diminutive and flat lesions in the right colon compared with controls (p<0.05), with more patients with multiple adenomas (>3) detected using pan-chromoscopy (p<0.01). Hyperplastic lesions were more commonly detected in the pan-chromoscopy group compared with controls (p<0.001). More hyperplastic polyps were detected in the left colon (86% rectosigmoid) using chromoscopy compared with controls.\nChromoscopy improves the total number of adenomas detected and enhances the detection of diminutive and flat lesions. Importantly, eight diminutive lesions had foci of high grade dysplasia. Chromoscopy may benefit patients, assuming a high risk of colorectal cancer, and help in risk stratification and planning follow up colonoscopy intervals.", "title": "Detecting diminutive colorectal lesions at colonoscopy: a randomised controlled trial of pan-colonic versus targeted chromoscopy.", "date": "2004-02-13"}, {"article_id": "16767577", "content": "Colonoscopy is still considered the standard investigation for the detection of colorectal adenomas, but the miss rate, especially for small and flat lesions, remains unacceptably high. Chromoscopy has been shown to increase the yield for lesion detection in inflammatory bowel disease. The aim of this randomized prospective study was to determine whether a combination of chromoscopy and structure enhancement could increase the adenoma detection rate in high-risk patients.\nAll patients included in the trial had a personal history of colorectal adenomas and/or a family history of colorectal cancer (but excluding genetic syndromes). They were randomized to one of two tandem colonoscopy groups, with the first pass consisting of conventional colonoscopy for both groups, followed by either chromoscopy and structure enhancement (the \"study\" group) or a second conventional colonoscopy (the control group) for the second-pass colonoscopy. All detected lesions was examined histopathologically after endoscopic resection or biopsy. The principal outcome parameter was the adenoma detection rate; the number, histopathology, and location of lesions was also recorded.\nA total of 292 patients were included in the study (146 patients in each group). The patients' demographic characteristics, the indications for colonoscopy, and the quality of bowel preparation were similar in the two groups. There was a significant difference between the two groups with respect to the median duration of the examination (18.9 minutes in the control group vs. 27.1 minutes for the study group, P < 0.001). Although more hyperplastic lesions were detected throughout the colon in the study group ( P = 0.033), there was no difference between the two groups in either the proportion of patients with at least one adenoma or in the total number of adenomas detected. Chromoscopy and structure enhancement diagnosed significantly more diminutive adenomas (< 5mm) in the right colon, compared with controls ( P = 0.039).\nOn the basis of our results we cannot generally recommend the systematic use of chromoscopy and structure enhancement in a high-risk patient population, although the detection of small adenomas in the proximal colon was improved.", "title": "Does chromoendoscopy with structure enhancement improve the colonoscopic adenoma detection rate?", "date": "2006-06-13"}, {"article_id": "21159889", "content": "Colonoscopy is the accepted gold standard for detecting colorectal adenomas, but the miss rate, especially for small and flat lesions, remains unacceptably high. The aim of this study was to determine whether enhanced mucosal contrast using pancolonic chromoendoscopy (PCC) allows higher rates of adenoma detection.\nIn a prospective, randomised two-centre trial, PCC (with 0.4% indigo carmine spraying during continuous extubation) was compared with standard colonoscopy (control group) in consecutive patients attending for routine colonoscopy. The histopathology of the lesions detected was confirmed by evaluating the endoscopic resection or biopsy specimens.\nA total of 1008 patients were included (496 in the PCC group, 512 in the control group). The patients' demographic characteristics and indications for colonoscopy were similar in the two groups. The proportion of patients with at least one adenoma was significantly higher in the PCC group (46.2%) than in the control group (36.3%; p = 0.002). Chromoendoscopy increased the overall detection rate for adenomas (0.95 vs 0.66 per patient), flat adenomas (0.56 vs 0.28 per patient) and serrated lesions (1.19 vs 0.49 per patient) (p < 0.001). There was a non-significant trend towards increased detection of advanced adenomas (103 vs 81; p = 0.067). Mean extubation times were slightly but significantly longer in the PCC group in comparison with the control group (11.6 \u00b1 3.36 min vs 10.1 \u00b1 2.03 min; p < 0.001).\nPancolonic chromoendoscopy markedly enhances adenoma detection rates in an average-risk population and is practicable enough for routine application.", "title": "Pancolonic chromoendoscopy with indigo carmine versus standard colonoscopy for detection of neoplastic lesions: a randomised two-centre trial.", "date": "2010-12-17"}, {"article_id": "12196768", "content": "Small adenomas may be missed during colonoscopy, but chromoscopy has been reported to enhance detection. The aim of this randomized-controlled trial was to determine the effect of total colonic dye spray on adenoma detection during routine colonoscopy.\nConsecutive outpatients undergoing routine colonoscopy were randomized to a dye-spray group (0.1% indigo carmine used to coat the entire colon during withdrawal from the cecum) or control group (no dye).\nTwo hundred fifty-nine patients were randomized, 124 to the dye-spray and 135 to the control group; demographics, indication for colonoscopy, and quality of the preparation were similar between the groups. Extubation from the cecum took a median of 9:05 minutes (range: 2:48-24:44 min) in the dye-spray group versus 4:52 minutes (range: 1:42-15:21 min) in the control group (p < 0.0001). The proportion of patients with at least 1 adenoma and the total number of adenomas were not different between groups. However, in the dye-spray group significantly more diminutive adenomas (<5 mm) were detected proximal to the sigmoid colon (p = 0.026) and more patients were identified with 3 or more adenomas (p = 0.002). More non-neoplastic polyps were detected throughout the colon in the dye-spray group (p = 0.003). There were no complications.\nDye-spray increases the detection of small adenomas in the proximal colon and patients with multiple adenomas, but long-term outcomes should be studied to determine the clinical value of these findings.", "title": "Total colonic dye-spray increases the detection of diminutive adenomas during routine colonoscopy: a randomized controlled trial.", "date": "2002-08-28"}, {"article_id": "20179689", "content": "Flat and depressed colon neoplasms are an increasingly recognized precursor for colorectal cancer (CRC) in Western populations. High-definition chromoscopy is used to increase the yield of colonoscopy for flat and depressed neoplasms; however, its role in average-risk patients undergoing routine screening remains uncertain.\nAverage-risk patients referred for screening colonoscopy at four U.S. medical centers were randomized to high-definition chromocolonoscopy or high-definition white light colonoscopy. The primary outcomes, patients with at least one adenoma and the number of adenomas per patient, were compared between the two groups. The secondary outcome was patients with flat or depressed neoplasms, as defined by the Paris classification.\nA total of 660 patients were randomized (chromocolonoscopy: 321, white light: 339). Overall, the mean number of adenomas per patient was 1.2+/-2.1, the mean number of flat polyps per patient was 1.4+/-1.9, and the mean number of flat adenomas per patient was 0.5+/-1.0. The number of patients with at least one adenoma (55.5% vs. 48.4%, absolute difference 7.1%, 95% confidence interval (-0.5% to 14.7%), P=0.07), and the number of adenomas per patient (1.3+/-2.4 vs. 1.1+/-1.8, P=0.07) were marginally higher in the chromocolonoscopy group. There were no significant differences in the number of advanced adenomas per patient (0.06+/-0.37 vs. 0.04+/-0.25, P=0.3) and the number of advanced adenomas<10 mm per patient (0.02+/-0.26 vs. 0.01+/-0.14, P=0.4). Two invasive cancers were found, one in each group; neither was a flat neoplasm. Chromocolonoscopy detected significantly more flat adenomas per patient (0.6+/-1.2 vs. 0.4+/-0.9, P=0.01), adenomas<5 mm in diameter per patient (0.8+/-1.3 vs. 0.7+/-1.1, P=0.03), and non-neoplastic lesions per patient (1.8+/-2.3 vs. 1.0+/-1.3, P<0.0001).\nHigh-definition chromocolonoscopy marginally increased overall adenoma detection, and yielded a modest increase in flat adenoma and small adenoma detection, compared with high-definition white light colonoscopy. The yield for advanced neoplasms was similar for the two methods. Our findings do not support the routine use of high-definition chromocolonoscopy for CRC screening in average-risk patients. The high adenoma detection rates observed in this study may be due to the high-definition technology used in both groups.", "title": "High-definition chromocolonoscopy vs. high-definition white light colonoscopy for average-risk colorectal cancer screening.", "date": "2010-02-25"}]}
{"original_review": "26346232", "question_data": [{"question_id": 6, "question": "Is stroke prevention higher, lower, or the same when comparing Transcatheter Device Closure (TDC) to medical therapy?", "answer": "no difference", "evidence_quality": "", "fulltext_required": "no", "comment": "", "relevant_sources": ["22417252", "23514285", "23514286"]}, {"question_id": 7, "question": "Is the composite endpoint of recurrent stroke or TIA higher, lower, or the same when comparing Transcatheter Device Closure (TDC) to medical therapy?", "answer": "no difference", "evidence_quality": "", "fulltext_required": "no", "comment": "", "relevant_sources": ["22417252", "23514285"]}], "sources": [{"article_id": "23514285", "content": "The options for secondary prevention of cryptogenic embolism in patients with patent foramen ovale are administration of antithrombotic medications or percutaneous closure of the patent foramen ovale. We investigated whether closure is superior to medical therapy.\nWe performed a multicenter, superiority trial in 29 centers in Europe, Canada, Brazil, and Australia in which the assessors of end points were unaware of the study-group assignments. Patients with a patent foramen ovale and ischemic stroke, transient ischemic attack (TIA), or a peripheral thromboembolic event were randomly assigned to undergo closure of the patent foramen ovale with the Amplatzer PFO Occluder or to receive medical therapy. The primary end point was a composite of death, nonfatal stroke, TIA, or peripheral embolism. Analysis was performed on data for the intention-to-treat population.\nThe mean duration of follow-up was 4.1 years in the closure group and 4.0 years in the medical-therapy group. The primary end point occurred in 7 of the 204 patients (3.4%) in the closure group and in 11 of the 210 patients (5.2%) in the medical-therapy group (hazard ratio for closure vs. medical therapy, 0.63; 95% confidence interval [CI], 0.24 to 1.62; P=0.34). Nonfatal stroke occurred in 1 patient (0.5%) in the closure group and 5 patients (2.4%) in the medical-therapy group (hazard ratio, 0.20; 95% CI, 0.02 to 1.72; P=0.14), and TIA occurred in 5 patients (2.5%) and 7 patients (3.3%), respectively (hazard ratio, 0.71; 95% CI, 0.23 to 2.24; P=0.56).\nClosure of a patent foramen ovale for secondary prevention of cryptogenic embolism did not result in a significant reduction in the risk of recurrent embolic events or death as compared with medical therapy. (Funded by St. Jude Medical; ClinicalTrials.gov number, NCT00166257.).", "title": "Percutaneous closure of patent foramen ovale in cryptogenic embolism.", "date": "2013-03-22"}, {"article_id": "23514286", "content": "Whether closure of a patent foramen ovale is effective in the prevention of recurrent ischemic stroke in patients who have had a cryptogenic stroke is unknown. We conducted a trial to evaluate whether closure is superior to medical therapy alone in preventing recurrent ischemic stroke or early death in patients 18 to 60 years of age.\nIn this prospective, multicenter, randomized, event-driven trial, we randomly assigned patients, in a 1:1 ratio, to medical therapy alone or closure of the patent foramen ovale. The primary results of the trial were analyzed when the target of 25 primary end-point events had been observed and adjudicated.\nWe enrolled 980 patients (mean age, 45.9 years) at 69 sites. The medical-therapy group received one or more antiplatelet medications (74.8%) or warfarin (25.2%). Treatment exposure between the two groups was unequal (1375 patient-years in the closure group vs. 1184 patient-years in the medical-therapy group, P=0.009) owing to a higher dropout rate in the medical-therapy group. In the intention-to-treat cohort, 9 patients in the closure group and 16 in the medical-therapy group had a recurrence of stroke (hazard ratio with closure, 0.49; 95% confidence interval [CI], 0.22 to 1.11; P=0.08). The between-group difference in the rate of recurrent stroke was significant in the prespecified per-protocol cohort (6 events in the closure group vs. 14 events in the medical-therapy group; hazard ratio, 0.37; 95% CI, 0.14 to 0.96; P=0.03) and in the as-treated cohort (5 events vs. 16 events; hazard ratio, 0.27; 95% CI, 0.10 to 0.75; P=0.007). Serious adverse events occurred in 23.0% of the patients in the closure group and in 21.6% in the medical-therapy group (P=0.65). Procedure-related or device-related serious adverse events occurred in 21 of 499 patients in the closure group (4.2%), but the rate of atrial fibrillation or device thrombus was not increased.\nIn the primary intention-to-treat analysis, there was no significant benefit associated with closure of a patent foramen ovale in adults who had had a cryptogenic ischemic stroke. However, closure was superior to medical therapy alone in the prespecified per-protocol and as-treated analyses, with a low rate of associated risks. (Funded by St. Jude Medical; RESPECT ClinicalTrials.gov number, NCT00465270.).", "title": "Closure of patent foramen ovale versus medical therapy after cryptogenic stroke.", "date": "2013-03-22"}, {"article_id": "22417252", "content": "The prevalence of patent foramen ovale among patients with cryptogenic stroke is higher than that in the general population. Closure with a percutaneous device is often recommended in such patients, but it is not known whether this intervention reduces the risk of recurrent stroke.\nWe conducted a multicenter, randomized, open-label trial of closure with a percutaneous device, as compared with medical therapy alone, in patients between 18 and 60 years of age who presented with a cryptogenic stroke or transient ischemic attack (TIA) and had a patent foramen ovale. The primary end point was a composite of stroke or transient ischemic attack during 2 years of follow-up, death from any cause during the first 30 days, or death from neurologic causes between 31 days and 2 years.\nA total of 909 patients were enrolled in the trial. The cumulative incidence (Kaplan-Meier estimate) of the primary end point was 5.5% in the closure group (447 patients) as compared with 6.8% in the medical-therapy group (462 patients) (adjusted hazard ratio, 0.78; 95% confidence interval, 0.45 to 1.35; P=0.37). The respective rates were 2.9% and 3.1% for stroke (P=0.79) and 3.1% and 4.1% for TIA (P=0.44). No deaths occurred by 30 days in either group, and there were no deaths from neurologic causes during the 2-year follow-up period. A cause other than paradoxical embolism was usually apparent in patients with recurrent neurologic events.\nIn patients with cryptogenic stroke or TIA who had a patent foramen ovale, closure with a device did not offer a greater benefit than medical therapy alone for the prevention of recurrent stroke or TIA. (Funded by NMT Medical; ClinicalTrials.gov number, NCT00201461.).", "title": "Closure or medical therapy for cryptogenic stroke with patent foramen ovale.", "date": "2012-03-16"}]}
{"original_review": "28898386", "question_data": [{"question_id": 8, "question": "Is the length of hospital stay higher, lower, or the same when comparing Pancreatojejunostomy (PJ) to Pancreatogastrostomy (PG)?", "answer": "no difference", "evidence_quality": "low", "fulltext_required": "no", "comment": "Grendar/25799130 does not mention (16% wt) - this may bias results", "relevant_sources": ["16327486", "19092337", "7574936"]}, {"question_id": 9, "question": "Is the overall risk of postoperative pancreatic fistula higher, lower, or the same when comparing Pancreatojejunostomy (PJ) to Pancreatogastrostomy (PG)?", "answer": "no difference", "evidence_quality": "low", "fulltext_required": "no", "comment": "Keck 2016/26135690 (19% weight) only mentions B/C fistula -> but fulltext is available; not sure why Wellner/22744638 is excluded", "relevant_sources": ["24467711", "16327486", "15910726", "19092337", "24264781", "25799130", "26135690", "23643139", "7574936"]}], "sources": [{"article_id": "19092337", "content": "To compare the results of postoperative morbidity rate of a new pancreatogastrostomy technique, pylorus-preserving pancreaticoduodenectomy (PPPD) with gastric partition (PPPD-GP) with the conventional technique of pancreaticojejunostomy (PJ).\nPancreatojejunostomy and pancreatogastrostomy (PG) are the commonly preferred methods of anastomosis after pancreatoduodenectomy (PD). All randomized controlled trials failed to show advantage of a particular technique, suggesting that both PJ and PG provide equally results. However, postoperative morbidity remains high. The best technique in pancreatic anastomosis is still debated.\nDescribed here is a new technique, PPPD-GP; in this technique the gastroepiploic arcade is preserved. Gastric partition was performed using 2 endo-Gia staplers along the greater curvature of the stomach, 3 cm from the border. This gastric segment, 10 to 12 cm in length is placed in close proximity to the cut edge of the pancreatic stump. An end-to-side, duct-to-mucosa anastomosis (with pancreatic duct stent) is constructed. One hundred eight patients undergoing PPPD for benign and malignant diseases of the pancreatic head and the periampullary region were randomized to receive PG (PPPD-GP) or end-to-side PJ (PPPD-PJ).\nThe two treatment groups showed no differences in preoperative parameters and intraoperative factors. The overall postoperative complications were 23% after PPPD-GP and 44% after PPPD-PJ (P < 0.01). The incidence of pancreatic fistula was 4% after PPPD-GP and 18% after PPPD-PJ (P < 0.01). The mean + SD hospital stay was 12 +/- 2 days after PPPD-GP and 16 +/- 3 days after PPPD-PJ.\nThis study shows that PPPD-GP can be performed safely and is associated with less complication than PPPD-PJ. The advantage of this technique over other PG techniques is that the anastomosis is outside the area of the stomach where the contents empty into the jejunum, but pancreatic juice drains directly into the stomach.", "title": "Pancreatogastrostomy with gastric partition after pylorus-preserving pancreatoduodenectomy versus conventional pancreatojejunostomy: a prospective randomized study.", "date": "2008-12-19"}, {"article_id": "26135690", "content": "To assess pancreatic fistula rate and secondary endpoints after pancreatogastrostomy (PG) versus pancreatojejunostomy (PJ) for reconstruction in pancreatoduodenectomy in the setting of a multicenter randomized controlled trial.\nPJ and PG are established methods for reconstruction in pancreatoduodenectomy. Recent prospective trials suggest superiority of the PG regarding perioperative complications.\nA multicenter prospective randomized controlled trial comparing PG with PJ was conducted involving 14 German high-volume academic centers for pancreatic surgery. The primary endpoint was clinically relevant postoperative pancreatic fistula. Secondary endpoints comprised perioperative outcome and pancreatic function and quality of life measured at 6 and 12 months of follow-up.\nFrom May 2011 to December 2012, 440 patients were randomized, and 320 were included in the intention-to-treat analysis. There was no significant difference in the rate of grade B/C fistula after PG versus PJ (20% vs 22%, P = 0.617). The overall incidence of grade B/C fistula was 21%, and the in-hospital mortality was 6%. Multivariate analysis of the primary endpoint disclosed soft pancreatic texture (odds ratio: 2.1, P = 0.016) as the only independent risk factor. Compared with PJ, PG was associated with an increased rate of grade A/B bleeding events, perioperative stroke, less enzyme supplementation at 6 months, and improved results in some quality of life parameters.\nThe rate of grade B/C fistula after PG versus PJ was not different. There were more postoperative bleeding events with PG. Perioperative morbidity and mortality of pancreatoduodenectomy seem to be underestimated, even in the high-volume center setting.", "title": "Pancreatogastrostomy Versus Pancreatojejunostomy for RECOnstruction After PANCreatoduodenectomy (RECOPANC, DRKS 00000767): Perioperative and Long-term Results of a Multicenter Randomized Controlled Trial.", "date": "2015-07-03"}, {"article_id": "24264781", "content": "Anastomotic leakage of pancreaticojejunostomy (PJ) remains the single most important source of morbidity after pancreaticoduodenectomy (PD). The primary aim of this randomized clinical trial comparing PG with PJ after PD was to test the hypothesis that invaginated PG would result in a lower rate and severity of pancreatic fistula.\nPatients undergoing PD were randomized to receive either a duct-to-duct PJ or a double-layer invaginated PG. The primary endpoint was the rate of pancreatic fistula, using the definition of the International Study Group on Pancreatic Fistula. Secondary endpoints were the evaluation of severe abdominal complications (Clavien-Dindo grade IIIa or above), endocrine and exocrine function.\nOf 123 patients randomized, 58 underwent PJ and 65 had PG. The incidence of pancreatic fistula was significantly higher following PJ than for PG (20 of 58 versus 10 of 65 respectively; P\u2009=\u20090.014), as was the severity of pancreatic fistula (grade A: 2 versus 5 per cent; grade B-C: 33 versus 11 per cent; P\u2009=\u20090.006). The hospital readmission rate for complications was significantly lower after PG (6 versus 24 per cent; P\u2009=\u20090.005), weight loss was lower (P\u2009=\u20090.025) and exocrine function better (P\u2009=\u20090.022).\nThe rate and severity of pancreatic fistula was significantly lower with this PG technique compared with that following PJ.\nISRCTN58328599 (http://www.controlled-trials.com).", "title": "Randomized clinical trial of pancreaticogastrostomy versus pancreaticojejunostomy on the rate and severity of pancreatic fistula after pancreaticoduodenectomy.", "date": "2013-11-23"}, {"article_id": "7574936", "content": "The authors hypothesized that pancreaticogastrostomy is safer than pancreaticojejunostomy after pancreaticoduodenectomy and less likely to be associated with a postoperative pancreatic fistula.\nPancreatic fistula is a leading cause of morbidity and mortality after pancreaticoduodenectomy, occurring in 10% to 20% of patients. Nonrandomized reports have suggested that pancreaticogastrostomy is less likely than pancreaticojejunostomy to be associated with postoperative complications.\nBetween May 1993 and January 1995, the findings for 145 patients were analyzed in this prospective trial at The Johns Hopkins Hospital. After giving their appropriate preoperative informed consent, patients were randomly assigned to pancreaticogastrostomy or pancreaticojejunostomy after completion of the pancreaticoduodenal resection. All pancreatic anastomoses were performed in two layers without pancreatic duct stents and with closed suction drainage. Pancreatic fistula was defined as drainage of greater than 50 mL of amylase-rich fluid on or after postoperative day 10.\nThe pancreaticogastrostomy (n = 73) and pancreaticojejunostomy (n = 72) groups were comparable with regard to multiple parameters, including demographics, medical history, preoperative laboratory values, and intraoperative factors, such as operative time, blood transfusions, pancreatic texture, length of pancreatic remnant mobilized, and pancreatic duct diameter. The overall incidence of pancreatic fistula after pancreaticoduodenectomy was 11.7% (17/145). The incidence of pancreatic fistula was similar for the pancreaticogastrostomy (12.3%) and pancreaticojejunostomy (11.1%) groups. Pancreatic fistula was associated with a significant prolongation of postoperative hospital stay (36 +/- 5 vs. 15 +/- 1 days) (p < 0.001). Factors significantly increasing the risk of pancreatic fistula by univariate logistic regression analysis included ampullary or duodenal disease, soft pancreatic texture, longer operative time, greater intraoperative red blood cell transfusions, and lower surgical volume (p < 0.05). A multivariate logistic regression analysis revealed the factors most highly associated with pancreatic fistula to be lower surgical volume and ampullary or duodenal disease in the resected specimen.\nPancreatic fistula is a common complication after pancreaticoduodenectomy, with an incidence most strongly associated with surgical volume and underlying disease. These data do not support the hypothesis that pancreaticogastrostomy is safer than pancreaticojejunostomy or is associated with a lower incidence of pancreatic fistula.", "title": "A prospective randomized trial of pancreaticogastrostomy versus pancreaticojejunostomy after pancreaticoduodenectomy.", "date": "1995-10-01"}, {"article_id": "15910726", "content": "Only 2 large (more than 100 patients) prospective trials comparing pancreatogastrostomy (PG) with pancreatojejunostomy (PJ) after pancreatoduodenectomy (PD) have been reported until now. One nonrandomized study showed that there were less pancreatic and digestive tract fistula with PG, whereas the other, a randomized trial from a single high-volume center, found no significant differences between the two techniques.\nSingle blind, controlled randomized, multicenter trial. The main endpoint was intra-abdominal complications (IACs).\nOf 149 randomized patients, 81 underwent PG and 68 PJ. No significant difference was found between the two groups concerning pre- or intraoperative patient characteristics. The rate of patients with one or more IACs was 34% in each group. Twenty-seven patients sustained a pancreatoenteric fistula (18%), 13 in PG (16%; 95% confidence interval [CI] 8-24%) and 14 in PJ (20%; 95% CI 10.5-29.5%). No statistically significant difference was found between the 2 groups concerning the mortality rate (11% overall), the rate of reoperations and/or postoperative interventional radiology drainages (23%), or the length of hospital stay (median 20.5 days). Univariate analysis found the following risk factors: (1) age > or =70 years old, (2) extrapancreatic disease, (3) normal consistency of pancreas, (4) diameter of main pancreatic duct <3 mm, (5) duration of operation >6 hours, and (6) a center effect. Significantly more IAC, pancreatoenteric fistula, and deaths occurred in one center (that included the most patients) (P = .05), but there were significantly more high-risk patients in this center (normal pancreas consistency, extrapancreatic pathology, small pancreatic duct, higher transfusion requirements, and duration of operation >6 hours) compared with the other centers. In multivariate analysis, the center effect disappeared. Independent risk factors included duration of operation >6 hours for IAC and for pancreatoenteric fistula (P = .01), extrapancreatic disease for pancreatoenteric fistulas (P < .04), and age > or =70 years for mortality (P < .02).\nThe type of pancreatoenteric anastomosis (PJ or PG) after PD does not significantly influence the rate of patients with one or more IAC and/or pancreatic fistula or the severity of complications.", "title": "A controlled randomized multicenter trial of pancreatogastrostomy or pancreatojejunostomy after pancreatoduodenectomy.", "date": "2005-05-25"}, {"article_id": "23643139", "content": "Postoperative pancreatic fistula is the leading cause of death and morbidity after pancreaticoduodenectomy. However, the best reconstruction method to reduce occurrence of fistula is debated. We did a multicentre, randomised superiority trial to compare the outcomes of different reconstructive techniques in patients undergoing pancreaticoduodenectomy for pancreatic or periampullary tumours.\nPatients aged 18-85 years with confirmed or suspected neoplasms of the pancreas, distal bile duct, ampulla vateri, duodenum, or periampullary tumours were eligible for inclusion. An internet-based platform was used to randomly assign patients to either pancreaticojejunostomy or pancreaticogastrostomy as reconstruction after pancreaticoduodenectomy, using permuted blocks with six patients per block. Within each centre the randomisation was stratified on the pancreatic duct diameter (\u22643 mm vs >3 mm) measured at the time of surgery. The primary endpoint was the occurrence of clinical postoperative pancreatic fistula (grade B or C) as defined by the International Study Group on Pancreatic Fistula. The study was not masked and analyses were done by intention to treat. Patient follow-up was closed 2 months after discharge from the hospital. This study is registered with ClinicalTrials.gov, number NCT00830778.\nBetween June, 2009, and August, 2012, we randomly allocated 167 patients to receive pancreaticojejunostomy and 162 to receive pancreaticogastrostomy. 33 (19.8%) patients in the pancreaticojejunostomy group and 13 (8.0%) in the pancreaticogastrostomy group had clinical postoperative pancreatic fistula (OR 2.86, 95% CI 1.38-6.17; p=0.002). The overall incidence of postoperative complications did not differ significantly between the groups (99 in the pancreaticojejunostomy group vs 100 in the pancreaticogastrostomy group), although more events in the pancreaticojejunostomy group were of grade \u22653a than in the pancreaticogastrostomy group (39 vs 35).\nIn patients undergoing pancreaticoduodenectomy for pancreatic head or periampullary tumours, pancreaticogastrostomy is more efficient than pancreaticojejunostomy in reducing the incidence of postoperative pancreatic fistula.\nFunding Johnson & Johnson Medical Devices, Belgium.", "title": "Pancreaticojejunostomy versus pancreaticogastrostomy reconstruction after pancreaticoduodenectomy for pancreatic or periampullary tumours: a multicentre randomised trial.", "date": "2013-05-07"}, {"article_id": "22744638", "content": "The aim of this single-center randomized trial was to compare the perioperative outcome of pancreatoduodenectomy with pancreatogastrostomy (PG) vs pancreaticojejunostomy (PJ).\nRandomization was done intraoperatively. PG was performed via anterior and posterior gastrotomy with pursestring and inverting seromuscular suture; control intervention was PJ with duct-mucosa anastomosis. The primary endpoint was postoperative pancreatic fistula (POPF).\nFrom 2006 to 2011, n\u2009=\u2009268 patients were screened and n\u2009=\u2009116 were randomized to n\u2009=\u200959 PG and n\u2009=\u200957 PJ. There was no statistically significant difference regarding the primary endpoint (PG vs PJ, 10 % vs 12 %, p\u2009=\u20090.775). The subgroup of high-risk patients with a soft pancreas had a non-significantly lower pancreatic fistula rate with PG (PG vs PJ, 14 vs 24 %, p\u2009=\u20090.352). Analysis of secondary endpoints demonstrated a shorter operation time (404 vs 443 min, p\u2009=\u20090.005) and reduced hospital stay for PG (15 vs 17 days, p\u2009=\u20090.155). Delayed gastric emptying (DGE; PG vs PJ, 27 vs 17 %, p\u2009=\u20090.246) and intraluminal bleeding (PG vs PJ, 7 vs 2 %, p\u2009=\u20090.364) were more frequent with PG. Mortality was low in both groups (<2 %).\nOur randomized controlled trial shows no difference between PG and PJ as reconstruction techniques after partial pancreatoduodenectomy. POPF rate, DGE, and bleeding were not statistically different. Operation time was significantly shorter in the PG group.", "title": "Randomized controlled single-center trial comparing pancreatogastrostomy versus pancreaticojejunostomy after partial pancreatoduodenectomy.", "date": "2012-06-30"}, {"article_id": "16327486", "content": "To compare the results of pancreaticogastrostomy versus pancreaticojejunostomy following pancreaticoduodenectomy in a prospective and randomized setting.\nWhile several techniques have been proposed for reconstructing pancreatico-digestive continuity, only a limited number of randomized studies have been carried out.\nA total of 151 patients undergoing pancreaticoduodenectomy with soft residual tissue were randomized to receive either pancreaticogastrostomy (group PG) or end-to-side pancreaticojejunostomy (group PJ).\nThe 2 treatment groups showed no differences in vital statistics or underlying disease, mean duration of surgery, and need for intraoperative blood transfusion. Overall, the incidence of surgical complications was 34% (29% in PG, 39% in PJ, P = not significant). Patients receiving PG showed a significantly lower rate of multiple surgical complications (P = 0.002). Pancreatic fistula was the most frequent complication, occurring in 14.5% of patients (13% in PG and 16% in PJ, P = not significant). Five patients in each treatment arm required a second surgical intervention; the postoperative mortality rate was 0.6%. PG was favored over PJ due to significant differences in postoperative collections (P = 0.01), delayed gastric emptying (P = 0.03), and biliary fistula (P = 0.01). The mean postoperative hospitalization period stay was comparable in both groups.\nWhen compared with PJ, PG did not show any significant differences in the overall postoperative complication rate or incidence of pancreatic fistula. However, biliary fistula, postoperative collections and delayed gastric emptying are significantly reduced in patients treated by PG. In addition, pancreaticogastrostomy is associated with a significantly lower frequency of multiple surgical complications.", "title": "Reconstruction by pancreaticojejunostomy versus pancreaticogastrostomy following pancreatectomy: results of a comparative study.", "date": "2005-12-06"}, {"article_id": "24467711", "content": "The optimal strategy for the reconstruction of the pancreas following pancreaticoduodenectomy (PD) is still debated. The aim of this study was to compare the outcomes of isolated Roux loop pancreaticojejunostomy (IRPJ) with those of pancreaticogastrostomy (PG) after PD.\nConsecutive patients submitted to PD were randomized to either method of reconstruction. The primary outcome measure was the rate of postoperative pancreatic fistula (POPF). Secondary outcomes included operative time, day to resumption of oral feeding, postoperative morbidity and mortality, and exocrine and endocrine pancreatic functions.\nNinety patients treated by PD were included in the study. The median total operative time was significantly longer in the IRPJ group (320\u2009min versus 300\u2009min; P = 0.047). Postoperative pancreatic fistula developed in nine of 45 patients in the IRPJ group and 10 of 45 patients in the PG group (P = 0.796). Seven IRPJ patients and four PG patients had POPF of type B or C (P = 0.710). Time to resumption of oral feeding was shorter in the IRPJ group (P = 0.03). Steatorrhea at 1 year was reported in nine of 42 IRPJ patients and 18 of 41 PG patients (P = 0.029). Albumin levels at 1 year were 3.6\u2009g/dl in the IRPJ group and 3.3\u2009g/dl in the PG group (P = 0.001).\nIsolated Roux loop PJ was not associated with a lower rate of POPF, but was associated with a decrease in the incidence of postoperative steatorrhea. The technique allowed for early oral feeding and the maintenance of oral feeding even if POPF developed.", "title": "Isolated Roux loop pancreaticojejunostomy versus pancreaticogastrostomy after pancreaticoduodenectomy: a prospective randomized study.", "date": "2014-01-29"}, {"article_id": "25799130", "content": "It has been suggested that pancreaticogastrostomy (PG) is a safer reconstruction than pancreaticojejunostomy (PJ), resulting in lower morbidity, including lower pancreatic leak rates and decreased postoperative mortality. We compared PJ and PG after pancreaticoduodenectomy (PD).\nA randomized clinical trial was designed. It was stopped with 50% accrual. Patients underwent either PG or PJ reconstruction. The primary outcome was the pancreatic fistula rate, and the secondary outcomes were overall morbidity and mortality. We used the Student t, Mann-Whitney U and \u03c7(2) tests for intention to treat analysis. The effect of randomization, American Society of Anesthesiologists score, soft pancreatic texture and use of pancreatic stent on overall complications and fistula rates was calculated using logistic regression.\nOur trial included 98 patients. The rate of pancreatic fistula formation was 18% in the PJ and 25% in the PG groups (p = 0.40). Postoperative complications occurred in 48% of patients in the PJ and 58% in the PG groups (p = 0.31). There were no significant predictors of overall complications in the multivariate analysis. Only soft pancreatic gland predicted the occurrence of pancreatic fistula (odds ratio 5.89, p = 0.003).\nThere was no difference in the rates of pancreatic leak/fistula, overall complications or mortality between patients undergoing PG and and those undergoing PJ after PD.\nSelon certains, la pancr\u00e9atogastrostomie (PG) est une technique de reconstruction plus s\u00e9curitaire que la pancr\u00e9atoj\u00e9junostomie (PJ) et entra\u00eene une morbidit\u00e9 moindre, y compris un taux moins \u00e9lev\u00e9 de fuites pancr\u00e9atiques et une mortalit\u00e9 postop\u00e9ratoire diminu\u00e9e. Nous avons compar\u00e9 la PJ et la PG post-pancr\u00e9atoduod\u00e9nectomie.\nUn essai clinique randomis\u00e9 a \u00e9t\u00e9 con\u00e7u et cess\u00e9 \u00e0 l\u2019atteinte d\u2019un taux de participation de 50 %. Les patients ont subi une reconstruction par PG ou par PJ. Le param\u00e8tre principal \u00e9tait le taux de fistules pancr\u00e9atiques et les param\u00e8tres secondaires \u00e9taient la morbidit\u00e9 et la mortalit\u00e9 globales. Nous avons utilis\u00e9 les tests \nNotre essai a regroup\u00e9 98 patients. Le taux de fistules pancr\u00e9atiques a \u00e9t\u00e9 de 18 % dans le groupe soumis \u00e0 la PJ et de 25 % dans le groupe soumis \u00e0 la PG (\nNous n\u2019avons not\u00e9 aucune diff\u00e9rence quant aux taux de fuites ou de fistules pancr\u00e9atiques, de complications globales ou de mortalit\u00e9 entre les patients soumis \u00e0 la PG et \u00e0 la PJ post-pancr\u00e9atoduod\u00e9nectomie.", "title": "In search of the best reconstructive technique after pancreaticoduodenectomy: pancreaticojejunostomy versus pancreaticogastrostomy.", "date": "2015-03-24"}]}
{"original_review": "36999589", "question_data": [{"question_id": 10, "question": "Is the breast cancer detection rate in women with dense breasts higher, lower, or the same when comparing screening with a combination of mammography and ultrasonography to screening with mammography alone?", "answer": "higher", "evidence_quality": "high", "fulltext_required": "no", "comment": "Starikov/26549432 is not helpful but weight is <20%", "relevant_sources": ["34406400", "29571797", "23116728", "26549432"]}, {"question_id": 11, "question": "Is the frequency of interval carcinoma occurence higher, lower, or the same when comparing screening with a combination of mammography and ultrasonography to screening with mammography alone?", "answer": "lower", "evidence_quality": "high", "fulltext_required": "no", "comment": "model should basically strictly use J-START/34406400", "relevant_sources": ["34406400", "30882843"]}, {"question_id": 12, "question": "Is the breast cancer detection rate higher, lower, or the same when comparing screening with a combination of mammography and ultrasonography to screening with mammography alone?", "answer": "higher", "evidence_quality": "high", "fulltext_required": "no", "comment": "model should basically strictly use J-START/34406400", "relevant_sources": ["34406400", "29571797", "26549432", "23465737", "30882843"]}], "sources": [{"article_id": "21131295", "content": "In cancer screening, it is essential to undertake effective screening with appropriate methodology, which should be supported by evidence of a reduced mortality rate. At present, mammography is the only method for breast cancer screening with such evidence. However, mammography does not achieve sufficient accuracy in breasts with high density at ages below 50. Although ultrasonography achieves better accuracy in Breast Cancer detection even in dense breasts, the effectiveness has not been verified. We have planned a randomized controlled trial to assess the effectiveness of ultrasonography in women aged 40-49, with a design to study 50,000 women with mammography and ultrasonography (intervention group), and 50,000 controls with mammography only (control group). The participants are scheduled to take second round screening with the same modality 2 years on. The primary endpoints are sensitivity and specificity, and the secondary endpoint is the rate of advanced breast cancers.", "title": "Randomized controlled trial on effectiveness of ultrasonography screening for breast cancer in women aged 40-49 (J-START): research design.", "date": "2010-12-07"}, {"article_id": "23116728", "content": "Automated breast ultrasound (ABUS)was performed in 3418 asymptomatic women with mammographically dense breasts. The addition of ABUS to mammography in women with greater than 50% breast density resulted in the detection of 12.3 per 1,000 breast cancers, compared to 4.6 per 1,000 by mammography alone. The mean tumor size was 14.3 mm and overall attributable risk of breast cancer was 19.92 (95% confidence level, 16.75 - 23.61) in our screened population. These preliminary results may justify the cost-benefit of implementing the judicious us of ABUS in conjunction with mammography in the dense breast screening population.", "title": "Improved breast cancer detection in asymptomatic women using 3D-automated breast ultrasound in mammographically dense breasts.", "date": "2012-11-03"}, {"article_id": "26547101", "content": "Mammography is the only proven method for breast cancer screening that reduces mortality, although it is inaccurate in young women or women with dense breasts. We investigated the efficacy of adjunctive ultrasonography.\nBetween July, 2007, and March, 2011, we enrolled asymptomatic women aged 40-49 years at 42 study sites in 23 prefectures into the Japan Strategic Anti-cancer Randomized Trial (J-START). Eligible women had no history of any cancer in the previous 5 years and were expected to live for more than 5 years. Randomisation was done centrally by the Japan Clinical Research Support Unit. Participants were randomly assigned in 1:1 ratio to undergo mammography and ultrasonography (intervention group) or mammography alone (control group) twice in 2 years. The primary outcome was sensitivity, specificity, cancer detection rate, and stage distribution at the first round of screening. Analysis was by intention to treat. This study is registered, number UMIN000000757.\nOf 72,998 women enrolled, 36,859 were assigned to the intervention group and 36,139 to the control group. Sensitivity was significantly higher in the intervention group than in the control group (91\u00b71%, 95% CI 87\u00b72-95\u00b70 vs 77\u00b70%, 70\u00b73-83\u00b77; p=0\u00b70004), whereas specificity was significantly lower (87\u00b77%, 87\u00b73-88\u00b70 vs 91\u00b74%, 91\u00b71-91\u00b77; p<0\u00b70001). More cancers were detected in the intervention group than in the control group (184 [0\u00b750%] vs 117 [0\u00b732%], p=0\u00b70003) and were more frequently stage 0 and I (144 [71\u00b73%] vs 79 [52\u00b70%], p=0\u00b70194). 18 (0\u00b705%) interval cancers were detected in the intervention group compared with 35 (0\u00b710%) in the control group (p=0\u00b7034).\nAdjunctive ultrasonography increases sensitivity and detection rate of early cancers.\nMinistry of Health, Labour and Welfare of Japan.", "title": "Sensitivity and specificity of mammography and adjunctive ultrasonography to screen for breast cancer in the Japan Strategic Anti-cancer Randomized Trial (J-START): a randomised controlled trial.", "date": "2015-11-09"}, {"article_id": "30882843", "content": "Whole-breast ultrasonography has been advocated to supplement screening mammography to improve outcomes in women with dense breasts.\nTo determine the performance of screening mammography plus screening ultrasonography compared with screening mammography alone in community practice.\nObservational cohort study. Two Breast Cancer Surveillance Consortium registries provided prospectively collected data on screening mammography with vs without same-day breast ultrasonography from January 1, 2000, to December 31, 2013. The dates of analysis were March 2014 to December 2018. A total of 6081 screening mammography plus same-day screening ultrasonography examinations in 3386 women were propensity score matched 1:5 to 30\u202f062 screening mammograms without screening ultrasonography in 15\u202f176 women from a sample of 113\u202f293 mammograms. Exclusion criteria included a personal history of breast cancer and self-reported breast symptoms.\nScreening mammography with vs without screening ultrasonography.\nCancer detection rate and rates of interval cancer, false-positive biopsy recommendation, short-interval follow-up, and positive predictive value of biopsy recommendation were estimated and compared using log binomial regression.\nScreening mammography with vs without ultrasonography examinations was performed more often in women with dense breasts (74.3% [n\u2009=\u20094317 of 5810] vs 35.9% [n\u2009=\u200939\u202f928 of 111\u202f306] in the overall sample), in women who were younger than 50 years (49.7% [n\u2009=\u20093022 of 6081] vs 31.7% [n\u2009=\u200916\u202f897 of 112\u202f462]), and in women with a family history of breast cancer (42.9% [n\u2009=\u20092595 of 6055] vs 15.0% [n\u2009=\u200916\u202f897 of 112\u202f462]). While 21.4% (n\u2009=\u20091154 of 5392) of screening ultrasonography examinations were performed in women with high or very high (\u22652.50%) Breast Cancer Surveillance Consortium 5-year risk scores, 53.6% (n\u2009=\u20092889 of 5392) had low or average (<1.67%) risk. Comparing mammography plus ultrasonography with mammography alone, the cancer detection rate was similar at 5.4 vs 5.5 per 1000 screens (adjusted relative risk [RR], 1.14; 95% CI, 0.76-1.68), as were interval cancer rates at 1.5 vs 1.9 per 1000 screens (RR, 0.67; 95% CI, 0.33-1.37). The false-positive biopsy rates were significantly higher at 52.0 vs 22.2 per 1000 screens (RR, 2.23; 95% CI, 1.93-2.58), as was short-interval follow-up at 3.9% vs 1.1% (RR, 3.10; 95% CI, 2.60-3.70). The positive predictive value of biopsy recommendation was significantly lower at 9.5% vs 21.4% (RR, 0.50; 95% CI, 0.35-0.71).\nIn a relatively young population of women at low, intermediate, and high breast cancer risk, these results suggest that the benefits of supplemental ultrasonography screening may not outweigh associated harms.", "title": "Performance of Screening Ultrasonography as an Adjunct to Screening Mammography in Women Across the Spectrum of Breast Cancer Risk.", "date": "2019-03-19"}, {"article_id": "29571797", "content": "To compare the performance of screening with mammography combined with ultrasound versus mammography alone in women at average risk for breast cancer.\n66,680 women underwent physician-performed ultrasound as an adjunct to screening mammography. Histological results and follow-up at one year were used as reference standard for sensitivity. Main outcome measures were cancer detection rate, sensitivity, recall rate, biopsy rate, and positive predictive value of biopsy for combined screening with mammography plus ultrasound versus mammography alone.\nThe overall sensitivity of mammography only was 61.5% in women with dense breasts and 86.6% in women with non-dense breasts. The sensitivity of mammography plus ultrasound combined was 81.3% in women with dense breasts and 95.0% in women with non-dense breasts. Adjunctive ultrasound increased the recall rate from 10.5 to 16.5 per 1000 women screened, and increased the biopsy rate from 6.3 to 9.3 per 1000 women screened. The positive predictive value of biopsy was 55.5% (95% CI 50.6%-60.3%) for mammography alone and 43.3 (95% CI 39.4%-47.3%) for combined mammography plus ultrasound.\nSupplemental ultrasound improves cancer detection in screening of women at average risk for breast cancer. Recall rates and biopsy rates can be kept within acceptable limits.", "title": "Combined screening with mammography and ultrasound in a population-based screening program.", "date": "2018-03-25"}, {"article_id": "26549432", "content": "To determine which modalities [2D mammography (2D), digital breast tomosynthesis (DBT), whole breast sonography (WBS)] are optimal for screening depending on breast density.\nInstitutional retrospective cohort study of 2013 screening mammograms (16,789), sorted by modalities and density.\nCancer detection is increased by adding WBS to 2D (P=.02) for the overall study population. Recall rate was lowest with 2D+DBT (10.2%, P<.001) and highest with 2D+DBT+WBS (23.6%, P<.001) for the overall study population as well.\nWomen with dense and nondense breasts benefit from reduced recall rate with the addition of DBT; however, this benefit is negated with the addition of WBS.", "title": "2D mammography, digital breast tomosynthesis, and ultrasound: which should be used for the different breast densities in breast cancer screening?", "date": "2015-11-10"}, {"article_id": "34406400", "content": "Mammography has limited accuracy in breast cancer screening. Ultrasonography, when used in conjunction with mammography screening, is helpful to detect early-stage and invasive cancers for asymptomatic women with dense and nondense breasts.\nTo evaluate the performance of adjunctive ultrasonography with mammography for breast cancer screening, according to differences in breast density.\nThis study is a secondary analysis of the Japan Strategic Anti-cancer Randomized Trial. Between July 2007 and March 2011, asymptomatic women aged 40 to 49 years were enrolled in Japan. The present study used data from cases enrolled from the screening center in Miyagi prefecture during 2007 to 2020. Participants were randomly assigned in a 1:1 ratio to undergo either mammography with ultrasonography (intervention group) or mammography alone (control group). Data analysis was performed from February to March 2020.\nUltrasonography adjunctive to mammography for breast cancer screening regardless of breast density.\nSensitivity, specificity, recall rates, biopsy rates, and characteristics of screen-detected cancers and interval breast cancers were evaluated between study groups and for each modality according to breast density.\nA total of 76\u202f119 women were enrolled, and data for 19\u202f213 women (mean [SD] age, 44.5 [2.8] years) from the Miyagi prefecture were analyzed; 9705 were randomized to the intervention group and 9508 were randomized to the control group. A total of 11\u202f390 women (59.3%) had heterogeneously or extremely dense breasts. Among the overall group, 130 cancers were found. Sensitivity was significantly higher in the intervention group than the control group (93.2% [95% CI, 87.4%-99.0%] vs 66.7% [95% CI, 54.4%-78.9%]; P\u2009<\u2009.001). Similar trends were observed in women with dense breasts (sensitivity in intervention vs control groups, 93.2% [95% CI, 85.7%-100.0%] vs 70.6% [95% CI, 55.3%-85.9%]; P\u2009<\u2009.001) and nondense breasts (sensitivity in intervention vs control groups, 93.1% [95% CI, 83.9%-102.3%] vs 60.9% [95% CI, 40.9%-80.8%]; P\u2009<\u2009.001). The rate of interval cancers per 1000 screenings was lower in the intervention group compared with the control group (0.5 cancers [95% CI, 0.1-1.0 cancers] vs 2.0 cancers [95% CI, 1.1-2.9 cancers]; P\u2009=\u2009.004). Within the intervention group, the rate of invasive cancers detected by ultrasonography alone was significantly higher than that for mammography alone in both dense (82.4% [95% CI, 56.6%-96.2%] vs 41.7% [95% CI, 15.2%-72.3%]; P\u2009=\u2009.02) and nondense (85.7% [95% CI, 42.1%-99.6%] vs 25.0% [95% CI, 5.5%-57.2%]; P\u2009=\u2009.02) breasts. However, sensitivity of mammography or ultrasonography alone did not exceed 80% across all breast densities in the 2 groups. Compared with the control group, specificity was significantly lower in the intervention group (91.8% [95% CI, 91.2%-92.3%] vs 86.8% [95% CI, 86.2%-87.5%]; P\u2009<\u2009.001). Recall rates (13.8% [95% CI, 13.1%-14.5%] vs 8.6% [95% CI, 8.0%-9.1%]; P\u2009<\u2009.001) and biopsy rates (5.5% [95% CI, 5.1%-6.0%] vs 2.1% [95% CI, 1.8%-2.4%]; P\u2009<\u2009.001) were significantly higher in the intervention group than the control group.\nIn this secondary analysis of a randomized clinical trial, screening mammography alone demonstrated low sensitivity, whereas adjunctive ultrasonography was associated with increased sensitivity. These findings suggest that adjunctive ultrasonography has the potential to improve detection of early-stage and invasive cancers across both dense and nondense breasts. Supplemental ultrasonography should be considered as an appropriate imaging modality for breast cancer screening in asymptomatic women aged 40 to 49 years regardless of breast density.\nNIPH Clinical Trial Identifier: UMIN000000757.", "title": "Evaluation of Adjunctive Ultrasonography for Breast Cancer Detection Among Women Aged 40-49 Years With Varying Breast Density Undergoing Screening Mammography: A Secondary Analysis of a Randomized Clinical Trial.", "date": "2021-08-19"}, {"article_id": "38424893", "content": "To assess prospectively the interpretative performance of automated breast ultrasound (ABUS) as a supplemental screening after digital breast tomosynthesis (DBT) or as a standalone screening of women with dense breast tissue.\nUnder an IRB-approved protocol (written consent required), women with dense breasts prospectively underwent concurrent baseline DBT and ABUS screening. Examinations were independently evaluated, in opposite order, by two of seven Mammography Quality Standards Act-qualified radiologists, with the primary radiologist arbitrating disagreements and making clinical management recommendations. We report results for 1111 screening examinations (598 first year and 513 second year) for which all diagnostic workups are complete. Imaging was also retrospectively reviewed for all cancers. Statistical assessments used a 0.05 significance level and accounted for correlation between participants' examinations.\nOf 1111 women screened, primary radiologists initially \"recalled\" based on DBT alone (6.6%, 73/1111, CI: 5.2%-8.2%), of which 20 were biopsied, yielding 6/8 total cancers. Automated breast ultrasound increased recalls overall to 14.4% (160/1111, CI: 12.4%-16.6%), with 27 total biopsies, yielding 1 additional cancer. Double reading of DBT alone increased the recall rate to 10.7% (119/1111), with 21 biopsies, with no improvement in cancer detection. Double reading ABUS increased the recall rate to 15.2% (169/1111, CI: 13.2%-17.5%) of women, of whom 22 were biopsied, yielding the detection of 7 cancers, including one seen only on double reading ABUS. Inter-radiologist agreement was similar for recall recommendations from DBT (\u03ba = 0.24, CI: 0.14-0.34) and ABUS (\u03ba = 0.23, CI: 0.15-0.32). Integrated assessments from both readers resulted in a recall rate of 15.1% (168/1111, CI: 13.1%-17.4%).\nSupplemental or standalone ABUS screening detected cancers not seen on DBT, but substantially increased noncancer recall rates.", "title": "A Prospective Study of Automated Breast Ultrasound Screening of Women with Dense Breasts in a Digital Breast Tomosynthesis-based Practice.", "date": "2020-03-25"}, {"article_id": "23465737", "content": "To determine whether adding screening ultrasonography to screening mammography can reduce patient recall rates and increase cancer detection rates.\nWe analyzed the results of mammography and ultrasonography breast screenings performed at the Total Health Evaluation Center Tsukuba, Japan, between April 2011 and March 2012. We also reviewed the modalities and results of diagnostic examinations from women with mammographic abnormalities who visited the Tsukuba Medical Center Hospital for further testing.\nOf 11,753 women screened, cancer was diagnosed in 10 (0.22%) of the 4529 participants who underwent mammography alone, 23 (0.37%) of the 6250 participants who underwent ultrasonography alone, and 5 (0.51%) of the 974 participants who underwent mammography and ultrasonography. The recall rate due to mammographic abnormalities was 4.9% for women screened only with mammography and 2.6% for those screened with both modalities. The cancer detection rate was 0.22% for women screened only with mammography (positive predictive value, 4.5%) and 0.31% for those screened with both modalities (positive predictive value, 12.0%). Of the 211 lesions presenting as mammographic abnormalities investigated further, diagnostic ultrasonography found no abnormalities in 63 (29.9%) and benign findings in 69 (33.7%). The rest 36.4% needed mammography, cytological or histological examinations or follow-up in addition to diagnostic ultrasonography.\nIt is possible to reduce the recall rate in screening mammography by combining mammography and ultrasonography for breast screening.", "title": "Effect of adding screening ultrasonography to screening mammography on patient recall and cancer detection rates: a retrospective study in Japan.", "date": "2013-03-08"}, {"article_id": "23980217", "content": "The purpose of this study was to evaluate the use and performance of supplemental screening whole-breast sonography in conjunction with mammography in asymptomatic women with dense breast tissue.\nA total of 28,796 asymptomatic women underwent screening mammography. Among 20,864 women with dense breasts (72%), 8359 underwent additional sonography as part of their screening examinations. We classified women with mammographically dense breasts into mammography-only and mammography-plus-sonography groups. The reference standard was a combination of pathologic results and clinical follow-up at 2 years. We compared the recall rate, cancer detection yield, sensitivity, specificity, and positive predictive value in each group.\nAmong the 20,864 women with dense breasts, 35 cancers were diagnosed, with a mean size of 13 mm. The cancer detection yield was 0.480 per 1000 women in the mammography-only group and increased to 2.871 in the mammography-plus-sonography group. Of 24 cancers detected in the mammography-plus-sonography group, the mean size was 11 mm, and the axillary lymph nodes were negative in 19 of 20. The sensitivity was significantly higher in the mammography-plus-sonography group than the mammography-only group (100% versus 54.55%; P = .002). The positive predictive values of sonographically prompted biopsy were 11.1% for the mammography-plus-sonography group and 50% for the mammography-only group.\nSupplemental screening whole-breast sonography increases the cancer detection yield by 2.391 cancers per 1000 women with dense breast tissue over that of mammography alone. It is beneficial for increased detection of breast cancers that are predominantly small and node negative; however, it also raises the number of false-positive results.", "title": "Evaluation of screening whole-breast sonography as a supplemental tool in conjunction with mammography in women with dense breasts.", "date": "2013-08-28"}]}
{"original_review": "29489032", "question_data": [{"question_id": 13, "question": "Is food intake higher, lower, or the same when comparing grehlin to placebo?", "answer": "higher", "evidence_quality": "very low", "fulltext_required": "no", "comment": "", "relevant_sources": ["15181065"]}], "sources": [{"article_id": "20186829", "content": "The short-term provision of ghrelin to patients with cancer indicates that there may be benefits from long-term provision of ghrelin for the palliative treatment of weight-losing cancer patients. This hypothesis was evaluated in a randomized, double-blind, phase 2 study.\nWeight-losing cancer patients with solid gastrointestinal tumors were randomized to receive either high-dose ghrelin treatment (13 microg/kg daily; n = 17 patients) or low-dose ghrelin treatment (0.7 microg/kg daily; n = 14 patients) for 8 weeks as a once-daily, subcutaneous injections. Appetite was scored on a visual analog scale; and food intake, resting energy expenditure, and body composition (dual x-ray absorpitometry) were measured before the start of treatment and during follow-up. Serum levels of ghrelin, insulin, insulin-like growth factor 1, growth hormone (GH), triglycerides, free fatty acids, and glucose were measured. Health-related quality of life, anxiety, and depression were assessed by using standardized methods (the 36-item Short Form Health Survey and the Hospital Anxiety and Depression Scale). Physical activity, rest, and sleep were measured by using a multisensor body monitor.\nTreatment groups were comparable at inclusion. Appetite scores were increased significantly by high-dose ghrelin analyzed both on an intent-to-treat basis and according to the protocol. High-dose ghrelin reduced the loss of whole body fat (P < .04) and serum GH (P < .05). There was a trend for high-dose ghrelin to improve energy balance (P < .07; per protocol). Otherwise, no statistically significant differences in outcome variables were observed between the high-dose and low-dose groups. Adverse effects were not observed by high-dose ghrelin, such as serum levels of tumor markers (cancer antigen 125 [CA 125], carcinoembryonic antigen, and CA 19-9).\nThe current results suggested that daily, long-term provision of ghrelin to weight-losing cancer patients with solid tumors supports host metabolism, improves appetite, and attenuates catabolism.", "title": "Effects by daily long term provision of ghrelin to unselected weight-losing cancer patients: a randomized double-blind study.", "date": "2010-02-27"}, {"article_id": "15181065", "content": "There is a pressing need for more effective appetite-stimulatory therapies for many patient groups including those with cancer. We have previously demonstrated that the gastric hormone ghrelin potently enhances appetite in healthy volunteers. Here, we performed an acute, randomized, placebo-controlled, cross-over clinical trial to determine whether ghrelin stimulates appetite in cancer patients with anorexia. Seven cancer patients who reported loss of appetite were recruited from oncology clinics at Charing Cross Hospital. The main outcome measures were energy intake from a buffet meal during ghrelin or saline infusion and meal appreciation as assessed by visual analog scale. A marked increase in energy intake (31 +/- 7%; P = 0.005) was observed with ghrelin infusion compared with saline control, and every patient ate more. The meal appreciation score was greater by 28 +/- 8% (P = 0.02) with ghrelin treatment. No side effects were observed. The stimulatory effects of ghrelin on food intake and meal appreciation seen in this preliminary study suggest that ghrelin could be an effective treatment for cancer anorexia and possibly for appetite loss in other patient groups.", "title": "Ghrelin increases energy intake in cancer patients with impaired appetite: acute, randomized, placebo-controlled trial.", "date": "2004-06-08"}, {"article_id": "18182992", "content": "Twenty-one adult patients were randomised to receive ghrelin on days 1 and 8 and placebo on days 4 and 11 or vice versa, given intravenously over a 60-min period before lunch: 10 received 2 microg kg(-1) (lower-dose) ghrelin; 11 received 8 microg kg(-1) (upper-dose) ghrelin. Active and total ghrelin, growth hormone (GH), and insulin-like growth factor 1 levels were monitored at baseline (4-5 days before day 1), during treatment days, and at end of study (day 17/18). Drug-related adverse events (assessed by NCI-CTC-toxicity criteria and cardiac examination) did not differ between ghrelin and placebo. No grade 3/4 toxicity or stimulation of tumour growth was observed. The peak increase of GH, a biological marker of ghrelin action, was 25 ng ml(-1) with lower-dose and 42 ng ml(-1) with upper-dose ghrelin. Morning fasting total ghrelin levels were higher (P<0.05) for upper-dose patients at end of study (3580 pg ml(-1)) than at baseline (990 pg ml(-1)). Insulin-like growth factor 1 levels did not change. At day 8, 81% of patients preferred ghrelin to placebo as against 63% at the end of study. Nutritional intake and eating-related symptoms, measured to explore preliminary efficacy, did not differ between ghrelin and placebo. Ghrelin is well tolerated and safe in patients with advanced cancer. For safety, tolerance, and patients' preference for treatment, no difference was observed between the lower- and upper-dose group.", "title": "Safety, tolerability and pharmacokinetics of intravenous ghrelin for cancer-related anorexia/cachexia: a randomised, placebo-controlled, double-blind, double-crossover study.", "date": "2008-01-10"}]}
{"original_review": "33215474", "question_data": [{"question_id": 14, "question": "Is length gain higher, lower, or the same when comparing moderate protein concentration to low protein concentration?", "answer": "higher", "evidence_quality": "low", "fulltext_required": "no", "comment": "", "relevant_sources": ["27801753", "10838460", "8906139"]}, {"question_id": 15, "question": "Is head circumference gain higher, lower, or the same when comparing moderate protein concentration to low protein concentration?", "answer": "higher", "evidence_quality": "very low", "fulltext_required": "no", "comment": "", "relevant_sources": ["27801753", "10838460", "8906139"]}, {"question_id": 16, "question": "Is weight gain higher, lower, or the same when comparing moderate protein concentration to low protein concentration?", "answer": "higher", "evidence_quality": "very low", "fulltext_required": "no", "comment": "not sure why Sankaran/8906139 is excluded", "relevant_sources": ["27801753", "10838460"]}, {"question_id": 17, "question": "Is head circumference gain higher, lower, or the same when comparing high protein concentration low protein concentration?", "answer": "uncertain effect", "evidence_quality": "very low", "fulltext_required": "no", "comment": "", "relevant_sources": ["26488118", "22301933", "22987877", "29772833", "28727654"]}, {"question_id": 18, "question": "Is length gain higher, lower, or the same when comparing high protein concentration low protein concentration?", "answer": "uncertain effect", "evidence_quality": "very low", "fulltext_required": "no", "comment": "", "relevant_sources": ["26488118", "22301933", "22987877", "29772833", "28727654"]}], "sources": [{"article_id": "27801753", "content": "The aim of the study was to determine whether higher enteral protein intake leads to improved head growth at 40 weeks postmenstrual age (PMA) in preterm infants <32 weeks or 1500 g.\nRandomized controlled trial in which 120 infants were assigned to either group A with higher enteral protein intake achieved by fortification with higher protein containing fortifier (1 g/100 mL expressed breast milk) or to group B with lower enteral protein intake where fortification was done with standard available protein fortifier (0.4 g /100 mL expressed breast milk).\nThe mean (standard deviation) protein intake was higher in group A as compared to group B; 4.2 (0.47) compared with 3.6 (0.37) g\u200a\u00b7\u200akg\u200a\u00b7\u200aday, P\u200a<\u200a0.001. At 40 weeks PMA, the mean (standard deviation) weekly occipitofrontal circumference gain was significantly higher in group A as compared to group B; 0.66 (0.16) compared with 0.60 (0.15) cm/week (mean difference 0.064, 95% confidence interval [0.004-0.123], [P\u200a=\u200a0.04]). Weight growth velocity in group A was 11.95 (2.2) g\u200a\u00b7\u200akg\u200a\u00b7\u200aday as compared to 10.78 (2.6) g\u200a\u00b7\u200akg\u200a\u00b7\u200aday in group B (mean difference 1.10, 95% confidence interval [0.25-2.07], [P\u200a=\u200a0.01]). No difference was observed in the length between the 2 groups. There was no difference in growth indices and neurodevelopmental outcomes at 12 to 18 months corrected age in the 2 groups.\nFortification of expressed human milk with fortifier containing higher protein results in better head growth and weight gain at 40 weeks PMA in preterm infants <32 weeks or 1500 g without any benefits on long-term growth and neurodevelopment at 12 to 18 months corrected age (CTRI/2014/06/004661).", "title": "Effect of Differential Enteral Protein on Growth and Neurodevelopment in Infants <1500 g: A Randomized Controlled Trial.", "date": "2016-11-02"}, {"article_id": "22987877", "content": "To evaluate the growth, tolerance, and safety of a new ultraconcentrated liquid human milk fortifier (LHMF) designed to provide optimal nutrients for preterm infants receiving human breast milk in a safe, nonpowder formulation.\nPreterm infants with a body weight \u2264 1250 g fed expressed and/or donor breast milk were randomized to receive a control powder human milk fortifier (HMF) or a new LHMF for 28 days. When added to breast milk, the LHMF provided \u223c20% more protein than the control HMF. Weight, length, head circumference, and serum prealbumin, albumin, blood urea nitrogen, electrolytes, and blood gases were measured. The occurrence of sepsis, necrotizing enterocolitis, and serious adverse events were monitored.\nThis multicenter, third party-blinded, randomized controlled, prospective study enrolled 150 infants. Achieved weight and linear growth rate were significantly higher in the LHMF versus control groups (P = .04 and 0.03, respectively). Among infants who adhered closely to the protocol, the LHMF had a significantly higher achieved weight, length, head circumference, and linear growth rate than the control HMF (P = .004, P = .003, P = .04, and P = .01, respectively). There were no differences in measures of feeding tolerance or days to achieve full feeding volumes. Prealbumin, albumin, and blood urea nitrogen were higher in the LHMF group versus the control group (all P < .05). There was no difference in the incidence of confirmed sepsis or necrotizing enterocolitis.\nUse of a new LHMF in preterm infants instead of powder HMF is safe. Benefits of LHMF include improvements in growth and avoidance of the use of powder products in the NICU.", "title": "A new liquid human milk fortifier and linear growth in preterm infants.", "date": "2012-09-19"}, {"article_id": "27893064", "content": "Protein, supplied in currently available commercial fortifiers, may be inadequate to meet the requirements of very preterm infants; in addition, intraindividual and interindividual variability of human milk protein and energy content potentially contribute to unsatisfactory early postnatal growth.\nTo determine effects on growth of different levels of enteral protein supplementation in predominantly human milk-fed preterm infants.\nThis randomized clinical and partially blinded single-center trial was conducted in a neonatal tertiary referral center in Germany. Sixty preterm infants (gestation <32 weeks and weight <1500 g at birth) were recruited from October 2012 to October 2014 and included 35% of 173 eligible infants. Median (interquartile range [IQR]) gestational age at birth was 29.9 (28.7-31.2) weeks. All analyses were conducted in an intention-to-treat population.\nInfants were randomly assigned to either a lower-protein (adding 1 g of bovine protein/100 mL of breast milk through a commercial human milk fortifier; n\u2009=\u200930) or a higher-protein group at a median (IQR) postnatal age of 7 (6-8) days. The higher-protein group (n\u2009=\u200930) received either standardized higher-protein supplementation (study fortifier adding 1.8 g of bovine protein/100 mL of breast milk [n\u2009=\u200915]) or individualized high-protein supplementation based on protein and fat content of administered breast milk (n\u2009=\u200915). Study interventions were continued for a median (IQR) of 41 (30-57) days and until definite discharge planning.\nPrimary outcome was weight gain (g/kg/d) from birth to the end of intervention.\nSixty preterm infants (gestation <32 weeks and weight <1500 g at birth), 33 girls, were recruited from October 2012 to October 2014 and included 35% of 173 eligible infants. Median (IQR) gestational age at birth was 29.9 (28.7-31.2) weeks. Demographic characteristics and hospital courses were similar in both groups, and birth weights ranged from 580 to 1495 g in the lower-protein group and 490 to 1470 g in the higher-protein group. Weight gain was similar in the lower- and higher-protein groups: mean (95% CI), 16.3 g/kg/d (15.4-17.1 g/kg/d) in the lower-protein group vs 16.0 g/kg/d (15.1-16.9 g/kg/d) in the higher-protein group) (P\u2009=\u2009.70), despite an increase in actual protein intake by 0.6 g/kg/d (0.4-0.7 g/kg/d) (P\u2009<\u2009.001). Head circumference and lower leg longitudinal growth were also similar, as was the proportion of cumulative total enteral feeding volume provided as breast milk: median (IQR) proportion of breast milk, 92% (79%-98%) in the lower-protein group vs 94% (62%-99%) in the higher-protein group (P\u2009=\u2009.89).\nAn increase in protein intake by 0.6 g/kg/d to a mean intake of 4.3 g/kg/d did not further enhance growth of very preterm infants with a median birth weight of 1200 g, who achieved near-fetal growth rates. This might point to a ceiling effect for enteral protein intake with respect to its influence on growth.\nclinicaltrials.gov Identifier: NCT01773902.", "title": "Effect of Increased Enteral Protein Intake on Growth in Human Milk-Fed Preterm Infants: A Randomized Clinical Trial.", "date": "2016-11-29"}, {"article_id": "22301933", "content": "Preterm human milk-fed infants often experience suboptimal growth despite the use of human milk fortifier (HMF). The extra protein supplied in fortifiers may be inadequate to meet dietary protein requirements for preterm infants.\nWe assessed the effect of human milk fortified with a higher-protein HMF on growth in preterm infants.\nThis is a randomized controlled trial in 92 preterm infants born at <31 wk gestation who received maternal breast milk that was fortified with HMF containing 1.4 g protein/100 mL (higher-protein group) or 1.0 g protein/100 mL (current practice) until discharge or estimated due date, whichever came first. The HMFs used were isocaloric and differed only in the amount of protein or carbohydrate. Length, weight, and head-circumference gains were assessed over the study duration.\nLength gains did not differ between the higher- and standard-protein groups (mean difference: 0.06 cm/wk; 95% CI: -0.01, 0.12 cm/wk; P = 0.08). Infants in the higher-protein group achieved a greater weight at study end (mean difference: 220 g; 95% CI: 23, 419 g; P = 0.03). Secondary analyses showed a significant reduction in the proportion of infants who were less than the 10th percentile for length at the study end in the higher-protein group (risk difference: 0.186; 95% CI: 0.370, 0.003; P = 0.047).\nA higher protein intake results in less growth faltering in human milk-fed preterm infants. It is possible that a higher-protein fortifier than used in this study is needed. This trial was registered with the Australian New Zealand Clinical Trials Registry (http://www.anzctr.org.au/) as ACTRN12606000525583.", "title": "Effect of increasing protein content of human milk fortifier on growth in preterm infants born at <31 wk gestation: a randomized controlled trial.", "date": "2012-02-04"}, {"article_id": "28727654", "content": "The aim of this study was to assess growth and nutritional biomarkers of preterm infants fed human milk (HM) supplemented with a new powdered HM fortifier (nHMF) or a control HM fortifier (cHMF). The nHMF provides similar energy content, 16% more protein (partially hydrolyzed whey), and higher micronutrient levels than the cHMF, along with medium-chain triglycerides and docosahexaenoic acid.\nIn this controlled, multicenter, double-blind study, a sample of preterm infants \u226432 weeks or \u22641500\u200ag were randomized to receive nHMF (n\u200a=\u200a77) or cHMF (n\u200a=\u200a76) for a minimum of 21 days. Weight gain was evaluated for noninferiority (margin\u200a=\u200a-1\u200ag/day) and superiority (margin\u200a=\u200a0\u200ag/day). Nutritional status and gut inflammation were assessed by blood, urine, and fecal biochemistries. Adverse events were monitored.\nAdjusted mean weight gain (analysis of covariance) was 2.3\u200ag/day greater in nHMF versus cHMF; the lower limit of the 95% CI (0.4\u200ag/day) exceeded both noninferiority (P\u200a<\u200a0.001) and superiority margins (P\u200a=\u200a0.01). Weight gain rate (unadjusted) was 18.3 (nHMF) and 16.8\u200ag\u200a\u00b7\u200akg\u200a\u00b7\u200aday (cHMF) between study days 1 and 21 (D1-D21). Length and head circumference (HC) gains between D1 and D21 were not different. Adjusted weight-for-age z score at D21 and HC-for-age z score at week 40 corrected age were greater in nHMF versus cHMF (P\u200a=\u200a0.013, P\u200a=\u200a0.003 respectively). nHMF had higher serum blood urea nitrogen, pre-albumin, alkaline phosphatase, and calcium (all within normal ranges; all P\u200a\u2264\u200a0.019) at D21 versus cHMF. Both HMFs were well tolerated with similar incidence of gastrointestinal adverse events.\nnHMF providing more protein and fat compared to a control fortifier is safe, well-tolerated, and improves the weight gain of preterm infants.", "title": "Growth and Nutritional Biomarkers of Preterm Infants Fed a New Powdered Human Milk Fortifier: A Randomized Trial.", "date": "2017-07-21"}, {"article_id": "29772833", "content": "The aim of this study was to assess the effect of feeding high protein human milk fortifier (HMF) on growth in preterm infants. In this single-centre randomised trial, 60 infants born 28\u207b32 weeks' gestation were randomised to receive a higher protein HMF providing 1.8 g protein (", "title": "The Effect of Increasing the Protein Content of Human Milk Fortifier to 1.8 g/100 mL on Growth in Preterm Infants: A Randomised Controlled Trial.", "date": "2018-05-19"}, {"article_id": "8906139", "content": "To evaluate the added nutritional value of the two commercially available human breast milk fortifiers: Similac Natural Care (NC) and Enfamil Powder (EP).\nA randomized controlled evaluation in healthy preterm neonates.\nNeonatal Intensive Care Unit, Royal University Hospital, Saskatoon, Saskatchewan, and Neonatal Intensive Care Unit, Jewish General Hospital, Montreal, Quebec, Canada.\nHealthy preterm infants admitted to and cared for in the aforementioned neonatal intensive care units.\nHealthy preterm neonates who were receiving expressed breast milk from their own mothers were supplemented with human milk fortifiers (NC and EP) per manufacturer's recommendations.\nGestational age and birth weight, gender, and race. At entry to and exit from the study, serum concentrations of albumin, protein, calcium, phosphorus, and alkaline phosphatase. The age at which the supplements were added and the number of days the infant remained in the hospital. Daily weight gain, head circumference, length, and height were also measured.\nStudent's t test was used to test the differences between the groups and within the groups at entry to and exit from the study. Fisher's exact test was used to determine differences in race, size, and gestational age in each group. When necessary, a chi 2 test was used to analyze the preponderance of either sex in each group. A Wilcoxon rank test was applied to the true exit date to determine whether the bias was comparable in each group.\nThe mean (+/- standard error) gestational age and birth weight were similar in both groups: 30 +/- 0.3 weeks and 1,314 +/- 40 g, respectively, for NC vs 29.6 +/- 0.35 weeks and 1,262 +/- 45 g, respectively, for EP. At entry to the study, values for the NC group (N = 29) were albumin 31 +/- 1.2 g/L, serum protein 48 +/- 1.4 g/L, calcium 2.4 +/- 0.03 mmol/L, phosphorus 1.85 +/- 0.08 mmol/L, alkaline phosphatase 347 +/- 27 IU/L. The values for the EP group (N = 30) were albumin 32 +/- 0.9 g/L, serum protein 49 +/- 1.4 g/L, calcium 2.4 +/- 0.4 mmol/L, phosphorus 1.9 +/- 0.1 mmol/L, alkaline phosphatase 420 +/- 34 IU/L. At the study exit, the values for the NC group were albumin 30 +/- 0.7 g/L, serum protein 45 +/- 0.9 g/L, calcium 2.4 +/- 0.3 mmol/L, phosphorus 1.96 +/- 0.07 mmol/L, and alkaline phosphatase 371 +/- 23 IU/L. The values for the EP group were albumin 32 +/- 1.0 g/L, serum protein 46.0 +/- 1.4 g/L, calcium 2.5 +/- 0.03 mmol/L, serum phosphorus 2.2 +/- 0.1, and alkaline phosphatase 367 +/- 27 IU/L. No significant differences were observed between groups at entry to and exit from the study. However, in the EP group the alkaline phosphatase decreased significantly (P = .02) from entry to exit and calcium increased significantly during the same period compared with the NC group (P = .003). The mean daily weight gain was 33 +/- 0.7 g for the NC group and 31 +/- 1 g for the EP group. The weekly gain in head circumference and body length were also similar in both groups: approximately 1 cm/week. Both groups tolerated the fortifiers well.\nThese findings suggest that both products provide the additional nutritional support necessary for optimal overall postnatal growth in healthy preterm infants. The differences in calcium and alkaline phosphatase may be due to the differences in vitamin D content in fortifiers 88 IU/100 mL in mixed NC vs 270 IU/100 mL in mixed EP. This observation calls for careful monitoring of calcium and alkaline phosphatase values and possible adjustments of vitamin D intake when fortifiers are used for extended periods.", "title": "A randomized, controlled evaluation of two commercially available human breast milk fortifiers in healthy preterm neonates.", "date": "1996-11-01"}, {"article_id": "26488118", "content": "This study was a comparison of growth and tolerance in premature infants fed either standard powdered human milk fortifier (HMF) or a newly formulated concentrated liquid that contained extensively hydrolyzed protein.\nThis was an unblinded randomized controlled multicenter noninferiority study on preterm infants receiving human milk (HM) supplemented with 2 randomly assigned HMFs, either concentrated liquid HMF containing extensively hydrolyzed protein (LE-HMF) or a powdered intact protein HMF (PI-HMF) as the control. The study population consisted of preterm infants \u226433 weeks who were enterally fed HM. Infants were studied from the first day of HM fortification until day 29 or hospital discharge, whichever came first.\nA total of 147 preterm infants were enrolled. Noninferiority was observed in weight gain reported in the intent-to-treat (ITT) analysis was 18.2 and 17.5 g \u00b7 kg(-1) \u00b7 day(-1) for the LE-HMF and PI-HMF groups, respectively. In an a priori defined subgroup of strict protocol followers (n\u200a=\u200a75), the infants fed LE-HMF achieved greater weight over time than those fed PI-HMF (P\u200a=\u200a0.036). The LE-HMF group achieved greater linear growth over time compared to the PI-HMF (P\u200a=\u200a0.029). The protein intake from fortified HM was significantly higher in the LE-HMF group compared with the PI-HMF group (3.9 vs 3.3 g \u00b7 kg(-1) \u00b7 day(-1), P\u200a<\u200a0.0001). Both fortifiers were well tolerated with no significant differences in overall morbidity.\nBoth fortifiers showed excellent weight gain (grams per kilograms per day), tolerance, and low incidence of morbidity outcomes with the infants who were strict protocol followers fed LE-HMF having improved growth during the study. These data point to the safety and suitability of this new concentrated liquid HMF (LE-HMF) in preterm infants. Growth with this fortifier closely matches the recent recommendations for a weight gain of >18 g \u00b7 kg(-1) \u00b7 day(-1).", "title": "Growth and Tolerance of Preterm Infants Fed a New Extensively Hydrolyzed Liquid Human Milk Fortifier.", "date": "2015-10-22"}, {"article_id": "10838460", "content": "Human milk fortification has been advocated to enhance premature infants' growth. We, therefore, undertook this study of a new human milk fortifier containing more protein than a reference one.\nOpen, randomized, controlled, multiclinic trial, with weekly growth parameters and safety evaluations in premature infants <1,500 g.\nThe 2 groups did not differ in demographic and baseline characteristics. The adjusted daily milk intake was significantly higher in the infants fed reference human milk fortifier (n = 29; 154.2 +/- 2.1 vs. 144.4 +/- 2.5 ml/kg/day, mean +/- SE; p < 0.05). Both human milk fortifiers produced increases over baseline in weight, length, and head circumference, with greater gains observed in the new human milk fortifier-fed infants for the former two parameters (weight gain 26.8 +/- 1.3 and 20.4 +/- 1.2 g/day, p < 0.05; head circumference 1.0 +/- 0.1 and 0.8 +/- 0.1 cm/week; length 0.9 +/- 0.1 and 0.8 +/- 0.1 cm/week, respectively). Serum chemistries were normal and acceptable for age. Study events were typical for premature infants and similar in both groups.\nThis new human milk fortifier had comparable safety to the reference human milk fortifier and promoted faster weight gain and head circumference growth.", "title": "Growth in human milk-Fed very low birth weight infants receiving a new human milk fortifier.", "date": "2000-06-06"}]}
{"original_review": "31206168", "question_data": [{"question_id": 19, "question": "Is mortality higher, lower, or the same when comparing rapid ART to standard initiation?", "answer": "lower", "evidence_quality": "very low", "fulltext_required": "yes", "comment": "Elul/29136001 does not report mortality in the abstract for some reason but carries like 20+% weight. it's only reported in a table", "relevant_sources": ["28742880", "27658873", "29509839", "27163694", "29136001", "29112963", "28542080"]}, {"question_id": 20, "question": "Is retention in care at 12 months higher, lower, or the same when comparing rapid ART to standard initiation?", "answer": "higher", "evidence_quality": "low", "fulltext_required": "yes", "comment": "Labhardt/29509839 does not discuss in abstract, but running it anyway because labhardt only has 13% weight; 27163694/Rosen requires fulltext and has 20% weight", "relevant_sources": ["28742880", "29509839", "27163694", "29136001", "29112963", "28542080"]}, {"question_id": 21, "question": "Is ART uptake at 12 months higher, lower, or the same when comparing rapid ART to standard initiation?", "answer": "higher", "evidence_quality": "moderate", "fulltext_required": "yes", "comment": "missing in Labhardt/29509839 (where we only have the abstract), but it has only 10% weight so ignoring that", "relevant_sources": ["28742880", "27658873", "29509839", "27163694"]}, {"question_id": 22, "question": "Is viral suppression at 12 months higher, lower, or the same when comparing rapid ART to standard initiation?", "answer": "higher", "evidence_quality": "moderate", "fulltext_required": "yes", "comment": "Amanyire/27658873 requires fulltext but is 48% weight", "relevant_sources": ["28742880", "27658873", "29509839", "27163694"]}], "sources": [{"article_id": "28742880", "content": "Attrition during the period from HIV testing to antiretroviral therapy (ART) initiation is high worldwide. We assessed whether same-day HIV testing and ART initiation improves retention and virologic suppression.\nWe conducted an unblinded, randomized trial of standard ART initiation versus same-day HIV testing and ART initiation among eligible adults \u226518 years old with World Health Organization Stage 1 or 2 disease and CD4 count \u2264500 cells/mm3. The study was conducted among outpatients at the Haitian Group for the Study of Kaposi's Sarcoma and Opportunistic infections (GHESKIO) Clinic in Port-au-Prince, Haiti. Participants were randomly assigned (1:1) to standard ART initiation or same-day HIV testing and ART initiation. The standard group initiated ART 3 weeks after HIV testing, and the same-day group initiated ART on the day of testing. The primary study endpoint was retention in care 12 months after HIV testing with HIV-1 RNA <50 copies/ml. We assessed the impact of treatment arm with a modified intention-to-treat analysis, using multivariable logistic regression controlling for potential confounders. Between August 2013 and October 2015, 762 participants were enrolled; 59 participants transferred to other clinics during the study period, and were excluded as per protocol, leaving 356 in the standard and 347 in the same-day ART groups. In the standard ART group, 156 (44%) participants were retained in care with 12-month HIV-1 RNA <50 copies, and 184 (52%) had <1,000 copies/ml; 20 participants (6%) died. In the same-day ART group, 184 (53%) participants were retained with HIV-1 RNA <50 copies/ml, and 212 (61%) had <1,000 copies/ml; 10 (3%) participants died. The unadjusted risk ratio (RR) of being retained at 12 months with HIV-1 RNA <50 copies/ml was 1.21 (95% CI: 1.04, 1.38; p = 0.015) for the same-day ART group compared to the standard ART group, and the unadjusted RR for being retained with HIV-1 RNA <1,000 copies was 1.18 (95% CI: 1.04, 1.31; p = 0.012). The main limitation of this study is that it was conducted at a single urban clinic, and the generalizability to other settings is uncertain.\nSame-day HIV testing and ART initiation is feasible and beneficial in this setting, as it improves retention in care with virologic suppression among patients with early clinical HIV disease.\nThis study is registered with ClinicalTrials.gov number NCT01900080.", "title": "Same-day HIV testing with initiation of antiretroviral therapy versus standard care for persons living with HIV: A randomized unblinded trial.", "date": "2017-07-26"}, {"article_id": "27163694", "content": "High rates of patient attrition from care between HIV testing and antiretroviral therapy (ART) initiation have been documented in sub-Saharan Africa, contributing to persistently low CD4 cell counts at treatment initiation. One reason for this is that starting ART in many countries is a lengthy and burdensome process, imposing long waits and multiple clinic visits on patients. We estimated the effect on uptake of ART and viral suppression of an accelerated initiation algorithm that allowed treatment-eligible patients to be dispensed their first supply of antiretroviral medications on the day of their first HIV-related clinic visit.\nRapIT (Rapid Initiation of Treatment) was an unblinded randomized controlled trial of single-visit ART initiation in two public sector clinics in South Africa, a primary health clinic (PHC) and a hospital-based HIV clinic. Adult (\u226518 y old), non-pregnant patients receiving a positive HIV test or first treatment-eligible CD4 count were randomized to standard or rapid initiation. Patients in the rapid-initiation arm of the study (\"rapid arm\") received a point-of-care (POC) CD4 count if needed; those who were ART-eligible received a POC tuberculosis (TB) test if symptomatic, POC blood tests, physical exam, education, counseling, and antiretroviral (ARV) dispensing. Patients in the standard-initiation arm of the study (\"standard arm\") followed standard clinic procedures (three to five additional clinic visits over 2-4 wk prior to ARV dispensing). Follow up was by record review only. The primary outcome was viral suppression, defined as initiated, retained in care, and suppressed (\u2264400 copies/ml) within 10 mo of study enrollment. Secondary outcomes included initiation of ART \u226490 d of study enrollment, retention in care, time to ART initiation, patient-level predictors of primary outcomes, prevalence of TB symptoms, and the feasibility and acceptability of the intervention. A survival analysis was conducted comparing attrition from care after ART initiation between the groups among those who initiated within 90 d. Three hundred and seventy-seven patients were enrolled in the study between May 8, 2013 and August 29, 2014 (median CD4 count 210 cells/mm3). In the rapid arm, 119/187 patients (64%) initiated treatment and were virally suppressed at 10 mo, compared to 96/190 (51%) in the standard arm (relative risk [RR] 1.26 [1.05-1.50]). In the rapid arm 182/187 (97%) initiated ART \u226490 d, compared to 136/190 (72%) in the standard arm (RR 1.36, 95% confidence interval [CI], 1.24-1.49). Among 318 patients who did initiate ART within 90 d, the hazard of attrition within the first 10 mo did not differ between the treatment arms (hazard ratio [HR] 1.06; 95% CI 0.61-1.84). The study was limited by the small number of sites and small sample size, and the generalizability of the results to other settings and to non-research conditions is uncertain.\nOffering single-visit ART initiation to adult patients in South Africa increased uptake of ART by 36% and viral suppression by 26%. This intervention should be considered for adoption in the public sector in Africa.\nClinicalTrials.gov NCT01710397, and South African National Clinical Trials Register DOH-27-0213-4177.", "title": "Initiating Antiretroviral Therapy for HIV at a Patient's First Clinic Visit: The RapIT Randomized Controlled Trial.", "date": "2016-05-11"}, {"article_id": "29112963", "content": "Gaps in the HIV care continuum contribute to poor health outcomes and increase HIV transmission. A combination of interventions targeting multiple steps in the continuum is needed to achieve the full beneficial impact of HIV treatment.\nLink4Health, a cluster-randomized controlled trial, evaluated the effectiveness of a combination intervention strategy (CIS) versus the standard of care (SOC) on the primary outcome of linkage to care within 1 month plus retention in care at 12 months after HIV-positive testing. Ten clusters of HIV clinics in Swaziland were randomized 1:1 to CIS versus SOC. The CIS included point-of-care CD4+ testing at the time of an HIV-positive test, accelerated antiretroviral therapy (ART) initiation for treatment-eligible participants, mobile phone appointment reminders, health educational packages, and noncash financial incentives. Secondary outcomes included each component of the primary outcome, mean time to linkage, assessment for ART eligibility, ART initiation and time to ART initiation, viral suppression defined as HIV-1 RNA < 1,000 copies/mL at 12 months after HIV testing among patients on ART \u22656 months, and loss to follow-up and death at 12 months after HIV testing. A total of 2,197 adults aged \u226518 years, newly tested HIV positive, were enrolled from 19 August 2013 to 21 November 2014 (1,096 CIS arm; 1,101 SOC arm) and followed for 12 months. The median participant age was 31 years (IQR 26-39), and 59% were women. In an intention-to-treat analysis, 64% (705/1,096) of participants at the CIS sites achieved the primary outcome versus 43% (477/1,101) at the SOC sites (adjusted relative risk [RR] 1.52, 95% CI 1.19-1.96, p = 0.002). Participants in the CIS arm versus the SOC arm had the following secondary outcomes: linkage to care regardless of retention at 12 months (RR 1.08, 95% CI 0.97-1.21, p = 0.13), mean time to linkage (2.5 days versus 7.5 days, p = 0.189), retention in care at 12 months regardless of time to linkage (RR 1.48, 95% CI 1.18-1.86, p = 0.002), assessment for ART eligibility (RR 1.20, 95% CI 1.07-1.34, p = 0.004), ART initiation (RR 1.16, 95% CI 0.96-1.40, p = 0.12), mean time to ART initiation from time of HIV testing (7 days versus 14 days, p < 0.001), viral suppression among those on ART for \u22656 months (RR 0.97, 95% CI 0.88-1.07, p = 0.55), loss to follow-up at 12 months after HIV testing (RR 0.56, 95% CI 0.40-0.79, p = 0.002), and death (N = 78) within 12 months of HIV testing (RR 0.80, 95% CI 0.46-1.35, p = 0.41). Limitations of this study include a small number of clusters and the inability to evaluate the incremental effectiveness of individual components of the combination strategy.\nA combination strategy inclusive of 5 evidence-based interventions aimed at multiple steps in the HIV care continuum was associated with significant increase in linkage to care plus 12-month retention. This strategy offers promise of enhanced outcomes for HIV-positive patients.\nClinicalTrials.gov NCT01904994.", "title": "Effectiveness of a combination strategy for linkage and retention in adult HIV care in Swaziland: The Link4Health cluster randomized trial.", "date": "2017-11-08"}, {"article_id": "29509839", "content": "Home-based HIV testing is a frequently used strategy to increase awareness of HIV status in sub-Saharan Africa. However, with referral to health facilities, less than half of those who test HIV positive link to care and initiate antiretroviral therapy (ART).\nTo determine whether offering same-day home-based ART to patients with HIV improves linkage to care and viral suppression in a rural, high-prevalence setting in sub-Saharan Africa.\nOpen-label, 2-group, randomized clinical trial (February 22, 2016-September 17, 2017), involving 6 health care facilities in northern Lesotho. During home-based HIV testing in 6655 households from 60 rural villages and 17 urban areas, 278 individuals aged 18 years or older who tested HIV positive and were ART naive from 268 households consented and enrolled. Individuals from the same household were randomized into the same group.\nParticipants were randomly assigned to be offered same-day home-based ART initiation (n\u2009=\u2009138) and subsequent follow-up intervals of 1.5, 3, 6, 9, and 12 months after treatment initiation at the health facility or to receive usual care (n\u2009=\u2009140) with referral to the nearest health facility for preparatory counseling followed by ART initiation and monthly follow-up visits thereafter.\nPrimary end points were rates of linkage to care within 3 months (presenting at the health facility within 90 days after the home visit) and viral suppression at 12 months, defined as a viral load of less than 100 copies/mL from 11 through 14 months after enrollment.\nAmong 278 randomized individuals (median age, 39 years [interquartile range, 28.0-52.0]; 180 women [65.7%]), 274 (98.6%) were included in the analysis (137 in the same-day group and 137 in the usual care group). In the same-day group, 134 (97.8%) indicated readiness to start ART that day and 2 (1.5%) within the next few days and were given a 1-month supply of ART. At 3 months, 68.6% (94) in same-day group vs 43.1% (59) in usual care group had linked to care (absolute difference, 25.6%; 95% CI, 13.8% to 36.3%; P\u2009<\u2009.001). At 12 months, 50.4% (69) in the same-day group vs 34.3% (47) in usual care group achieved viral suppression (absolute difference, 16.0%; 4.4%-27.2%; P\u2009=\u2009.007). Two deaths (1.5%) were reported in the same-day group, none in usual care group.\nAmong adults in rural Lesotho, a setting of high HIV prevalence, offering same-day home-based ART initiation to individuals who tested positive during home-based HIV testing, compared with usual care and standard clinic referral, significantly increased linkage to care at 3 months and HIV viral suppression at 12 months. These findings support the practice of offering same-day ART initiation during home-based HIV testing.\nclinicaltrials.gov Identifier: NCT02692027.", "title": "Effect of Offering Same-Day ART vs Usual Health Facility Referral During Home-Based HIV Testing on Linkage to Care and Viral Suppression Among Adults With HIV in Lesotho: The CASCADE Randomized Clinical Trial.", "date": "2018-03-07"}, {"article_id": "29136001", "content": "Concerning gaps in the HIV care continuum compromise individual and population health. We evaluated a combination intervention strategy (CIS) targeting prevalent barriers to timely linkage and sustained retention in HIV care in Mozambique.\nIn this cluster-randomized trial, 10 primary health facilities in the city of Maputo and Inhambane Province were randomly assigned to provide the CIS or the standard of care (SOC). The CIS included point-of-care CD4 testing at the time of diagnosis, accelerated ART initiation, and short message service (SMS) health messages and appointment reminders. A pre-post intervention 2-sample design was nested within the CIS arm to assess the effectiveness of CIS+, an enhanced version of the CIS that additionally included conditional non-cash financial incentives for linkage and retention. The primary outcome was a combined outcome of linkage to care within 1 month and retention at 12 months after diagnosis. From April 22, 2013, to June 30, 2015, we enrolled 2,004 out of 5,327 adults \u226518 years of age diagnosed with HIV in the voluntary counseling and testing clinics of participating health facilities: 744 (37%) in the CIS group, 493 (25%) in the CIS+ group, and 767 (38%) in the SOC group. Fifty-seven percent of the CIS group achieved the primary outcome versus 35% in the SOC group (relative risk [RR]CIS vs SOC = 1.58, 95% CI 1.05-2.39). Eighty-nine percent of the CIS group linked to care on the day of diagnosis versus 16% of the SOC group (RRCIS vs SOC = 9.13, 95% CI 1.65-50.40). There was no significant benefit of adding financial incentives to the CIS in terms of the combined outcome (55% of the CIS+ group achieved the primary outcome, RRCIS+ vs CIS = 0.96, 95% CI 0.81-1.16). Key limitations include the use of existing medical records to assess outcomes, the inability to isolate the effect of each component of the CIS, non-concurrent enrollment of the CIS+ group, and exclusion of many patients newly diagnosed with HIV.\nThe CIS showed promise for making much needed gains in the HIV care continuum in our study, particularly in the critical first step of timely linkage to care following diagnosis.\nClinicalTrials.gov NCT01930084.", "title": "A combination intervention strategy to improve linkage to and retention in HIV care following diagnosis in Mozambique: A cluster-randomized study.", "date": "2017-11-15"}, {"article_id": "28542080", "content": "Lack of accessible laboratory infrastructure limits HIV antiretroviral therapy (ART) initiation, monitoring, and retention in many resource-limited settings. Point-of-care testing (POCT) is advocated as a mechanism to overcome these limitations. We executed a pragmatic, prospective, randomized, controlled trial comparing the impact of POCT vs. standard of care (SOC) on treatment initiation and retention in care.\nSelected POC technologies were embedded at 3 primary health clinics in South Africa. Confirmed HIV-positive participants were randomized to either SOC or POC: SOC participants were venesected and specimens referred to the laboratory with patient follow-up as per algorithm (\u223c3 visits); POC participants had phlebotomy and POCT immediately on-site using Pima CD4 to assess ART eligibility followed by hematology, chemistry, and tuberculosis screening with the goal of receiving same-day adherence counseling and treatment initiation. Participant outcomes measured at recruitment 6 and 12 months after initiation.\nFour hundred thirty-two of 717 treatment eligible participants enrolled between May 2012 and September 2013: 198 (56.7%) SOC; 234 (63.6%) POC. Mean age was 37.4 years; 60.5% were female. Significantly more participants were initiated using POC [adjusted prevalence ratio (aPR) 0.83; 95% confidence interval (CI): 0.74 to 0.93; P < 0.0001], the median time to initiation was 1 day for POC and 26.5 days for SOC. The proportion of patients in care and on ART was similar for both arms at 6 months (47 vs. 50%) (aPR 0.96; 95% CI: 0.79 to 1.16) and 12 months (32 vs. 32%) (aPR 1.05; 95% CI: 0.80 to 1.38), with similar mortality rates. Loss to follow-up at 12 months was higher for POC (36% vs. 51%) (aPR 0.82; 95% CI: 0.65 to 1.04).\nAdoption of POCT accelerated ART initiation but once on treatment, there was unexpectedly higher loss to follow-up on POC and no improvement in outcomes at 12 months over SOC.", "title": "Multidisciplinary Point-of-Care Testing in South African Primary Health Care Clinics Accelerates HIV ART Initiation but Does Not Alter Retention in Care.", "date": "2017-05-26"}, {"article_id": "27658873", "content": "In Africa, up to 30% of HIV-infected patients who are clinically eligible for antiretroviral therapy (ART) do not start timely treatment. We assessed the effects of an intervention targeting prevalent health systems barriers to ART initiation on timing and completeness of treatment initiation.\nIn this stepped-wedge, non-blinded, cluster-randomised controlled trial, 20 clinics in southwestern Uganda were randomly assigned in groups of five clinics every 6 months to the intervention by a computerised random number generator. This procedure continued until all clinics had crossed over from control (standard of care) to the intervention, which consisted of opinion-leader-led training and coaching of front-line health workers, a point-of-care CD4 cell count testing platform, a revised counselling approach without mandatory multiple pre-initiation sessions, and feedback to the facilities on their ART initiation rates and how they compared with other facilities. Treatment-naive, HIV-infected adults (aged \u226518 years) who were clinically eligible for ART during the study period were included in the study population. The primary outcome was ART initiation 14 days after first clinical eligibility for ART. This study is registered with ClinicalTrials.gov, number NCT01810289.\nBetween April 11, 2013, and Feb 2, 2015, 12\u2008024 eligible patients visited one of the 20 participating clinics. Median CD4 count was 310 cells per \u03bcL (IQR 179-424). 3753 of 4747 patients (weighted proportion 80%) in the intervention group had started ART by 2 weeks after eligibility compared with 2585 of 7066 patients (38%) in the control group (risk difference 41\u00b79%, 95% CI 40\u00b71-43\u00b78). Vital status was ascertained in a random sample of 208 patients in the intervention group and 199 patients in the control group. Four deaths (2%) occurred in the intervention group and five (3%) occurred in the control group.\nA multicomponent intervention targeting health-care worker behaviour increased the probability of ART initiation 14 days after eligibility. This intervention consists of widely accessible components and has been tested in a real-world setting, and is therefore well positioned for use at scale.\nNational Institute of Allergy and Infectious Diseases (NIAID) and the President's Emergency Fund for AIDS Relief (PEPFAR).", "title": "Effects of a multicomponent intervention to streamline initiation of antiretroviral therapy in Africa: a stepped-wedge cluster-randomised trial.", "date": "2016-10-30"}]}
{"original_review": "28334427", "question_data": [{"question_id": 23, "question": "Is the rate of hypotension higher, lower, or the same when comparing capnography to standard monitoring?", "answer": "no difference", "evidence_quality": "moderate", "fulltext_required": "no", "comment": "", "relevant_sources": ["27006732"]}], "sources": [{"article_id": "19783324", "content": "We determine whether the use of capnography is associated with a decreased incidence of hypoxic events than standard monitoring alone during emergency department (ED) sedation with propofol.\nAdults underwent ED propofol sedation with standard monitoring (pulse oximetry, cardiac and blood pressure) and capnography and were randomized into a group in which treating physicians had access to the capnography and a blinded group in which they did not. All patients received supplemental oxygen (3 L/minute) and opioids greater than 30 minutes before. Propofol was dosed at 1.0 mg/kg, followed by 0.5 mg/kg as needed. Capnographic and SpO2 data were recorded electronically every 5 seconds. Hypoxia was defined as SpO2 less than 93%; respiratory depression, as end tidal CO2 (ETCO2) greater than 50 mm Hg, ETCO2 change from baseline of 10%, or loss of the waveform.\nOne hundred thirty-two subjects were evaluated and included in the final analysis. We observed hypoxia in 17 of 68 (25%) subjects with capnography and 27 of 64 (42%) with blinded capnography (P=.035; difference 17%; 95% confidence interval 1.3% to 33%). Capnography identified all cases of hypoxia before onset (sensitivity 100%; specificity 64%), with the median time from capnographic evidence of respiratory depression to hypoxia 60 seconds (range 5 to 240 seconds).\nIn adults receiving ED propofol sedation, the addition of capnography to standard monitoring reduced hypoxia and provided advance warning for all hypoxic events.", "title": "Does end tidal CO2 monitoring during emergency department procedural sedation and analgesia with propofol decrease the incidence of hypoxic events? A randomized, controlled trial.", "date": "2009-09-29"}, {"article_id": "27006732", "content": "This prospective, randomized trial was undertaken to evaluate the utility of adding end-tidal capnometry (ETC) to pulse oximetry (PO) in patients undergoing procedural sedation and analgesia (PSA) in the emergency department (ED).\nThe patients were randomized to monitoring with or without ETC in addition to the current standard of care. Primary endpoints included respiratory adverse events, with secondary endpoints of level of sedation, hypotension, other PSA-related adverse events and patient satisfaction.\nOf 986 patients, 501 were randomized to usual care and 485 to additional ETC monitoring. In this series, 48% of the patients were female, with a mean age of 46 years. Orthopedic manipulations (71%), cardioversion (12%) and abscess incision and drainage (12%) were the most common procedures, and propofol and fentanyl were the sedative/analgesic combination used for most patients. There was no difference in patients experiencing de-saturation (SaO2<90%) between the two groups; however, patients in the ETC group were more likely to require airway repositioning (12.9% vs. 9.3%, P=0.003). Hypotension (SBP<100 mmHg or <85 mmHg if baseline <100 mmHg) was observed in 16 (3.3%) patients in the ETC group and 7 (1.4%) in the control group (P=0.048).\nThe addition of ETC does not appear to change any clinically significant outcomes. We found an increased incidence of the use of airway repositioning maneuvers and hypotension in cases where ETC was used. We do not believe that ETC should be recommended as a standard of care for the monitoring of patients undergoing PSA.", "title": "End-tidal capnometry during emergency department procedural sedation and analgesia: a randomized, controlled study.", "date": "2016-03-24"}, {"article_id": "25445871", "content": "Data suggest that capnography is a more sensitive measure of ventilation than standard modalities and detects respiratory depression before hypoxemia occurs. We sought to determine if adding capnography to standard monitoring during sedation of children increased the frequency of interventions for hypoventilation, and whether these interventions would decrease the frequency of oxygen desaturations.\nWe enrolled 154 children receiving procedural sedation in a pediatric emergency department. All subjects received standard monitoring and capnography, but were randomized to whether staff could view the capnography monitor (intervention) or were blinded to it (controls). Primary outcome were the rate of interventions provided by staff for hypoventilation and the rate of oxygen desaturation less than 95%.\nSeventy-seven children were randomized to each group. Forty-five percent had at least 1 episode of hypoventilation. The rate of hypoventilation per minute was significantly higher among controls (7.1% vs 1.0%, P = .008). There were significantly fewer interventions in the intervention group than in the control group (odds ratio, 0.25; 95% confidence interval [CI], 0.13-0.50). Interventions were more likely to occur contemporaneously with hypoventilation in the intervention group (2.26; 95% CI, 1.34-3.81). Interventions not in time with hypoventilation were associated with higher odds of oxygen desaturation less than 95% (odds ratio, 5.31; 95% CI, 2.76-10.22).\nHypoventilation is common during sedation of pediatric emergency department patients. This can be difficult to detect by current monitoring methods other than capnography. Providers with access to capnography provided fewer but more timely interventions for hypoventilation. This led to fewer episodes of hypoventilation and of oxygen desaturation.", "title": "A randomized controlled trial of capnography during sedation in a pediatric emergency setting.", "date": "2014-12-03"}]}
{"original_review": "25280769", "question_data": [{"question_id": 24, "question": "Is the risk of HIV transmission at 12 months higher, lower, or the same when comparing triple antiretroviral prophylaxis during pregnancy and breastfeeding to short antiretroviral prophylaxis?", "answer": "lower", "evidence_quality": "low", "fulltext_required": "no", "comment": "", "relevant_sources": ["21237718"]}, {"question_id": 25, "question": "Is the risk of infant HIV infection higher, lower, or the same when comparing zidovudine, lamivudine, and lopinavir/ritonavir to zidovudine, lamivudine, and abacavir from 26\u201034 weeks gestation through six months of breastfeeding?", "answer": "no difference", "evidence_quality": "", "fulltext_required": "no", "comment": "", "relevant_sources": ["20554983"]}, {"question_id": 26, "question": "Is the risk of infant mortality after 12 months higher, lower, or the same when comparing six weeks of infant nevirapine to single dose infant nevirapine?", "answer": "lower", "evidence_quality": "moderate", "fulltext_required": "no", "comment": "", "relevant_sources": ["21330912"]}, {"question_id": 33, "question": "Is the risk of infant HIV infection or death after 12 months higher, lower, or the same when comparing six weeks of infant nevirapine to single dose infant nevirapine?", "answer": "no difference", "evidence_quality": "moderate", "fulltext_required": "no", "comment": "", "relevant_sources": ["21330912"]}, {"question_id": 34, "question": "Is the risk of infant HIV infection after 12 months higher, lower, or the same when comparing six weeks of infant nevirapine to single dose infant nevirapine?", "answer": "no difference", "evidence_quality": "moderate", "fulltext_required": "no", "comment": "", "relevant_sources": ["21330912"]}, {"question_id": 35, "question": "Is the risk of HIV transmission at 24 months higher, lower, or the same when comparing nevirapine up to 14 weeks plus one week zidovudine to only single dose nevirapine plus one week zidovudine?", "answer": "lower", "evidence_quality": "moderate", "fulltext_required": "no", "comment": "", "relevant_sources": ["21423025"]}, {"question_id": 36, "question": "Is the risk of HIV transmission or death at 24 months higher, lower, or the same when comparing nevirapine up to 14 weeks plus one week zidovudine to only single dose nevirapine plus one week zidovudine?", "answer": "lower", "evidence_quality": "moderate", "fulltext_required": "no", "comment": "", "relevant_sources": ["21423025"]}, {"question_id": 37, "question": "Is the risk of HIV transmission or death at 24 months higher, lower, or the same when comparing nevirapine plus zidovudine with dual prophylaxis up to 14 weeks to only single dose nevirapine plus one week zidovudine?", "answer": "lower", "evidence_quality": "moderate", "fulltext_required": "no", "comment": "", "relevant_sources": ["21423025"]}, {"question_id": 38, "question": "Is the risk of HIV transmission at 24 months higher, lower, or the same when comparing nevirapine plus zidovudine with dual prophylaxis up to 14 weeks to only single dose nevirapine plus one week zidovudine?", "answer": "lower", "evidence_quality": "moderate", "fulltext_required": "no", "comment": "", "relevant_sources": ["21423025"]}, {"question_id": 39, "question": "Is the risk of HIV infection or death higher, lower, or the same when comparing a maternal extended triple\u2010drug antiretroviral regimen to no extended postnatal antiretroviral regimen?", "answer": "lower", "evidence_quality": "moderate", "fulltext_required": "no", "comment": "", "relevant_sources": ["20554982"]}, {"question_id": 40, "question": "Is the risk of HIV infection higher, lower, or the same when comparing a maternal extended triple\u2010drug antiretroviral regimen to no extended postnatal antiretroviral regimen?", "answer": "lower", "evidence_quality": "moderate", "fulltext_required": "no", "comment": "", "relevant_sources": ["20554982"]}, {"question_id": 41, "question": "Is the risk of HIV infection higher, lower, or the same when comparing an extended infant nevirapine regimen to no extended postnatal antiretroviral regimen?", "answer": "lower", "evidence_quality": "low", "fulltext_required": "no", "comment": "", "relevant_sources": ["20554982"]}, {"question_id": 42, "question": "Is the risk of HIV infection or death higher, lower, or the same when comparing an extended infant nevirapine regimen to no extended postnatal antiretroviral regimen?", "answer": "lower", "evidence_quality": "moderate", "fulltext_required": "no", "comment": "", "relevant_sources": ["20554982"]}, {"question_id": 43, "question": "Is the risk of HIV infection higher, lower, or the same when comparing an extended infant nevirapine regimen to a maternal extended triple\u2010drug antiretroviral regimen?", "answer": "no difference", "evidence_quality": "moderate", "fulltext_required": "no", "comment": "", "relevant_sources": ["20554982"]}, {"question_id": 44, "question": "Is the risk of HIV infection or death higher, lower, or the same when comparing an extended infant nevirapine regimen to a maternal extended triple\u2010drug antiretroviral regimen?", "answer": "no difference", "evidence_quality": "moderate", "fulltext_required": "no", "comment": "", "relevant_sources": ["20554982"]}], "sources": [{"article_id": "20554982", "content": "We evaluated the efficacy of a maternal triple-drug antiretroviral regimen or infant nevirapine prophylaxis for 28 weeks during breast-feeding to reduce postnatal transmission of human immunodeficiency virus type 1 (HIV-1) in Malawi.\nWe randomly assigned 2369 HIV-1-positive, breast-feeding mothers with a CD4+ lymphocyte count of at least 250 cells per cubic millimeter and their infants to receive a maternal antiretroviral regimen, infant nevirapine, or no extended postnatal antiretroviral regimen (control group). All mothers and infants received perinatal prophylaxis with single-dose nevirapine and 1 week of zidovudine plus lamivudine. We used the Kaplan-Meier method to estimate the cumulative risk of HIV-1 transmission or death by 28 weeks among infants who were HIV-1-negative 2 weeks after birth. Rates were compared with the use of the log-rank test.\nAmong mother-infant pairs, 5.0% of infants were HIV-1-positive at 2 weeks of life. The estimated risk of HIV-1 transmission between 2 and 28 weeks was higher in the control group (5.7%) than in either the maternal-regimen group (2.9%, P=0.009) or the infant-regimen group (1.7%, P<0.001). The estimated risk of infant HIV-1 infection or death between 2 and 28 weeks was 7.0% in the control group, 4.1% in the maternal-regimen group (P=0.02), and 2.6% in the infant-regimen group (P<0.001). The proportion of women with neutropenia was higher among those receiving the antiretroviral regimen (6.2%) than among those in either the nevirapine group (2.6%) or the control group (2.3%). Among infants receiving nevirapine, 1.9% had a hypersensitivity reaction.\nThe use of either a maternal antiretroviral regimen or infant nevirapine for 28 weeks was effective in reducing HIV-1 transmission during breast-feeding. (ClinicalTrials.gov number, NCT00164736.)", "title": "Maternal or infant antiretroviral drugs to reduce HIV-1 transmission.", "date": "2010-06-18"}, {"article_id": "22196945", "content": "Nevirapine given once-daily for the first 6, 14, or 28 weeks of life to infants exposed to HIV-1 via breastfeeding reduces transmission through this route compared with single-dose nevirapine at birth or neonatally. We aimed to assess incremental safety and efficacy of extension of such prophylaxis to 6 months.\nIn our phase 3, randomised, double-blind, placebo-controlled HPTN 046 trial, we assessed the incremental benefit of extension of once-daily infant nevirapine from age 6 weeks to 6 months. We enrolled breastfeeding infants born to mothers with HIV-1 in four African countries within 7 days of birth. Following receipt of nevirapine from birth to 6 weeks, infants without HIV infection were randomly allocated (by use of a computer-generated permuted block algorithm with random block sizes and stratified by site and maternal antiretroviral treatment status) to receive extended nevirapine prophylaxis or placebo until 6 months or until breastfeeding cessation, whichever came first. The primary efficacy endpoint was HIV-1 infection in infants at 6 months and safety endpoints were adverse reactions in both groups. We used Kaplan-Meier analyses to compare differences in the primary outcome between groups. This study is registered with ClinicalTrials.gov, number NCT00074412.\nBetween June 19, 2008, and March 12, 2010, we randomly allocated 1527 infants (762 nevirapine and 765 placebo); five of whom had HIV-1 infection at randomisation and were excluded from the primary analyses. In Kaplan-Meier analysis, 1\u00b71% (95% CI 0\u00b73-1\u00b78) of infants who received extended nevirapine developed HIV-1 between 6 weeks and 6 months compared with 2\u00b74% (1\u00b73-3\u00b76) of controls (difference 1\u00b73%, 95% CI 0-2\u00b76), equating to a 54% reduction in transmission (p=0\u00b7049). However, mortality (1\u00b72% for nevirapine vs 1\u00b71% for placebo; p=0\u00b781) and combined HIV infection and mortality rates (2\u00b73%vs 3\u00b72%; p=0\u00b727) did not differ between groups at 6 months. 125 (16%) of 758 infants given extended nevirapine and 116 (15%) of 761 controls had serious adverse events, but frequency of adverse events, serious adverse events, and deaths did not differ significantly between treatment groups.\nNevirapine prophylaxis can safely be used to provide protection from mother-to-child transmission of HIV-1 via breastfeeding for infants up to 6 months of age.\nUS National Institutes of Health.", "title": "Efficacy and safety of an extended nevirapine regimen in infant children of breastfeeding mothers with HIV-1 infection for prevention of postnatal HIV-1 transmission (HPTN 046): a randomised, double-blind, placebo-controlled trial.", "date": "2011-12-27"}, {"article_id": "18684096", "content": "Single-dose nevirapine (SD NVP) at birth plus NVP prophylaxis for the infant up to 6 weeks of age is superior to SD NVP alone for prevention of vertical transmission of human immunodeficiency virus (HIV) through breastfeeding. We analyzed NVP resistance in HIV-infected Ugandan infants who received either SD NVP or extended NVP prophylaxis.\nWe tested plasma HIV by using a genotyping assay (ViroSeq; Celera Diagnostics), a phenotypic resistance assay (PhenoSense; Monogram Biosciences), and sensitive point mutation assay (LigAmp, for K103N, Y181C, and G190A).\nWhen infants were 6 weeks old, ViroSeq detected NVP resistance in a higher proportion of infants in the extended NVP arm than in the SD NVP arm (21 of 25 [84%] vs. 12 of 24 [50%]; P = .01). Similar results were obtained with LigAmp and PhenoSense. In both study arms, infants who were HIV infected at birth frequently had NVP resistance detected. In contrast, infants in the extended NVP arm who were HIV infected after birth were more likely to have resistance detected at 6 weeks, compared with infants in the SD NVP arm. The use of extended NVP prophylaxis was also associated with detection of NVP resistance by ViroSeq at 6 months (7 of 7 [100%] infants in the extended NVP arm had resistance detected, compared with 1 of 6 [16.7%] infants in the SD NVP arm; P = .005).\nThe use of extended NVP prophylaxis was associated with increased selection for and persistence of NVP resistance in HIV-infected Ugandan infants.", "title": "Analysis of nevirapine (NVP) resistance in Ugandan infants who were HIV infected despite receiving single-Dose (SD) NVP versus SD NVP plus daily NVP up to 6 weeks of age to prevent HIV vertical transmission.", "date": "2008-08-08"}, {"article_id": "21237718", "content": "Breastfeeding is essential for child health and development in low-resource settings but carries a significant risk of transmission of HIV-1, especially in late stages of maternal disease. We aimed to assess the efficacy and safety of triple antiretroviral compared with zidovudine and single-dose nevirapine prophylaxis in pregnant women infected with HIV.\nPregnant women with WHO stage 1, 2, or 3 HIV-1 infection who had CD4 cell counts of 200-500 cells per \u03bcL were enrolled at five study sites in Burkina Faso, Kenya, and South Africa to start study treatment at 28-36 weeks' gestation. Women were randomly assigned (1:1) by a computer generated random sequence to either triple antiretroviral prophylaxis (a combination of 300 mg zidovudine, 150 mg lamivudine, and 400 mg lopinavir plus 100 mg ritonavir twice daily until cessation of breastfeeding to a maximum of 6\u00b75 months post partum) or zidovudine and single-dose nevirapine (300 mg zidovudine twice daily until delivery and a dose of 600 mg zidovudine plus 200 mg nevirapine at the onset of labour and, after a protocol amendment in December, 2006, 1 week post-partum zidovudine 300 mg twice daily and lamivudine 150 mg twice daily). All infants received a 0\u00b76 mL dose of nevirapine at birth and, from December, 2006, 4 mg/kg twice daily of zidovudine for 1 week after birth. Patients and investigators were not masked to treatment. The primary endpoints were HIV-free infant survival at 6 weeks and 12 months; HIV-free survival at 12 months in infants who were ever breastfed; AIDS-free survival in mothers at 18 months; and serious adverse events in mothers and babies. Analysis was by intention to treat. This trial is registered with Current Controlled Trials, ISRCTN71468401.\nFrom June, 2005, to August, 2008, 882 women were enrolled, 824 of whom were randomised and gave birth to 805 singleton or first, liveborn infants. The cumulative rate of HIV transmission at 6 weeks was 3\u00b73% (95% CI 1\u00b79-5\u00b76%) in the triple antiretroviral group compared with 5\u00b70% (3\u00b73-7\u00b77%) in the zidovudine and single-dose nevirapine group, and at 12 months was 5\u00b74% (3\u00b76-8\u00b71%) in the triple antiretroviral group compared with 9\u00b75% (7\u00b70-12\u00b79%) in the zidovudine and single-dose nevirapine group (p=0\u00b7029). The cumulative rate of HIV transmission or death at 12 months was 10\u00b72% (95% CI 7\u00b76-13\u00b76%) in the triple antiretroviral group compared with 16\u00b70% (12\u00b77-20\u00b70%) in the zidovudine and single-dose nevirapine group (p=0\u00b7017). In infants whose mothers declared they intended to breastfeed, the cumulative rate of HIV transmission at 12 months was 5\u00b76% (95% CI 3\u00b74-8\u00b79%) in the triple antiretroviral group compared with 10\u00b77% (7\u00b76-14\u00b78%) in the zidovudine and single-dose nevirapine group (p=0\u00b702). AIDS-free survival in mothers at 18 months will be reported in a different publication. The incidence of laboratory and clinical serious adverse events in both mothers and their babies was similar between groups.\nTriple antiretroviral prophylaxis during pregnancy and breastfeeding is safe and reduces the risk of HIV transmission to infants. Revised WHO guidelines now recommend antiretroviral prophylaxis (either to the mother or to the baby) during breastfeeding if the mother is not already receiving antiretroviral treatment for her own health.\nAgence nationale de recherches sur le sida et les h\u00e9patites virales, Department for International Development, European and Developing Countries Clinical Trials Partnership, Thrasher Research Fund, Belgian Directorate General for International Cooperation, Centers for Disease Control and Prevention, Eunice Kennedy Shriver National Institute of Child Health and Human Development, and UNDP/UNFPA/World Bank/WHO Special Programme of Research, Development and Research Training in Human Reproduction.", "title": "Triple antiretroviral compared with zidovudine and single-dose nevirapine prophylaxis during pregnancy and breastfeeding for prevention of mother-to-child transmission of HIV-1 (Kesho Bora study): a randomised controlled trial.", "date": "2011-01-18"}, {"article_id": "21423025", "content": "This analysis updates and extends efficacy estimates of the PEPI-Malawi trial through age 24 months at study completion in September 2009.\nInfants of breastfeeding HIV-infected women were randomized at birth to the following: (1) single-dose nevirapine (NVP) + 1-week zidovudine (ZDV) (control); (2) control + extended daily NVP (ExtNVP) through 14 weeks; (3) control + extended daily NVP + ZDV (ExtNVP/ZDV) through 14 weeks. We estimated rates of HIV infection, death and HIV infection, or death using Kaplan-Meier analysis.\nThis analysis includes 3126 infants uninfected at birth as follows: 1004 control, 1071 ExtNVP, and 1051 ExtNVP/ZDV. By 9 months, HIV infection rates were 5.0% in ExtNVP, 6.0% in ExtNVP/ZDV, and 11.1% in control (P < 0.001 comparing extended regimens with control). At age 24 months, HIV infection rates had risen to ~11% in the extended arms compared with 15.6% in the controls (P < 0.05). The rates of HIV infection or death were also significantly lower in extended arms. There were no differences in severe adverse events with the exception of higher possibly related events in the ExtNVP/ZDV arm.\nDaily infant antiretroviral prophylaxis reduces postnatal HIV infection by ~70% during the period of prophylaxis. But continued HIV transmission after prophylaxis stops suggests more prolonged infant prophylaxis is needed.", "title": "Postexposure prophylaxis of breastfeeding HIV-exposed infants with antiretroviral drugs to age 14 weeks: updated efficacy results of the PEPI-Malawi trial.", "date": "2011-03-23"}, {"article_id": "21330912", "content": "We previously reported combined analysis of 6-week and 6-month endpoints of three randomized controlled trials [Six Week Extended Dose Nevirapine (SWEN) trials] that compared extended-dose nevirapine through 6 weeks of age to single-dose nevirapine to prevent HIV transmission via breastfeeding and mortality. We now present endpoints through 12 months of age.\nInfants in Ethiopia, India, and Uganda born to HIV-infected women who chose to breastfeed were randomized to receive single-dose or extended-dose nevirapine.\nHIV transmission, mortality, HIV transmission or death.\nPrimary analysis included 987 and 903 infants in the single-dose and the extended-dose arms, respectively. HIV transmission was 8.9% in the extended-dose group compared to 10.4% in the single-dose group, but the difference was not significant [risk ratio: 0.87, 95% confidence interval (CI): 0.65-1.15]. Cumulative mortality at 12 months was half in the extended-dose group compared to the single-dose group (risk ratio: 0.53, 95% CI: 0.32-0.85). The impact of extended-dose nevirapine was highest in infants of mothers with CD4 cell count more than 350 cells/\u03bcl. Risk ratios for death (risk ratio: 0.38, 95% CI: 0.17-0.84) and HIV transmission or death (risk ratio: 0.54, 95% CI: 0.35-0.85) were statistically significant for the CD4 cell counts more than 350 cells/\u03bcl category, whereas none of the risk ratios were significant for the CD4 cell counts 200 cells/\u03bcl or less and CD4 cell counts 201-350 cells/\u03bcl categories.\nFor populations with limited access to HAART, our results provide evidence for the use of extended-dose regimens to prevent infant deaths and increase HIV-free survival in infants of HIV-infected breastfeeding women, particularly for infants of women with CD4 cell counts more than 350 cells/\u03bcl.", "title": "Twelve-month follow-up of Six Week Extended Dose Nevirapine randomized controlled trials: differential impact of extended-dose nevirapine on mother-to-child transmission and infant death by maternal CD4 cell count.", "date": "2011-02-19"}, {"article_id": "20554983", "content": "The most effective highly active antiretroviral therapy (HAART) to prevent mother-to-child transmission of human immunodeficiency virus type 1 (HIV-1) in pregnancy and its efficacy during breast-feeding are unknown.\nWe randomly assigned 560 HIV-1-infected pregnant women (CD4+ count, > or = 200 cells per cubic millimeter) to receive coformulated abacavir, zidovudine, and lamivudine (the nucleoside reverse-transcriptase inhibitor [NRTI] group) or lopinavir-ritonavir plus zidovudine-lamivudine (the protease-inhibitor group) from 26 to 34 weeks' gestation through planned weaning by 6 months post partum. A total of 170 women with CD4+ counts of less than 200 cells per cubic millimeter received nevirapine plus zidovudine-lamivudine (the observational group). Infants received single-dose nevirapine and 4 weeks of zidovudine.\nThe rate of virologic suppression to less than 400 copies per milliliter was high and did not differ significantly among the three groups at delivery (96% in the NRTI group, 93% in the protease-inhibitor group, and 94% in the observational group) or throughout the breast-feeding period (92% in the NRTI group, 93% in the protease-inhibitor group, and 95% in the observational group). By 6 months of age, 8 of 709 live-born infants (1.1%) were infected (95% confidence interval [CI], 0.5 to 2.2): 6 were infected in utero (4 in the NRTI group, 1 in the protease-inhibitor group, and 1 in the observational group), and 2 were infected during the breast-feeding period (in the NRTI group). Treatment-limiting adverse events occurred in 2% of women in the NRTI group, 2% of women in the protease-inhibitor group, and 11% of women in the observational group.\nAll regimens of HAART from pregnancy through 6 months post partum resulted in high rates of virologic suppression, with an overall rate of mother-to-child transmission of 1.1%. (ClinicalTrials.gov number, NCT00270296.)", "title": "Antiretroviral regimens in pregnancy and breast-feeding in Botswana.", "date": "2010-06-18"}, {"article_id": "18525035", "content": "Effective strategies are urgently needed to reduce mother-to-child transmission of human immunodeficiency virus type 1 (HIV-1) through breast-feeding in resource-limited settings.\nWomen with HIV-1 infection who were breast-feeding infants were enrolled in a randomized, phase 3 trial in Blantyre, Malawi. At birth, the infants were randomly assigned to one of three regimens: single-dose nevirapine plus 1 week of zidovudine (control regimen) or the control regimen plus daily extended prophylaxis either with nevirapine (extended nevirapine) or with nevirapine plus zidovudine (extended dual prophylaxis) until the age of 14 weeks. Using Kaplan-Meier analyses, we assessed the risk of HIV-1 infection among infants who were HIV-1-negative on DNA polymerase-chain-reaction assay at birth.\nAmong 3016 infants in the study, the control group had consistently higher rates of HIV-1 infection from the age of 6 weeks through 18 months. At 9 months, the estimated rate of HIV-1 infection (the primary end point) was 10.6% in the control group, as compared with 5.2% in the extended-nevirapine group (P<0.001) and 6.4% in the extended-dual-prophylaxis group (P=0.002). There were no significant differences between the two extended-prophylaxis groups. The frequency of breast-feeding did not differ significantly among the study groups. Infants receiving extended dual prophylaxis had a significant increase in the number of adverse events (primarily neutropenia) that were deemed to be possibly related to a study drug.\nExtended prophylaxis with nevirapine or with nevirapine and zidovudine for the first 14 weeks of life significantly reduced postnatal HIV-1 infection in 9-month-old infants. (ClinicalTrials.gov number, NCT00115648.)", "title": "Extended antiretroviral prophylaxis to reduce breast-milk HIV-1 transmission.", "date": "2008-06-06"}, {"article_id": "19119321", "content": "Daily nevirapine (NVP) prophylaxis to HIV-exposed infants significantly reduces breast-milk HIV transmission. We assessed NVP-resistance in Indian infants enrolled in the \"six-week extended-dose nevirapine\" (SWEN) trial who received single-dose NVP (SD-NVP) or SWEN for prevention of breast-milk HIV transmission but who also acquired subtype C HIV infection during the first year of life.\nStandard population sequencing and cloning for viral subpopulations present at > or =5% frequency were used to determine HIV genotypes from 94% of the 79 infected Indian infants studied. Timing of infection was defined based on when an infant's blood sample first tested positive for HIV DNA. SWEN-exposed infants diagnosed with HIV by six weeks of age had a significantly higher prevalence of NVP-resistance than those who received SD-NVP, by both standard population sequencing (92% of 12 vs. 38% of 29; p = 0.002) and low frequency clonal analysis (92% of 12 vs. 59% of 29; p = 0.06). Likelihood of infection with NVP-resistant HIV through breast-milk among infants infected after age six weeks was substantial, but prevalence of NVP-resistance did not differ among SWEN or SD-NVP exposed infants by standard population sequencing (15% of 13 vs. 15% of 20; p = 1.00) and clonal analysis (31% of 13 vs. 40% of 20; p = 0.72). Types of NVP-resistance mutations and patterns of persistence at one year of age were similar between the two groups. NVP-resistance mutations did differ by timing of HIV infection; the Y181C variant was predominant among infants diagnosed in the first six weeks of life, compared to Y188C/H during late breast-milk transmission.\nUse of SWEN to prevent breast-milk HIV transmission carries a high likelihood of resistance if infection occurs in the first six weeks of life. Moreover, there was a continued risk of transmission of NVP-resistant HIV through breastfeeding during the first year of life, but did not differ between SD-NVP and SWEN groups. As with SD-NVP, the value of preventing HIV infection in a large number of infants should be considered alongside the high risk of resistance associated with extended NVP prophylaxis.\nClinicalTrials.gov NCT00061321.", "title": "Nevirapine resistance and breast-milk HIV transmission: effects of single and extended-dose nevirapine prophylaxis in subtype C HIV-infected infants.", "date": "2009-01-03"}, {"article_id": "18657709", "content": "UNICEF/WHO recommends that infants born to HIV-infected mothers who do not have access to acceptable, feasible, affordable, sustainable, and safe replacement feeding should be exclusively breastfed for at least 6 months. The aim of three trials in Ethiopia, India, and Uganda was to assess whether daily nevirapine given to breastfed infants through 6 weeks of age can decrease HIV transmission via breastfeeding.\nHIV-infected women breastfeeding their infants were eligible for participation. Participants were randomly assigned to receive either single-dose nevirapine (nevirapine 200 mg to women in labour and nevirapine 2 mg/kg to newborns after birth) or 6 week extended-dose nevirapine (nevirapine 200 mg to women in labour and nevirapine 2 mg/kg to newborn babies after birth plus nevirapine 5 mg daily from days 8-42 for the infant). The randomisation sequences were generated by computer at a central data coordinating centre. The primary endpoint was HIV infection at 6 months of age in infants who were HIV PCR negative at birth. Analyses were by modified intention to treat, excluding infants with missing specimens and those with indeterminate or confirmed HIV infection at birth. These studies are registered with ClinicalTrials.gov, numbers NCT00074399, NCT00061321, and NCT00639938.\n2024 liveborn infants randomised in the study had at least one specimen tested before 6 months of age (1047 infants in the single-dose group and 977 infants in the extended-dose group). The modified intention-to-treat population included 986 infants in the single-dose group and 901 in the extended-dose group. At 6 months, 87 children in the single-dose group and 62 in the extended-dose group were infected with HIV (relative risk 0.80, 95% CI 0.58-1.10; p=0.16). At 6 weeks of age, 54 children in the single-dose group and 25 in the extended-dose group were HIV positive (0.54, 0.34-0.85; p=0.009). 393 infants in the single-dose group and 346 in the extended-dose group experienced grade 3 or 4 serious adverse events during the study (p=0.54).\nAlthough a 6-week regimen of daily nevirapine might be associated with a reduction in the risk of HIV transmission at 6 weeks of age, the lack of a significant reduction in the primary endpoint-risk of HIV transmission at 6 months-suggests that a longer course of daily infant nevirapine to prevent HIV transmission via breast milk might be more effective where access to affordable and safe replacement feeding is not yet available and where the risks of replacement feeding are high.\nUS National Institutes of Health; US National Institute of Allergy and Infectious Diseases; Fogarty International Center.", "title": "Extended-dose nevirapine to 6 weeks of age for infants to prevent HIV transmission via breastfeeding in Ethiopia, India, and Uganda: an analysis of three randomised controlled trials.", "date": "2008-07-29"}, {"article_id": "16052084", "content": "Single-dose nevirapine (NVP) prophylaxis to mother and infant is widely used in resource-constrained settings for preventing mother-to-child transmission (MTCT) of HIV-1. Where women do not access antenatal care or HIV testing, postexposure prophylaxis to the infant may be an important preventative strategy.\nThis multicentre, randomized, open-label clinical trial (October 2000 to September 2002) in South Africa compared single-dose NVP with 6 weeks of zidovudine (ZDV), commenced within 24 h of delivery among 1051 infants whose mothers had no prior antiretroviral therapy. HIV-1 infection rates were ascertained at birth, and at 6 and 12 weeks of age. Kaplan-Meier survival methods were used to estimate HIV-1 infection rates in an intention-to-treat analysis.\nOverall, 6 week and 12 week MTCT probability was 12.8% [95% confidence interval (CI),10.5-15.0] and 16.3% (95% CI,13.4-19.2), respectively. At 12 weeks, among infants who were not infected at birth, 24 (7.9%) infections occurred in the NVP arm and 41 (13.1%) in the ZDV arm (log rank P = 0.06). Using multivariate analysis, factors associated with infection following birth were ZDV use [odds ratio (OR), 1.8; 95% CI,1.1-3.2; P = 0.032), maternal CD4 cell count < 500 x 10(6) cells/l (OR, 2.5; 95% CI,1.3-5.0; P = 0.007), maternal viral load > 50 000 copies/ml (OR, 3.6; 95% CI,2.0-6.2; P < 0.0001) and breastfeeding (OR, 2.2; 95% CI,1.3-3.8; P = 0.006).\nA single-dose of NVP given to infants offers protection against HIV-1 infection and should be a strategy used in infants of mothers with untreated HIV infection.", "title": "A randomized trial of two postexposure prophylaxis regimens to reduce mother-to-child HIV-1 transmission in infants of untreated mothers.", "date": "2005-07-30"}]}
{"original_review": "26447539", "question_data": [{"question_id": 27, "question": "Is fatigue severity higher, lower, or the same when comparing doxepin to placebo?", "answer": "insufficient data", "evidence_quality": "low", "fulltext_required": "no", "comment": "", "relevant_sources": ["23561946"]}, {"question_id": 28, "question": "Is the progression of physical aspects of fatigue higher, lower, or the same when comparing rasagiline to placebo?", "answer": "lower", "evidence_quality": "high", "fulltext_required": "no", "comment": "", "relevant_sources": ["21482191"]}, {"question_id": 29, "question": "Is subjective fatigue higher, lower, or the same when comparing modafinil to placebo?", "answer": "no difference", "evidence_quality": "", "fulltext_required": "no", "comment": "", "relevant_sources": ["19620846", "16291885"]}, {"question_id": 30, "question": "Is fatigue severity higher, lower, or the same when comparing exercise to usual care?", "answer": "no difference", "evidence_quality": "low", "fulltext_required": "no", "comment": "", "relevant_sources": ["22257506", "21953509"]}, {"question_id": 31, "question": "Is subjective fatigue higher, lower, or the same when comparing memantine to placebo?", "answer": "no difference", "evidence_quality": "moderate", "fulltext_required": "no", "comment": "", "relevant_sources": ["21193343"]}, {"question_id": 32, "question": "Is subjective fatigue higher, lower, or the same when comparing methylphenidate to placebo?", "answer": "no difference", "evidence_quality": "moderate", "fulltext_required": "no", "comment": "cochrane authors disagree with paper authors", "relevant_sources": ["17674415"]}], "sources": [{"article_id": "18695158", "content": "Fatigue is a common complaint in Parkinson disease (PD). We investigated fatigue in a cohort of previously untreated patients with early PD enrolled in the Earlier vs Later Levodopa (ELLDOPA) clinical trial.\nA total of 361 patients were enrolled in the randomized, double-blind, placebo-controlled ELLDOPA trial and assigned to receive placebo or carbidopa-levodopa 37.5/150 mg, 75/300 mg, or 150/600 mg daily for 40 weeks, followed by a 2-week medication washout period. Subjects who scored >4 on the Fatigue Severity Scale were classified as fatigued. PD severity was assessed using the Unified Parkinson's Disease Rating Scale (UPDRS), Hoehn-Yahr scale, and Schwab-England Activities of Daily Living Scale. A subgroup of subjects underwent [(123)I]-beta-CIT SPECT to measure striatal dopamine transporter density.\nOf the 349 ELLDOPA subjects who completed fatigue measures, 128 were classified as fatigued at baseline. The fatigued group was significantly more impaired neurologically (UPDRS, all subscales and Hoehn and Yahr staging) and functionally (Schwab-England Scale) but no significant differences were observed in beta-CIT measurements between the two groups. Analysis of covariance showed a greater increase in fatigue score from baseline to the end of the 2-week washout in the placebo group (0.75 points) than in the three groups receiving levodopa (increases of 0.30 [150 mg/day], 0.36 [300 mg/day], and 0.33 [600 mg/day]; p = 0.03 for heterogeneity).\nFatigue is a frequent symptom in early, untreated, non-depressed patients with Parkinson disease (PD), affecting over 1/3 of the patients in this cohort at baseline and 50% by week 42. Fatigue was associated with the severity of PD, and progressed less in patients treated with levodopa.", "title": "Fatigue in levodopa-naive subjects with Parkinson disease.", "date": "2008-08-13"}, {"article_id": "19842011", "content": "Fatigue is common in Parkinson's disease (PD), occurring in up to 42% of patients (2). There is no recognized treatment. This is a study of modafinil for Parkinson's disease related fatigue. Ethical approval was given. Patients with idiopathic PD were recruited from a Movement Disorders clinic. Those with depression, dementia, and other causes for fatigue were excluded. Patients were assessed using the Fatigue Severity Scale (FSS), Hospital Anxiety and Depression Scale (HADS), self-rating of improvement, Epworth Sleepiness Scale (ESS), and UPDRS. Modafinil was titrated up over 4 weeks to maximum of 400 mg/day. There followed a 5 week maintenance phase before reassessment. Thirteen patients participated. No significant change was seen in any safety measure. The FSS did not change significantly, however those on modafinil rated an improvement in their fatigue compared to placebo. The Modafinil group had a statistically significant improvement on ESS (p < 0.05). This is a small study of modafinil in selected PD patients. There is a suggestion of improvement on the global clinical impression scale for fatigue, but no significant change on FSS. A larger study is needed to further evaluate this drug in PD fatigue. This study highlights the problems with recruitment when trialing treatments of non-motor symptoms in PD. A significant improvement in EDS was seen.", "title": "Modafinil for Parkinson's disease fatigue.", "date": "2009-10-21"}, {"article_id": "16291885", "content": "Excessive daytime somnolence (EDS) commonly complicates Parkinson's disease (PD). The aetiology of EDS is probably multifactorial but is probably exacerbated by dopaminergic medications. Modafinil is a wake-promoting agent approved for use in narcolepsy, but it is often used to treat a variety of somnolent conditions.\nA double blind, placebo controlled parallel design trial was conducted to assess the efficacy of modafinil (200-400 mg/day) for the treatment of EDS in PD. The primary efficacy measure was the Epworth Sleepiness (ES) scale score. Secondary efficacy points included the Unified Parkinson's Disease Rating Scale (UPDRS), the Fatigue Severity Scale, the Hamilton Depression Scale, and the multiple sleep latency test (MSLT).\nOf a total of 40 subjects (29 men, mean (SD) age 64.8 (11.3) years), randomised to modafinil or placebo, 37 completed the study. Modafinil failed to significantly improve ES scores compared with placebo (2.7 v 1.5 points improvement, respectively, p = 0.28). MSLT failed to improve with modafinil relative to placebo (-0.16 v -0.70, respectively, p = 0.14). UPDRS, global impressions, Fatigue Severity Scale, and Hamilton Depression Scale scores were unchanged. Adverse events were minimal.\nModafinil failed to significantly improve EDS in PD compared with placebo. The drug did not alter motor symptoms in PD and was well tolerated.", "title": "Modafinil for daytime somnolence in Parkinson's disease: double blind, placebo controlled parallel trial.", "date": "2005-11-18"}, {"article_id": "22257506", "content": "To investigate the feasibility and effectiveness of six weeks of home-based treadmill training in people with mild Parkinson's disease.\nPilot randomized controlled trial of a six-week intervention followed by a further six weeks follow-up.\nHome-based treadmill training with outcome measures taken at a hospital clinic.\nTwenty cognitively intact participants with mild Parkinson's disease and gait disturbance. Two participants from the treadmill training group and one from the control group dropped out.\nThe treadmill training group undertook a semi-supervised home-based programme of treadmill walking for 20-40 minutes, four times a week for six weeks. The control group received usual care.\nThe feasibility of the intervention was assessed by recording exercise adherence and acceptability, exercise intensity, fatigue, muscle soreness and adverse events. The primary outcome measure of efficacy was walking capacity (6-minute walk test distance).\nHome-based treadmill training was feasible, acceptable and safe with participants completing 78% (SD 36) of the prescribed training sessions. The treadmill training group did not improve their walking capacity compared to the control group. The treadmill training group showed a greater improvement than the control group in fatigue at post test (P = 0.04) and in quality of life at six weeks follow-up testing (P = 0.02).\nSemi-supervised home-based treadmill training is a feasible and safe form of exercise for cognitively intact people with mild Parkinson's disease. Further investigation regarding the effectiveness of home-based treadmill training is warranted.", "title": "Home-based treadmill training for individuals with Parkinson's disease: a randomized controlled pilot trial.", "date": "2012-01-20"}, {"article_id": "21193343", "content": "To perform an exploratory study evaluating memantine for several common non-motor problems in Parkinson's disease.\nWe conducted a single center, double-blind, placebo controlled pilot trial of memantine, titrated to 20 mg/day, in PD subjects. Inclusion criteria were intentionally broad and included both fluctuating and non-fluctuating patients. After baseline assessments, subjects (N = 40) were randomized to drug and placebo groups. They received a battery of traditional and non-motor assessments. After a safety call (2 weeks after baseline) they returned for identical assessments at week 8. An 8-week open label extension was started if desired.\nSubject demographics (age 69.1 \u00b1 7.8; 24 males), were similar in the drug and placebo groups. Four dropped from the study while on drug vs. none on placebo. Two of 36 remaining dropped out over the 8-week open label section. Of the 34 who completed the final open label visit, 24 elected to be prescribed memantine after the study. During the controlled trial, there was no significant change in UPDRS section I or II, Epworth sleepiness scale, fatigue severity scale, Hamilton depression scale, Conner adult inventory, PD Quality of Life-39, or clinical global impressions. UPDRS \"on\" motor scores tended to improved, p = 0.19.\nMemantine was well tolerated in PD; however, specific measures of sleepiness, fatigue, depression, and attention did not significantly improve. The majority of subjects elected to stay on the drug after the open label extension suggesting some unassessed benefit.", "title": "Memantine for non-motor features of Parkinson's disease: a double-blind placebo controlled exploratory pilot trial.", "date": "2011-01-05"}, {"article_id": "21953509", "content": "Fatigue is one of the most disabling non-motor symptoms for people with Parkinson's disease. Exercise may modify fatigue. This study examines prescribed exercise effects on physical activity levels, well-being, and fatigue in Parkinson's disease.\nIn this single-blinded trial, participants were randomly assigned to either a 12 week community exercise program or control group. Primary outcome measures were fatigue (Fatigue Severity Scale) and physical activity.\nThirty-nine people with Parkinson's disease were included: 20 in exercise and 19 in control. Sixty-five percent of the study group were fatigued (n = 24, mean 4.02, SD 1.48). Increased fatigue was associated with lower mobility and activity (P < .05). Individuals participated in a mean of 15 (SD 10) exercise sessions with no significant change in fatigue, mobility, well-being, or physical activity after exercise (P \u2265 .05).\nParticipation in weekly exercise did not improve fatigue in people with Parkinson's Disease.", "title": "Weekly exercise does not improve fatigue levels in Parkinson's disease.", "date": "2011-09-29"}, {"article_id": "23561946", "content": "Although a variety of pharmacologic and non-pharmacologic treatments are effective for insomnia in the general population, insomnia in Parkinson's disease differs in important ways and may need different treatments. No studies have conclusively demonstrated effective insomnia treatments in Parkinson's disease.\nWe conducted a three-arm six-week randomized pilot study assessing non-pharmacologic treatment (cognitive behavioural therapy with bright light therapy) or doxepin (10 mg daily), compared to an inactive placebo in Parkinson's patients with insomnia. Sleep outcomes included insomnia scales, clinical global impression, sleep diaries and actigraphy. Secondary outcomes included motor severity, fatigue, depression and quality of life.\n18 patients were randomized, 6 to each group. Compared to placebo, doxepin improved the Insomnia Severity Index (-9 \u00b1 5.4 vs. -2 \u00b1 3.9, p = 0.03), the SCOPA-night score (-5.2 \u00b1 1.5 vs. -2.3 \u00b1 2.8, p = 0.049), the Pittsburgh Sleep Quality Index-sleep disturbances subscale (-0.5 \u00b1 0.5 vs 0.2 \u00b1 0.4, p = 0.02), and both patient and examiner-rated clinical global impression of change (1.7 \u00b1 0.8 vs. 0.5 \u00b1 0.8, p = 0.03 and 1.4 \u00b1 0.5 vs. 0.3 \u00b1 0.5, p = 0.003). On secondary outcomes doxepin reduced the fatigue severity scale (p = 0.02) and improved scores on the Montreal Cognitive Assessment (p = 0.007). Non-pharmacological treatment reduced the Insomnia Severity Index (-7.8 \u00b1 3.8 vs. -2.0 \u00b1 3.9, p = 0.03), and the examiner-reported clinical global impression of change (p = 0.006), but was associated with decline in Parkinson Disease Questionnaire-39. There were no changes in other primary and secondary outcomes, including actigraphy outcomes. Adverse events were comparable in all groups.\nDoxepin and non-pharmacologic treatment substantially improved insomnia in Parkinson's disease. These potential benefits must be replicated in a full confirmatory randomized controlled trial.", "title": "Doxepin and cognitive behavioural therapy for insomnia in patients with Parkinson's disease -- a randomized study.", "date": "2013-04-09"}, {"article_id": "17674415", "content": "Fatigue is a common nonmotor symptom in idiopathic Parkinson disease (IPD) that can prominently affect everyday function. This study was a randomized, double-blind, placebo-controlled trial evaluating methylphenidate for the treatment of fatigue in patients with IPD maintained on their regular medications. Thirty-six patients were randomized to receive either methylphenidate (10 mg three times per day; n = 17) or placebo (n = 19) for 6 weeks. Primary outcomes were the change from baseline on two separate self-report fatigue questionnaires: the Fatigue Severity Scale (FSS) and the Multidimensional Fatigue Inventory (MFI). Secondary outcomes included the Unified Parkinson Disease Rating Scale (UPDRS) motor score and the five individual domains of the MFI. Fourteen patients in the methylphenidate group and 16 patients in the control group remained on the intervention for the entire study period. In the treatment arm, mean FSS score was reduced by 6.5 points (from a baseline of 43.8) and mean MFI score was reduced by 8.4 points (from a baseline of 51.0). Both these reductions were significant (P < 0.04). Smaller reductions in the placebo group were nonsignificant. Mean UPDRS motor score did not change significantly in either group. Analysis of MFI subscores showed a significant reduction in General Fatigue in the methylphenidate group (P < 0.001). Overall, adverse effects of medication were more frequent in the placebo group. In conclusion, methylphenidate was effective in lowering fatigue scores in patients with IPD following a 6-week treatment period.", "title": "Methylphenidate improves fatigue scores in Parkinson disease: a randomized controlled trial.", "date": "2007-08-04"}, {"article_id": "22855866", "content": "Epidemiologic studies consistently link caffeine, a nonselective adenosine antagonist, to lower risk of Parkinson disease (PD). However, the symptomatic effects of caffeine in PD have not been adequately evaluated.\nWe conducted a 6-week randomized controlled trial of caffeine in PD to assess effects upon daytime somnolence, motor severity, and other nonmotor features. Patients with PD with daytime somnolence (Epworth >10) were given caffeine 100 mg twice daily \u00d73 weeks, then 200 mg twice daily \u00d73 weeks, or matching placebo. The primary outcome was the Epworth Sleepiness Scale score. Secondary outcomes included motor severity, sleep markers, fatigue, depression, and quality of life. Effects of caffeine were analyzed with Bayesian hierarchical models, adjusting for study site, baseline scores, age, and sex.\nOf 61 patients, 31 were randomized to placebo and 30 to caffeine. On the primary intention-to-treat analysis, caffeine resulted in a nonsignificant reduction in Epworth Sleepiness Scale score (-1.71 points; 95% confidence interval [CI] -3.57, 0.13). However, somnolence improved on the Clinical Global Impression of Change (+0.64; 0.16, 1.13, intention-to-treat), with significant reduction in Epworth Sleepiness Scale score on per-protocol analysis (-1.97; -3.87, -0.05). Caffeine reduced the total Unified Parkinson's Disease Rating Scale score (-4.69 points; -7.7, -1.6) and the objective motor component (-3.15 points; -5.50, -0.83). Other than modest improvement in global health measures, there were no changes in quality of life, depression, or sleep quality. Adverse events were comparable in caffeine and placebo groups.\nCaffeine provided only equivocal borderline improvement in excessive somnolence in PD, but improved objective motor measures. These potential motor benefits suggest that a larger long-term trial of caffeine is warranted.\nThis study provides Class I evidence that caffeine, up to 200 mg BID for 6 weeks, had no significant benefit on excessive daytime sleepiness in patients with PD.", "title": "Caffeine for treatment of Parkinson disease: a randomized controlled trial.", "date": "2012-08-03"}, {"article_id": "21482191", "content": "The ADAGIO study investigated whether rasagiline has disease-modifying effects in Parkinson's disease. Rasagiline 1 mg per day, but not 2 mg per day, was shown to be efficacious in the primary analysis. Here, we report additional secondary and post-hoc analyses of the ADAGIO study.\nADAGIO was a placebo-controlled, double-blind, multicentre, delayed-start study, in which 1176 patients with untreated early Parkinson's disease were randomly assigned to receive rasagiline 1 mg or 2 mg per day for 72 weeks (early-start groups) or placebo for 36 weeks followed by rasagiline 1 mg or 2 mg per day for 36 weeks (delayed-start groups). We assessed the need for additional antiparkinsonian therapy and changes in non-motor experiences of daily living and fatigue scales (prespecified outcomes) and changes in unified Parkinson's disease rating scale (UPDRS) scores and subscores in placebo and active groups (post-hoc outcomes). The ADAGIO study is registered with ClinicalTrials.gov, number NCT00256204.\nThe need for additional antiparkinsonian therapy was reduced with rasagiline 1 mg (25 of 288 [9%] patients) and 2 mg (26 of 293 [9%]) versus placebo (108 of 593 [18%]; odds ratio for 1 mg rasagiline vs placebo 0\u00b741, 95% CI 0\u00b725-0\u00b765, p=0\u00b70002; 2 mg rasagiline vs placebo 0\u00b741, 0\u00b726-0\u00b764, p=0\u00b70001). At week 36, both doses significantly improved UPDRS motor subscores compared with placebo (1 mg rasagiline mean difference -1\u00b788 [SE 0\u00b735]; 2 mg rasagiline -2\u00b718 [0\u00b735]; both p<0\u00b70001) and activities of daily living subscores (ADL; 1 mg rasagiline -0\u00b786 [0\u00b718]; 2 mg rasagiline -0\u00b788 [0\u00b718]; both p<0\u00b70001), and 1 mg rasagiline significantly improved UPDRS mentation subscore (-0\u00b722 [0\u00b708]; p=0\u00b7004). At week 72, the only significant difference between early-start and delayed-start groups was for ADL subscore with the 1 mg dose (-0\u00b762 [0\u00b729]; p=0\u00b7035). When assessed for the effect on non-motor symptoms at week 36, both doses showed benefits on the Parkinson fatigue scale versus placebo (1 mg rasagiline mean difference -0\u00b714 [SE 0\u00b705], p=0\u00b70032; 2 mg rasagiline -0\u00b719 [0\u00b705], p<0\u00b70001), and the 1 mg dose showed benefits on the scale for non-motor experiences of daily living compared with placebo (mean difference -0\u00b733 [0\u00b717]; p=0\u00b7049). The rate of progression of total UPDRS score for patients in the placebo group was 4\u00b73 points [SE 0\u00b73] over 36 weeks, with extrapolation to about 6 units per year. In the placebo group, patients with the lowest quartile of baseline UPDRS scores (\u226414; n=160) progressed more slowly than did those with highest scores (>25\u00b75; n=145; mean difference -3\u00b746 [SE 0\u00b777]; p<0\u00b70001).\nThese findings show that rasagiline delayed the need for symptomatic antiparkinsonian drugs and emphasise the contribution of the UPDRS ADL in the response of the rasagiline 1 mg per day early-start versus delayed-start group. The rate of UPDRS deterioration was less than was anticipated from previous studies and correlated with baseline severity. Understanding of the pattern of UPDRS deterioration is essential to assess disease modification.\nTeva Pharmaceutical Industries and H Lundbeck A/S.", "title": "A double-blind, delayed-start trial of rasagiline in Parkinson's disease (the ADAGIO study): prespecified and post-hoc analyses of the need for additional therapies, changes in UPDRS scores, and non-motor outcomes.", "date": "2011-04-13"}, {"article_id": "19620846", "content": "Fatigue is a major nonmotor symptom in Parkinson disease(PD). It is associated with reduced activity and lower quality of life.\nTo determine if modafinil improves subjective fatigue and physical fatigability in PD.\nNineteen PD patients who reported significant fatigue in the Multidimensional Fatigue Inventory (MFI) participated in this 8-week study. Subjects took their regular medications and were randomly assigned to the treatment group (9 subjects, modafinil 100-mg capsule BID) or placebo group (10 subjects). We used the MFI to measure subjective fatigue and used finger tapping and intermittent force generation to evaluate physical fatigability. Subjects also completed the Epworth Sleepiness Scale (ESS) and the Center of Epidemiological Study-Depression Scale.\nThere were no significant differences at baseline and at 1 month in finger tapping and ESS between the modafinil and placebo groups. At 2 months, the modafinil group had a higher tapping frequency (P<0.05), shorter dwell time (P<0.05), and less fatigability in finger tapping and tended to have lower ESS scores (P<0.12) than the placebo group. However, there was no difference between groups over time for any dimension of the MFI .\nThis small study demonstrated that although modafinil may be effective in reducing physical fatigability in PD, it did not improve fatigue symptoms.", "title": "Using modafinil to treat fatigue in Parkinson disease: a double-blind, placebo-controlled pilot study.", "date": "2009-07-22"}]}
{"original_review": "33565123", "question_data": [{"question_id": 45, "question": "Is the coverage of appropriate treatment from an appropriate provider for any iCCM illness higher, lower, or the same when comparing iCCM to usual facility services plus CCM for malaria?", "answer": "uncertain effect", "evidence_quality": "very low", "fulltext_required": "yes", "comment": "", "relevant_sources": ["26787147"]}, {"question_id": 46, "question": "Is the coverage of careseeking to an appropriate provider for any iCCM illness higher, lower, or the same when comparing iCCM to usual facility services plus CCM for malaria?", "answer": "no difference", "evidence_quality": "low", "fulltext_required": "yes", "comment": "", "relevant_sources": ["23136276"]}], "sources": [{"article_id": "26267141", "content": "Evidence is limited on whether Integrated Community Case Management (iCCM) improves treatment coverage of the top causes of childhood mortality (acute respiratory illnesses (ARI), diarrhoea and malaria). The coverage impact of iCCM in Central Uganda was evaluated.\nBetween July 2010 and December 2012 a pre-post quasi-experimental study in eight districts with iCCM was conducted; 3 districts without iCCM served as controls. A two-stage household cluster survey at baseline (n = 1036 and 1042) and end line (n = 3890 and 3844) was done in the intervention and comparison groups respectively. Changes in treatment coverage and timeliness were assessed using difference in differences analysis (DID). Mortality impact was modelled using the Lives Saved Tool.\n5,586 Village Health Team members delivered 1,907,746 treatments to children under age five. Use of oral rehydration solution (ORS) and zinc treatment of diarrhoea increased in the intervention area, while there was a decrease in the comparison area (DID = 22.9, p = 0.001). Due to national stock-outs of amoxicillin, there was a decrease in antibiotic treatment for ARI in both areas; however, the decrease was significantly greater in the comparison area (DID = 5.18; p<0.001). There was a greater increase in Artemisinin Combination Therapy treatment for fever in the intervention areas than in the comparison area but this was not significant (DID = 1.57, p = 0.105). In the intervention area, timeliness of treatments for fever and ARI increased significantly higher in the intervention area than in the comparison area (DID = 2.12, p = 0.029 and 7.95, p<0.001, respectively). An estimated 106 lives were saved in the intervention area while 611 lives were lost in the comparison area.\niCCM significantly increased treatment coverage for diarrhoea and fever, mitigated the effect of national stock outs of amoxicillin on ARI treatment, improved timeliness of treatments for fever and ARI and saved lives.", "title": "Evaluation of Integrated Community Case Management in Eight Districts of Central Uganda.", "date": "2015-08-13"}, {"article_id": "22438367", "content": "To evaluate the Indian Integrated Management of Neonatal and Childhood Illness (IMNCI) programme, which integrates improved treatment of illness for children with home visits for newborn care, to inform its scale-up.\nCluster randomised trial.\n18 clusters (population 1.1 million) in Haryana, India.\n29,667 births in intervention clusters and 30,813 in control clusters.\nCommunity health workers were trained to conduct postnatal home visits and women's group meetings; physicians, nurses, and community health workers were trained to treat or refer sick newborns and children; supply of drugs and supervision were strengthened.\nNeonatal and infant mortality; newborn care practices.\nThe infant mortality rate (adjusted hazard ratio 0.85, 95% confidence interval 0.77 to 0.94) and the neonatal mortality rate beyond the first 24 hours (adjusted hazard ratio 0.86, 0.79 to 0.95) were significantly lower in the intervention clusters than in control clusters. The adjusted hazard ratio for neonatal mortality rate was 0.91 (0.80 to 1.03). A significant interaction was found between the place of birth and the effect of the intervention for all mortality outcomes except post-neonatal mortality rate. The neonatal mortality rate was significantly lower in the intervention clusters in the subgroup born at home (adjusted hazard ratio 0.80, 0.68 to 0.93) but not in the subgroup born in a health facility (1.06, 0.91 to 1.23) (P value for interaction = 0.001). Optimal newborn care practices were significantly more common in the intervention clusters.\nImplementation of the IMNCI resulted in substantial improvement in infant survival and in neonatal survival in those born at home. The IMNCI should be a part of India's strategy to achieve the millennium development goal on child survival.\nClinical trials NCT00474981; ICMR Clinical Trial Registry CTRI/2009/091/000715.", "title": "Effect of implementation of Integrated Management of Neonatal and Childhood Illness (IMNCI) programme on neonatal and infant mortality: cluster randomised controlled trial.", "date": "2012-03-23"}, {"article_id": "24053172", "content": "Efforts to improve access to treatment for common illnesses in children less than five years initially targeted malaria alone under the home management of malaria strategy. However under this strategy, children with other illnesses were often wrongly treated with anti-malarials. Integrated community case management of common childhood illnesses is now recommended but its effect on promptness of appropriate pneumonia treatment is unclear.\nTo determine the effect of integrated malaria and pneumonia management on receiving prompt and appropriate antibiotics for pneumonia symptoms and treatment outcomes as well as determine associated factors.\nA follow-up study was nested within a cluster-randomized trial that compared under-five mortality in areas where community health workers (CHWs) treated children with malaria and pneumonia (intervention areas) and where they treated children with malaria only (control areas). Children treated by CHWs were enrolled on the day of seeking treatment from CHWs (609 intervention, 667 control) and demographic, illness, and treatment seeking information was collected. Further information on illness and treatment outcomes was collected on day four. The primary outcome was prompt and appropriate antibiotics for pneumonia symptoms and the secondary outcome was treatment outcomes on day four.\nChildren in the intervention areas were more likely to receive prompt and appropriate antibiotics for pneumonia symptoms compared to children in the control areas (RR\u2009=\u20093.51, 95%CI\u2009=\u20091.75-7.03). Children in the intervention areas were also less likely to have temperature \u226537.5\u00b0C on day four (RR\u2009=\u20090.29, 95%CI\u2009=\u20090.11-0.78). The decrease in fast breathing between day one and four was greater in the intervention (9.2%) compared to the control areas (4.2%, p-value\u2009=\u20090.01).\nIntegrated community management of malaria and pneumonia increases prompt and appropriate treatment for pneumonia symptoms and improves treatment outcomes.\nISRCTN52966230.", "title": "Integrated community case management of malaria and pneumonia increases prompt and appropriate treatment for pneumonia symptoms in children under five years in Eastern Uganda.", "date": "2013-09-24"}, {"article_id": "25674350", "content": "A trial to evaluate the Integrated Management of Neonatal and Childhood Illness (IMNCI) strategy showed that the intervention resulted in lower infant mortality and improved infant care practices. In this paper, we present the results of a secondary analysis to examine the effect of the IMNCI strategy on inequities in health indicators.\nThe trial was a cluster-randomized controlled trial in 18 primary health centre areas. For this analysis, the population was divided into subgroups by wealth status (using Principal Component Analysis), religion and caste, education of mother and sex of the infant. Multiple linear regression analysis was used to examine inequity gradients in neonatal and post-neonatal mortality, care practices and care seeking, and the differences in these gradients between intervention and control clusters.\nInequity in post-neonatal infant mortality by wealth status was lower in the intervention as compared to control clusters (adjusted difference in gradients 2.2 per 1000, 95% confidence interval (CI) 0 to 4.4 per 1000, P\u2009=\u20090.053). The intervention had no effect on inequities in neonatal mortality. The intervention resulted in a larger effect on breastfeeding within one hour of birth in poorer families (difference in inequity gradients 3.0%, CI 1.5 to 4.5, P\u2009<\u20090.001), in lower caste and minorities families, and in infants of mothers with fewer years of schooling. The intervention also reduced gender inequity in care seeking for severe neonatal illness from an appropriate provider (difference in inequity gradients 9.3%, CI 0.4 to 18.2, P\u2009=\u20090.042).\nImplementation of IMNCI reduced inequities in post-neonatal mortality, and newborn care practices (particularly starting breastfeeding within an hour of birth) and health care-seeking for severe illness. In spite of the intervention substantial inequities remained in the intervention group and therefore further efforts to ensure that health programs reach the vulnerable population subgroups are required.\nClinicaltrials.gov NCT00474981; ICMR Clinical Trial Registry CTRI/2009/091/000715.", "title": "Impact on inequities in health indicators: Effect of implementing the integrated management of neonatal and childhood illness programme in Haryana, India.", "date": "2015-02-13"}, {"article_id": "26787147", "content": "We conducted a prospective evaluation of the \"Rapid Scale-Up\" (RSU) program in Burkina Faso, focusing on the integrated community case management (iCCM) component of the program. We used a quasi-experimental design in which nine RSU districts were compared with seven districts without the program. The evaluation included documentation of program implementation, assessments of implementation and quality of care, baseline and endline coverage surveys, and estimation of mortality changes using the Lives Saved Tool. Although the program trained large numbers of community health workers, there were implementation shortcomings related to training, supervision, and drug stockouts. The quality of care provided to sick children was poor, and utilization of community health workers was low. Changes in intervention coverage were comparable in RSU and comparison areas. Estimated under-five mortality declined by 6.2% (from 110 to 103 deaths per 1,000 live births) in the RSU area and 4.2% (from 114 to 109 per 1,000 live births) in the comparison area. The RSU did not result in coverage increases or mortality reductions in Burkina Faso, but we cannot draw conclusions about the effectiveness of the iCCM strategy, given implementation shortcomings. The evaluation results highlight the need for greater attention to implementation of iCCM programs.", "title": "Independent Evaluation of the Rapid Scale-Up Program to Reduce Under-Five Mortality in Burkina Faso.", "date": "2016-01-21"}, {"article_id": "25243929", "content": "To examine whether community health volunteers induced significant changes in care seeking and treatment of ill children under five 2 years after their deployment in two underserved districts of Sierra Leone.\nA pre-test-post-test study with intervention and comparison groups was used. A household cluster survey was conducted among caregivers of 5643 children at baseline and of 5259 children at endline.\nIn the intervention districts, treatments provided by community health volunteers increased from 0 to 14.3% for all three conditions combined (P < 0.001). Care seeking from an appropriate provider was not statistically significant (OR = 1.50, 95% CI: 0.88-2.54) between intervention and comparison districts and coverage of appropriate treatment increased in both study groups for all three illnesses. However, the presence of community health volunteers was associated with a 105% increase in appropriate treatment for pneumonia (OR = 2.05, 95% CI: 1.22-3.42) and a 55% drop in traditional treatment for diarrhoea (OR = 0.45, 95% CI: 0.21-0.96). Community health volunteers were also associated with fewer facility treatments for malaria (OR = 0.21, 95% CI: 0.07-0.62).\nAfter implementing free care, coverage for treatment for all three illnesses in both study groups improved. Deployment of community health volunteers was associated with a reduced treatment burden at facilities and less reliance on traditional treatments.", "title": "Influence of community health volunteers on care seeking and treatment coverage for common childhood illnesses in the context of free health care in rural Sierra Leone.", "date": "2014-09-23"}, {"article_id": "22905758", "content": "Curative interventions delivered by community health workers (CHWs) were introduced to increase access to health services for children less than five years and have previously targeted single illnesses. However, CHWs in the integrated community case management of childhood illnesses strategy adopted in Uganda in 2010 will manage multiple illnesses. There is little documentation about the performance of CHWs in the management of multiple illnesses. This study compared the performance of CHWs managing malaria and pneumonia with performance of CHWs managing malaria alone in eastern Uganda and the factors influencing performance.\nA mixed methods study was conducted among 125 CHWs providing either dual malaria and pneumonia management or malaria management alone for children aged four to 59 months. Performance was assessed using knowledge tests, case scenarios of sick children, review of CHWs' registers, and observation of CHWs in the dual management arm assessing respiratory symptoms. Four focus group discussions with CHWs were also conducted.\nCHWs in the dual- and single-illness management arms had similar performance with respect to: overall knowledge of malaria (dual 72%, single 70%); eliciting malaria signs and symptoms (50% in both groups); prescribing anti-malarials based on case scenarios (82% dual, 80% single); and correct prescription of anti-malarials from record reviews (dual 99%, single 100%). In the dual-illness arm, scores for malaria and pneumonia differed on overall knowledge (72% vs 40%, p < 0.001); and correct doses of medicines from records (100% vs 96%, p < 0.001). According to records, 82% of the children with fast breathing had received an antibiotic. From observations 49% of CHWs counted respiratory rates within five breaths of the physician (gold standard) and 75% correctly classified the children. The factors perceived to influence CHWs' performance were: community support and confidence, continued training, availability of drugs and other necessary supplies, and cooperation from formal health workers.\nCHWs providing dual-illness management handled malaria cases as well as CHWs providing single-illness management, and also performed reasonably well in the management of pneumonia. With appropriate training that emphasizes pneumonia assessment, adequate supervision, and provision of drugs and necessary supplies, CHWs can provide integrated treatment for malaria and pneumonia.", "title": "Performance of community health workers under integrated community case management of childhood illnesses in eastern Uganda.", "date": "2012-08-22"}, {"article_id": "23555980", "content": "Development of resistance to first line antimalarials led to recommendation of artemisinin based combination therapies (ACTs). High adherence to ACTs provided by community health workers (CHWs) gave reassurance that community based interventions did not increase the risk of drug resistance. Integrated community case management of illnesses (ICCM) is now recommended through which children will access both antibiotics and antimalarials from CHWs. Increased number of medicines has been shown to lower adherence.\nTo compare adherence to antimalarials alone versus antimalarials combined with antibiotics under ICCM in children less than five years.\nA cohort study was nested within a cluster randomized trial that had CHWs treating children less than five years with antimalarials and antibiotics (intervention areas) and CHWs treating children with antimalarials only (control areas). Children were consecutively sampled from the CHWs' registers in the control areas (667 children); and intervention areas (323 taking antimalarials only and 266 taking antimalarials plus antibiotics). The sampled children were visited at home on day one and four of treatment seeking. Adherence was assessed using self reports and pill counts.\nAdherence in the intervention arm to antimalarials alone and antimalarials plus antibiotics arm was similar (mean 99% in both groups) but higher than adherence in the control arm (antimalarials only) (mean 96%). Forgetfulness (38%) was the most cited reason for non-adherence. At adjusted analysis: absence of fever (OR = 3.3, 95%CI =1.6-6.9), seeking care after two or more days (OR = 2.2, 95%CI = 1.3-3.7), not understanding instructions given (OR = 24.5, 95%CI = 2.7-224.5), vomiting (OR = 2.6, 95%CI = 1.2-5.5), and caregivers' perception that the child's illness was not severe (OR = 2.0, 95%CI = 1.1-3.8) were associated with non-adherence.\nAddition of antibiotics to antimalarials did not lower adherence. However, caregivers should be adequately counseled to understand the dosing regimens; continue with medicines even when the child seems to improve; and re-administer doses that have been vomited.", "title": "High adherence to antimalarials and antibiotics under integrated community case management of illness in children less than five years in eastern Uganda.", "date": "2013-04-05"}, {"article_id": "23136276", "content": "We compared use of community medicine distributors (CMDs) and drug use under integrated community case management and home-based management strategies in children 6-59 months of age in eastern Uganda. A cross-sectional study with 1,095 children was nested in a cluster randomized trial with integrated community case management (CMDs treating malaria and pneumonia) as the intervention and home-based management (CMDs treating only malaria) as the control. Care-seeking from CMDs was higher in intervention areas (31%) than in control areas (22%; P = 0.01). Prompt and appropriate treatment of malaria was higher in intervention areas (18%) than in control areas (12%; P = 0.03) and among CMD users (37%) than other health providers (9%). The mean number of drugs among CMD users compared with other health providers was 1.6 versus 2.4 in intervention areas and 1.4 versus 2.3 in control areas. Use of CMDs was low. However, integrated community case management of childhood illnesses increased use of CMDs and rational drug use.", "title": "Increased use of community medicine distributors and rational use of drugs in children less than five years of age in Uganda caused by integrated community case management of fever.", "date": "2013-01-03"}, {"article_id": "25172514", "content": "To determine the effect of implementation of the Integrated Management of Neonatal and Childhood Illness strategy on treatment seeking practices and on neonatal and infant morbidity.\nCluster randomised trial.\nHaryana, India.\n29,667 births in nine intervention clusters and 30,813 births in nine control clusters.\nThe pre-specified outcome was the effect on treatment seeking practices. Post hoc exploratory analyses assessed morbidity, hospital admission, post-neonatal infant care, and nutritional status outcomes.\nThe Integrated Management of Neonatal and Childhood Illness intervention included home visits by community health workers, improved case management of sick children, and strengthening of health systems. Outcomes were ascertained through interviews with randomly selected caregivers: 6204, 3073, and 2045 in intervention clusters and 6163, 3048, and 2017 in control clusters at ages 29 days, 6 months, and 12 months, respectively.\nIn the intervention cluster, treatment was sought more often from an appropriate provider for severe neonatal illness (risk ratio 1.76, 95% confidence interval 1.38 to 2.24), for local neonatal infection (4.86, 3.80 to 6.21), and for diarrhoea at 6 months (1.96, 1.38 to 2.79) and 12 months (1.22, 1.06 to 1.42) and pneumonia at 6 months (2.09, 1.31 to 3.33) and 12 months (1.44, 1.00 to 2.08). Intervention mothers reported fewer episodes of severe neonatal illness (risk ratio 0.82, 0.67 to 0.99) and lower prevalence of diarrhoea (0.71, 0.60 to 0.83) and pneumonia (0.73, 0.52 to 1.04) in the two weeks preceding the 6 month interview and of diarrhoea (0.63, 0.49 to 0.80) and pneumonia (0.60, 0.46 to 0.78) in the two weeks preceding the 12 month interview. Infants in the intervention clusters were more likely to still be exclusively breast fed in the sixth month of life (risk ratio 3.19, 2.67 to 3.81).\nImplementation of the Integrated Management of Neonatal and Childhood Illness programme was associated with timely treatment seeking from appropriate providers and reduced morbidity, a likely explanation for the reduction in mortality observed following implementation of the programme in this study.Trial registration Clinical trials NCT00474981; ICMR Clinical Trial Registry CTRI/2009/091/000715.", "title": "Effect of implementation of integrated management of neonatal and childhood illness programme on treatment seeking practices for morbidities in infants: cluster randomised trial.", "date": "2014-08-31"}, {"article_id": "30024811", "content": "To assess a community health worker (CHW) program's impact on childhood illness treatment in rural Liberia.\nWe deployed CHWs in half of Rivercess County in August 2015 with the other half constituting a comparison group until July 2016. All CHWs were provided cash incentives, supply chain support, and monthly clinical supervision. We conducted stratified cluster-sample population-based surveys at baseline (March-April 2015) and follow-up (April-June 2016) and performed a difference-in-differences analysis, adjusted by inverse probability of treatment weighting, to assess changes in treatment of fever, diarrhea, and acute respiratory infection by a qualified provider.\nWe estimated a childhood treatment difference-in-differences of 56.4 percentage points (95% confidence interval [CI]\u2009=\u200936.4, 76.3). At follow-up, CHWs provided 57.6% (95% CI\u2009=\u200942.8, 71.2) of treatment in the intervention group. The difference-in-differences diarrhea oral rehydration therapy was 22.4 percentage points (95% CI\u2009=\u2009-0.7, 45.5).\nImplementation of a CHW program in Rivercess County, Liberia, was associated with large, statistically significant improvements treatment by a qualified provider; however, improvements in correct diarrhea treatment were lower than improvements in coverage. Findings from this study offer support for expansion of Liberia's new National Community Health Assistant Program.", "title": "A Community Health Worker Intervention to Increase Childhood Disease Treatment Coverage in Rural Liberia: A Controlled Before-and-After Evaluation.", "date": "2018-07-20"}, {"article_id": "27102196", "content": "Evidence suggests that community-based interventions that promote improved home-based practices and care-seeking behaviour can have a large impact on maternal and child mortality in regions where rates are high. We aimed to assess whether an intervention package based on the WHO Integrated Management of Childhood Illness handbook and community mobilisation could reduce under-5 mortality in rural Guinea-Bissau, where the health service infrastructure is weak.\nWe did a non-masked cluster-randomised controlled trial (EPICS) in the districts of Tombali and Quinara in Guinea-Bissau. Clusters of rural villages were stratified by ethnicity and distance from a regional health centre, and randomly assigned (1:1) to intervention or control using a computerised random number generator. Women were eligible if they lived in one of the clusters at baseline survey prior to randomisation and if they were aged 15-49 years or were primary caregivers of children younger than 5 years. Their children were eligible if they were younger than 5 years or were liveborn after intervention services could be implemented on July 1, 2008. In villages receiving the intervention, community health clubs were established, community health workers were trained in case management, and traditional birth attendants were trained to care for pregnant women and newborn babies, and promote facility-based delivery. Registered nurses supervised community health workers and offered mobile clinic services. Health centres were not improved. The control group received usual services. The primary outcome was the proportion of children dying under age 5 years, and was analysed in all eligible children up to final visits to villages between Jan 1 and March 31, 2011. This trial is registered with ISRCTN, number ISRCTN52433336.\nOn Aug 30, 2007, we randomly assigned 146 clusters to intervention (73 clusters, 5669 women, and 4573 children) or control (73 clusters, 5840 women, and 4675 children). From randomisation until the end of the trial (last visit by June 30, 2011), the intervention clusters had 3093 livebirths and the control clusters had 3194. 6729 children in the intervention group and 6894 in the control group aged 0-5 years on July 1, 2008, or liveborn subsequently were analysed for mortality outcomes. 311 (4\u00b76%) of 6729 children younger than 5 years died in the intervention group compared with 273 (4\u00b70%) of 6894 in the control group (relative risk 1\u00b716 [95% CI 0\u00b799-1\u00b737]).\nOur package of community-based interventions did not reduce under-5 mortality in rural Guinea-Bissau. The short timeframe and other trial limitations might have affected our results. Community-based health promotion and basic first-line services in fragile contexts with weak secondary health service infrastructure might be insufficient to reduce child deaths.\nEffective Intervention.", "title": "Effects of community health interventions on under-5 mortality in rural Guinea-Bissau (EPICS): a cluster-randomised controlled trial.", "date": "2016-04-23"}]}
{"original_review": "34753201", "question_data": [{"question_id": 47, "question": "Is the incidence of breast hematomas within 90 days of breast surgery higher, lower, or the same when comparing NSAIDs to placebo?", "answer": "no difference", "evidence_quality": "low", "fulltext_required": "yes", "comment": "Romundstad/16428536 used in original analysis but weight < 25%", "relevant_sources": ["27935990"]}, {"question_id": 48, "question": "Is pain intensity 24 (\u00b1 12) hours following surgery higher, lower, or the same when comparing NSAIDs to placebo?", "answer": "lower", "evidence_quality": "low", "fulltext_required": "yes", "comment": "8770304 and 16146476 both don't mention stats but they point toward the correct answer anyway", "relevant_sources": ["31800611", "16146476", "8770304"]}], "sources": [{"article_id": "19338879", "content": "Narcotics have traditionally been used to control pain after augmentation mammaplasty, but they have adverse side effects, including addiction potential, clouded sensorium, nausea, and respiratory depression. Alternative strategies for managing postoperative pain are expensive, cumbersome, and also have their own risks. While long-term use of celecoxib has been associated with an increased risk of serious adverse cardiovascular effects, no problems have been reported with short-term use.\nThe purpose of this study was to determine whether the addition of celecoxib, a selective nonsteroidal anti-inflammatory, to an analgesic regimen reduced narcotic use and pain following augmentation mammaplasty.\nOne hundred patients underwent submuscular augmentation mammaplasty with smooth saline-filled mammary prostheses using an intravenous sedation technique. Group A (N = 50) used hydrocodone to manage postoperative pain. Group B (N = 50) used celecoxib 400 mg 1 to 2 hours before surgery and then daily in addition to hydrocodone postoperatively. Narcotic use, incidence of nausea, and complications were recorded. Pain was assessed daily with a Likert pain scale from 0 (no pain) to 10 (severe pain).\nGroup A used 110 +/- 34 mg hydrocodone during the immediate 7-day postoperative period and reported an average pain scale score of 5.1. Group B, which used celecoxib, used 34 +/- 22 mg hydrocodone during the same period with an average pain scale score of 3.3. These differences were statistically significant (P < .05). Group B experienced 53% less nausea than Group A. There were no significant differences between the groups regarding age, implant size, or complications.\nPerioperative celecoxib administration in patients undergoing augmentation mammaplasty significantly reduced postoperative narcotic use, pain, and nausea. Its use should facilitate the patient's ability to resume everyday activities following surgery.", "title": "Celecoxib reduces narcotic use and pain following augmentation mammaplasty.", "date": "2006-01-01"}, {"article_id": "23908084", "content": "To explore whether perioperative intravenous flurbiprofen axetil can reduce the incidence and intensity of chronic pain for breast cancer after surgical treatment.\nThis randomized, double-blind, controlled trial enrolled 60 patients undergoing mastectomy and axillary lymph node dissection under general anesthesia. All patients accepted Hospital Anxiety and Depression Scale (HAD) tests the day before the surgery to evaluate depression and anxiety. The patients were randomly assigned to receive either 50 mg flurbiprofen axetil intravenously 15 minutes before the surgical incision and 6 hours later (group F) or intravenous 5 mL intralipid as a control (group C). All patients received patient-controlled intravenous analgesia (PCIA) with fentanyl postoperatively. Peripheral venous blood samples were drawn before the surgery, at 4 and 24 h after the surgery to detect the plasma level of PGE2 and tumor necrosis factor-\u03b1 (TNF-\u03b1). Postoperative fentanyl consumption, Numerical Rating Scale (NRS) scores and adverse effects were recorded at 2, 6, 12, 24 and 48 h after the surgery. The duration and intensity of pain were followed up by telephone at the 2nd-12th month after the surgery.\nThe incidence of pain at 2, 4, 6, and 12 months after the breast surgery was 33%, 20%, 15%, and 10%, respectively, and the average pain score was 0.77, 0.57, 0.28, and 0.18, respectively. Compared with group C, the scores of pain in group F were significantly lower at 2, 4, 6 and 12 months postoperatively (F=7.758, P=0.007). The incidence of pain in group F was significantly lower at 2, 4 and 6 months postoperatively (P<0.05). There was no significant difference in the incidence of pain between the groups at 12 months postoperatively (P>0.05). Preoperatively and at 4 and 24 h after the surgery, there was no significant difference in the level of TNF-\u03b1 between the two groups (F=0.530, P=0.470); but plasma concentration of PGE2 in group F was significantly lower than that in group C (F=5.646, P=0.021). No patients developed abnormal bleeding, peptic ulcer, impaired liver or renal function and respiratory depression.\nPerioperative intravenous infusion of 100 mg flurbiprofen axetil can decrease the intensity and incidence of chronic pain for breast cancer after surgical treatment.", "title": "Effect of perioperative intravenous flurbiprofen axetil on chronic postmastectomy pain.", "date": "2013-08-03"}, {"article_id": "27935990", "content": "Persistent pain is a challenging clinical problem after breast cancer treatment. After surgery, inflammatory pain and nociceptive input from nerve injury induce central sensitization which may play a role in the genesis of persistent pain. Using quantitative sensory testing, we tested the hypothesis that adding COX-2 inhibition to standard treatment reduces hyperalgesia after breast cancer surgery. A secondary hypothesis was that patients developing persistent pain would exhibit more postoperative hyperalgesia.\n138 women scheduled for lumpectomy/mastectomy under general anesthesia with paravertebral block were randomized to COX-2 inhibition (2x40mg parecoxib on day of surgery, thereafter 2x200mg celecoxib/day until day five) or placebo. Preoperatively and 1, 5, 15 days and 1, 3, 6, 12 months postoperatively, we determined electric and pressure pain tolerance thresholds in dermatomes C6/T4/L1 and a 100mm VAS score for pain. We calculated the sum of pain tolerance thresholds and analyzed change in these versus preoperatively using mixed models analysis with factor medication. To assess hyperalgesia in persistent pain patients we performed an additional analysis on patients reporting VAS>30 at 12 months.\n48 COX-2 inhibition and 46 placebo patients were analyzed in a modified intention to treat analysis. Contrary to our primary hypothesis, change in the sum of tolerance thresholds in the COX-2 inhibition group was not different versus placebo. COX-2 inhibition had an effect on pain on movement at postoperative day 5 (p<0.01). Consistent with our secondary hypothesis, change in sum of pressure pain tolerance thresholds in 11 patients that developed persistent pain was negative versus patients without pain (p<0.01) from day 5 to 1 year postoperatively.\nPerioperative COX-2 inhibition has limited value in preventing sensitization and persistent pain after breast cancer surgery. Central sensitization may play a role in the genesis of persistent postsurgical pain.", "title": "Hyperalgesia and Persistent Pain after Breast Cancer Surgery: A Prospective Randomized Controlled Trial with Perioperative COX-2 Inhibition.", "date": "2016-12-10"}, {"article_id": "8770304", "content": "Ketorolac is a parenteral nonsteroidal antiinflammatory drug (NSAID). Two features have limited its clinical utility: tendency to elicit kidney failure and inability to produce complete analgesia. Because most NSAIDs are weak acids (pKa 3-5) and become concentrated in acidic tissues, such as injured and inflamed tissues, we hypothesized that local administration may enhance its analgesic efficacy while lowering the potential for systemic complications.\nWe conducted a randomized, placebo-controlled study of 60 group I-II (American Society of Anesthesiology criteria) mastectomy patients, 20 in each group. Near the end of surgery and every 6 h postoperatively, 20 ml of the study solution containing normal saline with or without 30 mg of ketorolac were administered simultaneously either via a Jackson-Pratt drain or intravenously in a double-blind fashion. The quality of pain control, the amount and character of the drain fluid, incidence of nausea and vomiting, length of stay in the postoperative care unit, and amount of morphine used for treatment of break-through pain were recorded.\nIntraoperative administration of ketorolac resulted in better quality of pain control in the immediate postoperative period regardless of route of administration. The incidence of nausea was significantly higher in the placebo group, and drain output in the ketorolac groups did not exceed the output in the placebo group.\nAnalgesic of the locally administered ketorolac is equally effective to the efficacy of ketorolac administered intravenously.", "title": "Comparison of analgesic effect of locally and systemically administered ketorolac in mastectomy patients.", "date": "1996-01-01"}, {"article_id": "16328638", "content": "A study was conducted to determine whether a single 400-mg dose of oral celecoxib administered 30 min before surgery reduces the opioid requirement for patients undergoing aesthetic subpectoral breast augmentation. A total of 695 patients undergoing breast augmentation were randomly selected into either a placebo or a treatment group. The findings showed that patients who received 400 mg of celecoxib 30 min before surgery required significantly fewer opioid analgesics after the operation than those given a placebo (p < 0.001). It also was found that nonsmokers and multiparous women required significantly fewer opioids than smokers and nulliparous women (p < 0.001). On the basis of this prospective study, the authors recommend a single 400-mg dose of celecoxib administered 30 min before surgery to decrease opioid analgesic requirements after subpectoral breast augmentation.", "title": "The use of celecoxib for reduction of pain after subpectoral breast augmentation.", "date": "2005-12-06"}, {"article_id": "27454263", "content": "An intravenous form of ibuprofen has recently been approved by the Food and Drug Administration (FDA) and reports are rare on its co-administration with opioids.\nWe researched whether an intravenous ibuprofen-hydromorphone combination is synergistic, additive, or infra-additive on postoperative pain.\nA parallel-group, 1:1:1 allocation, randomized, double-blind controlled trial.\nUniversity teaching hospital in Korea.\nNinety patients, undergoing breast surgery, were divided into one of the 3 groups (I, H, IH groups). Positive analgesic efficacy was defined as a numeric rating scale (NRS)</= 3 on a 0 - 10 NRS, 30 minutes after the drug administration. Drugs were administered by the Dixon's up-and-down method. Starting doses were ibuprofen (I) 50 mg, hydromorphone (H) 0.25 mg, or ibuprofen 25 mg + hydromorphone 0.125 mg (IH). The maximum doses were ibuprofen 800 mg, hydromorphone 2 mg, or ibuprofen 400 mg + hydromorphone 1 mg. Combination index (CI) (additive: 0.9 - 1.1, synergism: < 0.9, antagonism: > 1.1), dose reduction index (DRI, a measure of how much the dose of each drug in a combination can be reduced), and isobologram were used to define the nature of their interaction.\nOne way ANOVA, Kruskal Wallis test, and Chi square test, significance level P < 0.05.\nThe median effective doses (ED50) of ibuprofen and hydromorphone were 1,447 mg and 1.5 mg, respectively. The median ED50 of the combination was ibuprofen 71 mg and hydromorphone 0.3 mg. Ibuprofen and hydromorphone showed a strong synergy (CI 0.2, DRI 20 and 5 for ibuprofen and hydromorphone at ED50).\nAnalgesic efficacy was observed during post-anesthesia care unit (PACU) period only.\nThe combination of intravenous ibuprofen and hydromorphone produces a strong synergistic analgesia on postoperative pain.", "title": "Synergistic Effect of Intravenous Ibuprofen and Hydromorphone for Postoperative Pain: Prospective Randomized Controlled Trial.", "date": "2016-07-28"}, {"article_id": "16146476", "content": "Breast cancer treatment with mastectomy and immediate breast reconstruction (IBR) is associated with intense pain in the primary post-operative period. The present prospective, placebo-controlled and double-blind study aimed to evaluate the analgesic efficacy of diclofenac, a non-steroid anti-inflammatory drug (NSAID), in combination with paracetamol and opioids. This was done by 64-h assessment of post-operative pain intensity, opioid consumption, blood loss, nausea and tiredness.\nFifty women selected for mastectomy and IBR with submuscular implants with or without axillary lymph node dissection (ALND) were randomized to receive diclofenac 50 mg x 3 or placebo rectally in addition to oral paracetamol and intravenous opioids delivered using a patient-controlled analgesia (PCA) technique.\nDuring the first 20 h post-surgery, patients who received diclofenac experienced significantly less pain when resting than those who received placebo. When moving, a non-significant estimated difference in pain in favour of diclofenac was also noted. Opioid consumption during the first 6 h post-operatively was 34% less with diclofenac than with placebo. Means (SD) were 16.9 (10.3) mg and 25.6 (10.2) mg, respectively (P = 0.007). After 64 h, the difference was no longer statistically significant. Post-operative bleeding was significantly higher with diclofenac than with placebo (P < 0.01). Nausea and tiredness did not differ between the groups.\nThe addition of NSAID to paracetamol and opioid-PCA reduced opioid consumption and improved pain relief during the first 20 h at rest but was not convincingly effective during mobilization. Post-operative blood loss was higher with diclofenac.", "title": "Analgesic efficacy of diclofenac in combination with morphine and paracetamol after mastectomy and immediate breast reconstruction.", "date": "2005-09-09"}, {"article_id": "31800611", "content": "Ketorolac has been associated with a lower risk of recurrence in retrospective studies, especially in patients with positive inflammatory markers. It is still unknown whether a single dose of pre-incisional ketorolac can prolong recurrence-free survival.\nThe KBC trial is a multicenter, placebo-controlled, randomized phase III trial in high-risk breast cancer patients powered for 33% reduction in recurrence rate (from 60 to 40%). Patients received one dose of ketorolac tromethamine or a placebo before surgery. Eligible patients were breast cancer patients, planned for curative surgery, and with a Neutrophil-to-Lymphocyte Ratio\u22654, node-positive disease or a triple-negative phenotype. The primary endpoint was Disease-Free Survival (DFS) at two years. Secondary endpoints included safety, pain assessment and overall survival.\nBetween February 2013 and July 2015, 203 patients were assigned to ketorolac (n = 96) or placebo (n = 107). Baseline characteristics were similar between arms. Patients had a mean age of 55.7 (SD14) years. At two years, 83.1% of the patients were alive and disease free in the ketorolac vs. 89.7% in the placebo arm (HR: 1.23; 95%CI: 0.65-2.31) and, respectively, 96.8% vs. 98.1% were alive (HR: 1.09; 95%CI: 0.34-3.51).\nA single administration of 30 mg of ketorolac tromethamine before surgery does not increase disease-free survival in high risk breast cancer patients. Overall survival difference between ketorolac tromethamine group and placebo group was not statistically significant. The study was however underpowered because of lower recurrence rates than initially anticipated. No safety concerns were observed.\nClinicalTrials.gov NCT01806259.", "title": "Intraoperative ketorolac in high-risk breast cancer patients. A prospective, randomized, placebo-controlled clinical trial.", "date": "2019-12-05"}, {"article_id": "8694219", "content": "One hundred patients undergoing breast lump excision using a standard anaesthetic technique in the Day Care Unit were randomly divided into five groups. Groups A and B received either saline or diclofenac in a double-blind fashion before and at the end of the procedure, with both groups receiving 10 ml of 0.5% bupivacaine infiltration at the end. Groups C and D also received saline or diclofenac before and after surgery but had no bupivacaine infiltration at the end. Group E did not receive any injections but had bupivacaine infiltration at the end of surgery. In the postoperative period, pain was assessed by a visual analogue scale at 30 min intervals until discharge. All patients were requested to complete a pain relief questionnaire over the 48 h following surgery. There were highly significant differences between those who received bupivacaine and those who did not in the visual analogue scale scores at 30 min (p < 0.001), 60 min (p < 0.001), 120 min postoperatively (p = 0.02) and at discharge (p = 0.03). Pain scores were lower in those who received bupivacaine and they were less likely to request rescue medication, although this did not reach significance (p = 0.07). There were significant differences between the groups who received bupivacaine and diclofenac injection and those who received bupivacaine alone for visual analogue scale scores at 60 min following surgery (p = 0.05) and at 48 h (p = 0.002). Pain relief was better in those patients who received both bupivacaine and diclofenac injection. Although not significant (p = 0.22), fewer patients required rescue medication when diclofenac was given before surgery (10%) rather than after surgery (22.5%). Fewer patients had a fair amount or a great deal of pain in the 48 h following surgery when diclofenac was injected before (7.5%) rather than after surgery (12.5%). The mean number of oral analgesics taken in the 48 h after surgery was also lower in those patients who had the diclofenac before the surgery rather than after.", "title": "Analgesia for day surgery. Evaluation of the effect of diclofenac given before or after surgery with or without bupivacaine infiltration.", "date": "1996-06-01"}, {"article_id": "16428536", "content": "We compared methylprednisolone 125 mg IV (n = 68) and parecoxib 40 mg IV (n = 68) with placebo (n = 68) given before breast augmentation surgery in a randomized, double-blind parallel group study. Surgery was performed under local anesthesia combined with propofol/fentanyl sedation. Methylprednisolone and parecoxib decreased pain at rest and dynamic pain intensity from 1 to 6 h after surgery compared with placebo (mean summed pain intensity(1-6 h): methylprednisolone [17.25; 95% confidence interval [CI], 14.85-19.65] versus placebo [21.7; 95% CI, 19.3-24.1]; P < 0.03; parecoxib [15.25; 95% CI, 13.25-17.25] versus placebo; P < 0.001; mean summed dynamic pain intensity(1-6 h): methylprednisolone [22.7; 95% CI, 20.1-23.3] versus placebo [28.4; 95% CI, 26.0-30.8]; P < 0.01; parecoxib [20.9; 95% CI, 18.6-23.2] versus placebo; P < 0.001). Both rescue drug consumption and actual pain (all observations before and after rescue) during the first 6 h were similar in the two active drug groups and significantly reduced compared with placebo. Using a composite score of actual pain intensity and rescue analgesic use, the active drugs were significantly superior to placebo (P < 0.001 for both active drugs). Postoperative nausea and vomiting was reduced after methylprednisolone administration (incidence, 30%), but not after parecoxib (incidence, 37%), during the first 24 h compared with placebo (incidence, 60%; P < 0.001). Fatigue was reduced by methylprednisolone (incidence, 44%), but not by parecoxib (incidence, 59%), compared with placebo (incidence, 66%; P < 0.05). In conclusion, methylprednisolone 125 mg IV given before breast augmentation surgery had analgesic and rescue analgesic-sparing effects comparable with those of parecoxib 40 mg IV. Methylprednisolone, but not parecoxib, reduced nausea, vomiting, and fatigue.", "title": "Methylprednisolone reduces pain, emesis, and fatigue after breast augmentation surgery: a single-dose, randomized, parallel-group study with methylprednisolone 125 mg, parecoxib 40 mg, and placebo.", "date": "2006-01-24"}, {"article_id": "24807396", "content": "Vascular endothelial growth factor-C (VEGF-C), tumor necrosis factor-\u03b1 (TNF-\u03b1), and interleukin-1\u00df(IL-1\u00df) have been shown to be associated with the recurrence and metastasis of breast cancer after surgery. This study tested the hypothesis that patients undergoing surgery for breast cancer, who received postoperative analgesia with flurbiprofen axetil combined with small doses of fentanyl (FA), exhibited reduced levels of VEGF-C, TNF-\u03b1, and IL-1\u00df compared with those patients receiving fentanyl alone (F).\nForty-women with primary breast cancer undergoing a modified radical mastectomy were randomized to receive postoperative analgesia with flurbiprofen axetil combined with fentanyl or fentanyl alone. Venous blood was sampled before anesthesia, at the end of surgery, and at 48 hours after surgery, and the serum was analyzed. The primary endpoint was changes in the VEGF-C concentrations in serum.\nGroup FA patients reported similar analgesic effects as group F patients at 2, 24, and 48 hours. At 48 hours, mean postoperative concentrations of VEGF-C in group F patients were higher than in group FA patients, 730.9 versus. 354.1 pg/mL (P = 0.003), respectively. The mean postoperative concentrations of TNF-\u03b1 in group F patients were also higher compared with group FA patients 27.1 vs. 15.8 pg/mL (P = 0.005). Finally, the mean postoperative concentrations of IL-1\u00df in group F were also significantly higher than in group FA 497.5 vs. 197.7 pg/mL (P = 0.001).\nIn patients undergoing a mastectomy, postoperative analgesia with flurbiprofen axetil, combined with fentanyl, were associated with decreases in serum concentrations of VEGF-C, TNF-\u03b1, and IL-1\u00df compared with patients receiving doses of only fentanyl.", "title": "A Comparison of Fentanyl and Flurbiprofen Axetil on Serum VEGF-C, TNF-\u03b1, and IL-1\u00df Concentrations in Women Undergoing Surgery for Breast Cancer.", "date": "2014-05-09"}, {"article_id": "26710216", "content": "Analgesics had been suspected of impairing various immune functions either directly or indirectly. Our primary objective was to compare the effects of intravenous (IV) morphine, tramadol, and ketorolac on stress and immune responses in patients who underwent modified radical mastectomy.\nSixty patients randomly assigned to receive IV morphine 5 mg (group M, n=20), tramadol 100 mg (group T, n=20), or ketorolac 60 mg (group K, n=20) at the end of surgery.\nSerum cortisol, prolactin were measured immediately, 40 minutes, and 24 hours postoperatively. Expressions of peripheral T lymphocytes (CD3, CD3CD4, CD3CD8) and natural killer cells (CD3, CD56) were measured as percentages of total lymphocytes by flow cytometry immediately, 90 minutes, and 24 hours postoperatively.\nAfter 40 minutes, cortisol level increased but prolactin decreased significantly (P=0.001), then both decreased after 24 hours (P=0.001) compared with baseline within the 3 groups. CD3, CD4, CD8, and CD56 significantly decreased at 90 minutes and 24 hours (P\u22640.033) compared with baseline in the 3 groups. CD4, CD8, and CD56 significantly decreased in group M, compared with group T and K (P\u22640.016) and CD3, CD8, and CD56 in group T compared with group K at 90 minutes (P\u22640.024) postoperatively. After 24 hours, CD4, and CD8 decreased in group M compared with group T (P\u22640.048) and CD4 and CD56 in groups M and T compared with group K (P\u22640.049).\nIV morphine, tramadol, and ketorolac suppressed stress and immune responses. Ketorolac was the least immunosuppressive among the 3 drugs.", "title": "Comparison Between the Effects of Intravenous Morphine, Tramadol, and Ketorolac on Stress and Immune Responses in Patients Undergoing Modified Radical Mastectomy.", "date": "2015-12-29"}]}
{"original_review": "27915460", "question_data": [{"question_id": 49, "question": "Is the incidence of pneumonia higher, lower, or the same when comparing zinc supplementation to placebo?", "answer": "lower", "evidence_quality": "low", "fulltext_required": "no", "comment": "Penny and Bobat excluded due to lack of specificity/statistical detail (14.5% weight)", "relevant_sources": ["9651405", "12052800", "16168782", "17593956"]}], "sources": [{"article_id": "10431116", "content": "To determine whether supplemental zinc, with or without additional micronutrients, affects the severity and duration of persistent childhood diarrhea and the rate of nutritional recovery.\nThe study was a community-based, double-blind, randomized trial implemented in a shanty town in Lima, Peru. Children aged 6 to 36 months with persistent (>/=14 days) diarrhea received daily, for 2 weeks, a placebo (group P, n = 136) or a supplement of 20 mg of zinc, either with (group Z+VM, n = 137) or without (group Z, n = 139) additional vitamins and minerals. Symptoms of illness were recorded daily, and biochemical and anthropometric assessments were completed at baseline and on day 15.\nThe treatment groups were similar at baseline with regard to the characteristics of the presenting episode, anthropometric data, and plasma zinc concentration. The children consumed, on average, 95% (group P), 94% (group Z), or 88% (group Z+VM) of the supplement (P <.001). The plasma zinc concentration did not change significantly from baseline to day 15 in group P (4 microg/dL) but increased by 38 microg/dL in group Z and 14 microg/dL in group Z+VM. The median duration of diarrhea after starting treatment was 1 day; among children who continued to have diarrhea, there was a significant effect of treatment on diarrheal duration (P =.04, analysis of covariance). Specifically, the duration of illness was significantly reduced by 28% in children in group Z (P =.01) and by 33% in girls in group Z+VM (P =.04). There were no differences in the severity of the episode by treatment group.\nThere was a significant reduction in the duration of persistent diarrhea in selected subgroups of zinc-supplemented ambulatory patients in this population.", "title": "Randomized, community-based trial of the effect of zinc supplementation, with and without other micronutrients, on the duration of persistent childhood diarrhea in Lima, Peru.", "date": "1999-08-04"}, {"article_id": "17593956", "content": "Prophylactic zinc supplementation has been shown to reduce diarrhea and respiratory illness in children in many developing countries, but its efficacy in children in Africa is uncertain.\nTo determine if zinc, or zinc plus multiple micronutrients, reduces diarrhea and respiratory disease prevalence.\nRandomized, double-blind, controlled trial.\nRural community in South Africa.\nTHREE COHORTS: 32 HIV-infected children; 154 HIV-uninfected children born to HIV-infected mothers; and 187 HIV-uninfected children born to HIV-uninfected mothers.\nChildren received either 1250 IU of vitamin A; vitamin A and 10 mg of zinc; or vitamin A, zinc, vitamins B1, B2, B6, B12, C, D, E, and K and copper, iodine, iron, and niacin starting at 6 months and continuing to 24 months of age. Homes were visited weekly.\nPrimary outcome was percentage of days of diarrhea per child by study arm within each of the three cohorts. Secondary outcomes were prevalence of upper respiratory symptoms and percentage of children who ever had pneumonia by maternal report, or confirmed by the field worker.\nAmong HIV-uninfected children born to HIV-infected mothers, median percentage of days with diarrhea was 2.3% for 49 children allocated to vitamin A; 2.5% in 47 children allocated to receive vitamin A and zinc; and 2.2% for 46 children allocated to multiple micronutrients (P = 0.852). Among HIV-uninfected children born to HIV-uninfected mothers, median percentage of days of diarrhea was 2.4% in 56 children in the vitamin A group; 1.8% in 57 children in the vitamin A and zinc group; and 2.7% in 52 children in the multiple micronutrient group (P = 0.857). Only 32 HIV-infected children were enrolled, and there were no differences between treatment arms in the prevalence of diarrhea. The prevalence of upper respiratory symptoms or incidence of pneumonia did not differ by treatment arms in any of the cohorts.\nWhen compared with vitamin A alone, supplementation with zinc, or with zinc and multiple micronutrients, did not reduce diarrhea and respiratory morbidity in rural South African children.\nClinicalTrials.gov NCT00156832.", "title": "Zinc or multiple micronutrient supplementation to reduce diarrhea and respiratory disease in South African children: a randomized controlled trial.", "date": "2007-06-28"}, {"article_id": "12052800", "content": "To evaluate the effect of daily zinc supplementation in children on the incidence of acute lower respiratory tract infections and pneumonia.\nDouble masked, randomised placebo controlled trial.\nA slum community in New Delhi, India.\n2482 children aged 6 to 30 months.\nDaily elemental zinc, 10 mg to infants and 20 mg to older children or placebo for four months. Both groups received single massive dose of vitamin A (100 000 IU for infants and 200 000 IU for older children) at enrollment.\nAll households were visited weekly. Any children with cough and lower chest indrawing or respiratory rate 5 breaths per minute less than the World Health Organization criteria for fast breathing were brought to study physicians.\nAt four months the mean plasma zinc concentration was higher in the zinc group (19.8 (SD 10.1) v 9.3 (2.1) micromol/l, P<0.001). The proportion of children who had acute lower respiratory tract infection during follow up was no different in the two groups (absolute risk reduction -0.2%, 95% confidence interval -3.9% to 3.6%). Zinc supplementation resulted in a lower incidence of pneumonia than placebo (absolute risk reduction 2.5%, 95% confidence interval 0.4% to 4.6%). After correction for multiple episodes in the same child by generalised estimating equations analysis the odds ratio was 0.74, 95% confidence interval 0.56 to 0.99.\nZinc supplementation substantially reduced the incidence of pneumonia in children who had received vitamin A.", "title": "Effect of routine zinc supplementation on pneumonia in children aged 6 months to 3 years: randomised controlled trial in an urban slum.", "date": "2002-06-08"}, {"article_id": "12663295", "content": "Prebiotics are nondigestible food ingredients that stimulate the growth of Bifidobacterium and other bacteria in the gastrointestinal tract. Improved gastrointestinal and other health effects have been attributed to them.\nThe objective of this study was to evaluate the effects of dietary supplementation with the prebiotic oligofructose with and without zinc on the prevalence of diarrhea in a community with a high burden of gastrointestinal and other infections.\nTwo consecutive randomized, blinded, controlled clinical trials were performed in a shantytown community near Lima, Peru. The first trial compared an infant cereal supplemented with oligofructose (0.55 g/15 g cereal) with nonsupplemented cereal. During the second trial, zinc (1 mg/15 g cereal) was added to both oligofructose-supplemented and control cereals.\nWe enrolled 282 infants in the first trial and 349 in the second. In the first trial, mean (+/- SD) days of diarrhea were 10.3 +/- 9.6 in the nonsupplemented cereal group and 9.8 +/- 11.0 in the prebiotic-supplemented cereal group (P = 0.66). In the second trial, mean days of diarrhea were 10.3 +/- 8.9 in the group consuming cereal fortified only with zinc and 9.5 +/- 8.9 in the group consuming cereal containing both zinc and prebiotics (P = 0.35). Postimmunization titers of antibody to Haemophilus influenzae type B were similar in all groups, as were gains in height, visits to clinic, hospitalizations, and use of antibiotics.\nCereal supplemented with prebiotics was not associated with any change in diarrhea prevalence, use of health care resources, or response to H. influenzae type B immunization. Infants and young children who continue to breast-feed may not benefit from prebiotic supplementation.", "title": "Oligofructose-supplemented infant cereal: 2 randomized, blinded, community-based trials in Peruvian infants.", "date": "2003-03-29"}, {"article_id": "7651474", "content": "In developing countries the duration and severity of diarrheal illnesses are greatest among infants and young children with malnutrition and impaired immune status, both factors that may be associated with zinc deficiency. In children with severe zinc deficiency, diarrhea is common and responds quickly to zinc supplementation.\nTo evaluate the effects of daily supplementation with 20 mg of elemental zinc on the duration and severity of acute diarrhea, we conducted a double-blind, randomized, controlled trial involving 937 children, 6 to 35 months of age, in New Delhi, India. All the children also received oral rehydration therapy and vitamin supplements.\nAmong the children who received zinc supplementation, there was a 23 percent reduction (95 percent confidence interval, 12 percent to 32 percent) in the risk of continued diarrhea. Estimates of the likelihood of recovery according to the day of zinc supplementation revealed a reduction of 7 percent (95 percent confidence interval, -9 percent to +22 percent) in the risk of continued diarrhea during days 1 through 3 and a reduction of 38 percent (95 percent confidence interval, 27 percent to 48 percent) after day 3. When zinc supplementation was initiated within three days of the onset of diarrhea, there was a 39 percent reduction (95 percent confidence interval, 7 percent to 61 percent) in the proportion of episodes lasting more than seven days. In the zinc-supplementation group there was a decrease of 39 percent (95 percent confidence interval, 6 percent to 70 percent) in the mean number of watery stools per day (P = 0.02) and a decrease of 21 percent (95 percent confidence interval, 10 percent to 31 percent) in the number of days with watery diarrhea. The reductions in the duration and severity of diarrhea were greater in children with stunted growth than in those with normal growth.\nFor infants and young children with acute diarrhea, zinc supplementation results in clinically important reductions in the duration and severity of diarrhea.", "title": "Zinc supplementation in young children with acute diarrhea in India.", "date": "1995-09-28"}, {"article_id": "14985222", "content": "Zinc supplements reduce childhood morbidity in populations in whom zinc deficiency is common. In such populations, deficiencies in other micronutrients may also occur.\nThe objective was to determine whether the administration of other micronutrients with zinc modifies the effect of zinc supplementation on children's morbidity and physical growth.\nTwo hundred forty-six children aged 6-35 mo with persistent diarrhea were randomly assigned to 1 of 3 groups to receive a daily supplement of 10 mg Zn alone (Zn; n = 81), zinc plus vitamins and other minerals at 1-2 times recommended daily intakes (Zn+VM; n = 82), or placebo (n = 83) for approximately 6 mo after the diarrhea episode ended. Morbidity information was collected on weekdays. Weight, length, and other anthropometric indicators were measured monthly, and plasma zinc and other indicators of micronutrient status were measured at baseline and 6 mo.\nSupplement consumption was high ( approximately 90%) in all groups, although slightly more vomiting was reported in the Zn+VM group (P < 0.0001, analysis of variance). The change in plasma zinc from baseline to 6 mo was greater in the 2 zinc groups (6.1, 27.3, and 16.2 micro g/dL in the placebo, Zn, and Zn+VM groups, respectively; P < 0.0001, analysis of variance). The Zn group had fewer episodes of diarrhea, dysentery, and respiratory illness and a lower prevalence of fever and cough than did the Zn+VM group and a lower prevalence of cough than did the placebo group (P = 0.05). No significant effects of supplementation on growth were observed.\nMorbidity was greater after supplementation with zinc plus multivitamins and minerals than it was after supplementation with zinc alone.", "title": "Randomized controlled trial of the effect of daily supplementation with zinc or multiple micronutrients on the morbidity, growth, and micronutrient status of young Peruvian children.", "date": "2004-02-27"}, {"article_id": "16168782", "content": "Pneumonia and diarrhoea cause much morbidity and mortality in children younger than 5 years. Most deaths occur during infancy and in developing countries. Daily regimens of zinc have been reported to prevent acute lower respiratory tract infection and diarrhoea, and to reduce child mortality. We aimed to examine whether giving zinc weekly could prevent clinical pneumonia and diarrhoea in children younger than 2 years.\n1665 poor, urban children aged 60 days to 12 months were randomly assigned zinc (70 mg) or placebo orally once weekly for 12 months. Children were assessed every week by field research assistants. Our primary outcomes were the rate of pneumonia and diarrhoea. The rates of other respiratory tract infections were the secondary outcomes. Growth, final serum copper, and final haemoglobin were also measured. Analysis was by intention to treat.\n34 children were excluded before random assignment to treatment group because they had tuberculosis. 809 children were assigned zinc, and 812 placebo. After treatment assignment, 103 children in the treatment group and 44 in the control group withdrew. There were significantly fewer incidents of pneumonia in the zinc group than the control group (199 vs 286; relative risk 0.83, 95% CI 0.73-0.95), and a small but significant effect on incidence of diarrhoea (1881 cases vs 2407; 0.94, 0.88-0.99). There were two deaths in the zinc group and 14 in the placebo group (p=0.013). There were no pneumonia-related deaths in the zinc group, but ten in the placebo group (p=0.013). The zinc group had a small gain in height, but not weight at 10 months compared with the placebo group. Serum copper and haemoglobin concentrations were not adversely affected after 10 months of zinc supplementation.\n70 mg of zinc weekly reduces pneumonia and mortality in young children. However, compliance with weekly intake might be problematic outside a research programme.", "title": "Effect of weekly zinc supplements on incidence of pneumonia and diarrhoea in children younger than 2 years in an urban, low-income population in Bangladesh: randomised controlled trial.", "date": "2005-09-20"}, {"article_id": "9651405", "content": "Increased acute lower respiratory infection incidence, severity, and mortality are associated with malnutrition, and reduced immunological competence may be a mechanism for this association. Because zinc deficiency results in impaired immunocompetence and zinc supplementation improves immune status, we hypothesized that zinc deficiency is associated with increased incidence and severity of acute lower respiratory infection.\nWe evaluated the effect of daily supplementation with 10 mg of elemental zinc on the incidence and prevalence of acute lower respiratory infection in a double-blind, randomized, controlled trial in 609 children (zinc, n = 298; control, n = 311) 6 to 35 months of age. Supplementation and morbidity surveillance were done for 6 months.\nAfter 120 days of supplementation, the percentage of children with plasma zinc concentrations <60 microg/dL decreased from 35.6% to 11.6% in the zinc group, whereas in the control group it increased from 36.8% to 43.6%. Zinc-supplemented children had 0.19 acute lower respiratory infection episodes/child/year compared with 0.35 episodes/child/year in the control children. After correction for correlation of data using generalized estimating equation regression methods, there was a reduction of 45% (95% confidence interval, 10% to 67%) in the incidence of acute lower respiratory infections in zinc-supplemented children.\nA dietary zinc supplement resulted in a significant reduction in respiratory morbidity in preschool children. These findings suggest that interventions to improve zinc intake will improve the health and survival of children in developing countries.", "title": "Zinc supplementation reduces the incidence of acute lower respiratory infections in infants and preschool children: a double-blind, controlled trial.", "date": "1998-07-04"}, {"article_id": "8632217", "content": "Persistent diarrhea (PD) and dysentery (DD) account for most diarrhea-associated deaths among children in developing countries. Zinc deficiency can cause stunting and impaired immune function, both of which are risk factors for these diarrheal illnesses. We investigated the effect of zinc supplementation on the incidence of PD and DD in a community-based, double-blind randomized trial in children 6-35 mo of age. Increase over baseline in plasma zinc concentrations in the supplemented group compared with a control group (3.61 vs. 0.009 mumol.L-1), indicated successful supplementation. The overall reductions in the zinc supplemented group of 21% in the incidence of PD (95% CI -6 to 42%) and 14% in the incidence of dysentery (95% CI -15 to 36%) were not significant. There was a significant interaction of treatment effect with baseline plasma zinc concentration and age for PD and with gender for DD. In the zinc-supplemented group compared with the control group, the incidence of PD was reduced by 73% (P < 0.05; 95% CI 34 to 91%) in children with a baseline zinc < 7.65 mumol.L-1 and by 49% (P < 0.05; 95%CI 24 to 66%) in children > 11 mo of age. Zinc supplementation resulted in a 38% (P < 0.05 95%CI 8 to 59%) reduction in the incidence of DD in boys. There was no effect on PD among children 6-11 mo old or on DD in girls. In conclusion, zinc supplementation had a significant impact on the incidence of persistent diarrhea in children > 1 y old and in children with low plasma zinc, as well as on dysentery in boys. These findings may have important implications for reducing diarrhea-related morbidity and mortality.", "title": "Zinc supplementation reduces the incidence of persistent diarrhea and dysentery among low socioeconomic children in India.", "date": "1996-02-01"}, {"article_id": "16310552", "content": "Zinc deficiency is associated with impaired immune function and an increased risk of infection. Supplementation can decrease the incidence of diarrhoea and pneumonia in children in resource-poor countries. However, in children with HIV-1 infection, the safety of zinc supplementation is uncertain. We aimed to assess the role of zinc in HIV-1 replication before mass zinc supplementation is recommended in regions of high HIV-1 prevalence.\nWe did a randomised double-blind placebo-controlled equivalence trial of zinc supplementation at Grey's Hospital in Pietermaritzburg, South Africa. 96 children with HIV-1 infection were randomly assigned to receive 10 mg of elemental zinc as sulphate or placebo daily for 6 months. Baseline measurements of plasma HIV-1 viral load and the percentage of CD4+ T lymphocytes were established at two study visits before randomisation, and measurements were repeated 3, 6, and 9 months after the start of supplementation. The primary outcome measure was plasma HIV-1 viral load. Analysis was per protocol.\nThe mean log(10) HIV-1 viral load was 5.4 (SD 0.61) for the placebo group and 5.4 (SD 0.66) for the zinc-supplemented group 6 months after supplementation began (difference 0.0002, 95% CI -0.27 to 0.27). 3 months after supplementation ended, the corresponding values were 5.5 (SD 0.77) and 5.4 (SD 0.61), a difference of 0.05 (-0.24 to 0.35). The mean percentage of CD4+ T lymphocytes and median haemoglobin concentrations were also similar between the two groups after zinc supplementation. Two deaths occurred in the zinc supplementation group and seven in the placebo group (p=0.1). Children given zinc supplementation were less likely to get watery diarrhoea than those given placebo. Watery diarrhoea was diagnosed at 30 (7.4%) of 407 clinic visits in the zinc-supplemented group versus 65 (14.5%) of 447 visits in the placebo group (p=0.001).\nZinc supplementation of HIV-1-infected children does not result in an increase in plasma HIV-1 viral load and could reduce morbidity caused by diarrhoea.\nProgrammes to enhance zinc intake in deficient populations with a high prevalence of HIV-1 infection can be implemented without concern for adverse effects on HIV-1 replication. In view of the reductions in diarrhoea and pneumonia morbidity, zinc supplementation should be used as adjunct therapy for children with HIV-1 infection.", "title": "Safety and efficacy of zinc supplementation for children with HIV-1 infection in South Africa: a randomised double-blind placebo-controlled trial.", "date": "2005-11-29"}]}
{"original_review": "31930743", "question_data": [{"question_id": 50, "question": "Is overall survival higher, lower, or the same when comparing cisplatin in combination with third\u2010generation drugs to carboplatin in combination with third\u2010generation drugs?", "answer": "no difference", "evidence_quality": "high", "fulltext_required": "no", "comment": "Sweeney/11745199 excluded for lack of detail", "relevant_sources": ["12928123", "11784875", "28643733", "12377641", "12826316", "12837811", "17409843"]}, {"question_id": 51, "question": "Is response rate higher, lower, or the same when comparing cisplatin in combination with third\u2010generation drugs to carboplatin in combination with third\u2010generation drugs?", "answer": "no difference", "evidence_quality": "high", "fulltext_required": "no", "comment": "", "relevant_sources": ["12928123", "11784875", "28643733", "12377641", "12826316", "12837811", "17409843", "28780466", "21047474", "11745199", "21333222"]}], "sources": [{"article_id": "21333222", "content": "To observe the efficacy and toxicity of gemcitabine plus carboplatin (GCarb) versus gemcitabine plus cisplatin (GCis) in the treatment of advanced non-small cell lung cancer (NSCLC).\nForty patients with histologically confirmed NSCLC were randomized to enter the study. GCarb group:Gemcitabine 1 000 mg/m\u00b2 IV on day 1,8; carboplatin AUC 4-6 IV on day 1. GCis group: Gemcitabine 1 000 mg/m\u00b2 IV on day 1,8; cisplatin 30-40 mg/m\u00b2 IV on day 1-3.\nThe response rate was 65% and 60% for GCarb group and GCis group respectively (P > 0.5). Toxicities included myelosuppression, digestive reaction, alopecia and rash. Digestive toxicity in GCarb group was less than that in GCis group (P < 0.05).\nBoth GCarb and GCis regimes can be used as first-line protocol in the chemotherapy of non small cell lung cancer.", "title": "[Comparison of efficacy and toxicity between gemcitabine plus carboplatin and gemcitabine plus cisplatin in the treatment of advanced non-small cell lung cancer].", "date": "2002-12-20"}, {"article_id": "12837811", "content": "To investigate whether docetaxel plus platinum regimens improve survival and affect quality of life (QoL) in advanced non-small-cell lung cancer (NSCLC) compared with vinorelbine plus cisplatin as first-line chemotherapy.\nPatients (n = 1,218) with stage IIIB to IV NSCLC were randomly assigned to receive docetaxel 75 mg/m2 and cisplatin 75 mg/m2 every 3 weeks (DC); docetaxel 75 mg/m2 and carboplatin area under the curve of 6 mg/mL * min every 3 weeks (DCb); or vinorelbine 25 mg/m2/wk and cisplatin 100 mg/m2 every 4 weeks (VC).\nPatients treated with DC had a median survival of 11.3 v 10.1 months for VC-treated patients (P =.044; hazard ratio, 1.183 [97.2% confidence interval, 0.989 to 1.416]). The 2-year survival rate was 21% for DC-treated patients and 14% for VC-treated patients. Overall response rate was 31.6% for DC-treated patients v 24.5% for VC-treated patients (P =.029). Median survival (9.4 v 9.9 months [for VC]; P =.657; hazard ratio, 1.048 [97.2 confidence interval, 0.877 to 1.253]) and response (23.9%) with DCb were similar to those results for VC. Neutropenia, thrombocytopenia, infection, and febrile neutropenia were similar with all three regimens. Grade 3 to 4 anemia, nausea, and vomiting were more common (P <.01) with VC than with DC or DCb. Patients treated with either docetaxel regimen had consistently improved QoL compared with VC-treated patients, who experienced deterioration in QoL.\nDC resulted in a more favorable overall response and survival rate than VC. Both DC and DCb were better tolerated and provided patients with consistently improved QoL compared with VC. These findings demonstrate that a docetaxel plus platinum combination is an effective treatment option with a favorable therapeutic index for first-line treatment of advanced or metastatic NSCLC.", "title": "Randomized, multinational, phase III study of docetaxel plus platinum combinations versus vinorelbine plus cisplatin for advanced non-small-cell lung cancer: the TAX 326 study group.", "date": "2003-07-03"}, {"article_id": "11784875", "content": "We conducted a randomized study to determine whether any of three chemotherapy regimens was superior to cisplatin and paclitaxel in patients with advanced non-small-cell lung cancer.\nA total of 1207 patients with advanced non-small-cell lung cancer were randomly assigned to a reference regimen of cisplatin and paclitaxel or to one of three experimental regimens: cisplatin and gemcitabine, cisplatin and docetaxel, or carboplatin and paclitaxel.\nThe response rate for all 1155 eligible patients was 19 percent, with a median survival of 7.9 months (95 percent confidence interval, 7.3 to 8.5), a 1-year survival rate of 33 percent (95 percent confidence interval, 30 to 36 percent), and a 2-year survival rate of 11 percent (95 percent confidence interval, 8 to 12 percent). The response rate and survival did not differ significantly between patients assigned to receive cisplatin and paclitaxel and those assigned to receive any of the three experimental regimens. Treatment with cisplatin and gemcitabine was associated with a significantly longer time to the progression of disease than was treatment with cisplatin and paclitaxel but was more likely to cause grade 3, 4, or 5 renal toxicity (in 9 percent of patients, vs. 3 percent of those treated with cisplatin plus paclitaxel). Patients with a performance status of 2 had a significantly lower rate of survival than did those with a performance status of 0 or 1.\nNone of four chemotherapy regimens offered a significant advantage over the others in the treatment of advanced non-small-cell lung cancer.", "title": "Comparison of four chemotherapy regimens for advanced non-small-cell lung cancer.", "date": "2002-01-11"}, {"article_id": "12826316", "content": "We conducted a phase II randomized study to assess the efficacy, with response as the primary endpoint, and the toxicity of gemcitabine/cisplatin (GP) and gemcitabine/carboplatin (GC) in patients with advanced non-small cell lung cancer (NSCLC).\nPatients were randomized to GP (gemcitabine 1200 mg/m(2), days 1 and 8 plus cisplatin 80 mg/m(2) day 2) or GC (gemcitabine 1200 mg/m(2), days 1 and 8 plus carboplatin AUC=5 day 2). Cycles were repeated every 3 weeks.\nSixty-two patients were randomized to GP and 58 to GC. A total of 533 cycles were delivered (264 GP, 269 GC), with a median of four cycles/patient. The objective response rate was 41.9% (95% C.I., 29.6-54.2%) for GP and 31.0% (95% C.I., 18.2-42.8%) for GC (P=0.29). No significant differences between arms were observed in median survival (10.4 months GP, 10.8 months GC) and median time to progression (5.4 months GP, 5.1 months GC). Both regimens were very well tolerated with no statistical differences between arms in grade 3/4 toxicities. When all toxicity grades were combined, emesis, neuropathy and renal toxicity occurred more frequently on the GP arm (P<0.005).\nGC arm did not provide a significant difference in response rate compared with GP arm, with better overall tolerability. Carboplatin could be a valid alternative to cisplatin in the palliative setting.", "title": "Randomized, multicenter, phase II study of gemcitabine plus cisplatin versus gemcitabine plus carboplatin in patients with advanced non-small cell lung cancer.", "date": "2003-06-27"}, {"article_id": "11745199", "content": "Eastern Cooperative Oncology Group (ECOG) Study E1594 compared paclitaxel and cisplatin with three newer chemotherapy doublets in the treatment of patients with advanced nonsmall cell lung carcinoma (NSCLC). The accrual of patients with an ECOG performance status (PS) of 2 was discontinued due to a perceived rate of unacceptable toxicity.\nPatients were stratified by PS and randomized to one of the following treatments: 1) paclitaxel (135 mg/m2) over 24 hours with cisplatin (75 mg/m2) on a 21-day cycle; 2) cisplatin (100 mg/m2) with gemcitabine (1 g/m2) on Days 1, 8, and 15 on a 28-day cycle; 3) cisplatin (75 mg/m2) with docetaxel (75 mg/m2) on a 21-day cycle; and 4) paclitaxel (225 mg/m2) over 3 hours with carboplatin (area under the curve, 6). All tests of statistical significance were two-sided.\nSixty-eight patients with an ECOG PS of 2 were enrolled, and 64 patients were evaluable for toxicity and response. Fifty-six percent of 64 evaluable patients were male, and 81% had Stage IV disease. Grade 3-4 hematologic toxicities occurred in > 50% of the patients in each treatment group. Nonhematologic Grade 3-4 toxicities occurred significantly less often in the paclitaxel and carboplatin arm (P = 0.0032). The overall rate of toxicity did not differ significantly from the rate of toxicity in the PS-0 or PS-1 cohorts. There were 5 deaths (7.35%) among 68 patients with a PS of 2 during therapy; however, only 2 of those deaths were attributed to therapy. The overall response rate for the 64 evaluable patients was 14%. The overall median survival of all 68 patients with a PS of 2, as determined by an intent-to-treat analysis, was 4.1 months.\nPatients with advanced NSCLC and a PS of 2 experienced a large number of adverse reactions and overall poor survival. A comparison with patients with a PS of 0-1 suggests that these events and the shorter survival were related to disease process rather than treatment. Alternative strategies need to be explored with therapy specifically tailored for this group of patients.", "title": "Outcome of patients with a performance status of 2 in Eastern Cooperative Oncology Group Study E1594: a Phase II trial in patients with metastatic nonsmall cell lung carcinoma .", "date": "2001-12-18"}, {"article_id": "28643733", "content": "The use of cisplatin (Cis) versus carboplatin (Carb) in the treatment of advanced nonsmall cell lung cancer (NSCLC) is controversial. The aim of the study was to compare the safety and efficacy of Cis versus Carb in squamous NSCLC.\nA prospective, randomized, controlled, open-label study was conducted on advanced squamous NSCLC patients who were randomly assigned to receive Cis (40 mg/m 2 [day 1 and day 8]) or Carb (area under the curve = 5 [day 1]) combined with gemcitabine [Gem] (1000 mg/m 2 [day 1 and day 8]) of a 3-week schedule for six cycles. Study objectives were a radiological response after three cycles and six cycles, 1-year progression-free survival (PFS), 1-year overall survival (OS), and quality of life (QOL) assessment using functional assessment of cancer therapy-lung at baseline, after three cycles, and after six cycles.\nStatistical analysis was done using Statistical Package for Social Science version 15. A P < 0.05 was considered statistically significant.\nSeventy-one patients were enrolled (Gem/Cis group [n = 36], Gem/Carb group [n = 35]). Response rates were comparable in both arms. Nonsignificant differences were found regarding 1-year PFS (P = 0.308) and 1-year OS (P = 0.929) between the two groups. Neutropenia was significantly higher in Gem/Carb group, while vomiting and ototoxicity were significantly higher in Gem/Cis group. The effect on QOL was similar in both groups.\nCis and Carb have similar efficacy, tolerability, and effect on QOL and both can be used as a first-line treatment of squamous NSCLC.", "title": "A prospective randomized controlled study of cisplatin versus carboplatin-based regimen in advanced squamous nonsmall cell lung cancer.", "date": "2017-06-24"}, {"article_id": "21047474", "content": "To compare the efficacy and toxicity of paclitaxel-carboplatin (TAX-CBP) and paclitaxel-cisplatin (TAX-DDP) chemotherapy protocols for advanced non-small cell lung cancer.\nOne hundred and twenty-six patients with non-small cell lung cancer were randomized into TAX-DDP and TAX-CBP groups. TAX-CBP group: TAX 175 mg/m\u00b2 and CBP 350 mg/m\u00b2, d1 iv; TAX-DDP group: TAX 175 mg/m\u00b2 and DDP 100 mg/m\u00b2 d1 iv. The therapy was repeated every 28 days. The response rate was assessed after three treatments.\nTAX-CBP group: response rate (RR) was 36% (22/61), 1-year survival rate was 34.1%. TAX-DDP group: RR was 33.9% (21/62),1-year survival rate was 33.1%. There was no significant difference of RR and 1-year survival rate between TAX-CBP and TAX-DDP group (P>0.05). The median survival time of TAX-CBP group (11.2 months) was significant higher than that of TAX-DDP group (9 months) (P<0.05). The major toxicity associated with paclitaxel included alpecia, myelosuppression, gastrointestinal reaction and myalgia or arthralgia. The thrombocytopenia in TAX-CBP group was more severe than that in TAX-DDP group (P<0.05). The Gastrointestinal and myalgia or arthralgia in TAX-DDP group were more severe than those in TAX-CBP group (P<0.05).\nTAX-CBP and TAX-DDP chemotherapy may be used as first choice protocol in the chemotherapy of non-small cell lung cancer.", "title": "[A randomized phase II trial of paclitaxel in combination chemotherapy with platinum in the treatment of non-small cell lung cancer].", "date": "2001-06-20"}, {"article_id": "28780466", "content": "Platinum-based combination chemotherapy is standard treatment for the majority of patients with advanced non-small-cell lung cancer (NSCLC). The trial investigates the importance of the choice of platinum agent and dose of cisplatin in relation to patient outcomes.\nThe three-arm randomised phase III trial assigned patients with chemo-na\u00efve stage IIIB/IV NSCLC in a 1:1:1 ratio to receive gemcitabine 1250\u00a0mg/m\nThe trial recruited 1363 patients. Survival time differed significantly across the three treatment arms (p\u00a0=\u00a00.046) with GC50 worst with median 8.2 months compared to 9.5 for GC80 and 10.0 for GCb6. HRs (adjusted) for GC50 compared to GC80 was 1.13 (95% confidence interval [CI] 0.99-1.29) and for GC50 compared to GCb6 was 1.23 (95% CI: 1.08-1.41). GCb6 was significantly non-inferior to GC80 (HR\u00a0=\u00a00.93, upper limit of one-sided 95% CI 1.04). Adjusting for QoL did not change the findings. Best objective response rates were 29% (GC80), 20% (GC50) and 27% (GCb6), p\u00a0<\u00a00.007. There were more dose reductions and treatment delays in the GCb6 arm and more adverse events (60% with at least one grade 3-4 compared to 43% GC80 and 30% GC50).\nIn combination with gemcitabine, carboplatin at AUC6 is not inferior to cisplatin at 80\u00a0mg/m\nNCT00112710.\n2004-003868-30.\nCRUK/04/009.", "title": "Carboplatin versus two doses of cisplatin in combination with gemcitabine in the treatment of advanced non-small-cell lung cancer: Results from a British Thoracic Oncology Group randomised phase III trial.", "date": "2017-08-07"}, {"article_id": "12928123", "content": "This randomized, multicenter, phase III trial was conducted to compare the tolerability of gemcitabine plus cisplatin (GP) vs. gemcitabine plus carboplatin (GC) in chemonaive patients with stage IIIb and IV non-small cell lung carcinoma (NSCLC). Secondary objectives were to evaluate response, duration of response, time to progressive disease (TTPD), and survival.\nEligible patients were required to have stage IIIb or IV NSCLC, no previous chemotherapy, Karnofsky performance status of at least 70, bidimensionally measurable disease, and age 18-75 years. Randomized patients in both arms were given gemcitabine 1200 mg/m(2) on days 1 and 8, followed on day 1 by cisplatin 80 mg/m(2) (GP) or carboplatin AUC=5 (GC). Treatment cycles were repeated every 21 days for a maximum of six cycles, or until disease progression or unacceptable toxicity occurred.\nEnrolled patients in both arms, 87 in GP and 89 in GC, were well balanced for demographics and disease characteristics. Dose intensity was 93.8 and 92.7% for gemcitabine in GP/GC arms, respectively; 97.7% for cisplatin and 99.9% for carboplatin. Patients with at least one grade 3/4 toxicity excluding nausea, vomiting or alopecia, were 44% in GP arm and 54% in GC arm. The only significantly different toxicities were, nausea and vomiting in GP and thrombocytopenia in GC group. The overall response rates, median TTPD, response duration and survival were, 41/29%, 5.87/4.75 months, 7.48/5.15 months, and 8.75/7.97 months for GP and GC arms, respectively.\nGP and GC are effective and feasible regimens for advanced NSCLC, and are comparable in efficacy and toxicity. GC may offer acceptable option to patients with advanced NSCLC, especially those who are unable to receive cisplatin.", "title": "Gemcitabine plus cisplatin vs. gemcitabine plus carboplatin in stage IIIb and IV non-small cell lung cancer: a phase III randomized trial.", "date": "2003-08-21"}, {"article_id": "12377641", "content": "The combination of paclitaxel with cisplatin or carboplatin has significant activity in non-small-cell lung cancer (NSCLC). This phase III study of chemotherapy-na\u00efve advanced NSCLC patients was designed to assess whether response rate in patients receiving a paclitaxel/carboplatin combination was similar to that in patients receiving a paclitaxel/cisplatin combination. Paclitaxel was given at a dose of 200 mg/m(2) (3-h intravenous infusion) followed by either carboplatin at an AUC of 6 or cisplatin at a dose of 80 mg/m(2), all repeated every 3 weeks. Survival, toxicity and quality of life were also compared.\nPatients were randomised to receive one of the two combinations, stratified according to centre, performance status, disease stage and histology. The primary analyses of response rate and survival were carried out on response-evaluable patients. Survival was also analysed for all randomised patients. Toxicity analyses were carried out on all treated patients.\nA total of 618 patients were randomised. The two treatment arms were well balanced with regard to gender (83% male), age (median 58 years), performance status (83% ECOG 0-1), stage (68% IV, 32% IIIB) and histology (38% squamous cell carcinoma). In the paclitaxel/carboplatin arm, 306 patients received a total of 1311 courses (median four courses, range 1-10 courses) while in the paclitaxel/cisplatin arm, 302 patients received a total of 1321 courses (median four courses, range 1-10 courses). In only 76% of courses, carboplatin was administered as planned at an AUC of 6, while in 96% of courses, cisplatin was given at the planned dose of 80 mg/m(2). The response rate was 25% (70 of 279) in the paclitaxel/carboplatin arm and 28% (80 of 284) in the paclitaxel/cisplatin arm (P = 0.45). Responses were reviewed by an independent radiological committee. For all randomised patients, median survival was 8.5 months in the paclitaxel/carboplatin arm and 9.8 months in the paclitaxel/cisplatin arm [hazard ratio 1.20, 90% confidence interval (CI) 1.03-1.40]; the 1-year survival rates were 33% and 38%, respectively. On the same dataset, a survival update after 22 months of additional follow-up yielded a median survival of 8.2 months in the paclitaxel/carboplatin arm and 9.8 months in the paclitaxel/cisplatin arm (hazard ratio 1.22, 90% CI 1.06-1.40; P = 0.019); the 2-year survival rates were 9% and 15%, respectively. Excluding neutropenia and thrombocytopenia, which were more frequent in the paclitaxel/carboplatin arm, and nausea/vomiting and nephrotoxicity, which were more frequent in the paclitaxel/cisplatin arm, the rate of severe toxicities was generally low and comparable between the two arms. Overall quality of life (EORTC QLQ-C30 and LC-13) was also similar between the two arms.\nThis is the first trial comparing carboplatin and cisplatin in the treatment of advanced NSCLC. Although paclitaxel/carboplatin yielded a similar response rate, the significantly longer median survival obtained with paclitaxel/cisplatin indicates that cisplatin-based chemotherapy should be the first treatment option.", "title": "Phase III randomised trial comparing paclitaxel/carboplatin with paclitaxel/cisplatin in patients with advanced non-small-cell lung cancer: a cooperative multinational trial.", "date": "2002-10-16"}, {"article_id": "17409843", "content": "Paclitaxel plus carboplatin (CAR) or cisplatin (CIS) has shown activity in the treatment of advanced non-small cell lung cancer (NSCLC). Our aim was to determine whether paclitaxel plus platinum is an appropriate regimen for chemo-na\u00efve NSCLC in patients aged 70 years or older. Patients were randomized into paclitaxel plus CAR or paclitaxel plus CIS treatment arms. Treatment consisted of paclitaxel 160 mg/m and carboplatin at AUC = 6 (predicted using measured clearances and the Calvert formula) IV infusion on day 1 every 3 weeks, or paclitaxel 160 mg/m and cisplatin 60 mg/m IV on day 1 every 3 weeks. In total, 81 patients were enrolled from September 2000 to February 2005, including 40 who received CAR treatment and 41 who received CIS treatment. In all, 152 cycles of CAR (median, four cycles per patient) and 172 cycles of CIS (median, four cycles per patient) were given. Each arm had one complete response and 15 partial responses to the treatment, with overall response rates of 40% and 39%, respectively. Myelosuppression was mild in both arms, and there was no statistical difference between the two arms. Alopecia (P < 0.001), peripheral neuropathy (P = 0.017), and fatigue (P < 0.001) were more severe in the CIS treatment arm than in the CAR treatment arm. Median time to disease progression was 6.6 months in the CAR arm and 6.9 months in the CIS arm. Median survival time was 10.3 months in the CAR arm and 10.5 months in the CIS arm. In conclusion, paclitaxel plus CAR or CIS treatment is feasible in elderly patients and has similar activity. However, paclitaxel plus CAR had less non-hematological toxicity than paclitaxel plus CIS.", "title": "A Phase II randomized study of paclitaxel plus carboplatin or cisplatin against chemo-naive inoperable non-small cell lung cancer in the elderly.", "date": "2007-04-06"}]}
{"original_review": "29067682", "question_data": [{"question_id": 52, "question": "Is the rate of 50% or greater reduction in seizure frequency higher, lower, or the same when comparing eslicarbazepine acetate (ESL) of any dose to placebo?", "answer": "higher", "evidence_quality": "moderate", "fulltext_required": "no", "comment": "", "relevant_sources": ["20299189", "17319919", "19243424", "19832771", "25528898"]}, {"question_id": 53, "question": "Is the rate of study withdrawal due to adverse effects higher, lower, or the same when comparing eslicarbazepine acetate (ESL) of any dose to placebo?", "answer": "higher", "evidence_quality": "moderate", "fulltext_required": "yes", "comment": "fulltext needed for 25528898; 19832771/Gil-Nagel is included for context but does not clearly specify study withdrawal rates", "relevant_sources": ["20299189", "17319919", "19243424", "19832771", "25528898"]}], "sources": [{"article_id": "25528898", "content": "To evaluate the efficacy and safety of adjunctive eslicarbazepine acetate (ESL) in patients with refractory partial-onset seizures.\nThis randomized, placebo-controlled, double-blind, parallel-group, phase III study was conducted at 173 centers in 19 countries, including the United States and Canada. Eligible patients were aged \u226516 years and had uncontrolled partial-onset seizures despite treatment with 1-2 antiepileptic drugs (AEDs). After an 8-week baseline period, patients were randomized to once-daily placebo (n = 226), ESL 800 mg (n = 216), or ESL 1,200 mg (n = 211). Following a 2-week titration period, patients received ESL 800 or 1,200 mg once-daily for 12 weeks. Seizure data were captured and documented using event-entry or daily entry diaries.\nStandardized seizure frequency (SSF) during the maintenance period (primary end point) was reduced with ESL 1,200 mg (p = 0.004), and there was a trend toward improvement with ESL 800 mg (p = 0.06), compared with placebo. When data for titration and maintenance periods were combined, ESL 800 mg (p = 0.001) and 1,200 mg (p < 0.001) both reduced SSF. There were no statistically significant interactions between treatment response and geographical region (p = 0.38) or diary version (p = 0.76). Responder rate (\u226550% reduction in SSF) was significantly higher with ESL 1,200 mg (42.6%, p < 0.001) but not ESL 800 mg (30.5%, p = 0.07) than placebo (23.1%). Incidence of treatment-emergent adverse events (TEAEs) and TEAEs leading to discontinuation increased with ESL dose. The most common TEAEs were dizziness, somnolence, nausea, headache, and diplopia.\nAdjunctive ESL 1,200 mg once-daily was more efficacious than placebo in adult patients with refractory partial-onset seizures. The once-daily 800 mg dose showed a marginal effect on SSF, but did not reach statistical significance. Both doses were well tolerated. Efficacy assessment was not affected by diary format used.", "title": "Eslicarbazepine acetate as adjunctive therapy in patients with uncontrolled partial-onset seizures: Results of a phase III, double-blind, randomized, placebo-controlled trial.", "date": "2014-12-23"}, {"article_id": "20299189", "content": "To investigate the efficacy and safety of once-daily eslicarbazepine acetate (ESL) when used as add-on treatment in adults with > or = 4 partial-onset seizures per 4-week despite treatment with 1 to 3 antiepileptic drugs (AEDs).\nThis double-blind, parallel-group, multicenter study consisted of an 8-week observational baseline period, after which patients were randomized to placebo (n=100) or once-daily ESL 400 mg (n=96), 800 mg (n=101), or 1200 mg (n=98). Patients then entered a 14-week double-blind treatment phase. All patients started on their full maintenance dose except for those in the ESL 1200 mg group who received once-daily ESL 800 mg for 2 weeks before reaching their full maintenance dose.\nSeizure frequency per 4-week (primary endpoint) over the 14-week double-blind treatment period was significantly lower than placebo in the ESL 800 mg and 1200 mg (p<0.001) groups. Responder rate (> or = 50% reduction in seizure frequency) was 13.0% (placebo), 16.7% (400 mg), 40.0% (800 mg, p<0.001), and 37.1% (1200 mg, p<0.001). Median relative reduction in seizure frequency was 0.8% (placebo), 18.7% (400 mg), 32.6% (800 mg, p<0.001), and 32.8% (1200 mg). Discontinuation rates due to adverse events (AEs) were 3.0% (placebo), 12.5% (400 mg), 18.8% (800 mg), and 26.5% (1200 mg). The most common (>5%) AEs in any group were dizziness, somnolence, headache, nausea, diplopia, abnormal coordination, vomiting, blurred vision, and fatigue. The majority of AEs were of mild or moderate severity.\nTreatment with once-daily eslicarbazepine acetate 800 mg and 1200 mg was more effective than placebo and generally well tolerated in patients with partial-onset seizures refractory to treatment with 1 to 3 concomitant AEDs.", "title": "Eslicarbazepine acetate as adjunctive therapy in adult patients with partial epilepsy.", "date": "2010-03-20"}, {"article_id": "19832771", "content": "To evaluate the efficacy and safety of eslicarbazepine acetate (ESL) as adjunctive therapy in adults with partial-onset seizures.\nDouble-blind, placebo-controlled, parallel-group, multicenter study consisting of an 8-week baseline period, after which patients were randomized to placebo (n = 87) or once-daily ESL 800 mg (n = 85) or 1200 mg (n = 80). Patients received half dose during 2 weeks preceding a 12-week maintenance period.\nSeizure frequency over the maintenance period was significantly (P < 0.05) lower than placebo in both ESL groups. Responder rate was 23% (placebo), 35% (800 mg), and 38% (1200 mg). Median relative reduction in seizure frequency was 17% (placebo), 38% (800 mg), and 42% (1200 mg). The most common adverse events (AEs) (>10%) were dizziness, somnolence, headache, and nausea. The majority of AEs were of mild or moderate severity.\nOnce-daily treatment with ESL 800 and 1200 mg was effective and generally well tolerated.", "title": "Efficacy and safety of 800 and 1200 mg eslicarbazepine acetate as adjunctive treatment in adults with refractory partial-onset seizures.", "date": "2009-10-17"}, {"article_id": "17319919", "content": "To explore the efficacy and safety of eslicarbazepine acetate (BIA 2-093), a new antiepileptic drug, as adjunctive therapy in adult patients with partial epilepsy.\nA multicenter, double-blind, randomized, placebo-controlled study was conducted in 143 refractory patients aged 18-65 years with >or=4 partial-onset seizures/month. The study consisted of a 12-week treatment period followed by a 1-week tapering off. Patients were randomly assigned to one of three groups: treatment with eslicarbazepine acetate once daily (QD, n=50), twice daily (BID, n=46), or placebo (PL, n=47). The daily dose was titrated from 400 mg to 800 mg and to 1,200 mg at 4-week intervals. The proportion of responders (patients with a >or=50% seizure reduction) was the primary end point.\nThe percentage of responders versus baseline showed a statistically significant difference between QD and PL groups (54% vs. 28%; 90% CI =-infinity, -14; p=0.008). The difference between the BID (41%) and PL did not reach statistical significance (90% CI =-infinity, -1; p=0.12). A significantly higher proportion of responders in weeks 5-8 was found in the QD group than in the BID group (58% vs. 33%, respectively, p=0.022). At the end of the 12-week treatment, the number of seizure-free patients in the QD and BID groups was 24%, which was significantly different from the PL group. The incidence of adverse events was similar between the treatment groups and no drug-related serious adverse events occurred.\nEslicarbazepine acetate was efficacious and well tolerated as an adjunctive therapy of refractory epileptic patients.", "title": "Eslicarbazepine acetate: a double-blind, add-on, placebo-controlled exploratory trial in adult patients with partial-onset seizures.", "date": "2007-02-27"}, {"article_id": "19243424", "content": "To study the efficacy and safety of eslicarbazepine acetate (ESL) as adjunctive therapy for refractory partial seizures in adults with >or=4 partial-onset seizures (simple or complex, with or without secondary generalization) per 4 weeks despite treatment with 1-2 antiepileptic drugs (AEDs).\nThis multicenter, parallel-group study had an 8-week, single-blind, placebo baseline phase, after which patients were randomized to placebo (n = 102) or once-daily ESL 400 mg (n = 100), 800 mg (n = 98), or 1,200 mg (n = 102) in the double-blind treatment phase. ESL starting dose was 400 mg; thereafter, ESL was titrated at weekly 400-mg steps to the full maintenance dose (12 weeks).\nSeizure frequency adjusted per 4 weeks over the maintenance period (primary endpoint) was significantly lower than placebo in the ESL 1,200-mg (p = 0.0003) and 800-mg (p = 0.0028) groups [analysis of covariance (ANCOVA) of log-transformed seizure frequency]. Responder rate was 20% (placebo), 23% (400 mg), 34% (800 mg), and 43% (1,200 mg). Median relative reduction in seizure frequency was 16% (placebo), 26% (400 mg), 36% (800 mg), and 45% (1,200 mg). The most frequent concomitant AEDs were carbamazepine (56-62% of patients), lamotrigine (25-27%), and valproic acid (22-28%). Similar efficacy results were obtained in patients administered ESL with or without carbamazepine as concomitant AED. Discontinuation rates caused by adverse events (AEs) were 3.9% (placebo), 4% (400 mg), 8.2% (800 mg), and 19.6% (1,200 mg). AEs in >10% of any group were dizziness, headache, and diplopia. Most AEs were mild or moderate.\nESL, 800 and 1,200 mg once-daily, was well tolerated and more effective than placebo in patients who were refractory to treatment with one or two concomitant AEDs.", "title": "Efficacy and safety of eslicarbazepine acetate as adjunctive treatment in adults with refractory partial-onset seizures: a randomized, double-blind, placebo-controlled, parallel-group phase III study.", "date": "2009-02-27"}]}
{"original_review": "27782297", "question_data": [{"question_id": 54, "question": "Is the overall cervical dystonia improvement higher, lower, or the same when comparing botulinum toxin type A (BtA) to botulinum toxin type B (BtB)?", "answer": "higher", "evidence_quality": "low", "fulltext_required": "no", "comment": "", "relevant_sources": ["16275831", "18098274"]}, {"question_id": 55, "question": "Is the rate of dysphagia higher, lower, or the same when comparing botulinum toxin type A (BtA) to botulinum toxin type B (BtB)?", "answer": "no difference", "evidence_quality": "low", "fulltext_required": "no", "comment": "relies on model inferring that in 16157918, dysphagia was an evaluated autonomic function with no significant difference found", "relevant_sources": ["16275831", "18098274", "16157918"]}, {"question_id": 56, "question": "Is the risk of treatment\u2010related sore throat/dry mouth higher, lower, or the same when comparing botulinum toxin type A (BtA) to botulinum toxin type B (BtB)?", "answer": "lower", "evidence_quality": "moderate", "fulltext_required": "no", "comment": "", "relevant_sources": ["16275831", "18098274"]}], "sources": [{"article_id": "16157918", "content": "To compare autonomic effects of botulinum toxin (BTX), we randomized patients with cervical dystonia to receive either BTX-A or BTX-B in a double-blind manner. Efficacy and physiologic questionnaire measures of autonomic function were assessed at baseline and 2 weeks after injection. Patients treated with BTX-B had less saliva production (p < 0.01) and greater severity of constipation (p = 0.037) than those treated with BTX-A, but did not differ in other tests of autonomic functions.", "title": "Autonomic function after botulinum toxin type A or B: a double-blind, randomized trial.", "date": "2005-09-15"}, {"article_id": "18098274", "content": "The objective of this study was to compare efficacy, safety, and duration of botulinum toxin type A (BoNT-A) and type B (BoNT-B) in toxin-na\u00efve cervical dystonia (CD) subjects. BoNT-na\u00efve CD subjects were randomized to BoNT-A or BoNT-B and evaluated in a double-blind trial at baseline and every 4-weeks following one treatment. The primary measure was the change in Toronto Western Spasmodic Torticollis Rating Scale (TWSTRS) from baseline to week 4 post-injection. Secondary measures included change in TWSTRS-subscale scores, pain, global impressions, and duration of response and safety assessments. The study was designed as a noninferiority trial of BoNT-B to BoNT-A. 111 subjects were randomized (55 BoNT-A; 56 BoNT-B). Improvement in TWSTRS-total scores 4 weeks after BoNT-B was noninferior to BoNT-A (adjusted means 11.0 (SE 1.2) and 8.8 (SE 1.2), respectively; per-protocol-population (PPP)). The median duration of effect of BoNT-A and BoNT-B was not different (13.1 vs. 13.7 weeks, respectively; P-value = 0.833; PPP). There were no significant differences in the occurrence of injection site pain and dysphagia. Mild dry mouth was more frequent with BoNT-B but there were no differences for moderate/severe dry mouth. In this study, both BoNT-A and B were shown to be effective and safe for the treatment of toxin-naive CD subjects.", "title": "Botulinum toxin type B vs. type A in toxin-na\u00efve patients with cervical dystonia: Randomized, double-blind, noninferiority trial.", "date": "2007-12-22"}, {"article_id": "16275831", "content": "To directly compare two serotypes of botulinum toxin (BoNTA and BoNTB) in cervical dystonia (CD) using a randomized, double-blind, parallel-arm study design.\nSubjects with CD who had a previous response from BoNTA were randomly assigned to BoNTA or BoNTB and evaluated in a blinded fashion at baseline, 4 weeks, 8 weeks, and 2-week intervals thereafter until loss of 80% of clinical effect or completion of 20 weeks of observation. CD severity was measured with the Toronto Western Spasmodic Torticollis Rating Scale (TWSTRS), and adverse events were assessed by structured interview. Statistical analysis included Wilcoxon rank sum test, log rank tests, and Kaplan-Meier survival curves for duration of effect.\nA total of 139 subjects (BoNTA, n = 74; BoNTB, n = 65) were randomized at 19 study sites. Improvement in TWSTRS score was found at 4 weeks after injection and did not differ between serotypes. Dysphagia and dry mouth were more frequent with BoNTB (dysphagia: BoNTA 19% vs BoNTB 48%, p = 0.0005; dry mouth (BoNTA 41% vs BoNTB 80%, p < 0.0001). In clinical responders, BoNT A had a modestly longer duration of benefit (BoNTA 14 weeks, BoNTB 12.1 weeks, p = 0.033).\nBoth serotypes of botulinum toxin (BoNTA and BoNTB) had equivalent benefit in subjects with cervical dystonia at 4 weeks. BoNTA had fewer adverse events and a marginally longer duration of effect in subjects showing a clinical response.", "title": "Comparison of botulinum toxin serotypes A and B for the treatment of cervical dystonia.", "date": "2005-11-09"}]}
{"original_review": "25620061", "question_data": [{"question_id": 57, "question": "Is the overall survival rate higher, lower, or the same when comparing percutaneous ethanol injection (PEI) to percutaneous acetic acid injection (PAI)?", "answer": "no difference", "evidence_quality": "low", "fulltext_required": "no", "comment": "", "relevant_sources": ["9425919", "16009687"]}, {"question_id": 58, "question": "Is the overall survival rate higher, lower, or the same when comparing percutaneous ethanol injection (PEI) to surgery?", "answer": "no difference", "evidence_quality": "very low", "fulltext_required": "no", "comment": "", "relevant_sources": ["15973099"]}, {"question_id": 221, "question": "Is the overall survival rate higher, lower, or the same when comparing percutaneous acetic acid injection (PAI) to sham intervention?", "answer": "insufficient data", "evidence_quality": "", "fulltext_required": "no", "comment": "", "relevant_sources": ["9425919", "16009687", "15973099"]}, {"question_id": 222, "question": "Is the overall survival rate higher, lower, or the same when comparing percutaneous ethanol injection (PEI) to sham intervention?", "answer": "insufficient data", "evidence_quality": "", "fulltext_required": "no", "comment": "", "relevant_sources": ["9425919", "16009687", "15973099"]}, {"question_id": 223, "question": "Is the overall survival rate higher, lower, or the same when comparing percutaneous acetic acid injection (PAI) to no intervention?", "answer": "insufficient data", "evidence_quality": "", "fulltext_required": "no", "comment": "", "relevant_sources": ["9425919", "16009687", "15973099"]}, {"question_id": 224, "question": "Is the overall survival rate higher, lower, or the same when comparing percutaneous ethanol injection (PEI) to no intervention?", "answer": "insufficient data", "evidence_quality": "", "fulltext_required": "no", "comment": "", "relevant_sources": ["9425919", "16009687", "15973099"]}], "sources": [{"article_id": "16009687", "content": "The aim of this study was to compare the outcomes of radiofrequency thermal ablation (RFTA), percutaneous ethanol injection (PEI), and percutaneous acetic acid injection (PAI) in the treatment of hepatocellular carcinoma (HCC).\nA total of 187 patients with HCCs of 3 cm or less were randomly assigned to RFTA (n = 62), PEI (n = 62), or PAI (n = 63). Tumour recurrence and survival rates were assessed.\nOne, two, and three year local recurrence rates were 10%, 14%, and 14% in the RFTA group, 16%, 34%, and 34% in the PEI group, and 14%, 31%, and 31% in the PAI group (RFTA v PEI, p = 0.012; RFTA v PAI, p = 0.017). One, two, and three year survival rates were 93%, 81%, and 74% in the RFTA group, 88%, 66%, and 51% in the PEI group, and 90%, 67%, and 53% in the PAI group (RFTA v PEI, p = 0.031; RFTA v PAI, p = 0.038). One, two, and three year cancer free survival rates were 74%, 60%, and 43% in the RFTA group, 70%, 41%, and 21% in the PEI group, and 71%, 43%, and 23% in the PAI group (RFTA v PEI, p = 0.038; RFTA v PAI, p = 0.041). Tumour size, tumour differentiation, and treatment methods (RFTA v PEI and PAI) were significant factors for local recurrence, overall survival, and cancer free survival. Major complications occurred in 4.8% of patients (two with haemothorax, one gastric perforation) in the RFTA group and in none in two other groups (RFTA v PEI and PAI, p = 0.035).\nRFTA was superior to PEI and PAI with respect to local recurrence, overall survival, and cancer free survival rates, but RFTA also caused more major complications.", "title": "Randomised controlled trial comparing percutaneous radiofrequency thermal ablation, percutaneous ethanol injection, and percutaneous acetic acid injection to treat hepatocellular carcinoma of 3 cm or less.", "date": "2005-07-13"}, {"article_id": "9730384", "content": "To assess whether ultrasound (US)-guided percutaneous acetic acid injection is more effective than percutaneous ethanol injection in the treatment of small hepatocellular carcinoma (HCC).\nsixty patients with 1 to 4 HCC smaller than 3 cm entered a randomized controlled trial from August 1993 to September 1995. Thirty one and 29 patients were treated by percutaneous acetic acid injection using 50% acetic acid and percutaneous ethanol injection using absolute ethanol, respectively. There were no significant differences in clinical characteristics and biochemical data between the two groups.\nAll original tumors were treated successfully by the chosen therapy. However, local recurrence occurred in 8% of the 38 tumors treated with percutaneous acetic acid injection and 37% of the 35 tumors treated with percutaneous ethanol injection P>0.001). The 1- and 2-year survival rates were 100% and 92% with percutaneous acetic acid injection and 83% and 63% with percutaneous ethanol injection (p=0.0017). Multivariate analysis of prognostic factors revealed that treatment was an independent predictor of survival.\npercutaneous acetic acid injection is more effective than percutaneous ethanol injection.", "title": "Comparison of percutaneous acetic acid injection and percutaneous ethanol injection for small hepatocellular carcinoma.", "date": "1998-09-08"}, {"article_id": "15973099", "content": "To compare disease recurrence and survival among patients with small hepatocellular carcinoma after surgical resection or percutaneous ethanol injection therapy, 2 treatments that have not been evaluated with a prospective study.\nA total of 76 patients were randomly assigned to 2 groups based on treatment; all had one or 2 tumors with diameter </=3 cm, with hepatitis without cirrhosis or Child class A or B cirrhosis without evident ascites or bleeding tendency.\nFollow-up ranged from 12 to 59 months. Among percutaneous injection patients, 18 had recurrence 1 to 37 months after treatment (true recurrence, 11; original safety margin inadequate, 3; limitation of imaging technology to detect tiny tumors, 4). Three injection therapy patients died of cancer 25, 37, and 57 months after treatment. For the surgical resection group, 15 had recurrence 2 to 54 months after treatment (true recurrence, 12; limitation of imaging, 2; neck metastasis, 1). Five resection patients died of cancer at 11, 20, 23, 26, and 52 months, respectively. By Cox regression model and Kaplan-Meier survival analysis, there is no statistical significance for recurrence and survival between treatment groups. However, tumor size larger than 2 cm and alpha-fetoprotein over 200 ng/mL correlated with higher recurrence rate, and Child class B liver cirrhosis correlated with shorter survival.\nPercutaneous ethanol injection therapy appears to be as safe and effective as resection, and both treatments can be considered first-line options for small hepatocellular carcinoma.", "title": "Percutaneous ethanol injection versus surgical resection for the treatment of small hepatocellular carcinoma: a prospective study.", "date": "2005-06-24"}, {"article_id": "9425919", "content": "To assess whether ultrasound-guided percutaneous acetic acid injection is superior to percutaneous ethanol injection in the treatment of small hepatocellular carcinoma (HCC), 60 patients with one to four HCCs smaller than 3 cm were entered onto a randomized controlled trial. Thirty-one and 29 patients, respectively, were treated by percutaneous acetic acid injection using 50% acetic acid or by percutaneous ethanol injection using absolute ethanol. There were no significant differences in age, sex ratio, Child-Pugh class, size of tumors, or number of tumors between the two groups. When there was no evidence of viable HCC from biopsy, plain and helical dynamic computed tomography, or angiography, the treatment was considered successful and was discontinued. All original tumors were treated successfully by either therapy. However, 8% of 38 tumors treated with percutaneous acetic acid injection and 37% of 35 tumors treated with percutaneous ethanol injection developed a local recurrence (P < .001) during the follow-up periods of 29 +/- 8 months and 23 +/- 10 months, respectively. The 1- and 2-year survival rates were 100% and 92% in percutaneous acetic acid injection and 83% and 63% in percutaneous ethanol injection (P = .0017). A multivariate analysis of prognostic factors revealed that treatment was an independent predictor of survival. The risk ratio of percutaneous acetic acid injection versus percutaneous ethanol injection was 0.120 (range, 0.027-0.528; P = .0050). In conclusion, percutaneous acetic acid injection is superior to percutaneous ethanol injection in the treatment of small HCC.", "title": "Prospective randomized controlled trial comparing percutaneous acetic acid injection and percutaneous ethanol injection for small hepatocellular carcinoma.", "date": "1998-01-13"}]}
{"original_review": "31608991", "question_data": [{"question_id": 59, "question": "Is the risk of seizure recurrence higher, lower, or the same when comparing six months of treatment to 12-24 months treatment?", "answer": "no difference", "evidence_quality": "low", "fulltext_required": "no", "comment": "", "relevant_sources": ["16441247", "12134175", "12134176"]}, {"question_id": 60, "question": "Is the risk of seizure recurrence higher, lower, or the same when comparing 6-12 months of treatment to 24 months of treatment?", "answer": "no difference", "evidence_quality": "low", "fulltext_required": "no", "comment": "", "relevant_sources": ["16441247", "14604159", "12134175"]}], "sources": [{"article_id": "16441247", "content": "The duration of antiepileptic drug (AED) therapy in cases of solitary cerebral cysticercus granuloma (SCCG) presents a major dilemma and the efficacy of short-term (6 months) vs long-term (2 years) AED therapy has been studied.\nProspective randomized study of short-term vs long-term AED treatment with SCCG has been undertaken. A total of 206 subjects with new onset seizures with SCCG were randomized into two groups: group A (98 patients) were treated for 6 months and group B (108 patients) were treated for 2 years with AED therapy. The patients were evaluated periodically during and at least 18 months after the tapering of drugs.\nPartial seizures with or without secondary generalization has been found to be the commonest manifestation occurring in 80.6% of patients with SCCG. In group A 66.3% and in group B 57.4% patients showed complete resolution of computerized tomographic lesion and rest had punctated residual calcification. Statistically, no significant difference in the recurrence of seizures was found in two groups with disappearance of lesion but the difference between calcified residua and complete resolution subset was significant. In patients having residual calcification, 42.2% in group A and 21.7% in group B had recurrence of seizures and the difference was statistically significant (Z = 1.97, P < 0.05).\nThe study revealed that SCCG with epilepsy is a benign self-limiting disease. A longer duration of therapy is not warranted in patients having total resolution of lesion. Calcified lesion was found to be the most common cause of recurrence of seizures. Higher recurrence rate was observed in short-term therapy in patients having calcified lesions and may require long-term AED treatment.", "title": "Outcome of short-term antiepileptic treatment in patients with solitary cerebral cysticercus granuloma.", "date": "2006-01-31"}, {"article_id": "12134176", "content": "The study was conducted in 81 patients of epilepsy with small single enhancing CT (SSECT) lesion in brain to determine the clinical profile and duration of antiepileptic drugs (AEDs) treatment. The patients were randomly divided into group A (41 cases) and group B (40 cases). Group A patients were treated for 6 months and group B for 1 year with AEDS without cysticidal drugs. The most common mode of presentation was simple partial motor seizures with secondary generalization in both the groups. Repeat imaging of brain (CT/MRI) at 6 months showed disappearance of lesion in 82.94% in group A and 87% in group B, while persistence of lesion was present only in 4.87% in group A and 5% in group B. 87.81% patients in group A and 87.17% in group B were seizure free. The recurrence of seizure occurred in 12.19% cases in group A, and 12.82% in group B. 80% of these patients had calcified lesion in both the groups. This study reveals that SSECT lesion with epilpesy is a benign self-limiting disease. It also reveals that 6 months AED treatment is as effective as one year treatment. Patients having calcified lesion or persistence of lesion might require long term AED treatment.", "title": "Randomized prospective study of outcome of short term antiepileptic treatment in small single enhancing CT lesion in brain.", "date": "2002-07-23"}, {"article_id": "12134175", "content": "The duration of anti epileptic drug therapy for single small enhancing CT lesions (SSECTL) presents a major dilemma. We studied the efficacy of short duration (6 months) antiepileptic drug therapy as compared to long duration (2 years) drug therapy. Seventy three patients presenting with seizures and showing SSECTL on cranial CT scans (plain and contrast) were randomized into group A (6 months therapy) and group B (2 years therapy). There were 47 patients in group A and 26 patients in group B. Patients were followed up for one year after withdrawal of anti epileptic drugs. CT Head (plain and contrast) was repeated after 3 months, or earlier in cases of recurrence to rule out reinfection. 53.2% in group A and 53.8% in group B showed complete resolution and were seizure free on one year follow up. Punctate residual calcification was seen in 46.8% in group A and 46.2% in group B. Eight patients (17%) in group A and three (11.5%) in group B had a recurrence. The difference in recurrence of seizure between the two groups was not statistically significant (p<0.77) in the calcified lesion subset. Since none of the patients in total resolution subset showed recurrence, the difference between calcified and total resolution subset was highly significant. The study shows that a short duration (6 months) AED therapy in patients with total resolution of lesion on follow up scan, may be adequate in comparison to those who have calcific speck as a residue. However, a longer duration of therapy in case of calcific group probably does not alter their chances of recurrence.", "title": "Acute symptomatic seizures due to single CT lesions: how long to treat with antiepileptic drugs?", "date": "2002-07-23"}, {"article_id": "14604159", "content": "The duration of anti-epileptic drug (AED) therapy in children with seizures due to single small enhancing CT lesions (SSECTL) is controversial. We sought to determine whether there is any difference in the rate of seizure recurrence after 1 vs. 2 years of AED therapy and to identify the factors predictive of seizure recurrence. A total of 115 consecutive children with seizures and SSECTL were randomly assigned to two groups. Group A received AED(s) for 1 year and Group B for 2 years seizure-free interval. CT scan and EEG were done prior to AED withdrawal and children were followed-up for seizure recurrence for at least 1 year. Association between seizure recurrence and clinical and CT characteristics was analysed. Groups A and B consisted of 55 and 51 children, respectively (nine were lost to follow-up). There were 61 boys and 45 girls; mean age 9.33 years. Most (93 per cent) had focal seizures: 36 per cent complex partial, 22 per cent simple partial, 35 per cent partial with secondary generalization; 21 per cent had status epilepticus. The two groups were comparable in clinical, EEG and CT characteristics. CT scan and EEG prior to AED withdrawal were abnormal in 44 per cent and 33 per cent respectively. Six children, three from each group had seizure recurrence. Significant association was found between seizure recurrence and abnormal CT (persistence/calcification of lesion) and abnormal EEG prior to AED withdrawal (p < 0.01). The relative risk of seizure recurrence in a child with abnormal CT and EEG prior to AED withdrawal was 26.2 (95 per cent confidence interval 3.3-210.2, p = 0.0003). No association was found between seizure recurrence and any of the other variables. There was no difference in seizure recurrence after 1 vs. 2 years of AED therapy. Combination of persistent/calcified CT lesion and abnormal EEG prior to AED withdrawal was the best predictor of seizure recurrence.", "title": "One vs. two years of anti-epileptic therapy in children with single small enhancing CT lesions.", "date": "2003-11-08"}]}
{"original_review": "29464690", "question_data": [{"question_id": 61, "question": "Is the duration of mechanical ventilation higher, lower, or the same when comparing bispectral index (BIS) monitoring to clinical assessment for sedation?", "answer": "no difference", "evidence_quality": "low", "fulltext_required": "no", "comment": "", "relevant_sources": ["17444309", "21473824"]}, {"question_id": 62, "question": "Is the incidence of restlessness after suction higher, lower, or the same when comparing bispectral index (BIS) monitoring to clinical assessment for sedation?", "answer": "uncertain effect", "evidence_quality": "very low", "fulltext_required": "no", "comment": "requires the model to be skeptical of the sample size and for failing to consider whether the adverse events were clinically relevant", "relevant_sources": ["21473824"]}, {"question_id": 63, "question": "Is the incidence of delirium after extubation higher, lower, or the same when comparing bispectral index (BIS) monitoring to clinical assessment for sedation?", "answer": "uncertain effect", "evidence_quality": "very low", "fulltext_required": "no", "comment": "requires the model to be skeptical of the sample size and for failing to consider whether the adverse events were clinically relevant", "relevant_sources": ["21473824"]}, {"question_id": 64, "question": "Is the incidence of endotracheal tube resistance higher, lower, or the same when comparing bispectral index (BIS) monitoring to clinical assessment for sedation?", "answer": "uncertain effect", "evidence_quality": "very low", "fulltext_required": "no", "comment": "requires the model to be skeptical of the sample size and for failing to consider whether the adverse events were clinically relevant", "relevant_sources": ["21473824"]}], "sources": [{"article_id": "19570344", "content": "To investigate the correlation between subjective scoring and bispectral index (BIS) monitoring, and to study the feasibility of BIS monitor in assessing the depth of sedation for mechanically ventilated patients.\nA prospective randomized controlled trial was conducted. The sedation target was sedation-agitation scale (SAS) 3-4. A total of 83 patients in the intensive care unit were assigned to receive the sedation based on BIS monitor (42 cases) or the sedation based on subjective scale SAS (41 cases). The parameters of respiration, circulation, and the depth of sedation (BIS, SAS, Ramsay) were recorded. The difference between the two groups was compared. The correlation index and significance were calculated.\nA total of 83 patients of two groups reached the sedation target. Statistically significant difference was found between two groups in respiratory rate, fraction of inspiratory oxygen, and pulse saturation of oxygen before and after sedation (all P<0.05). After sedation, respiratory rate and fraction of inspiratory oxygen declined in all patients, while pulse saturation of oxygen rose obviously (all P<0.05). Statistically significant difference was also found in different correlation index. There was positive correlation index between BIS and SAS (r=0.626, P<0.05), a negative correlation index between BIS and Ramsay (r=-0.650, P<0.05), and also a negative correlation index between SAS and Ramsay (r=-0.908, P<0.05). Statistically significant difference was not found in the parameters of respiration and circulation between the two groups.\nBIS monitor is feasible for assessing the depth of sedation in mechanically ventilated patients.", "title": "[A study of bispectral index monitoring in assessing the depth of sedation of patients under mechanical ventilation].", "date": "2009-07-03"}, {"article_id": "17444309", "content": "The aim of this prospective randomised controlled trial was to assess the effectiveness of the Bispectral Index (BIS) monitor in supporting clinical sedation management decisions in mechanically ventilated intensive care unit patients. Fifty adult mechanically ventilated surgical and general intensive care unit patients receiving sedative infusions of morphine and midazolam were randomly allocated to receive BIS monitoring (n=25) or standard sedation management (n=25). In the BIS group, sedation was titrated to maintain a BIS value of greater than 70. In the standard management group, sedative needs were titrated based on subjective assessment and clinical signs. There was no statistically significant difference in the amount of sedation administered (morphine P = 0.67 and midazolam P = 0.85). However, there was a statistically significant difference in sedation administration over time. Patients in the BIS group received increasing amounts of sedation over time whilst those in the control group received decreasing amounts of sedation over time. The same inverse relationship existed for both sedative agents (morphine P = 0.005, midazolam P = 0.03). Duration of mechanical ventilation was comparable in the two groups. We conclude that the use of BIS monitoring did not reduce the amount of sedation used, the length of mechanical ventilation time or the length of ICU stay.", "title": "The impact of bispectral index monitoring on sedation administration in mechanically ventilated patients.", "date": "2007-04-21"}, {"article_id": "21473824", "content": "To compare the value of bispectral index (BIS) monitoring and sedation agitation scale (SAS) in guiding intensive care unit (ICU) sedation therapy for the patients undergoing short term mechanical ventilation.\nOne hundred and five patients aged 18-60 years after operation receiving mechanical ventilation for longer than 12 hours in ICU were enrolled in this study. The patients were randomly divided into two groups: BIS guided group (n=42) and SAS guided group (n=63). All of them received protocolized continuous sedation and analgesia by using fentanyl for analgesia and propofol plus midazolam to sedate intravenously. The effect of sedation was assessed every hour till BIS reaching 50-70 or SAS reaching grade 3-4. Sedatives and analgesics were suspended at 6:00 am on next day after ICU admission , and BIS and the SAS were recorded every hour, sedation time, time to wake up, duration of mechanical ventilation, daily dosage of midazolam and propofol, and the incidence of adverse events including restlessness after suction, endotracheal tube resistance, pain tolerance during sedation, and delirium after extubation were all recorded accordingly.\nDosages of midazolam and propofol were found higher in BIS guided group than the SAS guided group [midazolam (mg\u00d7kg(-1) \u00d7h(-1) ): 0.10\u00b10.02 vs. 0.09\u00b10.02, propofol (mg\u00d7kg(-1) \u00d7h(-1) ): 0.95\u00b10.23 vs. 0.86\u00b10.20, both P<0.05]. The total time (D t) of patients under sedative control was significantly longer in BIS guided group compared with SAS guided group, and also in first three hours [D 1, D 2, D 3, D t: 75.2% (507) vs. 52.8% (421), D 1: 78.6% (33) vs. 22.2% (14), D 2: 88.1% (37) vs. 20.6% (13), D 3: 81.0% (34) vs. 31.7% (20), all P<0.01]. The time to wake up (minutes) was significantly shorter in BIS guided group compared with SAS guided group [0 (0, 20) vs. 15 (0, 47), P<0.05]. No significant difference in acute physiology and chronic health evaluation II (APACHE II) score (3.57\u00b12.60 vs. 4.19\u00b12.30), duration of mechanical ventilation [hours: 16.5 (14.5, 19.0) vs. 17.0 (15.0, 19.0)], sedation time [hours: 14.0 (12.9, 17.1) vs. 16.0 (13.0, 18.0)] and incidence of adverse events including restlessness after suction (81.0% vs. 79.4%), endotracheal tube resistance (71.4% vs. 74.6%), pain tolerance during sedation (92.8% vs. 93.6%) and delirium after extubation (4.8% vs. 1.6%) was found between BIS guided group and SAS guided group (all P>0.05).\nBIS monitoring is better in sedative control than SAS assessment for ICU patients undergoing short term mechanical ventilation.", "title": "[A comparison of bispectral index and sedation agitation scale in guiding sedation therapy: a randomized controlled study in patients undergoing short term mechanical ventilation].", "date": "2011-04-09"}, {"article_id": "17243646", "content": "BIS has not been evaluated for sedation in ICU. We examined BIS for evaluation of sedation with propofol in ICU.\nEighteen male patients undergoing head and neck surgery (ASA-PS 1 x 2 and age<75 years) were randomly allocated to one of two groups receiving postoperative sedation with propofol in ICU. One group was monitored for sedation by BIS, and the other group by Ramsay score.\nThere were no significant differences between the two groups in the total-dose of propofol, and recovery time.\nThe use of BIS for sedation could not improve the management of postoperative sedation in ICU.", "title": "[Comparison of BIS and Ramsay score for evaluation of sedation with propofol in ICU].", "date": "2007-01-25"}]}
{"original_review": "30566235", "question_data": [{"question_id": 65, "question": "Is improvement in depression symptoms higher, lower, or the same when comparing SSRIs to placebo?", "answer": "no difference", "evidence_quality": "very low", "fulltext_required": "no", "comment": "", "relevant_sources": ["25308771", "17136950"]}], "sources": [{"article_id": "17136950", "content": "The aim of this study was to determine whether treating concomitant depression improves quality of life and exercise tolerance in COPD patients. Out-patients with moderate to severe, stable COPD completed Hospital Anxiety-Depression (HAD) and General Health questionnaires. A psychiatrist interviewed those with high scores. In a randomised, double-blind fashion, 28 depressed COPD patients took a selective serotonin re-uptake inhibitor, Paroxetine 20 mg daily, or matched placebo for 6 weeks. Subsequently, all patients took un-blinded Paroxetine for 3 months. From these questionnaires, 35% of 135 patients had significant depression, but this was confirmed by psychiatric interview in only 21%. Throughout the study, there were no changes in laboratory lung function nor in home peak flow. Six weeks' treatment produced no significant differences between placebo and treatment group in either depression, quality of life scores or 6-minute walking distances, although overall improvements in depression, correlated with increases in walking distance. Three months of un-blinded treatment, significantly improved depression scores (self-complete HAD, Beck's Depression and psychiatrist-completed Montgomery-Asberg scores), walking distances (369 to 427 m, p = 0.0003) and St. George's Respiratory Questionnaire Total Scores (65 to 58, p = 0.033). Although self-complete questionnaires over-diagnose depression, the condition is nevertheless common in patients with moderately severe COPD. Six weeks of antidepressants is insufficient to improve either depression, quality of life or exercise tolerance. However, our study suggests that a longer course of treatment may be effective and that improvements in depression are associated with improvements in exercise tolerance. A larger, double blind study with a longer treatment period is indicated.", "title": "Effect of treating depression on quality-of-life and exercise tolerance in severe COPD.", "date": "2006-12-02"}, {"article_id": "25308771", "content": "There is a lack of randomized controlled trials to assess the effects of pharmacological treatments in patients with stable chronic obstructive pulmonary disease (COPD) complicated with moderate or severe depression.\nTo assess the efficacy of sertraline hydrochloride on improving the quality of life of patients with stable COPD complicated with depression.\nThis randomized controlled trial, conducted from May to November 2013 in the Huai'an Second Hospital, Huai'an, China, enrolled 120 patients with stable COPD who had moderate or severe depression. Patients were randomly assigned to control and interventional groups (n\u2009=\u200960 in each group). In addition to the treatment for COPD, interventional group also received sertraline hydrochloride tablets 50\u2009mg/day for 6\u2009weeks, while the control group received placebo. The primary end point included COPD assessment test (CAT) scores and the secondary endpoint included 6-min walk distance and 17-item Hamilton Depression Rating Scale (HAMD-17) scores. Parameters of spirometry and adverse events were also observed.\nThere was no significant difference in improvements in the parameters of spirometry tests before and after the treatment with sertraline hydrochloride tablets between the placebo and interventional groups (P\u2009>\u20090.05). Patients in the sertraline hydrochloride group showed more changes in the HAMD-17 scores and CAT scores after treatment (P\u2009<\u20090.05) and travelled longer distances in the 6-min walk test than in the placebo group (P\u2009<\u20090.05).\nAntidepressant treatment can improve the quality of life and exercise capacity of patients with depression, and it can also improve depression scores, but not lung function.", "title": "Sertraline hydrochloride treatment for patients with stable chronic obstructive pulmonary disease complicated with depression: a randomized controlled trial.", "date": "2014-10-14"}, {"article_id": "15679006", "content": "Although the underlying pathology is initially confined to the lungs, the associated emotional responses to chronic obstructive pulmonary disease (COPD) contribute greatly to the resulting morbidity. The objective of this study was to examine the effect of an antidepressant drug on disease-specific quality of life in patients with end-stage COPD who present significant depressive symptoms.\nWe conducted a 12-week, randomized double-blind placebo-controlled trial of Paroxetine in which quality of life measured by the Chronic Respiratory Questionnaire (CRQ), an evaluative COPD-specific quality-of-life questionnaire, was the primary outcome.\n23 patients were randomized and 15 completed the trial (8 on Paroxetine; 7 on placebo). In the per-protocol analysis, we observed statistically and clinically significant improvements favoring the active treatment in 2 of the 4 domains of the CRQ: emotional function (adjusted mean difference: 1.1; 95% confidence interval [CI]: 0.0 - 2.2) and mastery (difference: 1.1; CI: 0.4 - 1.8). Dyspnea and fatigue improved, but to an extent that did not reach statistical significance. In the intention-to-treat analysis, none of the differences in CRQ scores was significant. Paroxetine was not associated to any worsening of respiratory symptoms.\nThe results of this small randomized trial indicated that patients with end-stage COPD may benefit from antidepressant drug therapy when significant depressive symptoms are present. This study underlined the difficulties in conducting experimental studies in frail and elderly patients with COPD.", "title": "Randomized trial of paroxetine in end-stage COPD.", "date": "2005-02-01"}, {"article_id": "1557484", "content": "Although recent epidemiologic studies have established that patients with chronic medical illness and depressed mood are more disabled than euthymic patients, detailed data on the benefits and risks of antidepressant treatment in medically high-risk patients have been slow to accumulate. The authors have examined multiple outcome indicators in patients with disabling chronic obstructive pulmonary disease and comorbid depression. Thirty patients completed a 12-week, randomized controlled trial of nortriptyline. Nortriptyline was clearly superior to placebo for treatment of depression. Nortriptyline treatment was accompanied by marked improvements in anxiety, certain respiratory symptoms, overall physical comfort, and day-to-day function; placebo effects were negligible. Physiological measures reflecting pulmonary insufficiency were generally unaffected by treatment. These data provide impetus for renewed efforts to improve recognition and treatment of mood disorders in even severely disabled medical patients.", "title": "Improvement in mood, physical symptoms, and function with nortriptyline for depression in patients with chronic obstructive pulmonary disease.", "date": "1992-01-01"}]}
{"original_review": "29963686", "question_data": [{"question_id": 66, "question": "Is the risk of major postoperative bleeding requiring intervention higher, lower, or the same when comparing tranexamic acid (TXA) to placebo?", "answer": "lower", "evidence_quality": "moderate", "fulltext_required": "no", "comment": "", "relevant_sources": ["2648144", "8229393"]}], "sources": [{"article_id": "25528251", "content": "Oral anticoagulants are widely prescribed drugs. Interruption of anticoagulant therapy prior to oral surgery has been an issue of great controversy. The purpose of this study was to evaluate the incidence of bleeding complications after dental extractions in patients on anticoagulant therapy (warfarin) in whom different local hemostatic methods were used.\nPatients using warfarin and requiring extractions of at least two teeth were screened to participate in this prospective, randomized study. Extraction sites were considered as sampling units (statistically representative sample size) and were allocated to one of the three study groups (G1-4.8% tranexamic acid; G2-fibrin sponge; and G3-no local hemostatic agents).\nEighty-four extraction sites were obtained from patients with mitral valve prolapse (47.4%), prosthetic cardiac valve (23.7%), venous thromboembolism (21.1%), and pulmonary embolism (5.2%). International normalized ratio (INR) values ranged between 2.1 and 3.1 (mean 2.51\u2009\u00b1\u20090.1). Postoperative bleeding was observed in four surgical sites (p\u2009<\u20090.001) and was mainly in older patients (p\u2009=\u20090.005).\nThe three local hemostatic protocols were similarly effective in controlling postoperative bleeding in patients undergoing anticoagulant therapy with warfarin. The majority of teeth could be extracted with minimal problems in patients with cardiovascular diseases receiving treatment with anticoagulant therapy.", "title": "Postoperative hemostatic efficacy of gauze soaked in tranexamic acid, fibrin sponge, and dry gauze compression following dental extractions in anticoagulated patients with cardiovascular disease: a prospective, randomized study.", "date": "2014-12-22"}, {"article_id": "8229393", "content": "The hemostatic effect of tranexamic acid solution (4.8%) used as a mouthwash was compared with a placebo solution in 93 patients on continuous, unchanged, oral anticoagulant treatment undergoing oral surgery. The investigation was a randomized, double-blind, placebo-controlled, multicenter study. Before suturing, the surgically treated region was irrigated with 10 mL of tranexamic acid (46 patients) or placebo (47 patients) solution. The patients then performed mouthwash with 10 mL of the solution for 2 minutes four times daily for 7 days. The treatment groups were comparable regarding age, smoking habits, and surgery. Laboratory variables measuring vitamin K-dependent coagulation factors were within therapeutic ranges (international normalized ratio 4.00 to 2.10). One of the clinics used a different method for these measurements and therefore the levels of coagulation factor X in plasma obtained for the three clinics were compared. No significant difference in the range at which surgery was performed was found between clinics. In the placebo group, 10 patients developed bleeding requiring treatment, while none of the patients treated with tranexamic acid solution had bleeding. This difference was statistically significant (P < .01). The treatment with mouthwash was well tolerated. It was concluded that patients on oral anticoagulants can undergo oral surgery within the therapeutic range without reducing the dosage of anticoagulants, provided that local antifibrinolytic treatment with tranexamic acid solution is instituted.", "title": "Prevention of postsurgical bleeding in oral surgery using tranexamic acid without dose modification of oral anticoagulants.", "date": "1993-11-01"}, {"article_id": "10468454", "content": "The purpose of this study was to evaluate postoperative bleeding in patients treated with oral anticoagulant drugs who underwent dental extractions without interruption of the treatment and to compare the effect of 3 different hemostatic modalities.\nA total of 150 patients who underwent dental extractions were divided into 3 groups. Local hemostasis was carried out as follows: group 1 (119 extractions), with gelatin sponge and sutures; group 2 (117 extractions), with gelatin sponge, sutures, and mouthwash with tranexamic acid; group 3 (123 extractions), with fibrin glue, gelatin sponge, and sutures.\nOf 150 patients, 13 (8.6%) presented with postoperative bleeding: 3 patients from group 1, 6 patients from group 2, and 4 patients from group 3.\nDental extractions can be performed without interruption in patients treated with oral anticoagulant. Local hemostasis with gelatin sponge and sutures is sufficient.", "title": "Dental extractions in patients maintained on continued oral anticoagulant: comparison of local hemostatic modalities.", "date": "1999-09-01"}, {"article_id": "2648144", "content": "We carried out a placebo-controlled, double-blind, randomized study of the hemostatic effect of tranexamic acid mouthwash after oral surgery in 39 patients receiving anticoagulant agents because of the presence of cardiac valvular stenosis, a prosthetic cardiac valve, or a vascular prosthesis. Surgery was performed with no change in the level of anticoagulant therapy, and treatment with the anticoagulant agent was continued after surgery. Before it was sutured, the operative field was irrigated in 19 patients with 10 ml of a 4.8 percent aqueous solution of tranexamic acid (an inhibitor of fibrinolysis) and in 20 patients with a placebo solution. For seven days thereafter, patients were instructed to rinse their mouths with 10 ml of the assigned solution for two minutes four times a day. There were no significant differences between the two treatment groups in base-line variables, including the level of anticoagulation at the time of surgery. Eight patients in the placebo group had a total of 10 postoperative bleeding episodes, whereas only 1 patient in the tranexamic acid group had a bleeding episode (P = 0.01). There were no systemic side effects. We conclude that local antifibrinolytic therapy is effective in preventing bleeding after oral surgery in patients who are being treated with anticoagulants.", "title": "Hemostatic effect of tranexamic acid mouthwash in anticoagulant-treated patients undergoing oral surgery.", "date": "1989-03-30"}]}
{"original_review": "31038197", "question_data": [{"question_id": 67, "question": "Is the risk of developing microbiologically confirmed tuberculosis higher, lower, or the same when comparing MVA85A added to BCG to BCG alone?", "answer": "no difference", "evidence_quality": "moderate", "fulltext_required": "yes", "comment": "", "relevant_sources": ["25726088", "23391465"]}, {"question_id": 68, "question": "Is the risk of developing latent tuberculosis higher, lower, or the same when comparing MVA85A added to BCG to BCG alone? ", "answer": "no difference", "evidence_quality": "moderate", "fulltext_required": "yes", "comment": "21606542/scriba omitted with 0.3% weight", "relevant_sources": ["25726088", "23391465", "29028973"]}, {"question_id": 69, "question": "Is the risk of life\u2010threatening serious adverse effects higher, lower, or the same when comparing MVA85A added to BCG to BCG alone?", "answer": "no difference", "evidence_quality": "high", "fulltext_required": "yes", "comment": "", "relevant_sources": ["25726088", "23391465", "29028973"]}, {"question_id": 70, "question": "Is the risk of starting on tuberculosis treatment higher, lower, or the same when comparing MVA85A added to BCG to BCG alone?", "answer": "no difference", "evidence_quality": "moderate", "fulltext_required": "yes", "comment": "", "relevant_sources": ["25726088", "23391465", "29028973"]}], "sources": [{"article_id": "28215501", "content": "The value of quantitative interferon-\u03b3 release assay results for predicting progression from Mycobacterium tuberculosis infection to active disease is unknown. We aimed to investigate the relation between QuantiFERON-TB Gold In-Tube (QFT) conversion interferon-\u03b3 values and risk of subsequent active tuberculosis disease and of QFT reversion.\nWe analysed data from a reported vaccine efficacy trial of the tuberculosis vaccine MVA85A in South Africa. QFT negative, HIV uninfected young children aged 18-24 weeks were enrolled. We stratified participants by quantitative QFT result (interferon-\u03b3 <0\u00b735 IU/mL, 0\u00b735-4\u00b700 IU/mL, and >4\u00b700 IU/mL) at the intermediate study visit (day 336) and determined risk of progression to active tuberculosis disease over the subsequent 6-24 months. No QFT differences were observed between placebo and MVA85A groups at day 336 or end of study; therefore, both groups were included in analyses. Study clinicians were not masked to QFT values, but strict case definitions were used that excluded QFT results. We used generalised additive models to evaluate the quantitative relation between day 336 QFT value and subsequent disease risk, and we compared disease rates between QFT strata using a two-sample Poisson test.\nAmong 2512 young children with QFT tests done at day 336, 172 (7%) were positive; 87 (7%) of 1267 in placebo group and 85 (7%) of 1245 in the MVA85A group (p=1\u00b700). Compared with QFT non-converters (tuberculosis disease incidence 0\u00b77 per 100 person-years [95% CI 0\u00b74-1\u00b71]), children with QFT conversion at interferon-\u03b3 values between 0\u00b735-4\u00b700 IU/mL did not have significantly increased risk of disease (2\u00b75 per 100 person-years [95% CI 0\u00b74-9\u00b74]; incidence rate ratio (IRR) 3\u00b77 (95% CI 0\u00b74-15\u00b78; p=0\u00b723). However, QFT conversion at interferon-\u03b3 values higher than 4\u00b700 IU/mL was associated with substantially increased disease incidence (28\u00b70 per 100 person-years [95% CI 14\u00b79-45\u00b77]) compared with non-converters (IRR 42\u00b75 [95% CI 17\u00b72-99\u00b77]; p<0\u00b70001), and compared with children with interferon-\u03b3 values between 0\u00b735-4\u00b700 IU/mL (IRR 11\u00b74 [95% CI 2\u00b74-107\u00b72]; p=0\u00b700047). Among 91 QFT converters who were given a repeat test, 53 (58%) reverted from positive to negative. QFT reversion risk was inversely associated with interferon-\u03b3 value at QFT conversion and was highest with interferon-\u03b3 values less than 4\u00b700 IU/mL (47 [77%] of 61).\nIn young children, tuberculosis disease risk was not significantly increased, and QFT reversion was common, following QFT conversion at interferon-\u03b3 values up to 10 times the recommended test threshold (0\u00b735 IU/mL). By contrast, QFT conversion at very high interferon-\u03b3 values (>4\u00b700 IU/mL) warrants intensified diagnostic and preventive intervention because of the extremely high risk of tuberculosis disease in these young children.\nAeras, Wellcome Trust, and Oxford-Emergent Tuberculosis Consortium (OETC) were the funders of the MVA85A 020 Trial. National Institute of Allergy and Infectious Diseases supported this analysis.", "title": "Serial QuantiFERON testing and tuberculosis disease risk among young children: an observational cohort study.", "date": "2017-02-22"}, {"article_id": "21606542", "content": "BCG, the only licensed tuberculosis vaccine, affords poor protection against lung tuberculosis in infants and children. A new tuberculosis vaccine, which may enhance the BCG-induced immune response, is urgently needed. We assessed the safety of and characterized the T cell response induced by 3 doses of the candidate vaccine, MVA85A, in BCG-vaccinated infants from a setting where tuberculosis is endemic.\n\u2003Infants aged 5-12 months were vaccinated intradermally with either 2.5 \u00d7 10(7), 5 \u00d7 10(7), or 10 \u00d7 10(7) plaque-forming units of MVA85A, or placebo. Adverse events were documented, and T-cell responses were assessed by interferon \u03b3 (IFN-\u03b3) enzyme-linked immunospot assay and intracellular cytokine staining.\nThe 3 MVA85A doses were well tolerated, and no vaccine-related serious adverse events were recorded. MVA85A induced potent, durable T-cell responses, which exceeded prevaccination responses up to 168 d after vaccination. No dose-related differences in response magnitude were observed. Multiple CD4 T cell subsets were induced; polyfunctional CD4 T cells co-expressing T-helper cell 1 cytokines with or without granulocyte-macrophage colony-stimulating factor predominated. IFN-\u03b3-expressing CD8 T cells, which peaked later than CD4 T cells, were also detectable.\nMVA85A was safe and induced robust, polyfunctional, durable CD4 and CD8 T-cell responses in infants. These data support efficacy evaluation of MVA85A to prevent tuberculosis in infancy. Clinical Trials Registration.\u2003NCT00679159.", "title": "Dose-finding study of the novel tuberculosis vaccine, MVA85A, in healthy BCG-vaccinated infants.", "date": "2011-05-25"}, {"article_id": "28633702", "content": "South Africa.\nTo evaluate the long-term effectiveness of infant modified vaccinia Ankara virus-expressing antigen 85A (MVA85A) vaccination against tuberculosis (TB).\nWe analysed data from a double-blind randomised placebo-controlled Phase 2b MVA85A infant TB vaccine trial (2009-2012), with extended post-trial follow-up (2012-2014). Isoniazid preventive therapy (IPT) was provided by public health services according to national guidelines. The primary outcome was curative treatment for TB disease. Survival analysis and Poisson regression were used for study analysis.\nTotal follow-up was 10\u2009351 person-years of observation (pyo). Median follow-up age was 4.8 years (interquartile range 4.4-5.2). There were 328 (12%) TB cases. TB disease incidence was 3.2/100 pyo (95%CI 2.8-3.5) overall, and respectively 3.3 (95%CI 2.9-3.9) and 3.0 (95%CI 2.6-3.5)/100 pyo in the MVA85A vaccine and placebo arms. A total of 304 children (11%) received IPT, with respectively 880 and 9471 pyo among IPT and non-IPT recipients. There were 23 (7.6%) TB cases among 304 IPT recipients vs. 305 (12.9%) among 2374 non-IPT recipients (P = 0.008). IPT effectiveness was 85% (95%CI 76-91).\nExtended follow-up confirms no long-term effectiveness of infant MVA85A vaccination, but a six-fold reduction in TB risk can be attributed to IPT. National TB programmes in high TB burden countries should ensure optimal implementation of IPT for eligible children.\nEvaluer l'efficacit\u00e9e \u00e0 long terme de la vaccination des nourrissons par le MVA85A (modified vaccinia Ankara virus-expressing antigen 85A vaccine) contre la tuberculose (TB).\nNous avons analys\u00e9 les donn\u00e9es d'un essai vaccinal de phase 2b en double aveugle, randomis\u00e9, contre placebo du vaccin MVA85A contre la TB du nourrisson (2009\u20132012) avec un suivi prolong\u00e9 apr\u00e8s l'essai (2012\u20132014). Un traitement pr\u00e9ventif par isoniazide (IPT) a \u00e9t\u00e9 fourni par les services de sant\u00e9 publique en accord avec les directives nationales. Le r\u00e9sultat principal a \u00e9t\u00e9 le traitement curatif de la TB maladie. Notre analyse a utilis\u00e9 l'analyse de survie et la r\u00e9gression de Poisson.\nLa dur\u00e9e totale du suivi a \u00e9t\u00e9 de 10 351 personnes-ann\u00e9es d'observation (pyo). L'\u00e2ge m\u00e9dian du suivi a \u00e9t\u00e9 de 4,8 ans (intervalle interquartile 4,4\u20135,2). Sont survenus 328 (12%) cas de TB. L'incidence d'ensemble de la TB maladie a \u00e9t\u00e9 de 3,2/100 pyo (IC95% 2,8\u20133,5) ; et 3,3 (IC95% 2,9\u20133,9) contre 3,0 (IC95% 2,6\u20133,5)/100 pyo, dans les bras vaccin MVA85A et placebo, respectivement. Ont re\u00e7u l'IPT 304 (11%) enfants, avec 880 et 9471 pyo parmi les enfants IPT et non IPT, respectivement ; 23 (7,6%) cas de TB sont survenus parmi 304 enfants qui ont re\u00e7u l'IPT contre 305 (12,9%) parmi les 2374 enfants qui n'ont pas re\u00e7u l'IPT (\nUn suivi prolong\u00e9 confirme l'absence d'efficacit\u00e9 \u00e0 long terme de l'administration du vaccin MVA85A chez les nourrissons mais une division par six du risque de TB est attribu\u00e9e \u00e0 l'IPT. Les programmes nationaux TB dans les pays durement frapp\u00e9s par la TB devraient s'assurer de la mise en \u0153uvre optimale de l'IPT pour les enfants \u00e9ligibles.\nEvaluar la eficacia a largo plazo de la vacunaci\u00f3n antituberculosa de los lactantes con MVA85A.\nSe analizaron los datos de un ensayo cl\u00ednico aleatorizado, con doble anonimato y controlado con placebo en fase 2b de la vacuna antituberculosa MVA85A en lactantes (del 2009 al 2012), con un seguimiento prolongado despu\u00e9s del ensayo (del 2012 al 2014). Los servicios de atenci\u00f3n de salud del sector p\u00fablico suministraron el tratamiento preventivo con isoniazida (TPI) seg\u00fan las directrices nacionales. El principal criterio de valoraci\u00f3n fue el tratamiento curativo de la tuberculosis (TB) activa. En el an\u00e1lisis estad\u00edstico se aplicaron el an\u00e1lisis de supervivencia y la regresi\u00f3n de Poisson.\nEl seguimiento total fue de 107 351 a\u00f1os-persona de observaci\u00f3n (apo). La mediana de la edad del seguimiento fue 4,8 a\u00f1os (amplitud intercuartil [AIC] de 4,4 a 5,2). Se presentaron 328 casos de TB (12%). La incidencia global de enfermedad tuberculosa fue 3,2 por 100 apo (IC del 95% de 2,8 a 3,5); en el grupo que recibi\u00f3 la vacuna MVA85A la incidencia fue 3,3 por 100 apo (IC95% de 2,9 a 3,9), comparada con 3,0 por 100 apo en el grupo que recibi\u00f3 placebo (IC95% de 2,6 a 3,5). Trescientos cuatro ni\u00f1os recibieron el TPI (11%) y su seguimiento fue de 880 apo; el seguimiento de quienes no recibieron TPI fue de 9471 apo. En el grupo que recibi\u00f3 TPI se presentaron 23 casos de TB (7,6% de 304) contra 305 en el grupo sin TPI (12,9% de 2374; \nUn seguimiento prolongado confirm\u00f3 que la vacuna MVA85A no es eficaz a largo plazo en los lactantes, pero atribuy\u00f3 al TPI una disminuci\u00f3n de seis veces del riesgo de padecer TB. Los programas nacionales contra la TB de los pa\u00edses con alta carga de morbilidad deben velar por la aplicaci\u00f3n \u00f3ptima del TPI en los ni\u00f1os que cumplen las condiciones para recibirlo.", "title": "Impact of isoniazid preventive therapy on the evaluation of long-term effectiveness of infant MVA85A vaccination.", "date": "2017-06-22"}, {"article_id": "25726088", "content": "HIV-1 infection is associated with increased risk of tuberculosis and a safe and effective vaccine would assist control measures. We assessed the safety, immunogenicity, and efficacy of a candidate tuberculosis vaccine, modified vaccinia virus Ankara expressing antigen 85A (MVA85A), in adults infected with HIV-1.\nWe did a randomised, double-blind, placebo-controlled, phase 2 trial of MVA85A in adults infected with HIV-1, at two clinical sites, in Cape Town, South Africa and Dakar, Senegal. Eligible participants were aged 18-50 years, had no evidence of active tuberculosis, and had baseline CD4 counts greater than 350 cells per \u03bcL if they had never received antiretroviral therapy or greater than 300 cells per \u03bcL (and with undetectable viral load before randomisation) if they were receiving antiretroviral therapy; participants with latent tuberculosis infection were eligible if they had completed at least 5 months of isoniazid preventive therapy, unless they had completed treatment for tuberculosis disease within 3 years before randomisation. Participants were randomly assigned (1:1) in blocks of four by randomly generated sequence to receive two intradermal injections of either MVA85A or placebo. Randomisation was stratified by antiretroviral therapy status and study site. Participants, nurses, investigators, and laboratory staff were masked to group allocation. The second (booster) injection of MVA85A or placebo was given 6-12 months after the first vaccination. The primary study outcome was safety in all vaccinated participants (the safety analysis population). Safety was assessed throughout the trial as defined in the protocol. Secondary outcomes were immunogenicity and vaccine efficacy against Mycobacterium tuberculosis infection and disease, assessed in the per-protocol population. Immunogenicity was assessed in a subset of participants at day 7 and day 28 after the first and second vaccination, and M tuberculosis infection and disease were assessed at the end of the study. The trial is registered with ClinicalTrials.gov, number NCT01151189.\nBetween Aug 4, 2011, and April 24, 2013, 650 participants were enrolled and randomly assigned; 649 were included in the safety analysis (324 in the MVA85A group and 325 in the placebo group) and 645 in the per-protocol analysis (320 and 325). 513 (71%) participants had CD4 counts greater than 300 cells per \u03bcL and were receiving antiretroviral therapy; 136 (21%) had CD4 counts above 350 cells per \u03bcL and had never received antiretroviral therapy. 277 (43%) had received isoniazid prophylaxis before enrolment. Solicited adverse events were more frequent in participants who received MVA85A (288 [89%]) than in those given placebo (235 [72%]). 34 serious adverse events were reported, 17 (5%) in each group. MVA85A induced a significant increase in antigen 85A-specific T-cell response, which peaked 7 days after both vaccinations and was primarily monofunctional. The number of participants with negative QuantiFERON-TB Gold In-Tube findings at baseline who converted to positive by the end of the study was 38 (20%) of 186 in the MVA85A group and 40 (23%) of 173 in the placebo group, for a vaccine efficacy of 11\u00b77% (95% CI -41\u00b73 to 44\u00b79). In the per-protocol population, six (2%) cases of tuberculosis disease occurred in the MVA85A group and nine (3%) occurred in the placebo group, for a vaccine efficacy of 32\u00b78% (95% CI -111\u00b75 to 80\u00b73).\nMVA85A was well tolerated and immunogenic in adults infected with HIV-1. However, we detected no efficacy against M tuberculosis infection or disease, although the study was underpowered to detect an effect against disease. Potential reasons for the absence of detectable efficacy in this trial include insufficient induction of a vaccine-induced immune response or the wrong type of vaccine-induced immune response, or both.\nEuropean & Developing Countries Clinical Trials Partnership (IP.2007.32080.002), Aeras, Bill & Melinda Gates Foundation, Wellcome Trust, and Oxford-Emergent Tuberculosis Consortium.", "title": "Safety, immunogenicity, and efficacy of the candidate tuberculosis vaccine MVA85A in healthy adults infected with HIV-1: a randomised, placebo-controlled, phase 2 trial.", "date": "2015-03-03"}, {"article_id": "23391465", "content": "The aim of this clinical caries detection study was to compare the outcome of quantitative light-induced fluorescence (QLF) and meticulous visual inspection (VI) in detecting non-cavitated caries lesions on occlusal surfaces in young adolescents. It was hypothesized that the respective diagnostic performances of meticulous VI and QLF are similar.\nThe subjects were 34 fifteen-year-old students. Five-hundred-and-seventeen cleaned occlusal surfaces were air-dried and examined using VI. Fluorescence images were captured with QLF equipment and custom software was used to display, store and analyze the images. The area of the lesion (area; mm2), fluorescence loss (DeltaF;%) and DeltaQ (Area*DeltaF; mm2*%) were determined at a QLF threshold of -5%. The presence/absence of non-cavitated lesions was independently recorded with both methods.\n78.8% of all untreated surfaces were classified as sound or as having a non-cavitated lesion with both methods uniformly (VI+QLF). On 7.1% of all surfaces a lesion was detected by VI only and on 14.1% by QLF only. All parameters (Area, DeltaF, DeltaQ) differed significantly between lesions registered with both methods (VI+QLF) and lesions recorded with QLF only.\nIt was concluded that our hypothesis cannot be confirmed. The study shows that QLF detects (1) more non-cavitated occlusal lesions and (2) smaller lesions compared to VI. However, taking into consideration time-consuming image capturing and analysis, QLF is not really practical for use in the dental office.", "title": "In vivo detection of non-cavitated caries lesions on occlusal surfaces by visual inspection and quantitative light-induced fluorescence.", "date": "2007-05-22"}, {"article_id": "29028973", "content": "Vaccination of human immunodeficiency virus (HIV)-infected infants with bacille Calmette-Gu\u00e9rin (BCG) is contraindicated. HIV-exposed newborns need a new tuberculosis vaccination strategy that protects against tuberculosis early in life and avoids the potential risk of BCG disease until after HIV infection has been excluded.\nThis double-blind, randomized, controlled trial compared newborn MVA85A prime vaccination (1 \u00d7 108 PFU) vs Candin\u00ae control, followed by selective, deferred BCG vaccination at age 8 weeks for HIV-uninfected infants and 12 months follow-up for safety and immunogenicity.\nA total of 248 HIV-exposed infants were enrolled. More frequent mild-moderate reactogenicity events were seen after newborn MVA85A vaccination. However, no significant difference was observed in the rate of severe or serious adverse events, HIV acquisition (n = 1 per arm), or incident tuberculosis disease (n = 5 MVA85A; n = 3 control) compared to the control arm. MVA85A vaccination induced modest but significantly higher Ag85A-specific interferon gamma (IFN\u03b3)+ CD4+ T cells compared to control at weeks 4 and 8 (P < .0001). BCG did not further boost this response in MVA85A vaccinees. The BCG-induced Ag85A-specific IFN\u03b3+ CD4+ T-cell response at weeks 16 and 52 was of similar magnitude in the control arm compared to the MVA85A arm at all time points. Proliferative capacity, functional profiles, and memory phenotype of BCG-specific CD4 responses were similar across study arms.\nMVA85A prime vaccination of HIV-exposed newborns was safe and induced an early modest antigen-specific immune response that did not interfere with, or enhance, immunogenicity of subsequent BCG vaccination. New protein-subunit and viral-vectored tuberculosis vaccine candidates should be tested in HIV-exposed newborns.\nNCT01650389.", "title": "Safety and Immunogenicity of Newborn MVA85A Vaccination and Selective, Delayed Bacille Calmette-Guerin for Infants of Human Immunodeficiency Virus-Infected Mothers: A Phase 2 Randomized, Controlled Trial.", "date": "2017-10-14"}]}
{"original_review": "28535331", "question_data": [{"question_id": 71, "question": "Is improvement in health status up to 6 months (as measured by improvement on both CCQ and SGRQ) higher, lower, or the same when comparing smart technology for self\u2010management to face\u2010to\u2010face/digital and/or written support for self\u2010management?", "answer": "higher", "evidence_quality": "low", "fulltext_required": "yes", "comment": "", "relevant_sources": ["27502583", "26089656", "24293120"]}, {"question_id": 72, "question": "Is the risk of adverse events higher, lower, or the same when comparing smart technology for self\u2010management to face\u2010to\u2010face/digital and/or written support for self\u2010management?", "answer": "higher", "evidence_quality": "", "fulltext_required": "yes", "comment": "", "relevant_sources": ["27502583"]}, {"question_id": 73, "question": "Is activity level up to 6 months higher, lower, or the same when comparing smart technology for self\u2010management to face\u2010to\u2010face/digital and/or written support for self\u2010management?", "answer": "higher", "evidence_quality": "low", "fulltext_required": "no", "comment": "", "relevant_sources": ["25811395", "27502583"]}, {"question_id": 74, "question": "Is activity level at 12 months higher, lower, or the same when comparing smart technology for self\u2010management to face\u2010to\u2010face/digital and/or written support for self\u2010management?", "answer": "no difference", "evidence_quality": "", "fulltext_required": "no", "comment": "", "relevant_sources": ["25811395"]}, {"question_id": 75, "question": "Is the risk of hospital admission by 12 months higher, lower, or the same when comparing smart technology for self\u2010management to face\u2010to\u2010face/digital and/or written support for self\u2010management?", "answer": "no difference", "evidence_quality": "low", "fulltext_required": "yes", "comment": "", "relevant_sources": ["27502583"]}, {"question_id": 76, "question": "Is smoking cessation higher, lower, or the same when comparing smart technology for self\u2010management to face\u2010to\u2010face/digital and/or written support for self\u2010management?", "answer": "no difference", "evidence_quality": "moderate", "fulltext_required": "no", "comment": "", "relevant_sources": ["26089656"]}], "sources": [{"article_id": "24293120", "content": "First, to investigate the effects of a telerehabilitation intervention on health status and activity level of patients with Chronic Obstructive Pulmonary Disease (COPD), compared to usual care. Second, to investigate how patients comply with the intervention and whether compliance is related to treatment outcomes.\na randomized controlled pilot trial\nThirty-four patients diagnosed with COPD.\nThe telerehabilitation application consists of an activity coach (3D-accelerometer with smartphone) for ambulant activity registration and real-time feedback, complemented by a web portal with a symptom diary for self-treatment of exacerbations. The intervention group used the application for 4 weeks. The control group received usual care.\nActivity level measured by a pedometer (in steps/day), health status by the Clinical COPD Questionnaire at baseline and after intervention. Compliance was expressed as the time the activity coach was worn.\nFourteen intervention and 16 control patients completed the study. Activity level (steps/day) was not significantly affected by the intervention over time. There was a non-significant difference in improvement in health status between the intervention (-0.34\u00b10.55) and control group (0.02\u00b10.57, p=0.10). Health status significantly improved within the intervention group (p=0.05). The activity coach was used more than prescribed (108%) and compliance was related to the increase in activity level for the first two feedback weeks (r=0.62, p=0.03).\nThis pilot study shows the potential of the telerehabilitation intervention: compliance with the activity coach was high, which directly related to an improvement in activity levels.", "title": "A telerehabilitation intervention for patients with Chronic Obstructive Pulmonary Disease: a randomized controlled pilot trial.", "date": "2013-12-03"}, {"article_id": "27502583", "content": "Regular physical activity (PA) is recommended for persons with chronic obstructive pulmonary disease (COPD). Interventions that promote PA and sustain long-term adherence to PA are needed.\nWe examined the effects of an Internet-mediated, pedometer-based walking intervention, called Taking Healthy Steps, at 12 months.\nVeterans with COPD (N=239) were randomized in a 2:1 ratio to the intervention or wait-list control. During the first 4 months, participants in the intervention group were instructed to wear the pedometer every day, upload daily step counts at least once a week, and were provided access to a website with four key components: individualized goal setting, iterative feedback, educational and motivational content, and an online community forum. The subsequent 8-month maintenance phase was the same except that participants no longer received new educational content. Participants randomized to the wait-list control group were instructed to wear the pedometer, but they did not receive step-count goals or instructions to increase PA. The primary outcome was health-related quality of life (HRQL) assessed by the St George's Respiratory Questionnaire Total Score (SGRQ-TS); the secondary outcome was daily step count. Linear mixed-effect models assessed the effect of intervention over time. One participant was excluded from the analysis because he was an outlier. Within the intervention group, we assessed pedometer adherence and website engagement by examining percent of days with valid step-count data, number of log-ins to the website each month, use of the online community forum, and responses to a structured survey.\nParticipants were 93.7% male (223/238) with a mean age of 67 (SD 9) years. At 12 months, there were no significant between-group differences in SGRQ-TS or daily step count. Between-group difference in daily step count was maximal and statistically significant at month 4 (P<.001), but approached zero in months 8-12. Within the intervention group, mean 76.7% (SD 29.5) of 366 days had valid step-count data, which decreased over the months of study (P<.001). Mean number of log-ins to the website each month also significantly decreased over the months of study (P<.001). The online community forum was used at least once during the study by 83.8% (129/154) of participants. Responses to questions assessing participants' goal commitment and intervention engagement were not significantly different at 12 months compared to 4 months.\nAn Internet-mediated, pedometer-based PA intervention, although efficacious at 4 months, does not maintain improvements in HRQL and daily step counts at 12 months. Waning pedometer adherence and website engagement by the intervention group were observed. Future efforts should focus on improving features of PA interventions to promote long-term behavior change and sustain engagement in PA.\nClinicaltrials.gov NCT01102777; https://clinicaltrials.gov/ct2/show/NCT01102777 (Archived by WebCite at http://www.webcitation.org/6iyNP9KUC).", "title": "Long-Term Effects of an Internet-Mediated Pedometer-Based Walking Program for Chronic Obstructive Pulmonary Disease: Randomized Controlled Trial.", "date": "2016-08-10"}, {"article_id": "24491137", "content": "Low levels of physical activity are common in patients with chronic obstructive pulmonary disease (COPD), and a sedentary lifestyle is associated with poor outcomes including increased mortality, frequent hospitalizations, and poor health-related quality of life. Internet-mediated physical activity interventions may increase physical activity and improve health outcomes in persons with COPD.\nThis manuscript describes the design and rationale of a randomized controlled trial that tests the effectiveness of Taking Healthy Steps, an Internet-mediated walking program for Veterans with COPD. Taking Healthy Steps includes an uploading pedometer, a website, and an online community. Eligible and consented patients wear a pedometer to obtain one week of baseline data and then are randomized on a 2:1 ratio to Taking Healthy Steps or to a wait list control. The intervention arm receives iterative step-count feedback; individualized step-count goals, motivational and informational messages, and access to an online community. Wait list controls are notified that they are enrolled, but that their intervention will start in one year; however, they keep the pedometer and have access to a static webpage.\nParticipants include 239 Veterans (mean age 66.7 years, 93.7% male) with 155 randomized to Taking Healthy Steps and 84 to the wait list control arm; rural-living (45.2%); ever-smokers (93.3%); and current smokers (25.1%). Baseline mean St. George's Respiratory Questionnaire Total Score was 46.0; 30.5% reported severe dyspnea; and the average number of comorbid conditions was 4.9. Mean baseline daily step counts was 3497 (+/- 2220).Veterans with COPD can be recruited to participate in an online walking program. We successfully recruited a cohort of older Veterans with a significant level of disability including Veterans who live in rural areas using a remote national recruitment strategy.\nClinical Trials.gov NCT01102777.", "title": "Taking Healthy Steps: rationale, design and baseline characteristics of a randomized trial of a pedometer-based Internet-mediated walking program in veterans with chronic obstructive pulmonary disease.", "date": "2014-02-05"}, {"article_id": "26089656", "content": "COPD is a leading cause of morbidity and mortality. Self-management interventions are considered important in order to limit the progression of the disease. Computer-tailored interventions could be an effective tool to facilitate self-management.\nThis randomized controlled trial tested the effectiveness of a web-based, computer-tailored COPD self-management intervention on physical activity and smoking behavior. Participants were recruited from an online panel and through primary care practices. Those at risk for or diagnosed with COPD, between 40 and 70 years of age, proficient in Dutch, with access to the Internet, and with basic computer skills (n=1,325), were randomly assigned to either the intervention group (n=662) or control group (n=663). The intervention group received the web-based self-management application, while the control group received no intervention. Participants were not blinded to group assignment. After 6 months, the effect of the intervention was assessed for the primary outcomes, smoking cessation and physical activity, by self-reported 7-day point prevalence abstinence and the International Physical Activity Questionnaire - Short Form.\nOf the 1,325 participants, 1,071 (80.8%) completed the 6-month follow-up questionnaire. No significant treatment effect was found on either outcome. The application however, was used by only 36% of the participants in the experimental group.\nA possible explanation for the nonsignificant effect on the primary outcomes, smoking cessation and physical activity, could be the low exposure to the application as engagement with the program has been shown to be crucial for the effectiveness of computer-tailored interventions. (Netherlands Trial Registry number: NTR3421.).", "title": "A randomized controlled trial evaluating the effectiveness of a web-based, computer-tailored self-management intervention for people with or at risk for COPD.", "date": "2015-06-20"}, {"article_id": "25811395", "content": "Low levels of physical activity (PA) are associated with poor outcomes in people with COPD. Interventions to increase PA could improve outcomes.\nWe tested the efficacy of a novel Internet-mediated, pedometer-based exercise intervention. Veterans with COPD (N = 239) were randomized in a 2:1 ratio to the (1) intervention group (Omron HJ-720 ITC pedometer and Internet-mediated program) or (2) wait-list control group (pedometer). The primary outcome was health-related quality of life (HRQL), assessed by the St. George's Respiratory Questionnaire (SGRQ), at 4 months. We examined the SGRQ total score (SGRQ-TS) and three domain scores: Symptoms, Activities, and Impact. The secondary outcome was daily step counts. Linear regression models assessed the effect of intervention on outcomes.\nParticipants had a mean age of 67 \u00b1 9 years, and 94% were men. There was no significant between-group difference in mean 4-month SGRQ-TS (2.3 units, P = .14). Nevertheless, a significantly greater proportion of intervention participants than control subjects had at least a 4-unit improvement in SGRQ-TS, the minimum clinically important difference (53% vs 39%, respectively, P = .05). For domain scores, the intervention group had a lower (reflecting better HRQL) mean than the control group by 4.6 units for Symptoms (P = .046) and by 3.3 units for Impact (P = .049). There was no significant difference in Activities score between the two groups. Compared with the control subjects, intervention participants walked 779 more steps per day at 4 months (P = .005).\nAn Internet-mediated, pedometer-based walking program can improve domains of HRQL and daily step counts at 4 months in people with COPD.\nClinical Trials.gov; No.: NCT01102777; URL: www.clinicaltrials.gov.", "title": "An Internet-Mediated Pedometer-Based Program Improves Health-Related Quality-of-Life Domains and Daily Step Counts in COPD: A Randomized Controlled Trial.", "date": "2015-03-27"}, {"article_id": "23742208", "content": "Chronic Obstructive Pulmonary Disease (COPD) is a major cause of morbidity and mortality. Effective self-management support interventions are needed to improve the health and functional status of people with COPD or at risk for COPD. Computer-tailored technology could be an effective way to provide this support.\nThis paper presents the protocol of a randomised controlled trial testing the effectiveness of a web-based, computer-tailored self-management intervention to change health behaviours of people with or at risk for COPD. An intervention group will be compared to a usual care control group, in which the intervention group will receive a web-based, computer-tailored self-management intervention. Participants will be recruited from an online panel and through general practices. Outcomes will be measured at baseline and at 6 months. The primary outcomes will be smoking behaviour, measuring the 7-day point prevalence abstinence and physical activity, measured in minutes. Secondary outcomes will include dyspnoea score, quality of life, stages of change, intention to change behaviour and alternative smoking behaviour measures, including current smoking behaviour, 24-hour point prevalence abstinence, prolonged abstinence, continued abstinence and number of quit attempts.\nTo the best of our knowledge, this will be the first randomised controlled trial to test the effectiveness of a web-based, computer-tailored self-management intervention for people with or at risk for COPD. The results will be important to explore the possible benefits of computer-tailored interventions for the self-management of people with or at risk for COPD and potentially other chronic health conditions.\nNTR3421.", "title": "A randomised controlled trial testing a web-based, computer-tailored self-management intervention for people with or at risk for chronic obstructive pulmonary disease: a study protocol.", "date": "2013-06-08"}]}
{"original_review": "31120132", "question_data": [{"question_id": 77, "question": "Is malaria parasite prevalence higher, lower, or the same when comparing non\u2010pyrethroid\u2010like indoor residual spraying (IRS) plus insecticide\u2010treated nets (ITNs) to insecticide\u2010treated nets (ITNs) alone?", "answer": "lower", "evidence_quality": "low", "fulltext_required": "yes", "comment": " 22682536/corbel is vague and has no fulltext but is included because it's relevant", "relevant_sources": ["29229808", "22682536", "24736370", "29655496", "22682536"]}, {"question_id": 78, "question": "Is malaria incidence higher, lower, or the same when comparing non\u2010pyrethroid\u2010like indoor residual spraying (IRS) plus insecticide\u2010treated nets (ITNs) to insecticide\u2010treated nets (ITNs) alone?", "answer": "uncertain effect", "evidence_quality": "very low", "fulltext_required": "no", "comment": "uncertainty due to wide CIs and for significant inconsistencies between trials", "relevant_sources": ["29229808", "22682536"]}, {"question_id": 79, "question": "Is anaemia prevalence higher, lower, or the same when comparing non\u2010pyrethroid\u2010like indoor residual spraying (IRS) plus insecticide\u2010treated nets (ITNs) to insecticide\u2010treated nets (ITNs) alone?", "answer": "lower", "evidence_quality": "low", "fulltext_required": "yes", "comment": "", "relevant_sources": ["24736370", "29655496"]}, {"question_id": 80, "question": "Is malaria incidence higher, lower, or the same when comparing pyrethroid\u2010like indoor residual spraying (IRS) plus insecticide\u2010treated nets (ITNs) to insecticide\u2010treated nets (ITNs) alone?", "answer": "no difference", "evidence_quality": "moderate", "fulltext_required": "no", "comment": "", "relevant_sources": ["25498847", "29229808"]}, {"question_id": 81, "question": "Is malaria parasite prevalence higher, lower, or the same when comparing pyrethroid\u2010like indoor residual spraying (IRS) plus insecticide\u2010treated nets (ITNs) to insecticide\u2010treated nets (ITNs) alone?", "answer": "no difference", "evidence_quality": "moderate", "fulltext_required": "no", "comment": "Kafy/29229808 requires fulltext for this question but weight is <10%", "relevant_sources": ["25498847", "29229808", "21565149"]}], "sources": [{"article_id": "29655496", "content": "Progress in malaria control is under threat by wide-scale insecticide resistance in malaria vectors. Two recent vector control products have been developed: a long-lasting insecticidal net that incorporates a synergist piperonyl butoxide (PBO) and a long-lasting indoor residual spraying formulation of the insecticide pirimiphos-methyl. We evaluated the effectiveness of PBO long-lasting insecticidal nets versus standard long-lasting insecticidal nets as single interventions and in combination with the indoor residual spraying of pirimiphos-methyl.\nWe did a four-group cluster randomised controlled trial using a two-by-two factorial design of 48 clusters derived from 40 villages in Muleba (Kagera, Tanzania). We randomly assigned these clusters using restricted randomisation to four groups: standard long-lasting insecticidal nets, PBO long-lasting insecticidal nets, standard long-lasting insecticidal nets plus indoor residual spraying, or PBO long-lasting insecticidal nets plus indoor residual spraying. Both standard and PBO nets were distributed in 2015. Indoor residual spraying was applied only once in 2015. We masked the inhabitants of each cluster to the type of nets received, as well as field staff who took blood samples. Neither the investigators nor the participants were masked to indoor residual spraying. The primary outcome was the prevalence of malaria infection in children aged 6 months to 14 years assessed by cross-sectional surveys at 4, 9, 16, and 21 months after intervention. The endpoint for assessment of indoor residual spraying was 9 months and PBO long-lasting insecticidal nets was 21 months. This trial is registered with ClinicalTrials.gov, number NCT02288637.\n7184 (68\u00b70%) of 10\u2008560 households were selected for post-intervention survey, and 15\u2008469 (89\u00b70%) of 17\u2008377 eligible children from the four surveys were included in the intention-to-treat analysis. Of the 878 households visited in the two indoor residual spraying groups, 827 (94%) had been sprayed. Reported use of long-lasting insecticidal nets, across all groups, was 15\u2008341 (77\u00b73%) of 19\u2008852 residents after 1 year, decreasing to 12\u2008503 (59\u00b72%) of 21\u2008105 in the second year. Malaria infection prevalence after 9 months was lower in the two groups that received PBO long-lasting insecticidal nets than in the two groups that received standard long-lasting insecticidal nets (531 [29%] of 1852 children vs 767 [42%] of 1809; odds ratio [OR] 0\u00b737, 95% CI 0\u00b721-0\u00b765; p=0\u00b70011). At the same timepoint, malaria prevalence in the two groups that received indoor residual spraying was lower than in groups that did not receive indoor residual spraying (508 [28%] of 1846 children vs 790 [44%] of 1815; OR 0\u00b733, 95% CI 0\u00b719-0\u00b755; p<0\u00b70001) and there was evidence of an interaction between PBO long-lasting insecticidal nets and indoor residual spraying (OR 2\u00b743, 95% CI 1\u00b719-4\u00b797; p=0\u00b70158), indicating redundancy when combined. The PBO long-lasting insecticidal net effect was sustained after 21 months with a lower malaria prevalence than the standard long-lasting insecticidal net (865 [45%] of 1930 children vs 1255 [62%] of 2034; OR 0\u00b740, 0\u00b720-0\u00b781; p=0\u00b70122).\nThe PBO long-lasting insecticidal net and non-pyrethroid indoor residual spraying interventions showed improved control of malaria transmission compared with standard long-lasting insecticidal nets where pyrethroid resistance is prevalent and either intervention could be deployed to good effect. As a result, WHO has since recommended to increase coverage of PBO long-lasting insecticidal nets. Combining indoor residual spraying with pirimiphos-methyl and PBO long-lasting insecticidal nets provided no additional benefit compared with PBO long-lasting insecticidal nets alone or standard long-lasting insecticidal nets plus indoor residual spraying.\nUK Department for International Development, Medical Research Council, and Wellcome Trust.", "title": "Effectiveness of a long-lasting piperonyl butoxide-treated insecticidal net and indoor residual spray interventions, separately and together, against malaria transmitted by pyrethroid-resistant mosquitoes: a cluster, randomised controlled, two-by-two factorial design trial.", "date": "2018-04-16"}, {"article_id": "21565149", "content": "This paper examines the relationship between indoor residual spray (IRS) and malaria parasite infection in Gash Barka Zone, Eritrea, an area with near universal coverage of insecticide treated bednets (ITN) and already low malaria parasite prevalence. A community randomized control trial was conducted in 2009. Malaria parasite infection prevalence was 0.5% [95% confidence interval (CI): 0.37-0.78%], with no significant difference detected between treatment and control areas. ITN possession remains high, with over 70% of households reporting ITN ownership [95% CI: 68.4-72.9]. ITN use among individuals within ITN-owning households was just under half [46.7% (95% CI: 45.4-48.0)]. Slight differences in ITN possession and use were detected between treatment and control areas. There was no significant difference in malaria parasite infection prevalence among individuals in households with \u22651 ITN compared to those in households without ITNs, nor among individuals reporting ITN use. Among individuals in ITN-owning households, sleeping under an ITN offered no statistically significant protection from malaria parasite infection. Community participation in environmental and larval habitat management activities was low: 17.9% (95% CI: 16.0-19.7). It is likely that IRS, larval habitat management and ITN distribution alone may be insufficient to interrupt transmission without corresponding high ITN use, sustained IRS application in areas where infections are clustered, and promptly seeking laboratory diagnosis and treatment of all fevers. Eritrea is ready for elimination, irrespective of inconclusive impact evaluation results.", "title": "Evaluating indoor residual spray for reducing malaria infection prevalence in Eritrea: results from a community randomized control trial.", "date": "2011-05-14"}, {"article_id": "29229808", "content": "Insecticide-based interventions have contributed to \u223c78% of the reduction in the malaria burden in sub-Saharan Africa since 2000. Insecticide resistance in malaria vectors could presage a catastrophic rebound in disease incidence and mortality. A major impediment to the implementation of insecticide resistance management strategies is that evidence of the impact of resistance on malaria disease burden is limited. A cluster randomized trial was conducted in Sudan with pyrethroid-resistant and carbamate-susceptible malaria vectors. Clusters were randomly allocated to receive either long-lasting insecticidal nets (LLINs) alone or LLINs in combination with indoor residual spraying (IRS) with a pyrethroid (deltamethrin) insecticide in the first year and a carbamate (bendiocarb) insecticide in the two subsequent years. Malaria incidence was monitored for 3 y through active case detection in cohorts of children aged 1 to <10 y. When deltamethrin was used for IRS, incidence rates in the LLIN + IRS arm and the LLIN-only arm were similar, with the IRS providing no additional protection [incidence rate ratio (IRR) = 1.0 (95% confidence interval [CI]: 0.36-3.0; ", "title": "Impact of insecticide resistance in ", "date": "2017-12-13"}, {"article_id": "22682536", "content": "Malaria control efforts and elimination in Africa are being challenged by the development of resistance of parasites to antimalarial drugs and vectors to insecticides. We investigated whether the combination of long-lasting insecticidal mosquito nets (LLINs) with indoor residual spraying (IRS) or carbamate-treated plastic sheeting (CTPS) conferred enhanced protection against malaria and better management of pyrethroid-resistance in vectors than did LLINs alone.\nWe did a cluster randomised controlled trial in 28 villages in southern Benin, west Africa. Inclusion criteria of the villages were moderate level of pyrethroid resistance in malaria vectors and minimum distance between villages of 2 km. We assessed four malaria vector control interventions: LLIN targeted coverage to pregnant women and children younger than 6 years (TLLIN, reference group), LLIN universal coverage of all sleeping units (ULLIN), TLLIN plus full coverage of carbamate-IRS applied every 8 months (TLLIN+IRS), and ULLIN plus full coverage of CTPS lined up to the upper part of the household walls (ULLIN+CTPS). The interventions were allocated to villages by a block randomisation on the basis of preliminary surveys and children of each village were randomly selected to participate with computer-generated numbers. The primary endpoint was the incidence density rate of Plasmodium falciparum clinical malaria in children younger than 6 years as was analysed by Poisson regression taking into account the effect of age and the sampling design with a generalised estimating equation approach. Clinical and parasitological information were obtained by active case detection of malaria episodes during 12 periods of 6 consecutive days scheduled at six weekly intervals and by cross-sectional surveys of asymptomatic plasmodial infections. Children or study investigators were not masked to study group. This study is registered with Current Controlled Trials, number ISRCTN07404145.\nOf 58 villages assessed, 28 were randomly assigned to intervention groups. 413-429 children were followed up in each intervention group for 18 months. The clinical incidence density of malaria was not reduced in the children from the ULLIN group (incidence density rate 0\u00b795, 95% CI 0\u00b767-1\u00b736, p=0\u00b779), nor in those from the TLLIN+IRS group (1\u00b732, 0\u00b790-1\u00b793, p=0\u00b715) or from the ULLIN+CTPS group (1\u00b705, 0\u00b775-1\u00b748, p=0\u00b777) compared with the reference group (TLLIN). The same trend was observed with the prevalence and parasite density of asymptomatic infections (non significant regression coefficients).\nNo significant benefit for reducing malaria morbidity, infection, and transmission was reported when combining LLIN+IRS or LLIN+CTPS compared with a background of LLIN coverage. These findings are important for national malaria control programmes and should help the design of more cost-effective strategies for malaria control and elimination.\nMinist\u00e8re Fran\u00e7ais des Affaires Etrang\u00e8res et Europ\u00e9ennes (FSP project 2006-22), Institut de Recherche pour le D\u00e9veloppement, President's Malaria Initiative (PMI) of US Governement.", "title": "Combination of malaria vector control interventions in pyrethroid resistance area in Benin: a cluster randomised controlled trial.", "date": "2012-06-12"}, {"article_id": "25498847", "content": "Although many malaria control programmes in sub-Saharan Africa use indoor residual spraying with long-lasting insecticidal nets (LLINs), the two studies assessing the benefit of the combination of these two interventions gave conflicting results. We aimed to assess whether the addition of indoor residual spraying to LLINs provided a significantly different level of protection against clinical malaria in children or against house entry by vector mosquitoes.\nIn this two-arm cluster, randomised, controlled efficacy trial we randomly allocated clusters of Gambian villages using a computerised algorithm to LLINs alone (n=35) or indoor residual spraying with dichlorodiphenyltrichloroethane plus LLINs (n=35). In each cluster, 65-213 children, aged 6 months to 14 years, were surveyed at the start of the 2010 transmission season and followed in 2010 and 2011 by passive case detection for clinical malaria. Exposure to parasite transmission was assessed by collection of vector mosquitoes with both light and exit traps indoors. Primary endpoints were the incidence of clinical malaria assessed by passive case detection and number of Anopheles gambiae sensu lato mosquitoes collected per light trap per night. Intervention teams had no role in data collection and the data collection teams were not informed of the spray status of villages. The trial is registered at the ISRCTN registry, number ISRCTN01738840.\nLLIN coverage in 2011 was 3510 (93%) of 3777 children in the indoor residual spraying plus LLIN group and 3622 (95.5%) of 3791 in the LLIN group. In 2010, 7845 children were enrolled, 7829 completed passive case detection, and 7697 (98%) had complete clinical and covariate data. In 2011, 7009 children remained in the study, 648 more were enrolled, 7657 completed passive case detection, and 7545 (98.5%) had complete data. Indoor residual spraying coverage per cluster was more than 80% for both years in the indoor residual spraying plus LLIN group. Incidence of clinical malaria was 0.047 per child-month at risk in the LLIN group and 0.044 per child-month at risk in the indoor residual spraying plus LLIN group in 2010, and 0.032 per child-month at risk in the LLIN group and 0.034 per child-month at risk in the indoor residual spraying plus LLIN group in 2011. The incident rate ratio was 1.08 (95% CI 0.80-1.46) controlling for confounders and cluster by mixed-effect negative binomial regression on all malaria attacks for both years. No significant difference was recorded in the density of vector mosquitoes caught in light traps in houses over the two transmission seasons; the mean number of A gambiae sensu lato mosquitoes per trap per night was 6.7 (4.0-10.1) in the LLIN group and 4.5 (2.4-7.4) in the indoor residual spraying plus LLIN group (p=0.281 in the random-effects linear regression model).\nWe identified no significant difference in clinical malaria or vector density between study groups. In this area with high LLIN coverage, moderate seasonal transmission, and susceptible vectors, indoor residual spraying did not provide additional benefit.\nUK Medical Research Council.", "title": "Efficacy of indoor residual spraying with dichlorodiphenyltrichloroethane against malaria in Gambian communities with high usage of long-lasting insecticidal mosquito nets: a cluster-randomised controlled trial.", "date": "2014-12-17"}, {"article_id": "24736370", "content": "Insecticide-treated nets (ITNs) and indoor residual spraying (IRS) of houses provide effective malaria transmission control. There is conflicting evidence about whether it is more beneficial to provide both interventions in combination. A cluster randomised controlled trial was conducted to investigate whether the combination provides added protection compared to ITNs alone.\nIn northwest Tanzania, 50 clusters (village areas) were randomly allocated to ITNs only or ITNs and IRS. Dwellings in the ITN+IRS arm were sprayed with two rounds of bendiocarb in 2012. Plasmodium falciparum prevalence rate (PfPR) in children 0.5-14 y old (primary outcome) and anaemia in children <5 y old (secondary outcome) were compared between study arms using three cross-sectional household surveys in 2012. Entomological inoculation rate (secondary outcome) was compared between study arms. IRS coverage was approximately 90%. ITN use ranged from 36% to 50%. In intention-to-treat analysis, mean PfPR was 13% in the ITN+IRS arm and 26% in the ITN only arm, odds ratio\u200a=\u200a0.43 (95% CI 0.19-0.97, n\u200a=\u200a13,146). The strongest effect was observed in the peak transmission season, 6 mo after the first IRS. Subgroup analysis showed that ITN users were additionally protected if their houses were sprayed. Mean monthly entomological inoculation rate was non-significantly lower in the ITN+IRS arm than in the ITN only arm, rate ratio\u200a=\u200a0.17 (95% CI 0.03-1.08).\nThis is the first randomised trial to our knowledge that reports significant added protection from combining IRS and ITNs compared to ITNs alone. The effect is likely to be attributable to IRS providing added protection to ITN users as well as compensating for inadequate ITN use. Policy makers should consider deploying IRS in combination with ITNs to control transmission if local ITN strategies on their own are insufficiently effective. Given the uncertain generalisability of these findings, it would be prudent for malaria control programmes to evaluate the cost-effectiveness of deploying the combination.\nwww.ClinicalTrials.gov NCT01697852 Please see later in the article for the Editors' Summary.", "title": "Indoor residual spraying in combination with insecticide-treated nets compared to insecticide-treated nets alone for protection against malaria: a cluster randomised trial in Tanzania.", "date": "2014-04-17"}]}
{"original_review": "31829446", "question_data": [{"question_id": 82, "question": "Is the rate of major complications higher, lower, or the same when comparing restrictive fluid therapy (RFT) to goal\u2010directed fluid therapy (GDFT)?", "answer": "uncertain effect", "evidence_quality": "very low", "fulltext_required": "yes", "comment": "23132508/Srinivasa is a bit vague but included for context", "relevant_sources": ["26471495", "22710266", "25595308", "25342408", "23132508"]}, {"question_id": 83, "question": "Is the length of hospital stay higher, lower, or the same when comparing restrictive fluid therapy (RFT) to goal\u2010directed fluid therapy (GDFT)?", "answer": "no difference", "evidence_quality": "very low", "fulltext_required": "yes", "comment": "", "relevant_sources": ["23070341", "26471495", "22710266", "25342408", "23132508"]}], "sources": [{"article_id": "22710266", "content": "We aimed to investigate whether fluid therapy with a goal of near-maximal stroke volume (SV) guided by oesophageal Doppler (ED) monitoring result in a better outcome than that with a goal of maintaining bodyweight (BW) and zero fluid balance in patients undergoing colorectal surgery.\nIn a double-blinded clinical multicentre trial, 150 patients undergoing elective colorectal surgery were randomized to receive fluid therapy after either the goal of near-maximal SV guided by ED (Doppler, D group) or the goal of zero balance and normal BW (Zero balance, Z group). Stratification for laparoscopic and open surgery was performed. The postoperative fluid therapy was similar in the two groups. The primary endpoint was postoperative complications defined and divided into subgroups by protocol. Analysis was performed by intention-to-treat. The follow-up was 30 days. The trial had 85% power to show a difference between the groups.\nThe number of patients undergoing laparoscopic or open surgery and the patient characteristics were similar between the groups. No significant differences between the groups were found for overall, major, minor, cardiopulmonary, or tissue-healing complications (P-values: 0.79; 0.62; 0.97; 0.48; and 0.48, respectively). One patient died in each group. No significant difference was found for the length of hospital stay [median (range) Z: 5.00 (1-61) vs D: 5.00 (2-41); P=0.206].\nGoal-directed fluid therapy to near-maximal SV guided by ED adds no extra value to the fluid therapy using zero balance and normal BW in patients undergoing elective colorectal surgery.", "title": "Which goal for fluid therapy during colorectal surgery is followed by the best outcome: near-maximal stroke volume or zero fluid balance?", "date": "2012-06-20"}, {"article_id": "23070341", "content": "The optimal strategy for fluid management during gastrointestinal surgery remains unclear. Minimizing the variation in arterial pulse pressure, which is induced by mechanical ventilation, is a potential strategy to improve postoperative outcomes. We tested this hypothesis in a prospective, randomized study with lactated Ringer's solution and 6% hydroxyethyl starch solution.\nA total of 60 patients who were undergoing gastrointestinal surgery were randomized into a restrictive lactated Ringer's group (n = 20), a goal-directed lactated Ringer's group (n = 20) and a goal-directed hydroxyethyl starch group (n = 20). The goal-directed fluid treatment was guided by pulse pressure variation, which was recorded during surgery using a simple manual method with a Datex Ohmeda S/5 Monitor and minimized to 11% or less by volume loading with either lactated Ringer's solution or 6% hydroxyethyl starch solution (130/0.4). The postoperative flatus time, the length of hospital stay and the incidence of complications were recorded as endpoints.\nThe goal-directed lactated Ringer's group received the greatest amount of total operative fluid compared with the two other groups. The flatus time and the length of hospital stay in the goal-directed hydroxyethyl starch group were shorter than those in the goal-directed lactated Ringer's group and the restrictive lactated Ringer's group. No significant differences were found in the postoperative complications among the three groups.\nMonitoring and minimizing pulse pressure variation by 6% hydroxyethyl starch solution (130/0.4) loading during gastrointestinal surgery improves postoperative outcomes and decreases the discharge time of patients who are graded American Society of Anesthesiologists physical status I/II.", "title": "Intraoperative fluid management in open gastrointestinal surgery: goal-directed versus restrictive.", "date": "2012-10-17"}, {"article_id": "25342408", "content": "There is continued controversy regarding the benefits of goal-directed fluid therapy, with earlier studies showing marked improvement in morbidity and length-of-stay that have not been replicated more recently. The aim of this study was to compare patient outcomes in elective colorectal surgery patients having goal-directed versus restrictive fluid therapy. Inclusion criteria included suitability for an Enhanced Recovery After Surgery care pathway and patients with an American Society of Anesthesiologists Physical Status score of 1 to 3. Patients were intraoperatively randomised to either restrictive or Doppler-guided goal-directed fluid therapy. The primary outcome was length-of-stay; secondary outcomes included complication rate, change in haemodynamic variables and fluid volumes. Compared to restrictive therapy, goal-directed therapy resulted in a greater volume of intraoperative fluid, 2115 (interquartile range 1350 to 2560) ml versus 1500 (1200 to 2000) ml, P=0.008, and was associated with an increase in Doppler-derived stroke volume index from beginning to end of surgery, 43.7 (16.3) to 54.2 (21.1) ml/m(2), P <0.001, in the latter group. Length-of-stay was similar, 6.5 (5 to 9) versus 6 (4 to 9) days, P=0.421. The number of patients with any complication (minor or major) was similar; 0% (30) versus 52% (26), P=0.42, or major complications, 1 (2%) versus 4 (8%), P=0.36, respectively. The increased perioperative fluid volumes and increased stroke volumes at the end of surgery in patients receiving goal-directed therapy did not translate to a significant difference in length-of-stay and we did not observe a difference in the number of patients experiencing minor or major complications.", "title": "A randomised controlled trial of fluid restriction compared to oesophageal Doppler-guided goal-directed fluid therapy in elective major colorectal surgery within an Enhanced Recovery After Surgery program.", "date": "2014-10-25"}, {"article_id": "23132508", "content": "Goal-directed fluid therapy (GDFT) has been compared with liberal fluid administration in non-optimized perioperative settings. It is not known whether GDFT is of value within an enhanced recovery protocol incorporating fluid restriction. This study evaluated GDFT under these circumstances in patients undergoing elective colectomy.\nPatients undergoing elective laparoscopic or open colectomy within an established enhanced recovery protocol (including fluid restriction) were randomized to GDFT or no GDFT. Bowel preparation was permitted for left colonic operations at the surgeon's discretion. Exclusion criteria included rectal tumours and stoma formation. The primary outcome was a patient-reported surgical recovery score (SRS). Secondary endpoints included clinical outcomes and physiological measures of recovery.\nEighty-five patients were randomized, and there were 37 patients in each group for analysis. Nine patients in the GDFT and four in the fluid restriction group received oral bowel preparation for either anterior resection (12) or subtotal colectomy (1). Patients in the GDFT group received more colloid during surgery (mean 591 versus 297 ml; P = 0\u00b7012) and had superior cardiac indices (mean corrected flow time 374 versus 355 ms; P = 0\u00b7018). However, no differences were observed between the GDFT and fluid restriction groups with regard to surgical recovery (mean SRS after 7 days 47 versus 46 respectively; P = 0\u00b7853), other secondary outcomes (mean aldosterone/renin ratio 9 versus 8; P = 0\u00b7898), total postoperative fluid (median 3750 versus 2400 ml; P = 0\u00b7604), length of hospital stay (median 6 versus 5 days; P = 0\u00b7570) or number of patients with complications (26 versus 27; P = 1\u00b7000).\nGDFT did not provide clinical benefit in patients undergoing elective colectomy within a protocol incorporating fluid restriction.\nNCT00911391 (http://www.clinicaltrials.gov).", "title": "Randomized clinical trial of goal-directed fluid therapy within an enhanced recovery protocol for elective colectomy.", "date": "2012-11-08"}, {"article_id": "25595308", "content": "The use of adequate fluid therapy during cytoreductive surgery (CRS) and hyperthermic intraperitoneal chemotherapy (HIPEC) remains controversial. The aim of the study was to assess whether the use of fluid therapy protocol combined with goal-directed therapy (GDT) is associated with a significant change in morbidity, length of hospital stay, and mortality compared to standard fluid therapy. Patients American Society of Anesthesiologists (ASA) II-III undergoing CRS and HIPEC were randomized into two groups. The GDT group (N\u2009=\u200938) received fluid therapy according to a protocol guided by monitored hemodynamic parameters. The control group (N\u2009=\u200942) received standard fluid therapy. We evaluated incidence of major complications, total length of hospital stay, total amount of fluids administered, and mortality rate. The incidence of major abdominal complications was 10.5% in GDT group and 38.1% in the control group (P\u2009=\u20090.005). The median duration of hospitalization was 19 days in GDT group and 29 days in the control group (P\u2009<\u20090.0001). The mortality rate was zero in GDT group vs. 9.5% in the control group (P\u2009=\u20090.12). GDT group received a significantly (P\u2009<\u20090.0001) lower amount of fluid (5812\u2009\u00b1\u20091244 ml) than the control group (8269\u2009\u00b1\u20091452 ml), with a significantly (P\u2009<\u20090.0001) lower volume of crystalloids (3884\u2009\u00b1\u20091003 vs. 68,528\u2009\u00b1\u20091413 ml). In CRS and HIPEC, the use of a GDT improves outcome in terms of incidence of major abdominal and systemic postoperative complications and length of hospital stay, compared to standard fluid therapy protocol.", "title": "A randomized trial of goal directed vs. standard fluid therapy in cytoreductive surgery with hyperthermic intraperitoneal chemotherapy.", "date": "2015-01-18"}, {"article_id": "26471495", "content": "The use of goal directed fluid protocols in intermediate risk patients undergoing hip or knee replacement was studied in few trials using invasive monitoring. For this reason we have implemented two different fluid management protocols, both based on a novel totally non-invasive arterial pressure monitoring device and compared them to the standard (no-protocol) treatment applied before the transition in our academic institution.\nThree treatment groups were compared in this prospective study: the observational (CONTROL, N = 40) group before adoption of fluid protocols and two randomized groups after the transition to protocol fluid management with the use of the continuous non-invasive blood pressure monitoring (CNAP\u00ae) device. In the PRESSURE group (N = 40) standard variables were used for restrictive fluid therapy. Goal directed fluid therapy using pulse pressure variation was used in the GDFT arm (N = 40). The influence on the rate of postoperative complications, on the hospital length of stay and other parameters was assessed.\nBoth protocols were associated with decreased fluid administration and maintained hemodynamic stability. Reduced rate of postoperative infection and organ complications (22 (55 %) vs. 33 (83 %) patients; p = 0.016; relative risk 0.67 (0.49-0.91)) was observed in the GDFT group compared to CONTROL. Lower number of patients receiving transfusion (4 (10 %) in GDFT vs. 17 (43 %) in CONTROL; p = 0.005) might contribute to this observation. No significant differences were observed in other end-points.\nIn our study, the use of the fluid protocol based on pulse pressure variation assessed using continuous non-invasive arterial pressure measurement seems to be associated with a reduction in postoperative complications and transfusion needs as compared to standard no-protocol treatment.\nACTRN12612001014842.", "title": "Fluid management guided by a continuous non-invasive arterial pressure device is associated with decreased postoperative morbidity after total knee and hip replacement.", "date": "2015-10-17"}]}
{"original_review": "26904970", "question_data": [{"question_id": 84, "question": "Is survival to hospital discharge higher, lower, or the same when comparing biphasic waveform defibrillation to monophasic waveform defibrillation?", "answer": "no difference", "evidence_quality": "", "fulltext_required": "no", "comment": "", "relevant_sources": ["11719116", "17060379", "12867305", "15992986"]}, {"question_id": 85, "question": "Is survival to hospital admission higher, lower, or the same when comparing biphasic waveform defibrillation to monophasic waveform defibrillation?", "answer": "no difference", "evidence_quality": "", "fulltext_required": "no", "comment": "van Alem/12867305 omitted because it does not specify hospital admission rate but has only 13% weight", "relevant_sources": ["11719116", "17060379"]}], "sources": [{"article_id": "12867305", "content": "Evidence suggests that biphasic waveforms are more effective than monophasic waveforms for defibrillation in out-of-hospital cardiac arrest (OHCA), yet their performance has only been compared in un-blinded studies.\nWe compared the success of biphasic truncated exponential (BTE) and monophasic damped sine (MDS) shocks for defibrillation in OHCA in a prospective, randomised, double blind clinical trial. First responders were equipped with MDS and BTE automated external defibrillators (AEDs) in a random fashion. Patients in ventricular fibrillation (VF) received BTE or MDS first shocks of 200 J. The ECG was recorded for subsequent analysis continuously. The success of the first shock as a primary endpoint was removal of VF and required a return of an organized rhythm for at least two QRS complexes, with an interval of <5 s, within 1 min after the first shock. The secondary endpoint was termination of VF at 5 s. VF was the initial recorded rhythm in 120 patients in OHCA, 51 patients received BTE and 69 received MDS shocks. The success rate of 200 J first shocks was significantly higher for BTE than for MDS shocks, 35/51 (69%) and 31/69 (45%), P=0.01. In a logistic regression model the odds ratio of success for a BTE shock was 4.01 (95% CI 1.01-10.0), adjusted for baseline cardiopulmonary resuscitation, VF-amplitude and time between collapse and first shock. No difference was found with respect to the secondary endpoint, termination of VF at 5 s (RR 1.07 95% CI: 0.99-1.11) and with respect to survival to hospital discharge (RR 0.73 95% CI: 0.31-1.70).\nBTE-waveform AEDs provide significantly higher rates of successful defibrillation with return of an organized rhythm in OHCA than MDS waveform AEDs.", "title": "A prospective, randomised and blinded comparison of first shock success of monophasic and biphasic waveforms in out-of-hospital cardiac arrest.", "date": "2003-07-18"}, {"article_id": "11719116", "content": "Advances in early defibrillation access, key to the \"Chain of Survival\", will depend on innovations in defibrillation waveforms, because of their impact on device size and weight. This study compared standard monophasic waveform automatic external defibrillators (AEDs) to an innovative biphasic waveform AED.\nImpedance-compensated biphasic truncated exponential (ICBTE) and either monophasic truncated exponential (MTE) or monophasic damped sine (MDS) AEDs were prospectively, randomly assigned by date in four emergency medical services. The study design compared ICBTE with MTE and MDS combined. This subset analysis distinguishes between the two classes of monophasic waveform, MTE and MDS, and compares their performance to each other and to the biphasic waveform, contingent on significant overall effects (ICBTE vs. MTE vs. MDS). Primary endpoint: Defibrillation efficacy with < or =3 shocks. Secondary endpoints: shock efficacy with < or =1 shock, < or =2 shocks, and survival to hospital admission and discharge. Observations included return of spontaneous circulation (ROSC), refibrillation, and time to first shock and to first successful shock.\nOf 338 out-of-hospital cardiac arrests, 115 had a cardiac aetiology, presented with ventricular fibrillation, and were shocked by an AED. Defibrillation efficacy for the first \"stack\" of up to 3 shocks, for up to 2 shocks and for the first shock alone was superior for the ICBTE waveform than for either the MTE or the MDS waveform, while there was no difference between the efficacy of MTE and MDS. Time from the beginning of analysis by the AED to the first shock and to the first successful shock was also superior for the ICBTE devices compared to either the MTE or the MDS devices, while again there was no difference between the MTE and MDS devices. More ICBTE patients achieved ROSC pre-hospital than did MTE patients. While the rates of ROSC were identical for MTE and MDS patients, the difference between ICBTE and MDS was not significant. Rates of refibrillation and survival to hospital admission and discharge did not differ among the three populations.\nICBTE was superior to MTE and MDS in defibrillation efficacy and speed and to MTE in ROSC. MTE and MDS did not differ in efficacy. There were no differences among the waveforms in refibrillation or survival.", "title": "Optimal Response to Cardiac Arrest study: defibrillation waveform effects.", "date": "2001-11-24"}, {"article_id": "17060379", "content": "Although biphasic, as compared with monophasic, waveform defibrillation for cardiac arrest is increasing in use and popularity, whether it is truly a more lifesaving waveform is unproven.\nConsecutive adults with nontraumatic out-of-hospital ventricular fibrillation cardiac arrest were randomly allocated to defibrillation according to the waveform from automated external defibrillators administered by prehospital medical providers. The primary event of interest was admission alive to the hospital. Secondary events included return of rhythm and circulation, survival, and neurological outcome. Providers were blinded to automated defibrillator waveform. Of 168 randomized patients, 80 (48%) and 68 (40%) consistently received only monophasic or biphasic waveform shocks, respectively, throughout resuscitation. The prevalence of ventricular fibrillation, asystole, or organized rhythms at 5, 10, or 20 seconds after each shock did not differ significantly between treatment groups. The proportion of patients admitted alive to the hospital was relatively high: 73% in monophasic and 76% in biphasic treatment groups (P=0.58). Several favorable trends were consistently associated with receipt of biphasic waveform shock, none of which reached statistical significance. Notably, 27 of 80 monophasic shock recipients (34%), compared with 28 of 68 biphasic shock recipients (41%), survived (P=0.35). Neurological outcome was similar in both treatment groups (P=0.4). Earlier administration of shock did not significantly alter the performance of one waveform relative to the other, nor did shock waveform predict any clinical outcome after multivariate adjustment.\nNo statistically significant differences in outcome could be ascribed to use of one waveform over another when out-of-hospital ventricular fibrillation was treated.", "title": "Transthoracic incremental monophasic versus biphasic defibrillation by emergency responders (TIMBER): a randomized comparison of monophasic with biphasic waveform ascending energy defibrillation for the resuscitation of out-of-hospital cardiac arrest due to ventricular fibrillation.", "date": "2006-10-25"}, {"article_id": "11023932", "content": "In the present study, we compared an automatic external defibrillator (AED) that delivers 150-J biphasic shocks with traditional high-energy (200- to 360-J) monophasic AEDs.\nAEDs were prospectively randomized according to defibrillation waveform on a daily basis in 4 emergency medical services systems. Defibrillation efficacy, survival to hospital admission and discharge, return of spontaneous circulation, and neurological status at discharge (cerebral performance category) were compared. Of 338 patients with out-of-hospital cardiac arrest, 115 had a cardiac etiology, presented with ventricular fibrillation, and were shocked with an AED. The time from the emergency call to the first shock was 8.9+/-3.0 (mean+/-SD) minutes.\nThe 150-J biphasic waveform defibrillated at higher rates, resulting in more patients who achieved a return of spontaneous circulation. Although survival rates to hospital admission and discharge did not differ, discharged patients who had been resuscitated with biphasic shocks were more likely to have good cerebral performance.", "title": "Multicenter, randomized, controlled trial of 150-J biphasic shocks compared with 200- to 360-J monophasic shocks in the resuscitation of out-of-hospital cardiac arrest victims. Optimized Response to Cardiac Arrest (ORCA) Investigators.", "date": "2000-10-12"}, {"article_id": "15992986", "content": "Although biphasic defibrillation waveforms appear to be superior to monophasic waveforms in terminating VF, their relative benefits in out-of-hospital resuscitation are incompletely understood. Prior comparisons of defibrillation waveform efficacy in out-of-hospital cardiac arrest (OHCA) are confined to patients presenting in a shockable rhythm and resuscitated by first responder (basic life support). This effectiveness study compared monophasic and biphasic defibrillation waveform for conversion of ventricular arrhythmias in all OHCA treated with advance life support (ALS).\nThis prospective randomized controlled trial compared the rectilinear biphasic (RLB) waveform with the monophasic damped sine (MDS) waveform, using step-up energy levels. The study enrolled OHCA patients requiring at least one shock delivered by ALS providers, regardless of initial presenting rhythm. Shock success was defined as conversion at 5s to organized rhythm after one to three escalating shocks. We report efficacy results for the cohort of patients treated by ALS paramedics who presented with an initially shockable rhythm who had not received a shock from a first responder (MDS: n=83; RLB: n=86). Shock success within the first three ascending energy shocks for RLB (120, 150, 200J) was superior to MDS (200, 300, 360J) for patients initially presenting in a shockable rhythm (52% versus 34%, p=0.01). First shock conversion was 23% and12%, for RLB and MDS, respectively (p=0.07). There were no significant differences in return of spontaneous circulation (47% versus 47%), survival to 24h (31% versus 27%), and survival to discharge (9% versus 7%). Mean 24h survival rates of bystander witnessed events showed differences between waveforms in the early circulatory phase at 4-10 min post event (mean (S.D.) RLB 0.45 (0.07) versus MDS 0.31 (0.06), p=0.0002) and demonstrated decline as time to first shock increased to 20 min.\nShock success to an organized rhythm comparing step-up protocol for energy settings demonstrated the RLB waveform was superior to MDS in ALS treatment of OHCA. Survival rates for both waveforms are consistent with current theories on the circulatory and metabolic phases of out-of-hospital cardiac arrest.", "title": "Out-of-hospital cardiac arrest rectilinear biphasic to monophasic damped sine defibrillation waveforms with advanced life support intervention trial (ORBIT).", "date": "2005-07-05"}, {"article_id": "17114971", "content": "The definition of defibrillation shock \"success\" endorsed by the International Liaison Committee on Resuscitation since the publication of Guidelines 2000 for Cardiopulmonary Resuscitation and Emergency Cardiac Care has been removal of ventricular fibrillation at 5 secs after shock delivery. Although this success criterion provides a direct assessment of the primary task of a shock, it may not be the only clinically useful measure of shock outcome. We evaluated a different defibrillation success criterion to determine whether it could provide additional insight into the relative performance of different defibrillation shocks.\nA randomized study comparing monophasic and biphasic waveform shocks is reported with return of organized rhythm as the primary outcome measure of defibrillation success.\nA total of 120 patients with out-of-hospital ventricular fibrillation as the first recorded rhythm were treated with defibrillation with automated external defibrillators.\nReturn of organized rhythm (two QRS complexes, <5 secs apart, <60 secs after defibrillation) was achieved in 31 monophasic shock (45%) and 35 biphasic shock (69%) patients (relative risk, 1.53, 95% confidence interval, 1.11-2.10). Logistic regression analysis revealed that shock waveform was the strongest independent predictor of return of organized rhythm (odds ratio, 4.0; 95% confidence interval, 1.67-10.0). Defibrillation success with the conventional International Liaison Committee on Resuscitation criterion was very high (91% and 98%, respectively) and not significantly different between groups.\nReturn of organized rhythm proved to be a more sensitive measure of relative defibrillation shock performance than the conventional shock success criterion. Inclusion of return of organized rhythm as an end point in future clinical research could help discern more subtle defibrillation shock effects and contribute to further optimization of defibrillation technology.", "title": "Definition of successful defibrillation.", "date": "2006-11-23"}]}
{"original_review": "31557310", "question_data": [{"question_id": 86, "question": "Is the incidence of surgical site infection (SSI) for patients undergoing breast cancer surgery without reconstruction higher, lower, or the same when comparing prophylactic antibiotics administered preoperatively to no antibiotic or placebo?", "answer": "lower", "evidence_quality": "moderate", "fulltext_required": "no", "comment": "", "relevant_sources": ["11063500", "23001082", "16989011", "7480237", "9776150", "23052809", "10873356", "19673597", "2403655", "2403697"]}, {"question_id": 87, "question": "Is the rate of hospital readmission higher, lower, or the same when comparing prophylactic antibiotics administered preoperatively to placebo?", "answer": "uncertain effect", "evidence_quality": "low", "fulltext_required": "no", "comment": "", "relevant_sources": ["9776150", "2403655"]}, {"question_id": 88, "question": "Is the incidence of surgical site infection (SSI) for patients undergoing breast cancer surgery without reconstruction higher, lower, or the same when comparing prophylactic antibiotics administered preoperatively to no antibiotic?", "answer": "lower", "evidence_quality": "moderate", "fulltext_required": "no", "comment": "", "relevant_sources": ["11063500", "23001082", "16989011"]}, {"question_id": 89, "question": "Is the incidence of surgical site infection (SSI) for patients undergoing breast cancer surgery without reconstruction higher, lower, or the same when comparing prophylactic antibiotics administered preoperatively to placebo?", "answer": "lower", "evidence_quality": "moderate", "fulltext_required": "no", "comment": "", "relevant_sources": ["7480237", "9776150", "23052809", "10873356", "19673597", "2403655", "2403697"]}, {"question_id": 90, "question": "Is the cost of care higher, lower, or the same when comparing prophylactic antibiotics administered preoperatively to no antibiotic or placebo?", "answer": "uncertain effect", "evidence_quality": "low", "fulltext_required": "no", "comment": "original authors did not pool due to heterogeneity and lack of detail", "relevant_sources": ["23001082", "9776150"]}], "sources": [{"article_id": "19673597", "content": "Preoperative core needle biopsies may increase the risk of surgical site infection (SSI) in breast cancer surgery. The purpose of this randomized trial was to determine whether a prophylactic antibiotic would prevent SSI under these conditions.\nImaging-guided multiple core needle biopsies were performed one to two weeks prior to surgery to obtain confirmation of the presence of breast cancer. Then the patients were randomized to receive either a single intravenous dose of 1.0 g of dicloxacillin (n = 144) or placebo infusion of saline (n = 148) 30 min prior to operation. After breast surgery, incisional morbidity was monitored for 30 days. The number of SSIs was compared with that in 672 patients treated before the implementation of core needle biopsies.\nThe patient characteristics and risk factors for SSI were similar in the antibiotic prophylaxis and placebo groups. The incidence of SSI was 7.2% (21/292) in the prospective trial compared with 6.8% (46/672) in the retrospective cohort (p = 0.890). The incidence of postoperative SSIs was 5.6% (8/144) in the dicloxacillin group and 8.8% (13/148) in the placebo group (p = 0.371). For the first two weeks, there was a non-significant trend to fewer SSIs in the antibiotic group (n = 1) than the placebo group (n = 4). Body mass index, smoking, or previous illness did not affect the likelihood of SSI.\nCore needle biopsy did not increase the incidence of SSI. Antibiotic prophylaxis did not prevent SSI, probably because so few infections occurred.", "title": "Does preoperative core needle biopsy increase surgical site infections in breast cancer surgery? Randomized study of antibiotic prophylaxis.", "date": "2009-08-14"}, {"article_id": "7480237", "content": "Over a 9-month period from September of 1991 to May of 1992, 339 patients were included in a randomized, double-blind, placebo-controlled study using azithromycin as the prophylactic agent to determine whether it effects a clinically meaningful reduction in postoperative surgical infections in plastic surgery. Azithromycin was given as prophylaxis in 171 patients and placebo in 168 patients. The study medication was a single oral dose taken at 8 P.M. the day before surgery. The patients were followed up for a minimum of 4 weeks after surgery. The patients who received wound infection prophylaxis had 5.1 percent infections compared with 20.5 percent in the placebo group (p = 0.00009). Eighty percent of all wound infections were first seen after discharge, explaining why plastic surgeons might overlook their infectious complications. There was a significant reduction in postoperative complications (p = 0.04) and in the additional use of antibiotics postoperatively (p = 0.007) in the prophylaxis group. Subgroup analysis showed a significant reduction in surgical infections in breast surgery (p < 0.05) and reconstructive surgery with flaps (p < 0.05). No effect of the prophylactic regime was demonstrated in patients undergoing secondary surgery for cleft lip and palate disease.", "title": "A prospective, double-blind, placebo-controlled trial of a single dose of azithromycin on postoperative wound infections in plastic surgery.", "date": "1995-11-01"}, {"article_id": "16989011", "content": "The aim of this randomized clinical trial was to determine whether a single intravenous dose of 2 g flucloxacillin could prevent wound infection after primary non-reconstructive breast surgery.\nThe study included 618 patients undergoing local excision (n = 490), mastectomy (n = 107) or microdochectomy (n = 21). Patients were randomized to receive either a single dose of flucloxacillin immediately after the induction of anaesthesia or no intervention. Wound morbidity was monitored by an independent research nurse for 42 days after surgery.\nThe incidence of wound infection was similar in the two groups: 10 of 311 (3.2 percent) in the flucloxacillin group and 14 of 307 (4.6 percent) in the control group (chi(2) = 0.75, P = 0.387; relative risk 0.71, 95 percent confidence interval 0.32 to 1.53). The groups also had similar wound scores and rates of moderate or severe cellulitis. Wound infection presented a median of 16 days after surgery.\nThe administration of a single dose of flucloxacillin failed to reduce the rate of wound infection after non-reconstructive breast surgery.", "title": "Randomized clinical trial of single-dose antibiotic prophylaxis for non-reconstructive breast surgery.", "date": "2006-09-22"}, {"article_id": "2403697", "content": "The ability of perioperative cefazolin to reduce the incidence of postoperative wound infection in patients undergoing ablative surgical treatment for carcinoma of the breast was tested in this prospective, randomized, double-blinded study. From May 1983 until December 1985, 118 women were divided into two groups at random. Group 1 consisted of 59 patients and received cefazolin and group 2 was made up of 59 patients who received a placebo. The groups were similar with respect to age, operative procedure, operative time and time to discharge after operation. Three infections occurred among those in group 1 and five among those in group 2 (p = 0.72). The time to onset of infection was delayed in the patients in group 1 versus those in group 2 (17.7 days versus 9.6 days, p = 0.04). Six of eight infections occurred in patients in whom an interval between biopsy and definitive surgical treatment was present. Prophylactic antibiotics in mammary operations did not reduce postoperative wound infections in this study.", "title": "A prospective, randomized double-blind study of the use of antibiotics at the time of mastectomy.", "date": "1990-01-01"}, {"article_id": "20926017", "content": "This study investigated the effects of Gentacoll implants on healing in patients (n = 44) undergoing modified radical mastectomy and axillary dissection. Group I, the Gentacoll group (n = 22), underwent surgery followed by insertion of 10 \u00d7 10 \u00d7 0.5 cm Gentacoll implants (280 mg collagen sponge plus 200 mg gentamicin sulphate) into the axillary area and under the flap area of the breast before wound closure. Group II, the control group (n = 22), underwent surgery without the application of Gentacoll. Neither group received oral or parenteral post-operative antibiotic therapy. Outcome measures included wound infection, seroma formation, total drainage volumes, drain removal time and duration of hospital stay. Post-operative infection rate, seroma formation, drainage volumes and duration of hospital stay were significantly reduced in the Gentacoll group compared with the control group. In conclusion, the application of Gentacoll significantly improved post-operative outcomes in patients undergoing modified radical mastectomy.", "title": "Effect of local gentamicin application on healing and wound infection in patients with modified radical mastectomy: a prospective randomized study.", "date": "2010-10-12"}, {"article_id": "9776150", "content": "Antibiotic prophylaxis is controversial in patients undergoing axillary lymph node dissection (ALND). We determined whether preoperative antibiotics decreased incidence or treatment cost of infectious complications following ALND.\nTwo hundred patients entered this prospective, randomized, double-blind trial. Patients received either placebo or cefonicid preoperatively. Loco-regional signs of infection were monitored for 4 weeks postoperatively.\nThere was a trend toward fewer infections in the prophylactic group (placebo 13% versus cefonicid 6%; P = 0.080). Cefonicid significantly decreased severe infections requiring hospitalization (placebo 8% versus cefonicid 1%; P = 0.033). Cefonicid also decreased the treatment cost of infection per patient ($49.80 versus $364.87).\nWe demonstrated a trend toward fewer overall infections and significantly fewer severe infections in patients given prophylactic antibiotics, which translated into a decrease in the cost of treatment for infectious complications. These findings support antibiotic prophylaxis for patients undergoing ALND.", "title": "Prospective, randomized, double-blind study of prophylactic antibiotics in axillary lymph node dissection.", "date": "1998-10-17"}, {"article_id": "11063500", "content": "Based on the observation that administration of clarithromycin led to an attenuation of the inflammatory response induced by surgical trauma in a guinea pig model, we investigated the potential beneficial effects of clarithromycin on the local and systemic inflammatory response in patients undergoing mastectomy in an open-label prospective study. During a 16-month period, 54 patients who underwent mastectomy were randomly divided into two groups. In one group, the patients received oral clarithromycin at a dose of 500 mg twice a day, from the day before to 3 days after mastectomy. There was no significant difference in the incidence of antibiotic prophylaxis-related toxicities or postoperative infections between the patients who received clarithromycin and those who did not. Clarithromycin treatment was significantly associated with an attenuation of febrile response, tachycardia, tachypnea, and an increase in monocyte counts (P, <0.0001, <0.01, <0.05, and <0.01, respectively). Clarithromycin also reduced the intensity and duration of postoperative pain (P, <0.05 and <0.005, respectively) and increased the range of motion of the involved shoulder (P < 0.05 for abduction and flexion). We conclude that clarithromycin effectively modulates the acute inflammatory response associated with mastectomy and produces a better clinical outcome.", "title": "Clarithromycin attenuates mastectomy-induced acute inflammatory response.", "date": "2000-11-04"}, {"article_id": "23052809", "content": "The effectiveness of antibiotic prophylaxis for prevention of surgical site infection (SSI) following specific types of breast cancer surgery remains uncertain. This study assessed the effectiveness of prophylaxis in modified radical mastectomy (MRM).\nWomen undergoing MRM for breast cancer were recruited. Women were excluded who had diabetes mellitus, severe malnutrition or known allergy to cephalosporins; were receiving corticosteroid therapy or were treated with antibiotics within one week prior to surgery; were scheduled for simultaneous breast reconstruction or bilateral oophorectomy; had existing local infection. Participants were randomized to receive either intravenous cefazolin 1 g or placebo within 30 min prior to skin incision. Standard skin preparation and operative technique for MRM were carried out. Wounds were assessed for SSI and other complications weekly for 30 days.\nA total of 254 women were recruited. Age, clinical stage, prior chemotherapy, and operative time were similar for antibiotic and placebo groups. The overall incidence of SSI was 14.2 %. There were no significant differences in the infection rate over the 30-day follow-up period between the placebo and antibiotic groups (15 % vs 13.4 %; p = 0.719) or at each week. The majority of SSI were either cellulitis or superficial infection for both groups. There were no significant differences between groups in treatments required for SSI, incidence of hematoma or seroma.\nThe findings of this study, alone and when meta-analyzed with data from studies in similar surgical populations, do not support the use of antibiotic prophylaxis in MRM.", "title": "A randomized, double-blinded placebo-controlled clinical trial of the routine use of preoperative antibiotic prophylaxis in modified radical mastectomy.", "date": "2012-10-12"}, {"article_id": "23001082", "content": "To assess the impact of prophylactic antibiotics on the prevention of surgical site infection (SSI) and the cost-effectiveness of this prophylaxis for breast cancer surgery in overweight or obese women.\nSSI is higher than expected after breast surgery. Obesity was found to be one of the risk factors.\nThe trial was designed as a phase IV randomized, controlled, parallel-group efficacy trial. It was conducted at a tertiary university hospital. Overweight or obese women with clinically early-stage breast cancer who had been assigned to undergo surgery were eligible. Patients were randomly allocated to either a prophylaxis or a control group by using a computer-generated list. The prophylaxis group received 1 g ampicillin-sulbactam intravenously at anesthesia. The control group received no intervention. Patients and observers were blinded to the assignments. The primary outcome was the comparison of SSI incidences of the 2 groups. Patients were monitored for 30 days.\nA total of 369 patients were included in final analysis, out of which 187 were allocated for prophylaxis and 182 were randomly assigned to the control group. Analysis was done according to the intention-to-treat principle. Prophylaxis significantly reduced the SSI rate (4.8%) in the prophylaxis group when compared with that in the control group [13.7%; relative risk (RR) 0.35; 95% CI: 0.17-0.73]. No adverse reaction was observed. The mean SSI-related cost (20.26 USD) was found to be significantly higher in the control group when compared with that (8.48 USD) in the prophylaxis group.\nAntibiotic prophylaxis significantly decreased SSI incidence after elective surgery and was shown to be cost-effective in obese breast cancer patients. ClinicalTrials.gov Identifier: NCT00356148.", "title": "Efficacy of prophylactic antibiotic administration for breast cancer surgery in overweight or obese patients: a randomized controlled trial.", "date": "2012-09-25"}, {"article_id": "10873356", "content": "Antibiotic prophylaxis has been used to good effect in the prevention of post-operative wound infections in patients undergoing gastrointestinal operations. We have assessed the use of a single dose of intravenous antibiotic (Augmentin 1.2 g), given with induction of anaesthesia as prophylaxis, against post-operative wound infection in women undergoing clean, elective breast surgery. Three hundred and thirty-four patients were recruited. Of the 164 receiving antibiotic prophylaxis 29 (17.7%) had wound infections compared with 32 (18.8%) in the placebo group (P=0.79). There were no significant differences in any other post-operative infective complications. Antibiotic prophylaxis is probably not required in clean, elective breast surgery.", "title": "Antibiotic prophylaxis for post-operative wound infection in clean elective breast surgery.", "date": "2000-06-30"}, {"article_id": "2403655", "content": "We assessed the efficacy of perioperative antibiotic prophylaxis for surgery in a randomized, double-blind trial of 1218 patients undergoing herniorrhaphy or surgery involving the breast, including excision of a breast mass, mastectomy, reduction mammoplasty, and axillary-node dissection. The prophylactic regimen was a single dose of cefonicid (1 g intravenously) administered approximately half an hour before surgery. The patients were followed up for four to six weeks after surgery. Blinding was maintained until the last patient completed the follow-up and all diagnoses of infection had been made. The patients who received prophylaxis had 48 percent fewer probable or definite infections than those who did not (Mantel-Haenszel risk ratio, 0.52; 95 percent confidence interval, 0.32 to 0.84; P = 0.01). For patients undergoing a procedure involving the breast, infection occurred in 6.6 percent of the cefonicid recipients (20 of 303) and 12.2 percent of the placebo recipients (37 of 303); for those undergoing herniorrhaphy, infection occurred in 2.3 percent of the cefonicid recipients (7 of 301) and 4.2 percent of the placebo recipients (13 of 311). There were comparable reductions in the numbers of definite wound infections (Mantel-Haenszel risk ratio, 0.49), wounds that drained pus (risk ratio, 0.43), Staphylococcus aureus wound isolates (risk ratio, 0.49), and urinary tract infections (risk ratio, 0.40). There were also comparable reductions in the need for postoperative antibiotic therapy, non-routine visits to a physician for problems involving wound healing, incision and drainage procedures, and readmission because of problems with wound healing. We conclude that perioperative antibiotic prophylaxis with cefonicid is useful for herniorrhaphy and certain types of breast surgery.", "title": "Perioperative antibiotic prophylaxis for herniorrhaphy and breast surgery.", "date": "1990-01-18"}]}
{"original_review": "32068247", "question_data": [{"question_id": 91, "question": "Is the likelihood of detoxification at six\u2010month follow\u2010up higher, lower, or the same when comparing dihydrocodeine (DHC) to buprenorphine?", "answer": "no difference", "evidence_quality": "low", "fulltext_required": "no", "comment": "", "relevant_sources": ["19196468", "17210079"]}, {"question_id": 92, "question": "Is the likelihood of treatment retention higher, lower, or the same when comparing dihydrocodeine (DHC) to buprenorphine?", "answer": "no difference", "evidence_quality": "low", "fulltext_required": "yes", "comment": "", "relevant_sources": ["19196468", "17210079"]}], "sources": [{"article_id": "17210079", "content": "Many drug users present to primary care requesting detoxification from illicit opiates. There are a number of detoxification agents but no recommended drug of choice. The purpose of this study is to compare buprenorphine with dihydrocodeine for detoxification from illicit opiates in primary care.\nOpen label randomised controlled trial in NHS Primary Care (General Practices), Leeds, UK. Sixty consenting adults using illicit opiates received either daily sublingual buprenorphine or daily oral dihydrocodeine. Reducing regimens for both interventions were at the discretion of prescribing doctor within a standard regimen of not more than 15 days. Primary outcome was abstinence from illicit opiates at final prescription as indicated by a urine sample. Secondary outcomes during detoxification period and at three and six months post detoxification were recorded.\nOnly 23% completed the prescribed course of detoxification medication and gave a urine sample on collection of their final prescription. Risk of non-completion of detoxification was reduced if allocated buprenorphine (68% vs 88%, RR 0.58 CI 0.35-0.96, p = 0.065). A higher proportion of people allocated to buprenorphine provided a clean urine sample compared with those who received dihydrocodeine (21% vs 3%, RR 2.06 CI 1.33-3.21, p = 0.028). People allocated to buprenorphine had fewer visits to professional carers during detoxification and more were abstinent at three months (10 vs 4, RR 1.55 CI 0.96-2.52) and six months post detoxification (7 vs 3, RR 1.45 CI 0.84-2.49).\nInformative randomised trials evaluating routine care within the primary care setting are possible amongst drug using populations. This small study generates unique data on commonly used treatment regimens.", "title": "Buprenorphine versus dihydrocodeine for opiate detoxification in primary care: a randomised controlled trial.", "date": "2007-01-11"}, {"article_id": "17210080", "content": "In the United Kingdom (UK), there is an extensive market for the class 'A' drug heroin. Many heroin users spend time in prison. People addicted to heroin often require prescribed medication when attempting to cease their drug use. The most commonly used detoxification agents in UK prisons are buprenorphine, dihydrocodeine and methadone. However, national guidelines do not state a detoxification drug of choice. Indeed, there is a paucity of research evaluating the most effective treatment for opiate detoxification in prisons. This study seeks to address the paucity by evaluating routinely used interventions amongst drug using prisoners within UK prisons.\nThe Leeds Evaluation of Efficacy of Detoxification Study (LEEDS) Prisons Pilot Study will use randomised controlled trial methodology to compare the open use of buprenorphine and dihydrocodeine for opiate detoxification, given in the context of routine care, within HMP Leeds. Prisoners who are eligible and give informed consent will be entered into the trial. The primary outcome measure will be abstinence status at five days post detoxification, as determined by a urine test. Secondary outcomes during the detoxification and then at one, three and six months post detoxification will be recorded.", "title": "The Leeds Evaluation of Efficacy of Detoxification Study (LEEDS) prisons project pilot study: protocol for a randomised controlled trial comparing dihydrocodeine and buprenorphine for opiate detoxification.", "date": "2007-01-11"}, {"article_id": "19196468", "content": "Many opiate users entering British prisons require prescribed medication to help them achieve abstinence. This commonly takes the form of a detoxification regime. Previously, a range of detoxification agents have been prescribed without a clear evidence base to recommend a drug of choice. There are few trials and very few in the prison setting. This study compares dihydrocodeine with buprenorphine.\nOpen label, pragmatic, randomised controlled trial in a large remand prison in the North of England. Ninety adult male prisoners requesting an opiate detoxification were randomised to receive either daily sublingual buprenorphine or daily oral dihydrocodeine, given in the context of routine care. All participants gave written, informed consent. Reducing regimens were within a standard regimen of not more than 20 days and were at the discretion of the prescribing doctor. Primary outcome was abstinence from illicit opiates as indicated by a urine test at five days post detoxification. Secondary outcomes were collected during the detoxification period and then at one, three and six months post detoxification. Analysis was undertaken using relative risk tests for categorical data and unpaired t-tests for continuous data.\n64% of those approached took part in the study. 63 men (70%) gave a urine sample at five days post detoxification. At the completion of detoxification, by intention to treat analysis, a higher proportion of people allocated to buprenorphine provided a urine sample negative for opiates (abstinent) compared with those who received dihydrocodeine (57% vs 35%, RR 1.61 CI 1.02-2.56). At the 1, 3 and 6 month follow-up points, there were no significant differences for urine samples negative for opiates between the two groups. Follow up rates were low for those participants who had subsequently been released into the community.\nThese findings would suggest that dihydrocodeine should not be routinely used for detoxification from opiates in the prison setting. The high relapse rate amongst those achieving abstinence would suggest the need for an increased emphasis upon opiate maintenance programmes in the prison setting.\nCurrent Controlled Trials ISRCTN07752728.", "title": "The Leeds Evaluation of Efficacy of Detoxification Study (LEEDS) prisons project: a randomised controlled trial comparing dihydrocodeine and buprenorphine for opiate detoxification.", "date": "2009-02-07"}, {"article_id": "15117415", "content": "Heroin is a synthetic opioid with an extensive illicit market leading to large numbers of people becoming addicted. Heroin users often present to community treatment services requesting detoxification and in the UK various agents are used to control symptoms of withdrawal. Dissatisfaction with methadone detoxification 8 has lead to the use of clonidine, lofexidine, buprenorphine and dihydrocodeine; however, there remains limited evaluative research. In Leeds, a city of 700,000 people in the North of England, dihydrocodeine is the detoxification agent of choice. Sublingual buprenorphine, however, is being introduced. The comparative value of these two drugs for helping people successfully and comfortably withdraw from heroin has never been compared in a randomised trial. Additionally, there is a paucity of research evaluating interventions among drug users in the primary care setting. This study seeks to address this by randomising drug users presenting in primary care to receive either dihydrocodeine or buprenorphine.\nThe Leeds Evaluation of Efficacy of Detoxification Study (LEEDS) project is a pragmatic randomised trial which will compare the open use of buprenorphine with dihydrocodeine for illicit opiate detoxification, in the UK primary care setting. The LEEDS project will involve consenting adults and will be run in specialist general practice surgeries throughout Leeds. The primary outcome will be the results of a urine opiate screening at the end of the detoxification regimen. Adverse effects and limited data to three and six months will be acquired.", "title": "The Leeds Evaluation of Efficacy of Detoxification Study (LEEDS) project: an open-label pragmatic randomised control trial comparing the efficacy of differing therapeutic agents for primary care detoxification from either street heroin or methadone [ISRCTN07752728].", "date": "2004-05-01"}, {"article_id": "17156174", "content": "The aim of this study is to define the efficacy of dihydrocodeine as an alternative to methadone in the maintenance treatment of opiate dependence.\nA pragmatic open-label randomized controlled study of patients recommended for opiate maintenance treatment to test equivalence of the two treatment options with follow-up continuing for up to 42 months after recruitment.\nAssessment at either Edinburgh's Community Drug Problem Service or at two general practitioner practices with specialist drug community psychiatric nurses, then with shared care follow-up.\nTwo hundred and thirty-five subjects (168 male, 67 female) with opiate dependence syndrome were recruited. Subjects selected were suitable for opiate maintenance treatment. Routine treatment was offered throughout.\nPatients were randomized to receive either methadone mixture 1 mg/ml or dihydrocodeine, 30 mg or 60 mg tablets.\nThe primary outcome measure was retention in treatment. Eight secondary outcomes included total illicit opiate use, reported crime, physical health, mental health, injecting drug use, overdoses, selling drugs and being in education or work. Measures were compared over 42 months follow-up.\nThere was no difference in groups for retention in treatment at follow-up and there was improvement in all secondary outcomes from baseline. No significant difference in outcomes was found between randomized groups over time. Compliance with randomized treatment differed by randomized group and was affected by experiences in custody during follow-up. Those randomized to dihydrocodeine were more likely to switch treatments.\nThese results, combined with existing clinical experience, provide evidence that dihydrocodeine is a viable alternative to methadone as a maintenance treatment for opiate dependence. Indirect comparisons with other studies show dihydrocodeine (and methadone) to be superior to placebo.", "title": "Addressing the efficacy of dihydrocodeine versus methadone as an alternative maintenance treatment for opiate dependence: A randomized controlled trial.", "date": "2006-12-13"}]}
{"original_review": "30746689", "question_data": [{"question_id": 93, "question": "Is the time to postvoid residual volume of urine \u2264 50 mL higher, lower, or the same when comparing nerve\u2010sparing radical hysterectomy to standard radical hysterectomy?", "answer": "lower", "evidence_quality": "low", "fulltext_required": "no", "comment": "", "relevant_sources": ["22209773", "25872890"]}, {"question_id": 94, "question": "Is the likelihood of disease-free survival higher, lower, or the same when comparing nerve\u2010sparing radical hysterectomy to standard radical hysterectomy?", "answer": "no difference", "evidence_quality": "very low", "fulltext_required": "no", "comment": "", "relevant_sources": ["25872890"]}], "sources": [{"article_id": "22209773", "content": "This study evaluated histopathology and clinical outcome of autonomic nerve trauma and vessels removal within the cardinal ligament (CL) during nerve-sparing radical hysterectomy (NSRH) compared with radical hysterectomy (RH).\n25 women with FIGO stage Ib1-IIa cervical cancer underwent RH (n=13) or NSRH (n=12). Removed CLs lengths were measured. Biopsies were collected from the proximal, middle and distal segment of CLs and fixed. Different markers were used for immunohistochemisty analysis: tyrosine hydroxylase for sympathetic nerves; vasoactive intestinal polypeptide for parasympathetic nerves; CD34 for blood vessels; and D2-40 for lymphatic vessels. The volume density (Vv), a parameter of biological stereology, was used to quantitatively measure CL components, while post-operative functions, such as defecation, micturition and two-year disease free survival in RH and NSRH groups were compared.\nThe nerves mainly existed in the middle and distal segments of CLs. The Vv was greater in RH compared with NSRH for both sympathetic and parasympathetic nerve markers (P<0.05), while the Vv of blood and lymphatic vessels were same in the two groups. Average time to achieve residual urine\u226450ml and first defecation were shorter in NSRH than in RH (P<0.05).\nLess autonomic nerves within CL are transected in NSRH than in RH, while blood/lymphatic vessels are efficiently removed in both treatments. Compared to RH, NSRH decreases iatrogenic injury, which leads to reduced post-operative co-morbidities, with ensure the same radicality.", "title": "Classical and nerve-sparing radical hysterectomy: an evaluation of the nerve trauma in cardinal ligament.", "date": "2012-01-03"}, {"article_id": "25872890", "content": "A prospective, randomized controlled trial was conducted to evaluate the efficacy of nerve-sparing radical hysterectomy (NSRH) in preserving bladder function and its oncologic safety in the treatment of cervical cancer.\nFrom March 2003 to November 2005, 92 patients with cervical cancer stage IA2 to IIA were randomly assigned for surgical treatment with conventional radical hysterectomy (CRH) or NSRH, and 86 patients finally included in the analysis. Adequacy of nerve sparing, radicality, bladder function, and oncologic safety were assessed by quantifying the nerve fibers in the paracervix, measuring the extent of paracervix and harvested lymph nodes (LNs), urodynamic study (UDS) with International Prostate Symptom Score (IPSS), and 10-year disease-free survival (DFS), respectively.\nThere were no differences in clinicopathologic characteristics between two groups. The median number of nerve fiber was 12 (range, 6 to 21) and 30 (range, 17 to 45) in the NSRH and CRH, respectively (p<0.001). The extent of resected paracervix and number of LNs were not different between the two groups. Volume of residual urine and bladder compliance were significantly deteriorated at 12 months after CRH. On the contrary, all parameters of UDS were recovered no later than 3 months after NSRH. Evaluation of the IPSS showed that the frequency of long-term urinary symptom was higher in CRH than in the NSRH group. The median duration before the postvoid residual urine volume became less than 50 mL was 11 days (range, 7 to 26 days) in NSRH group and was 18 days (range, 10 to 85 days) in CRH group (p<0.001). No significant difference was observed in the 10-year DFS between two groups.\nNSRH appears to be effective in preserving bladder function without sacrificing oncologic safety.", "title": "Efficacy and oncologic safety of nerve-sparing radical hysterectomy for cervical cancer: a randomized controlled trial.", "date": "2015-04-16"}, {"article_id": "25605211", "content": "To investigate bladder and intestinal function recovery and quality of sexual life after laparoscopic nerve-sparing radical hysterectomy (LNRH) for treatment of early invasive cervical carcinoma.\nSubjects included patients who underwent radical hysterectomy by laparotomy who were randomly assigned to 2 groups: 30 patients who underwent LNRH and 35 classical laparoscopic radical hysterectomy (LRH). We assessed the patients general clinical information, surgical characteristics, pathological findings, and adjuvant therapies. A urodynamic study was used to assess bladder function. Intestinal function recovery and quality of sexual life were evaluated by questionnaire.\nNo significant differences were found in age, surgery characteristics, pathological findings, adjuvant therapies, and main adverse effects between the 2 groups. The mean duration of the postoperative catheterization (DPC) in group LNRH was shorter than that in group LRH (P < 0.001). The maximum flow rate, maximum cystometric capacity , maximum detrusor pressure and urinary complications in group LNRH were better than those in group LRH. The quality of sexual life evaluated according to the female sexual function index (FSFI) was better in group LNRH than in those who underwent LRH. The intestinal function of patients in group LNRH also recovered better compared with patients in group LRH.", "title": "Effect of laparoscopic nerve-sparing radical hysterectomy on bladder function, intestinal function recovery and quality of sexual life in patients with cervical carcinoma.", "date": "2015-01-22"}, {"article_id": "20606542", "content": "To investigate the bladder function recovery and quality of life (QOL) using nerve-sparing radical hysterectomy (NSRH) in treating early invasive cervical carcinoma.\nSubjects included patients who underwent radical hysterectomy by laparotomy for early-stage cervical carcinoma. Thirty-one patients were randomly assigned to 2 groups: group A, 15 patients who underwent NSRH; and group B, 16 patients who underwent classical radical hysterectomy. We observed the patients' general clinical information, surgical characteristics, postoperative vital signs, pathological findings, adjuvant therapies, and adverse effects. A urodynamic study was used to assess the bladder function. The patients' QOL was evaluated by Functional Assessment of Cervical Cancer Therapy (FACT-Cx).\nTwenty-nine patients completed the study. No significant differences were found in age, body mass index, surgery characteristics, pathological findings, adjuvant therapies, and main adverse effects between the 2 groups (P > 0.05). The postoperative time of bladder function recovery in group A was obviously earlier than that in group B (P < 0.05). The urodynamic study showed that the extent of bladder function recovery in group A was better than that in group B (P < 0.05). The QOL in group A evaluated 1 year after operation was improved compared with that in group B (P < 0.05). The QOL analysis showed that group A did much better than group B in social and family life, emotional well-being, working status, and the symptom correlated with the operation (P < 0.05). No significant differences were found in basic bodily functions (P > 0.05).\nNerve-sparing radical hysterectomy is a safe and reliable technique for early invasive cervical carcinoma. The postoperative bladder function recovery and the patients' QOL were improved after NSRH compared with the control group. Therefore, NSRH could be an alternative management to modify the classical surgery for cervical carcinoma with International Federation of Gynecology and Obstetrics stages IB1 to IIA.", "title": "Effect of nerve-sparing radical hysterectomy on bladder function recovery and quality of life in patients with cervical carcinoma.", "date": "2010-07-08"}]}
{"original_review": "37162250", "question_data": [{"question_id": 95, "question": "Is functional capacity higher, lower, or the same when comparing prehabilitation to no prehabilitation before colorectal cancer surgery?", "answer": "higher", "evidence_quality": "very low", "fulltext_required": "no", "comment": "", "relevant_sources": ["29327644", "25076007"]}, {"question_id": 96, "question": "Is the risk of complications higher, lower, or the same when comparing prehabilitation to no prehabilitation before colorectal cancer surgery?", "answer": "no difference", "evidence_quality": "low", "fulltext_required": "no", "comment": "29327644 omitted with 18.4% weight", "relevant_sources": ["31968063", "25076007"]}], "sources": [{"article_id": "25076007", "content": "The preoperative period (prehabilitation) may represent a more appropriate time than the postoperative period to implement an intervention. The impact of prehabilitation on recovery of function al exercise capacity was thus studied in patients undergoing colorectal resection for cancer.\nA parallel-arm single-blind superiority randomized controlled trial was conducted. Seventy-seven patients were randomized to receive either prehabilitation (n = 38) or rehabilitation (n = 39). Both groups received a home-based intervention of moderate aerobic and resistance exercises, nutritional counseling with protein supplementation, and relaxation exercises initiated either 4 weeks before surgery (prehabilitation) or immediately after surgery (rehabilitation), and continued for 8 weeks after surgery. Patients were managed with an enhanced recovery pathway. Primary outcome was functional exercise capacity measured using the validated 6-min walk test.\nMedian duration of prehabilitation was 24.5 days. While awaiting surgery, functional walking capacity increased (\u2265 20 m) in a higher proportion of the prehabilitation group compared with the rehabilitation group (53 vs. 15%, adjusted P = 0.006). Complication rates and duration of hospital stay were similar. The difference between baseline and 8-week 6-min walking test was significantly higher in the prehabilitation compared with the rehabilitation group (+23.7 m [SD, 54.8] vs. -21.8 m [SD, 80.7]; mean difference 45.4 m [95% CI, 13.9 to 77.0]). A higher proportion of the prehabilitation group were also recovered to or above baseline exercise capacity at 8 weeks compared with the rehabilitation group (84 vs. 62%, adjusted P = 0.049).\nMeaningful changes in postoperative functional exercise capacity can be achieved with a prehabilitation program.", "title": "Prehabilitation versus rehabilitation: a randomized control trial in patients undergoing colorectal resection for cancer.", "date": "2014-07-31"}, {"article_id": "31968063", "content": "Research supports use of prehabilitation to optimize physical status before and after colorectal cancer resection, but its effect on postoperative complications remains unclear. Frail patients are a target for prehabilitation interventions owing to increased risk for poor postoperative outcomes.\nTo assess the extent to which a prehabilitation program affects 30-day postoperative complications in frail patients undergoing colorectal cancer resection compared with postoperative rehabilitation.\nThis single-blind, parallel-arm, superiority randomized clinical trial recruited patients undergoing colorectal cancer resection from September 7, 2015, through June 19, 2019. Patients were followed up for 4 weeks before surgery and 4 weeks after surgery at 2 university-affiliated tertiary hospitals. A total of 418 patients 65 years or older were assessed for eligibility. Of these, 298 patients were excluded (not frail [n\u2009=\u2009290], unable to exercise [n\u2009=\u20093], and planned neoadjuvant treatment [n\u2009=\u20095]), and 120 frail patients (Fried Frailty Index,\u22652) were randomized. Ten patients were excluded after randomization because they refused surgery (n\u2009=\u20093), died before surgery (n\u2009=\u20093), had no cancer (n\u2009=\u20091), had surgery without bowel resection (n\u2009=\u20091), or were switched to palliative care (n\u2009=\u20092). Hence, 110 patients were included in the intention-to-treat analysis (55 in the prehabilitation [Prehab] and 55 in the rehabilitation [Rehab] groups). Data were analyzed from July 25 through August 21, 2019.\nMultimodal program involving exercise, nutritional, and psychological interventions initiated before (Prehab group) or after (Rehab group) surgery. All patients were treated within a standardized enhanced recovery pathway.\nThe primary outcome included the Comprehensive Complications Index measured at 30 days after surgery. Secondary outcomes were 30-day overall and severe complications, primary and total length of hospital stay, 30-day emergency department visits and hospital readmissions, recovery of walking capacity, and patient-reported outcome measures.\nOf 110 patients randomized, mean (SD) age was 78 (7) years; 52 (47.3%) were men and 58 (52.7%) were women; 31 (28.2%) had rectal cancer; and 87 (79.1%) underwent minimally invasive surgery. There was no between-group difference in the primary outcome measure, 30-day Comprehensive Complications Index (adjusted mean difference, -3.2; 95% CI, -11.8 to 5.3; P\u2009=\u2009.45). Secondary outcome measures were also not different between groups.\nIn frail patients undergoing colorectal cancer resection (predominantly minimally invasive) within an enhanced recovery pathway, a multimodal prehabilitation program did not affect postoperative outcomes. Alternative strategies should be considered to optimize treatment of frail patients preoperatively.\nClinicalTrials.gov identifier: NCT02502760.", "title": "Effect of Multimodal Prehabilitation vs Postoperative Rehabilitation on 30-Day Postoperative Complications for Frail Patients Undergoing Resection of Colorectal Cancer: A Randomized Clinical Trial.", "date": "2020-01-23"}, {"article_id": "29327644", "content": "Prehabilitation has been previously shown to be more effective in enhancing postoperative functional capacity than rehabilitation alone. The purpose of this study was to determine whether a weekly supervised exercise session could provide further benefit to our current prehabilition program, when comparing to standard post-surgical rehabilitation.\nA parallel-arm single-blind randomized control trial was conducted in patients scheduled for non-metastatic colorectal cancer resection. Patients were assigned to either a once weekly supervised prehabilitation (PREHAB+, n\u2009=\u200941) or standard rehabilitation (REHAB, n\u2009=\u200939) program. Both multimodal programs were home-based program and consisted of moderate intensity aerobic and resistance exercise, nutrition counseling with daily whey protein supplementation and anxiety-reduction strategies. Perioperative care was standardized for both groups as per enhanced recovery after surgery (ERAS\nBoth groups were comparable for baseline walking capacity (PREHAB+: 448\u00a0m [IQR 375-525] vs. REHAB: 461\u00a0m [419-556], p=.775) and included a similar proportion of patients who improved walking capacity (>20\u00a0m) during the preoperative period (PREHAB+: 54% vs. REHAB: 38%, p\u2009=\u2009.222). After surgery, changes in 6MWD were also similar in both groups. In PREHAB+, however, there was a significant association between physical activity energy expenditure and 6MWD (p\u2009<\u2009.01). Previously inactive patients were more likely to improve functional capacity due to PREHAB+ (OR 7.07 [95% CI 1.10-45.51]).\nThe addition of a weekly supervised exercise session to our current prehabilitation program did not further enhance postoperative walking capacity when compared to standard REHAB care. Sedentary patients, however, seemed more likely to benefit from PREHAB+. An association was found between energy spent in physical activity and 6MWD. This information is important to consider when designing cost-effective prehabilitation programs.", "title": "Evaluation of supervised multimodal prehabilitation programme in cancer patients undergoing colorectal resection: a randomized control trial.", "date": "2018-01-13"}]}
{"original_review": "36200610", "question_data": [{"question_id": 97, "question": "Is moderate to severe anaemia prevalence higher, lower, or the same when comparing house modifications to no modifications?", "answer": "lower", "evidence_quality": "high", "fulltext_required": "yes", "comment": "", "relevant_sources": ["33838737", "19732949", "33640067"]}, {"question_id": 98, "question": "Is malaria parasite prevalence higher, lower, or the same when comparing housing modifications to no modifications?", "answer": "lower", "evidence_quality": "moderate", "fulltext_required": "yes", "comment": "", "relevant_sources": ["19732949", "34022912", "35437129", "32950061", "33838737"]}, {"question_id": 99, "question": "Is clinical malaria incidence higher, lower, or the same when comparing housing modifications to no modifications?", "answer": "uncertain effect", "evidence_quality": "very low", "fulltext_required": "no", "comment": "fulltext is available", "relevant_sources": ["29452110", "33838737", "33640067"]}, {"question_id": 100, "question": "Is indoor adult mosquito density higher, lower, or the same when comparing housing modifications to no modifications?", "answer": "lower", "evidence_quality": "low", "fulltext_required": "yes", "comment": "", "relevant_sources": ["19732949", "34022912", "33640067", "33838737"]}], "sources": [{"article_id": "33640067", "content": "New vector control tools are required to sustain the fight against malaria. Lethal house lures, which target mosquitoes as they attempt to enter houses to blood feed, are one approach. Here we evaluated lethal house lures consisting of In2Care (Wageningen, Netherlands) Eave Tubes, which provide point-source insecticide treatments against host-seeking mosquitoes, in combination with house screening, which aims to reduce mosquito entry.\nWe did a two-arm, cluster-randomised controlled trial with 40 village-level clusters in central C\u00f4te d'Ivoire between Sept 26, 2016, and April 10, 2019. All households received new insecticide-treated nets at universal coverage (one bednet per two people). Suitable households within the clusters assigned to the treatment group were offered screening plus Eave Tubes, with Eave Tubes treated using a 10% wettable powder formulation of the pyrethroid \u03b2-cyfluthrin. Because of the nature of the intervention, treatment could not be masked for households and field teams, but all analyses were blinded. The primary endpoint was clinical malaria incidence recorded by active case detection over 2 years in cohorts of children aged 6 months to 10 years. This trial is registered with ISRCTN, ISRCTN18145556.\n3022 houses received screening plus Eave Tubes, with an average coverage of 70% across the intervention clusters. 1300 eligible children were recruited for active case detection in the control group and 1260 in the intervention group. During the 2-year follow-up period, malaria case incidence was 2\u00b729 per child-year (95% CI 1\u00b797-2\u00b761) in the control group and 1\u00b743 per child-year (1\u00b721-1\u00b765) in the intervention group (hazard ratio 0\u00b762, 95% CI 0\u00b751-0\u00b776; p<0\u00b70001). Cost-effectiveness simulations suggested that screening plus Eave Tubes has a 74\u00b70% chance of representing a cost-effective intervention, compared with existing healthcare activities in C\u00f4te d'Ivoire, and is similarly cost-effective to other core vector control interventions across sub-Saharan Africa. No serious adverse events associated with the intervention were reported during follow-up.\nScreening plus Eave Tubes can provide protection against malaria in addition to the effects of insecticide-treated nets, offering potential for a new, cost-effective strategy to supplement existing vector control tools. Additional trials are needed to confirm these initial results and further optimise Eave Tubes and the lethal house lure concept to facilitate adoption.\nThe Bill & Melinda Gates Foundation.", "title": "Impact and cost-effectiveness of a lethal house lure against malaria transmission in central C\u00f4te d'Ivoire: a two-arm, cluster-randomised controlled trial.", "date": "2021-03-01"}, {"article_id": "35437129", "content": "Increases in bed net coverage and antimalarial treatment have reduced the risk of malaria in sub-Saharan Africa. However, the pace of reduction has slowed, and new tools are needed to reverse this trend. We evaluated houses screened with insecticide-treated ceiling nets using a cluster randomized-controlled trial in western Kenya. The primary endpoints were \n[Figure: see text]", "title": "Effectiveness of screened ceilings over the current best practice in reducing malaria prevalence in western Kenya: a cluster randomised controlled trial.", "date": "2022-04-20"}, {"article_id": "32950061", "content": "Mosquito-proofing of houses using wire mesh screens is gaining greater recognition as a practical intervention for reducing exposure to malaria transmitting mosquitoes. Screening potentially protects all persons sleeping inside the house against transmission of mosquito-borne diseases indoors. The study assessed the effectiveness of house eaves screening in reducing indoor vector densities and malaria prevalence in Nyabondo, western Kenya.\n160 houses were selected for the study, with half of them randomly chosen for eaves screening with fibre-glass coated wire mesh (experimental group) and the other half left without screening (control group). Randomization was carried out by use of computer-generated list in permuted blocks of ten houses and 16 village blocks, with half of them allocated treatment in a ratio of 1:1. Cross-sectional baseline entomological and parasitological data were collected before eave screening. After baseline data collection, series of sampling of indoor adult mosquitoes were conducted once a month in each village using CDC light traps. Three cross-sectional malaria parasitological surveys were conducted at three month intervals after installation of the screens. The primary outcome measures were indoor Anopheles mosquito density and malaria parasite prevalence.\nA total of 15,286 mosquitoes were collected over the two year period using CDC light traps in 160 houses distributed over 16 study villages (mean mosquitoes\u2009=\u20094.35, SD\u2009=\u200911.48). Of all mosquitoes collected, 2,872 (18.8%) were anophelines (2,869 Anopheles gambiae sensu lato, 1 Anopheles funestus and 2 other Anopheles spp). Overall, among An. gambiae collected, 92.6% were non-blood fed, 3.57% were blood fed and the remaining 0.47% were composed of gravid and half gravid females. More indoor adult mosquitoes were collected in the control than experimental arms of the study. Results from cross-sectional parasitological surveys showed that screened houses recorded relatively low malaria parasite prevalence rates compared to the control houses. Overall, malaria prevalence was 5.6% (95% CI: 4.2-7.5) n\u2009=\u20091,918, with baseline prevalence rate of 6.1% (95% CI: 3.9-9.4), n\u2009=\u2009481 and 3\nThe study demonstrated that house eave screening has potential to reduce indoor vector densities and malaria prevalence in high transmission areas.", "title": "Evaluating effectiveness of screening house eaves as a potential intervention for reducing indoor vector densities and malaria prevalence in Nyabondo, western Kenya.", "date": "2020-09-21"}, {"article_id": "29452110", "content": "House is the major site for malaria infection where most human-vector contact takes place. Hence, improving housing might reduce the risk of malaria infection by limiting house entry of vectors. This study aimed to explore the impact of screening doors and windows with wire meshes on density and entomological inoculation rate (EIR) of malaria vector, and malaria incidence, and assess the acceptability, durability, and cost of the intervention. The susceptibility status of malaria vector was also assessed. A two-arm randomized trial was done in Arba Minch Town, southwest Ethiopia. 92 houses were randomly included in the trial. The baseline entomological and malaria prevalence data were collected. The mosquito sampling was done twice per household per month by Centers for Diseases Control and Prevention (CDC) light traps for six months. The baseline prevalence of malaria was assessed by testing 396 (83% of the 447 study participants) household members in all the eligible houses. The 92 houses were then randomized into control and intervention groups using mosquito and malaria prevalence baseline data to make the two groups comparable except the intervention. Then, we put wire-mesh on doors and windows of 46 houses. Post-screening mosquito collection was done in each household twice per month for three months. Each household member was visited twice per month for six months to assess malaria episodes. The frequency of damage to different structure of screening was measured twice. In-depth interview was conducted with 24 purposely selected household heads from intervention group. Speciation of Anopheles mosquito was done by morphological key, and the circum-sporozoite proteins (CSPs) analysis was done using enzyme-linked immunosorbent assay. A generalized estimating equation with a negative binomial distribution was used to assess the impact of the intervention on the indoor density of vectors. Clinical malaria case data were analyzed using Poisson regression with generalized linear model. Screening doors and windows reduced the indoor density of An. arabiensis by 48% (mean ratio of intervention to control\u202f=\u202f0.85/1.65; 0.52) (P\u202f=\u202f.001). Plasmodium falciparum CSP rate was 1.6% (3/190) in the intervention houses, while it was 2.7% (10/372) in the control houses. The protective efficacy of screening intervention from CSP positive An. arabiensis was 41% (mean ratio of intervention to control\u202f=\u202f1.6/2.7; 0.59), but was not statistically significant (P\u202f=\u202f.6). The EIR of An. arabiensis was 1.91 in the intervention group, whereas it was 6.45 in the control group. 477 participants were followed for clinical malaria (50.1% from intervention and 49.9% from the control group). Of 49 RDT positive cases, 45 were confirmed to be positive with microscopy. 80% (n\u202f=\u202f36) cases were due to P. falciparum and the rest 20% (n\u202f=\u202f9) were due to P. vivax. The incidence of P. falciparum in the intervention group was lower (IRR: 0.39, 95% CI: 0.2-0.80; P\u202f=\u202f.01) than in the control group. Using incidence of P. falciparum infection, the protective efficacy of intervention was 61% (95% CI: 18-83; P\u202f=\u202f.007). 97.9% of screened windows and 63.8% of screened doors were intact after eleven months of installation. Malaria mosquito was resistance (mortality rate of 75%) to the insecticide used for bed nets treatment. Almost all participants of intervention arm were willing to continue using screened doors and windows. Screening doors and windows reduced the indoor exposure to malaria vectors. The intervention is effective, durable and well-accepted. Hence, the existing interventions can be supplemented with house screening intervention for further reduction and ultimately elimination of malaria by reducing insecticide pressure on malaria vectors. However, further research could be considered in broad setting on different housing improvement and in the way how to scale-up for wider community.", "title": "Exploring the impact of house screening intervention on entomological indices and incidence of malaria in Arba Minch town, southwest Ethiopia: A randomized control trial.", "date": "2018-02-17"}, {"article_id": "36271422", "content": "The housing stock of rural sub-Saharan Africa is changing rapidly. With millions of new homes required over the coming decades, there is an opportunity to protect residents by screening homes from malaria mosquitoes. This study, undertaken in the Upper River Region of The Gambia, explores local perceptions of what a good house should provide for its inhabitants and responses to living in a house that has been modified as part of a randomized control trial designed to assess whether improved housing provided additional protection against clinical malaria in children (the RooPfs trial).\nThis descriptive, exploratory study was undertaken over 22\u00a0months using mixed-methods (informal conversations, observations, focus group discussions, photovoice, and a questionnaire survey) in a parallel convergent design. Analysis was conducted across the data sets using a framework approach. Following coding, the textual data were charted by a priori and emerging themes. These themes were compared with the quantitative survey results. The nature and range of views about housing and the RooPfs study modifications and the relationships among them were identified and described.\nThe data were derived from a total of 35 sets of observations and informal conversations in 10 villages, 12 discussions with the photovoice photographers, 26 focus group discussions (across 13 villages) and 391 completed questionnaires. The study participants described a 'good house' as one with a corrugate-metal roof, cement walls (preferably cement block, but mud block covered with cement plaster was also an acceptable and cheaper substitute) and well-fitting doors. These features align with local perceptions of a modern house that provides social status and protection from physical harms. The RooPfs modifications were largely appreciated, although poor workmanship caused concerns that houses had become insecure. However, the long-term trusting relationship with the implementing institution and the actions taken to rectify problems provided reassurance and enhanced acceptability.\nIn developing housing to address population needs in Africa, attention should be paid to local perceptions of what is required to make a house secure for its inhabitants, as well as providing a healthy environment.", "title": "House screening for malaria control: views and experiences of participants in the RooPfs trial.", "date": "2022-10-23"}, {"article_id": "34022912", "content": "Current standard interventions are not universally sufficient for malaria elimination. The effects of community-based house improvement (HI) and larval source management (LSM) as supplementary interventions to the Malawi National Malaria Control Programme (NMCP) interventions were assessed in the context of an intensive community engagement programme.\nThe study was a two-by-two factorial, cluster-randomized controlled trial in Malawi. Village clusters were randomly assigned to four arms: a control arm; HI; LSM; and HI\u2009+\u2009LSM. Malawi NMCP interventions and community engagement were used in all arms. Household-level, cross-sectional surveys were conducted on a rolling, 2-monthly basis to measure parasitological and entomological outcomes over 3\u00a0years, beginning with one baseline year. The primary outcome was the entomological inoculation rate (EIR). Secondary outcomes included mosquito density, Plasmodium falciparum prevalence, and haemoglobin levels. All outcomes were assessed based on intention to treat, and comparisons between trial arms were conducted at both cluster and household level.\nEighteen clusters derived from 53 villages with 4558 households and 20,013 people were randomly assigned to the four trial arms. The mean nightly EIR fell from 0.010 infectious bites per person (95% CI 0.006-0.015) in the baseline year to 0.001 (0.000, 0.003) in the last year of the trial. Over the full trial period, the EIR did not differ between the four trial arms (p\u2009=\u20090.33). Similar results were observed for the other outcomes: mosquito density and P. falciparum prevalence decreased over 3\u00a0years of sampling, while haemoglobin levels increased; and there were minimal differences between the trial arms during the trial period.\nIn the context of high insecticide-treated bed net use, neither community-based HI, LSM, nor HI\u2009+\u2009LSM contributed to further reductions in malaria transmission or prevalence beyond the reductions observed over two years across all four trial arms. This was the first trial, as far as the authors are aware, to test the potential complementary impact of LSM and/or HI beyond levels achieved by standard interventions. The unexpectedly low EIR values following intervention implementation indicated a promising reduction in malaria transmission for the area, but also limited the usefulness of this outcome for measuring differences in malaria transmission among the trial arms. Trial registration PACTR, PACTR201604001501493, Registered 3 March 2016, https://pactr.samrc.ac.za/ .", "title": "The effect of community-driven larval source management and house improvement on malaria transmission when added to the standard malaria control strategies in Malawi: a cluster-randomized controlled trial.", "date": "2021-05-24"}, {"article_id": "33838737", "content": "In malaria-endemic areas, residents of modern houses have less malaria than those living in traditional houses. We aimed to assess whether children in The Gambia received an incremental benefit from improved housing, where current best practice of insecticide-treated nets, indoor residual spraying, seasonal malaria chemoprevention in children younger than 5 years, and prompt treatment against clinical malaria was in place.\nIn this randomised controlled study, 800 households with traditional thatched-roofed houses were randomly selected from 91 villages in the Upper River Region of The Gambia. Within each village, equal numbers of houses were randomly allocated to the control and intervention groups using a sampling frame. Houses in the intervention group were modified with metal roofs and screened doors and windows, whereas houses in the control group received no modifications. In each group, clinical malaria in children aged 6 months to 13 years was monitored by active case detection over 2 years (2016-17). We did monthly collections from indoor light traps to estimate vector densities. Primary endpoints were the incidence of clinical malaria in study children with more than 50% of observations each year and household vector density. The trial is registered at ISRCTN02622179.\nIn June, 2016, 785 houses had one child each recruited into the study (398 in unmodified houses and 402 in modified houses). 26 children in unmodified houses and 28 children in modified houses did not have at least 50% of visits in a year and so were excluded from analysis. 38 children in unmodified houses were recruited after study commencement, as were 21 children in modified houses, meaning 410 children in unmodified houses and 395 in modified houses were included in the parasitological analyses. At the end of the study, 659 (94%) of 702 children were reported to have slept under an insecticide-treated net; 662 (88%) of 755 children lived in houses that received indoor residual spraying; and 151 (90%) of 168 children younger than 5 years had seasonal malaria chemoprevention. Incidence of clinical malaria was 0\u00b712 episodes per child-year in children in the unmodified houses and 0\u00b720 episodes per child-year in the modified houses (unadjusted incidence rate ratio [RR] 1\u00b768 [95% CI 1\u00b711-2\u00b755], p=0\u00b7014). Household vector density was 3\u00b730 Anopheles gambiae per house per night in the unmodified houses compared with 3\u00b760 in modified houses (unadjusted RR 1\u00b728 [0\u00b787-1\u00b789], p=0\u00b721).\nImproved housing did not provide protection against clinical malaria in this area of low seasonal transmission with high coverage of insecticide-treated nets, indoor residual spraying, and seasonal malaria chemoprevention.\nGlobal Health Trials funded by Medical Research Council, UK Department for International Development, and Wellcome Trust.", "title": "Improved housing versus usual practice for additional protection against clinical malaria in The Gambia (RooPfs): a household-randomised controlled trial.", "date": "2021-04-12"}, {"article_id": "34120608", "content": "House improvement (HI) to prevent mosquito house entry, and larval source management (LSM) targeting aquatic mosquito stages to prevent development into adult forms, are promising complementary interventions to current malaria vector control strategies. Lack of evidence on costs and cost-effectiveness of community-led implementation of HI and LSM has hindered wide-scale adoption. This study presents an incremental cost analysis of community-led implementation of HI and LSM, in a cluster-randomized, factorial design trial, in addition to standard national malaria control interventions in a rural area (25,000 people), in southern Malawi.\nIn the trial, LSM comprised draining, filling, and Bacillus thuringiensis israelensis-based larviciding, while house improvement (henceforth HI) involved closing of eaves and gaps on walls, screening windows/ventilation spaces with wire mesh, and doorway modifications. Communities implemented all interventions. Costs were estimated retrospectively using the 'ingredients approach', combining 'bottom-up' and 'top-down approaches', from the societal perspective. To estimate the cost of independently implementing each intervention arm, resources shared between trial arms (e.g. overheads) were allocated to each consuming arm using proxies developed based on share of resource input quantities consumed. Incremental implementation costs (in 2017 US$) are presented for HI-only, LSM-only and HI\u2009+\u2009LSM arms. In sensitivity analyses, the effect of varying costs of important inputs on estimated costs was explored.\nThe total economic programme costs of community-led HI and LSM implementation was $626,152. Incremental economic implementation costs of HI, LSM and HI\u2009+\u2009LSM were estimated as $27.04, $25.06 and $33.44, per person per year, respectively. Project staff, transport and labour costs, but not larvicide or screening material, were the major cost drivers across all interventions. Costs were sensitive to changes in staff costs and population covered.\nIn the trial, the incremental economic costs of community-led HI and LSM implementation were high compared to previous house improvement and LSM studies. Several factors, including intervention design, year-round LSM implementation and low human population density could explain the high costs. The factorial trial design necessitated use of proxies to allocate costs shared between trial arms, which limits generalizability where different designs are used. Nevertheless, costs may inform planners of similar intervention packages where cost-effectiveness is known. Trial registration Not applicable. The original trial was registered with The Pan African Clinical Trials Registry on 3 March 2016, trial number PACTR201604001501493.", "title": "Cost of community-led larval source management and house improvement for malaria control: a cost analysis within a cluster-randomized trial in a rural district in Malawi.", "date": "2021-06-15"}, {"article_id": "19732949", "content": "House screening should protect people against malaria. We assessed whether two types of house screening--full screening of windows, doors, and closing eaves, or installation of screened ceilings--could reduce house entry of malaria vectors and frequency of anaemia in children in an area of seasonal malaria transmission.\nDuring 2006 and 2007, 500 occupied houses in and near Farafenni town in The Gambia, an area with low use of insecticide-treated bednets, were randomly assigned to receive full screening, screened ceilings, or no screening (control). Randomisation was done by computer-generated list, in permuted blocks of five houses in the ratio 2:2:1. Screening was not treated with insecticide. Exposure to mosquitoes indoors was assessed by fortnightly light trap collections during the transmission season. Primary endpoints included the number of female Anopheles gambiae sensu lato mosquitoes collected per trap per night. Secondary endpoints included frequency of anaemia (haemoglobin concentration <80 g/L) and parasitaemia at the end of the transmission season in children (aged 6 months to 10 years) who were living in the study houses. Analysis was by modified intention to treat (ITT), including all randomised houses for which there were some outcome data and all children from those houses who were sampled for haemoglobin and parasitaemia. This study is registered as an International Standard Randomised Controlled Trial, number ISRCTN51184253.\n462 houses were included in the modified ITT analysis (full screening, n=188; screened ceilings, n=178; control, n=96). The mean number of A gambiae caught in houses without screening was 37.5 per trap per night (95% CI 31.6-43.3), compared with 15.2 (12.9-17.4) in houses with full screening (ratio of means 0.41, 95% CI 0.31-0.54; p<0.0001) and 19.1 (16.1-22.1) in houses with screened ceilings (ratio 0.53, 0.40-0.70; p<0.0001). 755 children completed the study, of whom 731 had complete clinical and covariate data and were used in the analysis of clinical outcomes. 30 (19%) of 158 children from control houses had anaemia, compared with 38 (12%) of 309 from houses with full screening (adjusted odds ratio [OR] 0.53, 95% CI 0.29-0.97; p=0.04), and 31 (12%) of 264 from houses with screened ceilings (OR 0.51, 0.27-0.96; p=0.04). Frequency of parasitaemia did not differ between intervention and control groups.\nHouse screening substantially reduced the number of mosquitoes inside houses and could contribute to prevention of anaemia in children.\nMedical Research Council.", "title": "Effect of two different house screening interventions on exposure to malaria vectors and on anaemia in children in The Gambia: a randomised controlled trial.", "date": "2009-09-08"}]}
{"original_review": "32104914", "question_data": [{"question_id": 101, "question": "Is global cognitive function level at the end of intervention higher, lower, or the same when comparing computerised cognitive training (CCT) of at least 12 weeks to active control interventions?", "answer": "higher", "evidence_quality": "low", "fulltext_required": "no", "comment": "", "relevant_sources": ["21311196", "29261218"]}, {"question_id": 102, "question": "Is speed of processing at the end of intervention higher, lower, or the same when comparing computerised cognitive training (CCT) of at least 12 weeks to active control interventions?", "answer": "uncertain effect", "evidence_quality": "very low", "fulltext_required": "yes", "comment": "\"We downgraded the evidence for imprecision, inconsistency, and risk of bias. Therefore we are very uncertain of this result.\"", "relevant_sources": ["23531885", "27698558"]}, {"question_id": 103, "question": "Is episodic memory at the end of intervention higher, lower, or the same when comparing computerised cognitive training (CCT) of at least 12 weeks to no intervention?", "answer": "higher", "evidence_quality": "low", "fulltext_required": "no", "comment": "", "relevant_sources": ["20418350"]}, {"question_id": 104, "question": "Is speed of processing at the end of intervention higher, lower, or the same when comparing computerised cognitive training (CCT) of at least 12 weeks to no intervention?", "answer": "uncertain effect", "evidence_quality": "very low", "fulltext_required": "yes", "comment": "\"We downgraded the evidence for imprecision, inconsistency, and risk of bias. Therefore we are very uncertain of this result.\"", "relevant_sources": ["23531885", "25511081"]}, {"question_id": 105, "question": "Is working memory at the end of intervention higher, lower, or the same when comparing computerised cognitive training (CCT) of at least 12 weeks to no intervention?", "answer": "no difference", "evidence_quality": "low", "fulltext_required": "yes", "comment": "", "relevant_sources": ["23531885"]}, {"question_id": 230, "question": "Is global cognitive function at the end of intervention higher, lower, or the same when comparing computerised cognitive training (CCT) of at least 12 weeks to no intervention?", "answer": "insufficient data", "evidence_quality": "", "fulltext_required": "no", "comment": "", "relevant_sources": ["23531885", "25511081", "20418350"]}, {"question_id": 231, "question": "Is the rate of activities of daily living higher, lower, or the same when comparing computerised cognitive training (CCT) of at least 12 weeks to no intervention?", "answer": "insufficient data", "evidence_quality": "", "fulltext_required": "no", "comment": "", "relevant_sources": ["23531885", "25511081", "20418350"]}, {"question_id": 232, "question": "Is the rate of activities of daily living higher, lower, or the same when comparing computerised cognitive training (CCT) of at least 12 weeks to active control interventions?", "answer": "insufficient data", "evidence_quality": "", "fulltext_required": "no", "comment": "", "relevant_sources": ["27698558", "29261218", "21615936", "26417460", "21311196", "23531885"]}, {"question_id": 233, "question": "Is the risk of adverse effects higher, lower, or the same when comparing computerised cognitive training (CCT) of at least 12 weeks to no intervention?", "answer": "insufficient data", "evidence_quality": "", "fulltext_required": "no", "comment": "", "relevant_sources": ["23531885", "25511081", "20418350"]}, {"question_id": 234, "question": "Is the risk of adverse effects higher, lower, or the same when comparing computerised cognitive training (CCT) of at least 12 weeks to active control interventions?", "answer": "insufficient data", "evidence_quality": "", "fulltext_required": "no", "comment": "", "relevant_sources": ["27698558", "29261218", "21615936", "26417460", "21311196", "23531885"]}, {"question_id": 235, "question": "Is quality of life higher, lower, or the same when comparing computerised cognitive training (CCT) of at least 12 weeks to no intervention?", "answer": "insufficient data", "evidence_quality": "", "fulltext_required": "no", "comment": "", "relevant_sources": ["23531885", "25511081", "20418350"]}, {"question_id": 236, "question": "Is quality of life higher, lower, or the same when comparing computerised cognitive training (CCT) of at least 12 weeks to active control interventions?", "answer": "insufficient data", "evidence_quality": "", "fulltext_required": "no", "comment": "", "relevant_sources": ["27698558", "29261218", "21615936", "26417460", "21311196", "23531885"]}], "sources": [{"article_id": "25805989", "content": "Computerized cognitive training (CCT) is a safe and inexpensive intervention to enhance cognitive performance in the elderly. However, the neural underpinning of CCT-induced effects and the timecourse by which such neural changes occur are unknown. Here, we report on results from a pilot study of healthy older adults who underwent three 1-h weekly sessions of either multidomain CCT program (n = 7) or an active control intervention (n = 5) over 12 weeks. Multimodal magnetic resonance imaging (MRI) scans and cognitive assessments were performed at baseline and after 9 and 36 h of training. Voxel-based structural analysis revealed a significant Group \u00d7 Time interaction in the right post-central gyrus indicating increased gray matter density in the CCT group compared to active control at both follow-ups. Across the entire sample, there were significant positive correlations between changes in the post-central gyrus and change in global cognition after 36 h of training. A post-hoc vertex-based analysis found a significant between-group difference in rate of thickness change between baseline and post-training in the left fusiform gyrus, as well as a large cluster in the right parietal lobe covering the supramarginal and post-central gyri. Resting-state functional connectivity between the posterior cingulate and the superior frontal gyrus, and between the right hippocampus and the superior temporal gyrus significantly differed between the two groups after 9 h of training and correlated with cognitive change post-training. No significant interactions were found for any of the spectroscopy and diffusion tensor imaging data. Though preliminary, our results suggest that functional change may precede structural and cognitive change, and that about one-half of the structural change occurs within the first 9 h of training. Future studies are required to determine the role of these brain changes in the mechanisms underlying CCT-induced cognitive effects.", "title": "Cognitive training-induced short-term functional and long-term structural plastic change is related to gains in global cognition in healthy older adults: a pilot study.", "date": "2015-03-26"}, {"article_id": "25511081", "content": "Exercise interventions often do not combine physical and cognitive training. However, this combination is assumed to be more beneficial in improving walking and cognitive functioning compared to isolated cognitive or physical training.\nA multicenter parallel randomized controlled trial was conducted to compare a motor to a cognitive-motor exercise program. A total of 182 eligible residents of homes-for-the-aged (n\u2009=\u2009159) or elderly living in the vicinity of the homes (n\u2009=\u200923) were randomly assigned to either strength-balance (SB) or strength-balance-cognitive (SBC) training. Both groups conducted similar strength-balance training during 12 weeks. SBC additionally absolved computerized cognitive training. Outcomes were dual task costs of walking, physical performance, simple reaction time, executive functions, divided attention, fear of falling and fall rate. Participants were analysed with an intention to treat approach.\nThe 182 participants (mean age\u2009\u00b1\u2009SD: 81.5\u2009\u00b1\u20097.3 years) were allocated to either SB (n\u2009=\u200998) or SBC (n\u2009=\u200984). The attrition rate was 14.3%. Interaction effects were observed for dual task costs of step length (preferred walking speed: F(1,174)\u2009=\u20094.94, p\u2009=\u20090.028, \u03b72\u2009=\u20090.027, fast walking speed: F(1,166)\u2009=\u20096.14, p\u2009=\u20090.009, \u03b72\u2009=\u20090.040) and dual task costs of the standard deviation of step length (F(1,166)\u2009=\u20096.14, p\u2009=\u20090.014, \u03b72\u2009=\u20090.036), in favor of SBC. Significant interactions in favor of SBC revealed for in gait initiation (F(1,166)\u2009=\u20099.16, p\u2009=\u20090.003, \u03b72\u2009=\u20090.052), 'reaction time' (F(1,180)\u2009=\u20095.243, p\u2009=\u20090.023, \u03b7\u00b2\u2009=\u20090.028) & 'missed answers' (F(1,180)\u2009=\u200911.839, p\u2009=\u20090.001, \u03b7\u00b2\u2009=\u20090.062) as part of the test for divided attention. Within-group comparison revealed significant improvements in dual task costs of walking (preferred speed; velocity (p\u2009=\u20090.002), step time (p\u2009=\u20090.018), step length (p\u2009=\u20090.028), fast speed; velocity (p\u2009<\u20090.001), step time (p\u2009=\u20090.035), step length (p\u2009=\u20090.001)), simple reaction time (p\u2009<\u20090.001), executive functioning (Trail making test B; p\u2009<\u20090.001), divided attention (p\u2009<\u20090.001), fear of falling (p\u2009<\u20090.001), and fall rate (p\u2009<\u20090.001).\nCombining strength-balance training with specific cognitive training has a positive additional effect on dual task costs of walking, gait initiation, and divided attention. The findings further confirm previous research showing that strength-balance training improves executive functions and reduces falls.\nThis trial has been registered under ISRCTN75134517.", "title": "Strength-balance supplemented with computerized cognitive training to improve dual task gait and divided attention in older adults: a multicenter randomized-controlled trial.", "date": "2014-12-17"}, {"article_id": "21615936", "content": "The efficacy of non-pharmacological intervention approaches such as physical activity, strength, and cognitive training for improving brain health has not been established. Before definitive trials are mounted, important design questions on participation/adherence, training and interventions effects must be answered to more fully inform a full-scale trial.\nSHARP-P was a single-blinded randomized controlled pilot trial of a 4-month physical activity training intervention (PA) and/or cognitive training intervention (CT) in a 2 \u00d7 2 factorial design with a health education control condition in 73 community-dwelling persons, aged 70-85 years, who were at risk for cognitive decline but did not have mild cognitive impairment.\nIntervention attendance rates were higher in the CT and PACT groups: CT: 96%, PA: 76%, PACT: 90% (p=0.004), the interventions produced marked changes in cognitive and physical performance measures (p\u22640.05), and retention rates exceeded 90%. There were no statistically significant differences in 4-month changes in composite scores of cognitive, executive, and episodic memory function among arms. Four-month improvements in the composite measure increased with age among participants assigned to physical activity training but decreased with age for other participants (intervention*age interaction p=0.01). Depending on the choice of outcome, two-armed full-scale trials may require fewer than 1,000 participants (continuous outcome) or 2,000 participants (categorical outcome).\nGood levels of participation, adherence, and retention appear to be achievable for participants through age 85 years. Care should be taken to ensure that an attention control condition does not attenuate intervention effects. Depending on the choice of outcome measures, the necessary sample sizes to conduct four-year trials appear to be feasible.\nClinicaltrials.gov Identifier: NCT00688155.", "title": "Designing clinical trials for assessing the effects of cognitive training and physical activity interventions on cognitive outcomes: the Seniors Health and Activity Research Program Pilot (SHARP-P) study, a randomized controlled trial.", "date": "2011-05-28"}, {"article_id": "27698558", "content": "Physical exercise and cognitive training have been shown to enhance cognition among older adults. However, few studies have looked at the potential synergetic effects of combining physical and cognitive training in a single study. Prior trials on combined training have led to interesting yet equivocal results. The aim of this study was to examine the effects of combined physical and cognitive interventions on physical fitness and neuropsychological performance in healthy older adults.\nSeventy-six participants were randomly assigned to one of four training combinations using a 2\u00d72 factorial design. The physical intervention was a mixed aerobic and resistance training program, and the cognitive intervention was a dual-task (DT) training program. Stretching and toning exercises and computer lessons were used as active control conditions. Physical and cognitive measures were collected pre- and postintervention.\nAll groups showed equivalent improvements in measures of functional mobility. The aerobic-strength condition led to larger effect size in lower body strength, independently of cognitive training. All groups showed improved speed of processing and inhibition abilities, but only participants who took part in the DT training, independently of physical training, showed increased task-switching abilities. The level of functional mobility after intervention was significantly associated with task-switching abilities.\nCombined training did not yield synergetic effects. However, DT training did lead to transfer effects on executive performance in neuropsychological tests. Both aerobic-resistance training and stretching-toning exercises can improve functional mobility in older adults.", "title": "Effects of combined physical and cognitive training on fitness and neuropsychological outcomes in healthy older adults.", "date": "2016-10-05"}, {"article_id": "23531885", "content": "Cognitive training and aerobic training are known to improve cognitive functions. To examine the separate and combined effects of such training on cognitive performance, four groups of healthy older adults embarked on a 4 months cognitive and/or mild aerobic training. A first group [n = 33, mean age = 80 (66-90)] engaged in cognitive training, a second [n = 29, mean age = 81 (65-89)] in mild aerobic training, a third [n = 29, mean age = 79 (70-93)] in the combination of both, and a fourth [n = 31, mean age = 79 (71-92)] control group engaged in book-reading activity. The outcome was a well-validated multi-domain computerized cognitive evaluation for older adults. The results indicate that, when compared to older adults who did not engage in cognitive training (the mild aerobic and control groups) older adults who engaged in cognitive training (separate or combined training groups) showed significant improvement in cognitive performance on Hand-Eye Coordination, Global Visual Memory (GVM; working memory and long-term memory), Speed of Information Processing, Visual Scanning, and Naming. Indeed, individuals who did not engage in cognitive training showed no such improvements. Those results suggest that cognitive training is effective in improving cognitive performance and that it (and not mild aerobic training) is driving the improvement in the combined condition. Results are discussed in terms of the special circumstances of aerobic and cognitive training for older adults who are above 80 years of age.", "title": "Does combined cognitive training and physical activity training enhance cognitive abilities more than either alone? A four-condition randomized controlled trial among healthy older adults.", "date": "2013-03-28"}, {"article_id": "20418350", "content": "Several reports suggest beneficial impacts of either physical or mental activity on cognitive function in old age. However, the differential effects of complex mental and physical activities on cognitive performance in humans remain to be clarified.\nThis randomized controlled trial evaluates a cognitive and a physical standardized 6-month activity intervention (3 x 1.5 h/wk) conducted in Berlin (Germany). Two hundred fifty nine healthy women aged 70-93 years were randomized to a computer course (n = 92), an exercise course (n = 91), or a control group (n = 76), of whom 230 completed the 6-month assessment. Group differences in change over a period of 6 months in episodic memory (story recall, possible range, 0-21; word recall, possible range, 0-16), executive control (working memory, ie, time quotient of Trail Making Tests B/A), and verbal fluency were evaluated by analyses of covariance (intention to treat) adjusting for baseline, fluid intelligence, and educational level.\nIn contrast to the control group, both the exercise group, DeltaM (SD) = 2.09 (2.66), p < .001, and the computer group, DeltaM (SD) =1.89 (2.88), p < .001, showed improved delayed story recall. They maintained performance in delayed word recall and working memory (time measure) as opposed to the control group that showed a decline, DeltaM (SD) = -0.91 (2.15), p = .001, and DeltaM (SD) = 0.24 (0.68), p = .04, respectively.\nIn healthy older women, participation in new stimulating activities contributes to cognitive fitness and might delay cognitive decline. Exercise and computer classes seem to generate equivalent beneficial effects.", "title": "Complex mental and physical activity in older women and cognitive performance: a 6-month randomized controlled trial.", "date": "2010-04-27"}, {"article_id": "26417460", "content": "Increasing research has evidenced that our brain retains a capacity to change in response to experience until late adulthood. This implies that cognitive training can possibly ameliorate age-associated cognitive decline by inducing training-specific neural plastic changes at both neural and behavioral levels. This longitudinal study examined the behavioral effects of a systematic thirteen-week cognitive training program on attention and working memory of older adults who were at risk of cognitive decline. These older adults were randomly assigned to the Cognitive Training Group (n = 109) and the Active Control Group (n = 100). Findings clearly indicated that training induced improvement in auditory and visual-spatial attention and working memory. The training effect was specific to the experience provided because no significant difference in verbal and visual-spatial memory between the two groups was observed. This pattern of findings is consistent with the prediction and the principle of experience-dependent neuroplasticity. Findings of our study provided further support to the notion that the neural plastic potential continues until older age. The baseline cognitive status did not correlate with pre- versus posttraining changes to any cognitive variables studied, suggesting that the initial cognitive status may not limit the neuroplastic potential of the brain at an old age.", "title": "Neural Plastic Effects of Cognitive Training on Aging Brain.", "date": "2015-09-30"}, {"article_id": "21311196", "content": "Many studies have suggested that cognitive training can result in cognitive gains in healthy older adults. We investigated whether personalized computerized cognitive training provides greater benefits than those obtained by playing conventional computer games.\nThis was a randomized double-blind interventional study. Self-referred healthy older adults (n = 155, 68 \u00b1 7 years old) were assigned to either a personalized, computerized cognitive training or to a computer games group. Cognitive performance was assessed at baseline and after 3 months by a neuropsychological assessment battery. Differences in cognitive performance scores between and within groups were evaluated using mixed effects models in 2 approaches: adherence only (AO; n = 121) and intention to treat (ITT; n = 155).\nBoth groups improved in cognitive performance. The improvement in the personalized cognitive training group was significant (p < 0.03, AO and ITT approaches) in all 8 cognitive domains. However, in the computer games group it was significant (p < 0.05) in only 4 (AO) or 6 domains (ITT). In the AO analysis, personalized cognitive training was significantly more effective than playing games in improving visuospatial working memory (p = 0.0001), visuospatial learning (p = 0.0012) and focused attention (p = 0.0019).\nPersonalized, computerized cognitive training appears to be more effective than computer games in improving cognitive performance in healthy older adults. Further studies are needed to evaluate the ecological validity of these findings.", "title": "Computer-based, personalized cognitive training versus classical computer games: a randomized double-blind prospective trial of cognitive stimulation.", "date": "2011-02-12"}, {"article_id": "29261218", "content": "Home-based computerised cognitive training (CCT) is ineffective at enhancing global cognition, a key marker of cognitive ageing.\nTo test the effectiveness of supervised, group-based, multidomain CCT on global cognition in older adults and to characterise the dose-response relationship during and after training.\nA randomised, double-blind, longitudinal, active-controlled trial.\nCommunity-based training centre in Sydney, Australia Participants: Eighty nondemented community-dwelling older adults (mean age = 72.1, 68.8% females) with multiple dementia risk factors but no major neuropsychiatric or sensory disorder. Of the 80 participants admitted to the study, 65 completed post-training assessment and 55 were followed up one year after training cessation.\nThirty-six group-based sessions over three months of either CCT targeting memory, speed, attention, language and reasoning tasks, or active control training comprising audiovisual educational exercises.\nPrimary outcome was change from baseline in global cognition as defined by a composite score of memory, speed and executive function. Secondary outcome was 15-month change in Bayer Activities of Daily Living from baseline to one year post-training.\nIntention-to-treat analyses revealed significant effects on global cognition in the cognitive training group compared to active control after three weeks of training (ES = 0.33, P=.039) that increased after 3 months of training (ES = 0.49, P=.003) and persisted three months after training cessation (ES = 0.30, P=0.023). Significant and durable improvements were also noted in memory and processing speed. Dose-response characteristics differed among cognitive domains. Training effects waned gradually but residual gains were noted twelve months post-training. No significant effects on activities of daily living were noted and there were no adverse effects.\nIn older adults with multiple dementia risk factors, group-based CCT is a safe and effective intervention for enhancing overall cognition, memory and processing speed. Dose-response relationships vary for each cognitive domain, vital information for clinical and community implementation and further trial design.", "title": "The Timecourse of Global Cognitive Gains from Supervised Computer-Assisted Cognitive Training: A Randomised, Active-Controlled Trial in Elderly with Multiple Dementia Risk Factors.", "date": "2014-01-01"}]}
{"original_review": "38197546", "question_data": [{"question_id": 106, "question": "Is the risk of cholera at two-year follow-up higher, lower, or the same when comparing two doses of Dukoral with or without a booster dose to placebo?", "answer": "lower", "evidence_quality": "high", "fulltext_required": "no", "comment": "", "relevant_sources": ["7967990", "10823767"]}, {"question_id": 107, "question": "Is the risk of cholera at two-year follow-up higher, lower, or the same when comparing two doses of Shanchol to placebo?", "answer": "lower", "evidence_quality": "moderate", "fulltext_required": "no", "comment": "", "relevant_sources": ["19819004", "26164097"]}, {"question_id": 108, "question": "Is the risk of cholera at five-year follow-up higher, lower, or the same when comparing two doses of Shanchol to placebo?", "answer": "lower", "evidence_quality": "high", "fulltext_required": "no", "comment": "", "relevant_sources": ["24140390", "29463233"]}, {"question_id": 109, "question": "Is the risk of cholera at two-year follow-up higher, lower, or the same when comparing one dose of Shanchol to placebo?", "answer": "lower", "evidence_quality": "high", "fulltext_required": "no", "comment": "", "relevant_sources": ["29550406"]}, {"question_id": 110, "question": "Is the risk of severe dehydrating cholera at two-year follow-up higher, lower, or the same when comparing one dose of Shanchol to placebo?", "answer": "lower", "evidence_quality": "high", "fulltext_required": "no", "comment": "", "relevant_sources": ["29550406"]}], "sources": [{"article_id": "10823767", "content": "The protective efficacy of an oral inactivated whole cell Vibrio cholerae plus recombinant B subunit cholera vaccine was determined against El Tor cholera among Peruvian children and adults (2-65 years old) in a randomized, double-blind manner. Study subjects received 2 doses of vaccine or placebo 2 weeks apart, followed by a booster dose 10 months later. Surveillance for cholera was performed actively, with 2 visits per week to each household, and passively, at a local hospital. Stool samples were collected during diarrhea episodes and were cultured for V. cholerae. A total of 17,799 persons received 2 doses of vaccine or placebo, and 14,997 of these persons received the booster dose. After 2 doses (first surveillance period), V. cholerae biotype O1 was isolated from 17 vaccinees and 16 placebo recipients, demonstrating vaccine efficacy (VE) of -4%. After 3 doses (second surveillance period), V. cholerae O1 was isolated from 13 vaccinees and 32 placebo recipients, demonstrating VE of 61% (95% confidence interval \u00bfCI, 28%-79%). In the second surveillance period, the VE for illness requiring hospitalization was 82% (95% CI, 27%-96%). VE was also higher for persons >15 years old (VE, 72%; 95% CI, 28%-89%).", "title": "Two-year study of the protective efficacy of the oral whole cell plus recombinant B subunit cholera vaccine in Peru.", "date": "2000-05-24"}, {"article_id": "34146473", "content": "Killed whole-cell oral cholera vaccines (OCVs) are widely used for prevention of cholera in developing countries. However, few studies have evaluated the protection conferred by internationally recommended OCVs for durations beyond 2 years of follow-up.\nIn this study, we followed up the participants of a cluster-randomised controlled trial for 2 years after the end of the original trial. Originally, we had randomised 90 geographical clusters in Dhaka slums in Bangladesh in equal numbers (1:1:1) to a two-dose regimen of OCV alone (targeted to people aged 1 year or older), a two-dose regimen of OCV plus a water-sanitation-hygiene (WASH) intervention, or no intervention. There was no masking of group assignment. The WASH intervention conferred little additional protection to OCV and was discontinued at 2 years of follow-up. Surveillance for severe cholera was continued for 4 years. Because of the short duration and effect of the WASH intervention, we combined the two OCV intervention groups. The primary outcomes were OCV overall protection (protection of all members of the intervention clusters) and total protection (protection of individuals who got vaccinated in the intervention clusters) against severe cholera, which we assessed by multivariable survival models appropriate for cluster-randomised trials. This trial is registered on ClinicalTrials.gov, NCT01339845.\nThe study was done between April 17, 2011, and Nov 1, 2015. 268\u2009896 participants were present at the time of the first dose, with 188\u2009206 in the intervention group and 80\u2009690 in the control group. OCV coverage of the two groups receiving OCV was 66% (123\u2009659 of 187\u2009214 participants). During 4 years of follow-up, 441 first episodes of severe cholera were detected (243 episodes in the vaccinated groups and as 198 episodes in the unvaccinated group). Overall OCV protection was 36% (95% CI 19 to 49%) and total OCV protection was 46% (95% CI 32 to 58). Cumulative total vaccine protection was notably lower for people vaccinated before the age of 5 years (24%; -30 to 56) than for people vaccinated at age 5 years or older (49%; 35 to 60), although the differences in protection for the two age groups were not significant (p=0\u00b73308). Total vaccine protection dropped notably (p=0\u00b70115) after 3 years in children vaccinated at 1-4 years of age.\nThese findings provide further evidence of long-term effectiveness of killed whole-cell OCV, and therefore further support for the use of killed whole-cell OCVs to control endemic cholera, but indicate that protection is shorter-lived in children vaccinated before the age of 5 years than in people vaccinated at the age of 5 years or older.\nBill & Melinda Gates Foundation.\nFor the Bengali translation of the abstract see Supplementary Materials section.", "title": "Effectiveness of a killed whole-cell oral cholera vaccine in Bangladesh: further follow-up of a cluster-randomised trial.", "date": "2021-06-20"}, {"article_id": "28196715", "content": "Pregnant women are vulnerable to complications of cholera. Killed oral cholera vaccines (OCV) are not recommended for pregnant women though there is no evidence of harmful effects during pregnancy. We evaluated the effect of a killed OCV, Shanchol\u2122, on pregnancy outcomes during an effectiveness trial of the vaccine in urban Bangladesh.\nIndividuals \u2a7e1year were invited to participate in the trial, conducted in 2011 in Dhaka, Bangladesh. Pregnancy by history was an exclusion criterion and all women of reproductive age (15-49years) were verbally questioned about pregnancy at enrollment and prior to vaccination. Out of 48,414 women of reproductive age 286 women received the OCV unknowingly while pregnant. Out of these, we could recruit 69 women defined as exposed to OCV. Accordingly, we selected 69 pregnant women randomly from those who did not take the OCV (non-exposed to OCV). We evaluated adverse pregnancy outcome (spontaneous miscarriages, still births, or congenital malformations) between those who were exposed to OCV and those who were not exposed to OCV.\nAbout 16% of pregnant women exposed to OCV had pregnancy loss, as compared to 12% of unvaccinated pregnant women (P=0.38). One congenital anomaly was observed and occurred in women non-exposed to OCV group. Models that adjusted for baseline characteristics that were unbalanced between the exposed and non-exposed groups, revealed a no elevation of risk of adverse pregnancy outcomes in vaccinees versus non-vaccinees (Adj. OR (95% CI): 0.45 (0.11-1.88).\nNo excess of adverse fetal outcomes associated with receipt of OCV was observed in this study.\nClinical Trials.gov number NCT01339845.", "title": "Safety of the oral cholera vaccine in pregnancy: Retrospective findings from a subgroup following mass vaccination campaign in Dhaka, Bangladesh.", "date": "2017-02-16"}, {"article_id": "19819004", "content": "Oral cholera vaccines consisting of killed whole cells have been available for many years, but they have not been used extensively in populations with endemic disease. An inexpensive, locally produced oral killed-whole-cell vaccine has been used in high-risk areas in Vietnam. To expand the use of this vaccine, it was modified to comply with WHO standards. We assessed the efficacy and safety of this modified vaccine in a population with endemic cholera.\nIn this double-blind trial, 107 774 non-pregnant residents of Kolkata, India, aged 1 year or older, were cluster-randomised by dwelling to receive two doses of either modified killed-whole-cell cholera vaccine (n=52 212; 1966 clusters) or heat-killed Escherichia coli K12 placebo (n=55 562; 1967 clusters), both delivered orally. Randomisation was done by computer-generated sequence in blocks of four. The primary endpoint was prevention of episodes of culture-confirmed Vibrio cholerae O1 diarrhoea severe enough for the patient to seek treatment in a health-care facility. We undertook an interim, per-protocol analysis at 2 years of follow-up that included individuals who received two completely ingested doses of vaccine or placebo. We assessed first episodes of cholera that occurred between 14 days and 730 days after receipt of the second dose. This study is registered with ClinicalTrials.gov, number NCT00289224.\n31 932 participants assigned to vaccine (1721 clusters) and 34 968 assigned to placebo (1757 clusters) received two doses of study treatment. There were 20 episodes of cholera in the vaccine group and 68 episodes in the placebo group (protective efficacy 67%; one-tailed 99% CI, lower bound 35%, p<0.0001). The vaccine protected individuals in age-groups 1.0-4.9 years, 5.0-14.9 years, and 15 years and older, and protective efficacy did not differ significantly between age-groups (p=0.28). We recorded no vaccine-related serious adverse events.\nThis modified killed-whole-cell oral vaccine, compliant with WHO standards, is safe, provides protection against clinically significant cholera in an endemic setting, and can be used in children aged 1.0-4.9 years, who are at highest risk of developing cholera in endemic settings.\nBill & Melinda Gates Foundation, Swedish International Development Cooperation Agency, Governments of South Korea, Sweden, and Kuwait.", "title": "Efficacy and safety of a modified killed-whole-cell oral cholera vaccine in India: an interim analysis of a cluster-randomised, double-blind, placebo-controlled trial.", "date": "2009-10-13"}, {"article_id": "22028938", "content": "Killed oral cholera vaccines (OCVs) have been licensed for use in developing countries, but protection conferred by licensed OCVs beyond two years of follow-up has not been demonstrated in randomized, clinical trials.\nWe conducted a cluster-randomized, placebo-controlled trial of a two-dose regimen of a low-cost killed whole cell OCV in residents 1 year of age and older living in 3,933 clusters in Kolkata, India. The primary endpoint was culture-proven Vibrio cholerae O1 diarrhea episodes severe enough to require treatment in a health care facility. Of the 66,900 fully dosed individuals (31,932 vaccinees and 34,968 placebo recipients), 38 vaccinees and 128 placebo-recipients developed cholera during three years of follow-up (protective efficacy 66%; one-sided 95%CI lower bound\u200a=\u200a53%, p<0.001). Vaccine protection during the third year of follow-up was 65% (one-sided 95%CI lower bound\u200a=\u200a44%, p<0.001). Significant protection was evident in the second year of follow-up in children vaccinated at ages 1-4 years and in the third year in older age groups.\nThe killed whole-cell OCV conferred significant protection that was evident in the second year of follow-up in young children and was sustained for at least three years in older age groups. Continued follow-up will be important to establish the vaccine's duration of protection.\nClinicalTrials.gov NCT00289224.", "title": "Efficacy of a low-cost, inactivated whole-cell oral cholera vaccine: results from 3 years of follow-up of a randomized, controlled trial.", "date": "2011-10-27"}, {"article_id": "29463233", "content": "Oral cholera vaccine (OCV) is a feasible tool to prevent or mitigate cholera outbreaks. A better understanding of the vaccine's efficacy among different age groups and how rapidly its protection wanes could help guide vaccination policy.\nTo estimate the level and duration of OCV efficacy, we re-analyzed data from a previously published cluster-randomized, double-blind, placebo controlled trial with five years of follow-up. We used a Cox proportional hazards model and modeled the potentially time-dependent effect of age categories on both vaccine efficacy and risk of infection in the placebo group. In addition, we investigated the impact of an outbreak period on model estimation.\nVaccine efficacy was 38% (95% CI: -2%,62%) for those vaccinated from ages 1 to under 5 years old, 85% (95% CI: 67%,93%) for those 5 to under 15 years, and 69% (95% CI: 49%,81%) for those vaccinated at ages 15 years and older. Among adult vaccinees, efficacy did not appear to wane during the trial, but there was insufficient data to assess the waning of efficacy among child vaccinees.\nThrough this re-analysis we were able to detect a statistically significant difference in OCV efficacy when the vaccine was administered to children under 5 years old vs. children 5 years and older. The estimated efficacies are more similar to the previously published analysis based on the first two years of follow-up than the analysis based on all five years.\nClinicalTrials.gov identifier NCT00289224.", "title": "Efficacy of a bivalent killed whole-cell cholera vaccine over five years: a re-analysis of a cluster-randomized trial.", "date": "2018-02-22"}, {"article_id": "24140390", "content": "Efficacy and safety of a two-dose regimen of bivalent killed whole-cell oral cholera vaccine (Shantha Biotechnics, Hyderabad, India) to 3 years is established, but long-term efficacy is not. We aimed to assess protective efficacy up to 5 years in a slum area of Kolkata, India.\nIn our double-blind, cluster-randomised, placebo-controlled trial, we assessed incidence of cholera in non-pregnant individuals older than 1 year residing in 3933 dwellings (clusters) in Kolkata, India. We randomly allocated participants, by dwelling, to receive two oral doses of modified killed bivalent whole-cell cholera vaccine or heat-killed Escherichia coli K12 placebo, 14 days apart. Randomisation was done by use of a computer-generated sequence in blocks of four. The primary endpoint was prevention of episodes of culture-confirmed Vibrio cholerae O1 diarrhoea severe enough for patients to seek treatment in a health-care facility. We identified culture-confirmed cholera cases among participants seeking treatment for diarrhoea at a study clinic or government hospital between 14 days and 1825 days after receipt of the second dose. We assessed vaccine protection in a per-protocol population of participants who had completely ingested two doses of assigned study treatment.\n69 of 31\u2008932 recipients of vaccine and 219 of 34\u2008968 recipients of placebo developed cholera during 5 year follow-up (incidence 2\u00b72 per 1000 in the vaccine group and 6\u00b73 per 1000 in the placebo group). Cumulative protective efficacy of the vaccine at 5 years was 65% (95% CI 52-74; p<0\u00b70001), and point estimates by year of follow-up suggested no evidence of decline in protective efficacy.\nSustained protection for 5 years at the level we reported has not been noted previously with other oral cholera vaccines. Established long-term efficacy of this vaccine could assist policy makers formulate rational vaccination strategies to reduce overall cholera burden in endemic settings.\nBill & Melinda Gates Foundation and the governments of South Korea and Sweden.", "title": "5 year efficacy of a bivalent killed whole-cell oral cholera vaccine in Kolkata, India: a cluster-randomised, double-blind, placebo-controlled trial.", "date": "2013-10-22"}, {"article_id": "26164097", "content": "Cholera is endemic in Bangladesh with epidemics occurring each year. The decision to use a cheap oral killed whole-cell cholera vaccine to control the disease depends on the feasibility and effectiveness of vaccination when delivered in a public health setting. We therefore assessed the feasibility and protective effect of delivering such a vaccine through routine government services in urban Bangladesh and evaluated the benefit of adding behavioural interventions to encourage safe drinking water and hand washing to vaccination in this setting.\nWe did this cluster-randomised open-label trial in Dhaka, Bangladesh. We randomly assigned 90 clusters (1:1:1) to vaccination only, vaccination and behavioural change, or no intervention. The primary outcome was overall protective effectiveness, assessed as the risk of severely dehydrating cholera during 2 years after vaccination for all individuals present at time of the second dose. This study is registered with ClinicalTrials.gov, number NCT01339845.\nOf 268,896 people present at baseline, we analysed 267,270: 94,675 assigned to vaccination only, 92,539 assigned to vaccination and behavioural change, and 80,056 assigned to non-intervention. Vaccine coverage was 65% in the vaccination only group and 66% in the vaccination and behavioural change group. Overall protective effectiveness was 37% (95% CI lower bound 18%; p=0\u00b7002) in the vaccination group and 45% (95% CI lower bound 24%; p=0\u00b7001) in the vaccination and behavioural change group. We recorded no vaccine-related serious adverse events.\nOur findings provide the first indication of the effect of delivering an oral killed whole-cell cholera vaccine to poor urban populations with endemic cholera using routine government services and will help policy makers to formulate vaccination strategies to reduce the burden of severely dehydrating cholera in such populations.\nBill & Melinda Gates Foundation.", "title": "Feasibility and effectiveness of oral cholera vaccine in an urban endemic setting in Bangladesh: a cluster randomised open-label trial.", "date": "2015-07-15"}, {"article_id": "7967990", "content": "The cholera epidemic in South America has reinforced the need for safe and effective oral vaccines. In a randomised, double-blind, placebo-controlled efficacy trial among 1563 Peruvian military recruits we have investigated the protective efficacy of an oral inactivated whole-cell/recombinant-B-subunit (WC/rBS) cholera vaccine. Participants were given two oral doses of cholera vaccine or Escherichia coli K12 placebo, with an interval of 7-14 days. 1426 (91%) subjects received the two prescribed doses and were followed up for a mean of 18 weeks (median 21 weeks). After vaccination, Vibrio cholerae O1 El Tor Ogawa was isolated from 17 subjects with diarrhoea. 16 of the cholera cases occurred 2 weeks or longer after the second dose of vaccine (14 placebo recipients, 2 vaccinees). We also detected 14 symptomless infections (11 [7 placebo recipients, 4 vaccinees]) 2 weeks or longer after the second dose. The vaccine had significant protective efficacy against cholera (86% [95% CI 37-97], p < 0.01) but not against symptomless infection (42% [-96 to 85]). All cholera cases were in people of blood group O, who made up 76% of the study population (p < 0.01). Two doses of WC/rBS vaccine, given 1 to 2 weeks apart, provide rapid, short-term protection against symptomatic cholera in adult South Americans, who are predominantly of blood group O. Long-term efficacy studies in Peruvian adults and children are under way.", "title": "Protective efficacy of oral whole-cell/recombinant-B-subunit cholera vaccine in Peruvian military recruits.", "date": "1994-11-05"}, {"article_id": "29550406", "content": "A single-dose regimen of inactivated whole-cell oral cholera vaccine (OCV) is attractive because it reduces logistical challenges for vaccination and could enable more people to be vaccinated. Previously, we reported the efficacy of a single dose of an OCV vaccine during the 6 months following dosing. Herein, we report the results of 2 years of follow-up.\nIn this placebo-controlled, double-blind trial done in Dhaka, Bangladesh, individuals aged 1 year or older with no history of receipt of OCV were randomly assigned to receive a single dose of inactivated OCV or oral placebo. The primary endpoint was a confirmed episode of non-bloody diarrhoea for which the onset was at least 7 days after dosing and a faecal culture was positive for Vibrio cholerae O1 or O139. Passive surveillance for diarrhoea was done in 13 hospitals or major clinics located in or near the study area for 2 years after the last administered dose. We assessed the protective efficacy of the OCV against culture-confirmed cholera occurring 7-730 days after dosing with both crude and multivariable per-protocol analyses. This trial is registered at ClinicalTrials.gov, number NCT02027207.\nBetween Jan 10, 2014, and Feb 4, 2014, 205\u2008513 people were randomly assigned to receive either vaccine or placebo, of whom 204\u2008700 (102\u2008552 vaccine recipients and 102\u2008148 placebo recipients) were included in the per-protocol analysis. 287 first episodes of cholera (109 among vaccine recipients and 178 among placebo recipients) were detected during the 2-year follow-up; 138 of these episodes (46 in vaccine recipients and 92 in placebo recipients) were associated with severe dehydration. The overall incidence rates of initial cholera episodes were 0\u00b722 (95% CI 0\u00b718 to 0\u00b727) per 100\u2008000 person-days in vaccine recipients versus 0\u00b736 (0\u00b731 to 0\u00b742) per 100\u2008000 person-days in placebo recipients (adjusted protective efficacy 39%, 95% CI 23 to 52). The overall incidence of severe cholera was 0\u00b709 (0\u00b707 to 0\u00b712) per 100\u2008000 person-days versus 0\u00b719 (0\u00b715 to 0\u00b723; adjusted protective efficacy 50%, 29 to 65). Vaccine protective efficacy was 52% (8 to 75) against all cholera episodes and 71% (27 to 88) against severe cholera episodes in participants aged 5 years to younger than 15 years. For participants aged 15 years or older, vaccine protective efficacy was 59% (42 to 71) against all cholera episodes and 59% (35 to 74) against severe cholera. The protection in the older age groups was sustained throughout the 2-year follow-up. In participants younger than 5 years, the vaccine did not show protection against either all cholera episodes (protective efficacy -13%, -68 to 25) or severe cholera episodes (-44%, -220 to 35).\nA single dose of the inactivated whole-cell OCV offered protection to older children and adults that was sustained for at least 2 years. The absence of protection of young children might reflect a lesser degree of pre-existing natural immunity in this age group.\nBill & Melinda Gates Foundation to the International Vaccine Institute.", "title": "Efficacy of a single-dose regimen of inactivated whole-cell oral cholera vaccine: results from 2 years of follow-up of a randomised trial.", "date": "2018-03-20"}, {"article_id": "27144848", "content": "A single-dose regimen of the current killed oral cholera vaccines that have been prequalified by the World Health Organization would make them more attractive for use against endemic and epidemic cholera. We conducted an efficacy trial of a single dose of the killed oral cholera vaccine Shanchol, which is currently given in a two-dose schedule, in an urban area in which cholera is highly endemic.\nNonpregnant residents of Dhaka, Bangladesh, who were 1 year of age or older were randomly assigned to receive a single dose of oral cholera vaccine or oral placebo. The primary outcome was vaccine protective efficacy against culture-confirmed cholera occurring 7 to 180 days after dosing. Prespecified secondary outcomes included protective efficacy against severely dehydrating culture-confirmed cholera during the same interval, against cholera and severe cholera occurring 7 to 90 versus 91 to 180 days after dosing, and against cholera and severe cholera according to age at baseline.\nA total of 101 episodes of cholera, 37 associated with severe dehydration, were detected among the 204,700 persons who received one dose of vaccine or placebo. The vaccine protective efficacy was 40% (95% confidence interval [CI], 11 to 60%; 0.37 cases per 1000 vaccine recipients vs. 0.62 cases per 1000 placebo recipients) against all cholera episodes, 63% (95% CI, 24 to 82%; 0.10 vs. 0.26 cases per 1000 recipients) against severely dehydrating cholera episodes, and 63% (95% CI, -39 to 90%), 56% (95% CI, 16 to 77%), and 16% (95% CI, -49% to 53%) against all cholera episodes among persons vaccinated at the age of 5 to 14 years, 15 or more years, and 1 to 4 years, respectively, although the differences according to age were not significant (P=0.25). Adverse events occurred at similar frequencies in the two groups.\nA single dose of the oral cholera vaccine was efficacious in older children (\u22655 years of age) and in adults in a setting with a high level of cholera endemicity. (Funded by the Bill and Melinda Gates Foundation and others; ClinicalTrials.gov number, NCT02027207.).", "title": "Efficacy of a Single-Dose, Inactivated Oral Cholera Vaccine in Bangladesh.", "date": "2016-05-06"}, {"article_id": "31092224", "content": "Cholera increases the risk of harmful effects on foetuses. We prospectively followed pregnant women unaware of their pregnancy status who received a study agent in a clinical trial evaluating the association between exposure to an oral cholera vaccine (OCV) and foetal survival.\nStudy participants were selected from a randomized placebo-controlled trial conducted in Dhaka, Bangladesh. The vaccination campaign was conducted between January 10 and February 4, 2014. We enrolled women who were exposed to an OCV or placebo during pregnancy (Cohort 1) and women who were pregnant after the vaccination was completed (Cohort 2). Our primary endpoint was pregnancy loss (spontaneous miscarriage or stillbirth), and the secondary endpoints were preterm delivery and low birth weight. We employed a log-binomial regression to calculate the relative risk of having adverse outcomes among OCV recipients compared to that among placebo recipients.\nThere were 231 OCV and 234 placebo recipients in Cohort 1 and 277 OCV and 299 placebo recipients in Cohort 2. In Cohort 1, the incidence of pregnancy loss was 113/1000 and 115/1000 among OCV and placebo recipients, respectively. The adjusted relative risk for pregnancy loss was 0.97 (95% CI: 0.58-1.61; p\u2009=\u20090.91) in Cohort 1. We did not observe any variation in the risk of pregnancy loss between the two cohorts. The risks for preterm delivery and low birth weight were not significantly different between the groups in both cohorts.\nOur study provides additional evidence that exposure to an OCV during pregnancy does not increase the risk of pregnancy loss, preterm delivery, or low birth weight, suggesting that pregnant women in cholera-affected regions should not be excluded in a mass vaccination campaign.\nThe study is registered at ( http://clinicaltrials.gov ). Identifier: NCT02027207 .", "title": "Safety of a bivalent, killed, whole-cell oral cholera vaccine in pregnant women in Bangladesh: evidence from a randomized placebo-controlled trial.", "date": "2019-05-17"}]}
{"original_review": "36637057", "question_data": [{"question_id": 111, "question": "Is vision-related quality of life higher, lower, or the same when comparing telerehabilitation with a therapist to self-guided training?", "answer": "no difference", "evidence_quality": "very low", "fulltext_required": "yes", "comment": "", "relevant_sources": ["34081648"]}, {"question_id": 112, "question": "Is device abandonment after 2 weeks higher, lower, or the same when comparing telerehabilitation with a therapist to self-guided training?", "answer": "lower", "evidence_quality": "very low", "fulltext_required": "yes", "comment": "", "relevant_sources": ["34081649"]}, {"question_id": 113, "question": "Is device abandonment after 3 months higher, lower, or the same when comparing telerehabilitation with a therapist to self-guided training?", "answer": "no difference", "evidence_quality": "very low", "fulltext_required": "yes", "comment": "", "relevant_sources": ["34081649"]}], "sources": [{"article_id": "34081648", "content": "Head-mounted low vision devices have become a viable alternative to enhance residual vision. This study supports the use of a head-mounted display to improve aspects of functional vision and quality of life. Much is still unknown regarding the required frequency, duration, or potential effectiveness of this telerehabilitation training protocol or what characteristics best identify optimal users.\nA randomized study explored the effect of telerehabilitation on quality of life and functional vision in individuals with low vision using a head-mounted display.\nWe recruited 57 participants (age, 21 to 82 years; mean, 54.5 years) among new prospective eSight Eyewear users, randomized 1:1 into two parallel groups; the experimental group received the telerehabilitation training provided by a low vision therapist, whereas the control group received the self-training standard offered by the device manufacturer and without involvement of a low vision therapist. The primary outcome measures were the impact of telerehabilitation on validated measures of assistive technology-related quality of life: the Psychosocial Impact of Assistive Devices Scale and the Quebec User Evaluation of Satisfaction with Assistive Technology scale. Exploratory outcomes were the assessment of self-reported functional vision using the Veterans Affairs Low Vision Visual Functioning Questionnaire-48 and cybersickness associated with head-mounted display use with the Simulator Sickness Questionnaire.\nAssistive technology-related quality of life was improved when measured by the satisfaction scale but not the psychosocial scale within the first 3 months, independently of training type. Overall, functional vision improvement was observed within the first 2 weeks of device use and maintained during the 6-month study, independently of group type. Cybersickness outcomes were similar between training groups and did not change significantly for 6 months.\neSight Eyewear, either with telerehabilitation or with the manufacturer self-training comparison, improved functional vision and increased users' quality of life within the initial 3 months of device training and practice.", "title": "Head-mounted Visual Assistive Technology-related Quality of Life Changes after Telerehabilitation.", "date": "2021-06-04"}, {"article_id": "34081649", "content": "A recent trend in low vision rehabilitation has been the use of portable head-mounted displays to enhance residual vision. Our study confirms the feasibility of telerehabilitation and informs the development of evidence-based recommendations to improve telerehabilitation interventions to reduce device abandonment.\nTo develop evidence-based recommendations for telerehabilitation, we conducted a feasibility study in preparation for a future randomized trial on the use of head-mounted displays.\nWe recruited novice eSight Eyewear users, randomized 1:1: the experimental group received telerehabilitation by a low vision therapist using video conferencing; the control group completed at home self-training provided by the device manufacturer. The primary feasibility outcomes were whether the recruitment goal of 60 participants (30/group) was attainable within 1 year and how participants judged the accessibility and acceptability of the telerehabilitation. An exploratory outcome was the impact of telerehabilitation on eSight Eyewear use behavior.\nAmong 333 eSight users, 57 participants were enrolled, of which 35% withdrew from the study, whereas the remainder completed the 6-month follow-up. The withdrawal rate was higher in the control group but did not differ significantly from the experimental group. High accessibility (93% of participants accessed the platform) and global acceptability (100% overall satisfaction) were reported among those who completed the telerehabilitation protocol. The therapist had no difficulty judging the participants' reading performances qualitatively while participants used their device to read their eSkills and VisExc guides. Most participants improved their daily activities, based on qualitative reports of the attained goals. Seventy-nine percent of individuals declined to participate, whereas 16% of participants decided not to use eSight Eyewear anymore.\nThe data demonstrated the feasibility of a randomized controlled telerehabilitation study for people with low vision using a head-mounted display. Positive feedback from the participants and the therapist suggests the potential value of this modality for low vision services.", "title": "Personalized Telerehabilitation for a Head-mounted Low Vision Aid: A Randomized Feasibility Study.", "date": "2021-06-04"}, {"article_id": "31542748", "content": "A recent trend in low vision (LV) has been towards the use of portable head-mounted displays (HMDs) to enhance residual vision. The decision process around the (non-)use of such devices have been identified as multifactorial. Among important barriers identified in the context of magnifying LV aids were transportation issues and insufficient training. In recent years, telerehabilitation has become of growing interest in healthcare because it allows individuals to remain at home while receiving rehabilitation services. A recent pilot study indicated encouraging outcomes; however, very few applications of telerehabilitation for LV have been tested systematically.\nTo help guide evidence-based practice recommendations for this modality, we will carry out a feasibility study to assess the recruitment, retention, accessibility and acceptability of an eventual fully randomised trial of telerehabilitation for people with LV using HMDs. We will recruit 60 participants aged 18+ years among prospective eSight Eyewear owners, randomised 1:1 into two parallel groups. The active intervention will be the telerehabilitation operated by a LV therapist; the control arm will be the current self-training standard provided by the device vendor. The primary feasibility outcome measures will be: time to recruit participants, loss to follow-up, accessibility and acceptability of the telerehabilitation (satisfaction of the users and LV therapist). Exploratory outcomes will be the impact of telerehabilitation on eSight Eyewear use behaviour (discontinuance rate), and validated measures of assistive-technology-related quality of life.\nThe study was approved by the Ethics Review Board of the ", "title": "Measuring changes in device use of a head-mounted low vision aid after personalised telerehabilitation: protocol for a feasibility study.", "date": "2019-09-23"}]}
{"original_review": "32898300", "question_data": [{"question_id": 114, "question": "Is weight by 30 days of age higher, lower, or the same when comparing carbohydrate supplementation to no supplementation?", "answer": "higher", "evidence_quality": "very low", "fulltext_required": "yes", "comment": "", "relevant_sources": ["25538834"]}, {"question_id": 115, "question": "Is the risk of feeding intolerance higher, lower, or the same when comparing carbohydrate supplementation to no supplementation?", "answer": "no difference", "evidence_quality": "very low", "fulltext_required": "yes", "comment": "", "relevant_sources": ["25538834"]}, {"question_id": 116, "question": "Is the risk of necrotising enterocolitis (NEC) higher, lower, or the same when comparing carbohydrate supplementation to no supplementation?", "answer": "no difference", "evidence_quality": "very low", "fulltext_required": "no", "comment": "original paper authors use hazard ratio and claim significant difference; cochrane authors use risk ratio and find no significant difference", "relevant_sources": ["25538834"]}], "sources": [{"article_id": "26985433", "content": "This study was designed to compare the efficacy and safety of enteral supplementation of a prebiotic mixture (SCGOS/LCFOS) on faecal microbiota in very premature infants who fed exclusively with human-milk.\nThis double-center randomized control trial was conducted from December 2012 to November 2013 in the tertiary Neonatal Intensive Care Units of the Isfahan University of Medical Sciences. Fifty preterm infants (birth weight \u22641500 g who were not fed with formula) were randomly allocated to have enteral (tube feeding) supplementation with a prebiotic mixture (SCGOS/LCFOS; 9:1) or receive no prebiotics.\nThe primary outcome (e.g., the effect of the prebiotic mixture on fecal microbiota pattern) was clearly different between the two groups. Despite greater coliforms colony counts in first stool cultures in the prebiotic group (Group P) (P = 0.67), coliforms were significantly lower in the third stool cultures in the Group P (P < 0.001). Furthermore, despite the much higher Lactobacillus colony counts, in the first stool cultures, in the control group (Group C) (P = 0.005); there was a trend toward significantly increased Lactobacillus colony counts in the Group P during the study, but the difference between Lactobacillus colony counts, in the third stool cultures, between two groups was no longer statistically significant (P = 0.11). Interestingly, the median length of hospital stay was significantly less in the Group P (16 [12.50-23.50] vs. 25 [19.50-33.00] days; P = 0.003).\nThis suggests that it might have been \"the complete removal of formula\" which manifests a synergistic effect between nonhuman neutral oligosaccharides (prebiotics) and human oligosaccharides, which in turn, led to the rapid growth of beneficial Lactobacillus colonies in the gut of breast milk-fed preterm infants, while decreasing the number of pathogenic coliforms microorganisms. Therefore, further studies with larger sample sizes are recommended to investigate the issue.", "title": "The effect of neutral oligosaccharides on fecal microbiota in premature infants fed exclusively with breast milk: A randomized clinical trial.", "date": "2016-03-18"}, {"article_id": "25538834", "content": "Necrotizing enterocolitis (NEC) is one of the most destructive diseases associated with conditions of neonatal prematurity. Supplementation with enteral prebiotics may reduce the incidence of NEC, especially in infants who fed exclusively with breast-milk. Therefore, we compared the efficacy and safety of enteral supplementation of a prebiotic mixture (short chain galacto-oligosaccharides/long chain fructo-oligosaccharides [SCGOS/LCFOS]) versus no intervention on incidence of NEC in preterm infants.\nIn a single-center randomized control trial 75 preterm infants (birth weight [BW] \u22641500 g, gestational age \u226434 weeks and were not fed with formula) on 30 ml/kg/day volume of breast-milk were randomly allocated to have enteral supplementation with a prebiotic mixture (SCGOS/LCFOS; 9:1) or not receive any prebiotic. The incidence of suspected NEC, feeding intolerance, time to full enteral feeds, duration of hospitalization were investigated.\nDifferences in demographic characteristics were not statistically important. SCGOS/LCFOS mixture significantly reduced the incidence of suspected NEC, (1 [4.0%] vs. 11 [22.0%]; hazard ratio: 0.49 [95% confidence interval: 0.29-0.84]; P = 0.002), and time to full enteral feeds (11 [7-21] vs. 14 [8-36] days; P - 0.02]. Also duration of hospitalization was meaningfully shorter in the prebiotic group (16 [9-45] vs. 25 [11-80]; P - 0.004]. Prebiotic oligosaccharides were well tolerated by very low BW (VLBW) infants.\nEnteral supplementation with prebiotic significantly reduced the incidence of NEC in VLBW infants who were fed exclusively breast-milk. This finding suggests that it might have been the complete removal of formula which caused a synergistic effect between nonhuman neutral oligosaccharides (prebiotic) and human oligosaccharides.", "title": "The Effect of Neutral Oligosaccharides on Reducing the Incidence of Necrotizing Enterocolitis in Preterm infants: A Randomized Clinical Trial.", "date": "2014-12-30"}]}
{"original_review": "27078125", "question_data": [{"question_id": 117, "question": "Is the risk of severe post-partum haemorrhage (PPH) higher, lower, or the same when comparing oxytocin to no intervention?", "answer": "no difference", "evidence_quality": "very low", "fulltext_required": "yes", "comment": "", "relevant_sources": ["24130463"]}, {"question_id": 118, "question": "Is the risk of PPH (> 500 mL)  higher, lower, or the same when comparing oxytocin to no intervention?", "answer": "lower", "evidence_quality": "low", "fulltext_required": "no", "comment": "", "relevant_sources": ["24130463"]}, {"question_id": 119, "question": "Is transfer or referral of the mother to a healthcare facility higher, lower, or the same when comparing oxytocin to no intervention?", "answer": "no difference", "evidence_quality": "low", "fulltext_required": "yes", "comment": "", "relevant_sources": ["24130463"]}, {"question_id": 120, "question": "Is the risk of stillbirth higher, lower, or the same when comparing oxytocin to no intervention?", "answer": "no difference", "evidence_quality": "low", "fulltext_required": "yes", "comment": "", "relevant_sources": ["24130463"]}, {"question_id": 121, "question": "Is the risk of early infant death (0-3 days) higher, lower, or the same when comparing oxytocin to no intervention?", "answer": "no difference", "evidence_quality": "low", "fulltext_required": "yes", "comment": "", "relevant_sources": ["24130463"]}, {"question_id": 226, "question": "Is maternal anaemia higher, lower, or the same when comparing oxytocin to no intervention?", "answer": "insufficient data", "evidence_quality": "", "fulltext_required": "no", "comment": "", "relevant_sources": ["24130463"]}, {"question_id": 227, "question": "Is neonatal death within 28 days higher, lower, or the same when comparing oxytocin to no intervention?", "answer": "insufficient data", "evidence_quality": "", "fulltext_required": "no", "comment": "", "relevant_sources": ["24130463"]}, {"question_id": 228, "question": "Is the rate of breastfeeding higher, lower, or the same when comparing oxytocin to no intervention?", "answer": "insufficient data", "evidence_quality": "", "fulltext_required": "no", "comment": "", "relevant_sources": ["24130463"]}, {"question_id": 229, "question": "Is the rate of breastfeeding higher, lower, or the same when comparing oxytocin to no intervention?", "answer": "insufficient data", "evidence_quality": "", "fulltext_required": "no", "comment": "", "relevant_sources": ["24130463"]}], "sources": [{"article_id": "24130463", "content": "Oxytocin (10 IU) is the drug of choice for prevention of postpartum hemorrhage (PPH). Its use has generally been restricted to medically trained staff in health facilities. We assessed the effectiveness, safety, and feasibility of PPH prevention using oxytocin injected by peripheral health care providers without midwifery skills at home births.\nThis community-based, cluster-randomized trial was conducted in four rural districts in Ghana. We randomly allocated 54 community health officers (stratified on district and catchment area distance to a health facility: \u226510 km versus <10 km) to intervention (one injection of oxytocin [10 IU] one minute after birth) and control (no provision of prophylactic oxytocin) arms. Births attended by a community health officer constituted a cluster. Our primary outcome was PPH, using multiple definitions; (PPH-1) blood loss \u2265500 mL; (PPH-2) PPH-1 plus women who received early treatment for PPH; and (PPH-3) PPH-2 plus any other women referred to hospital for postpartum bleeding. Unsafe practice is defined as oxytocin use before delivery of the baby. We enrolled 689 and 897 women, respectively, into oxytocin and control arms of the trial from April 2011 to November 2012. In oxytocin and control arms, respectively, PPH-1 rates were 2.6% versus 5.5% (RR: 0.49; 95% CI: 0.27-0.88); PPH-2 rates were 3.8% versus 10.8% (RR: 0.35; 95% CI: 0.18-0.63), and PPH-3 rates were similar to those of PPH-2. Compared to women in control clusters, those in the intervention clusters lost 45.1 mL (17.7-72.6) less blood. There were no cases of oxytocin use before delivery of the baby and no major adverse events requiring notification of the institutional review boards. Limitations include an unblinded trial and imbalanced numbers of participants, favoring controls.\nMaternal health care planners can consider adapting this model to extend the use of oxytocin into peripheral settings including, in some contexts, home births.\nClinicalTrials.gov NCT01108289 Please see later in the article for the Editors' Summary.", "title": "Effect on postpartum hemorrhage of prophylactic oxytocin (10 IU) by injection by community health officers in Ghana: a community-based, cluster-randomized trial.", "date": "2013-10-17"}, {"article_id": "22676921", "content": "Hemorrhage is the leading direct cause of maternal death globally. While oxytocin is the drug of choice for postpartum hemorrhage prevention, its use has generally been limited to health facilities. This trial assesses the effectiveness, safety, and feasibility of expanding the use of prophylactic intramuscular oxytocin to peripheral health care providers at home births in four predominantly rural districts in central Ghana.\nThis study is designed as a community-based cluster-randomized trial in which Community Health Officers are randomized to provide (or not provide) an injection of oxytocin 10 IU via the Uniject\u2122 injection system within one minute of delivery of the baby to women who request their presence at home at the onset of labor. The primary aim is to determine if administration of prophylactic oxytocin via Uniject\u2122 by this cadre will reduce the risk of postpartum hemorrhage by 50 % relative to deliveries which do not receive the prophylactic intervention. Postpartum hemorrhage is examined under three sequential definitions: 1) blood loss \u2265500 ml (BL); 2) treatment for bleeding (TX) and/or BL; 3) hospital referral for bleeding and/or TX and/or BL. Secondary outcomes address safety and feasibility of the intervention and include adverse maternal and fetal outcomes and logistical concerns regarding assistance at home births and the storage and handling of oxytocin, respectively.\nResults from this trial will build evidence for the effectiveness of expanding the delivery of this established prophylactic intervention to peripheral settings. Complementary data on safety and logistical issues related to this intervention will assist policymakers in low-income countries in selecting both the best uterotonic and service delivery strategy for postpartum hemorrhage prevention. Results of this trial are expected in mid-2013. The trial is registered at ClinicalTrials.gov: NCT01108289.", "title": "Impact on postpartum hemorrhage of prophylactic administration of oxytocin 10 IU via Uniject\u2122 by peripheral health care providers at home births: design of a community-based cluster-randomized trial.", "date": "2012-06-09"}]}
{"original_review": "30590875", "question_data": [{"question_id": 122, "question": "Is the risk of gingivitis higher, lower, or the same when comparing scaling and polishing at a reguarly-scheduled 6 month interval to no yearly-or-shorter schedule for scaling and polishing?", "answer": "no difference", "evidence_quality": "high", "fulltext_required": "no", "comment": "", "relevant_sources": ["22204658", "29984691"]}, {"question_id": 123, "question": "Is the risk of gingivitis higher, lower, or the same when comparing scaling and polishing at a reguarly-scheduled 12 month interval to no yearly-or-shorter schedule for scaling and polishing?", "answer": "no difference", "evidence_quality": "high", "fulltext_required": "no", "comment": "", "relevant_sources": ["22204658", "29984691"]}, {"question_id": 124, "question": "Is the risk of gingivitis higher, lower, or the same when comparing scaling and polishing at a 6 month interval to scaling and polishing at a 12 month interval?", "answer": "no difference", "evidence_quality": "high", "fulltext_required": "no", "comment": "", "relevant_sources": ["22204658", "29984691"]}], "sources": [{"article_id": "24090395", "content": "Single visit scale and polish is frequently carried out in dental practices however there is little evidence to support (or refute) its clinical effectiveness. The purpose of this research was to compare patient-reported outcomes between groups receiving a scale and polish at 6-, 12-, and 24-month intervals. Outcomes recorded included participants' subjective assessment of their oral cleanliness; the perceived importance of scale and polish for oral health and aesthetics; and frequency at which this treatment is required.\nA practice-based randomised control trial was undertaken, with a 24-month follow-up period. Participants were healthy adults with no significant periodontal disease (BPE codes <3) randomly allocated to three groups to receive scale and polish at 6-, 12-, or 24-month intervals. Patient-reported outcomes were recorded at baseline and follow-up. Oral cleanliness was reported using a 5-point scale and recorded by examiners blinded to trial group allocation. A self-completed questionnaire enabled participants to report perceived importance of scale and polish (5-point scale), and required frequency of treatment (6-point scale). The main hypothesis was that participants receiving 6-monthly scale and polish would report higher levels of oral cleanliness compared to participants receiving scale and polish at 12- and 24-month intervals.\n369 participants were randomised: 125 to the 6-month group; 122 to the 12-month group; and 122 to the 24-month group. Complete data set analysis was carried out to include 107 (6-month group), 100 (12-month group) and 100 (24-month group) participants. Multiple imputation analyses were conducted where follow-up data was missing. The difference in the proportions of participants reporting a 'high' level of oral cleanliness at follow-up was significant (Chi-squared P = 0.003): 52.3% (6-month group), 47.0% (12-month group) and 30.0% (24-month group). Scale and polish was thought to be important by the majority in each group for keeping mouths clean and gums healthy, whitening teeth, and preventing bad breath and tooth decay; there were no statistically significant differences between groups at follow-up. Most participants at follow-up thought that the frequency of scale and polish should be \"every 6 months\" or more frequently: 77.9% (6-month group), 64.6% (12-month group), 71.7% (24-month group); differences between groups were not statistically significant (Chi squared P = 0.126). The results suggest that participants in the 24-month trial group were more likely to choose a scale and polish interval of \"once a year\" or less frequently (OR 2.89; 95% CI 1.36, 6.13).\nThe majority of healthy adults regarded 6-monthly single-visit scale and polish as being beneficial for their oral health. Receiving the treatment at different frequencies did not alter this belief; and those with the longest interval between scale and polish provision perceived that their mouth was less clean. In the absence of a strong evidence base to support (or refute) the effectiveness of single-visit scale and polish, the beliefs and preferences of patients regarding scale and polish may be influential drivers for maintaining provision of this treatment.", "title": "Patient perceptions regarding benefits of single visit scale and polish: a randomised controlled trial.", "date": "2013-10-05"}, {"article_id": "22204658", "content": "Practice-based general dental practitioners routinely provide \"scale and polish\" or \"oral prophylaxis\" to patients attending their practices. Despite its routine provision, there is no evidence to support the clinical effectiveness of single-visit scale and polish, nor the frequency at which it should be provided. A recent systematic review recommended that future trials investigating scale and polish should involve dental practice patients.\nA practice-based parallel randomised controlled trial with 24-month follow-up was conducted. Healthy adults (Basic Periodontal Examination [BPE] codes <3) were randomly assigned to 3 groups (6-month, 12-month, or 24-month interval between scale and polish). The primary outcome was gingival bleeding with the hypothesis that 6-monthly scale and polish would result in lower prevalence than 12-month or 24-month frequency. Follow-up measurements were recorded by examiners blinded to the allocation. 125, 122 and 122 participants were randomised to the 6-month, 12-month and 24-month groups respectively. Complete data set analyses were conducted for 307 participants: 107, 100, and 100 in the 6-month, 12-month and 24-month groups respectively. Chi-square test and ANOVA were used to compare treatment groups at follow-up. Logistic regression and ANCOVA were used to estimate the relationship between outcome and treatment group, adjusted for baseline values. Multiple imputation analyses were also carried out for participants with incomplete data sets.\nPrevalence of gingival bleeding at follow-up was 78.5% (6-month), 78% (12-month) and 82% (24-month) (p = 0.746). There were no statistically significant differences between groups with respect to follow-up prevalence of plaque and calculus. Statistically significant differences detected in the amount (millimetres) of calculus were too small to be clinically significant. Seventeen (4.6%) participants were withdrawn from the trial to receive additional treatment.\nThis trial could not identify any differences in outcomes for single-visit scale and polish provided at 6, 12 and 24 month frequencies for healthy patients (with no significant periodontal disease). However, this is the first trial of scale and polish which has been conducted in a general practice setting and the results are not conclusive. Larger trials with more comprehensive measurement and long-term follow up need to be undertaken to provide a firm evidence base for this intervention. This trial informs the design of future practice-based trials on this subject.", "title": "Clinical outcomes of single-visit oral prophylaxis: a practice-based randomised controlled trial.", "date": "2011-12-30"}, {"article_id": "29984691", "content": "Periodontal disease is preventable but remains the most common oral disease worldwide, with major health and economic implications. Stakeholders lack reliable evidence of the relative clinical effectiveness and cost-effectiveness of different types of oral hygiene advice (OHA) and the optimal frequency of periodontal instrumentation (PI).\nTo test clinical effectiveness and assess the economic value of the following strategies: personalised OHA versus routine OHA, 12-monthly PI (scale and polish) compared with 6-monthly PI, and no PI compared with 6-monthly PI.\nMulticentre, pragmatic split-plot, randomised open trial with a cluster factorial design and blinded outcome evaluation with 3 years' follow-up and a within-trial cost-benefit analysis. NHS and participant costs were combined with benefits [willingness to pay (WTP)] estimated from a discrete choice experiment (DCE).\nUK dental practices.\nAdult dentate NHS patients, regular attenders, with Basic Periodontal Examination (BPE) scores of 0, 1, 2 or 3.\nPractices were randomised to provide routine or personalised OHA. Within each practice, participants were randomised to the following groups: no PI, 12-monthly PI or 6-monthly PI (current practice).\nClinical - gingival inflammation/bleeding on probing at the gingival margin (3 years). Patient - oral hygiene self-efficacy (3 years). Economic - net benefits (mean WTP minus mean costs).\nA total of 63 dental practices and 1877 participants were recruited. The mean number of teeth and percentage of bleeding sites was 24 and 33%, respectively. Two-thirds of participants had BPE scores of \u2264 2. Under intention-to-treat analysis, there was no evidence of a difference in gingival inflammation/bleeding between the 6-monthly PI group and the no-PI group [difference 0.87%, 95% confidence interval (CI) -1.6% to 3.3%; \nBeing a pragmatic trial, we did not deny PIs to the no-PI group; there was clear separation in the mean number of PIs between groups.\nThere was no additional benefit from scheduling 6-monthly or 12-monthly PIs over not providing this treatment unless desired or recommended, and no difference between OHA delivery for gingival inflammation/bleeding and patient-centred outcomes. However, participants valued, and were willing to pay for, both interventions, with greater financial value placed on PI than on OHA.\nAssess the clinical effectiveness and cost-effectiveness of providing multifaceted periodontal care packages in primary dental care for those with periodontitis.\nCurrent Controlled Trials ISRCTN56465715.\nThis project was funded by the National Institute for Health Research (NIHR) Health Technology Assessment programme and will be published in full in ", "title": "Improving the Quality of Dentistry (IQuaD): a cluster factorial randomised controlled trial comparing the effectiveness and cost-benefit of oral hygiene advice and/or periodontal instrumentation with routine care for the prevention and management of periodontal disease in dentate adults attending dental primary care.", "date": "2018-07-10"}]}
{"original_review": "33631841", "question_data": [{"question_id": 125, "question": "Is the likelihood of 4\u2010point improvement in SELENA\u2010SLEDAI higher, lower, or the same when comparing belimumab to placebo?", "answer": "higher", "evidence_quality": "high", "fulltext_required": "no", "comment": "fulltext available but not needed for some studies", "relevant_sources": ["29295825", "28118533", "21296403", "22127708"]}, {"question_id": 126, "question": "Is the risk of one or more serious adverse events higher, lower, or the same when comparing belimumab to placebo?", "answer": "no difference", "evidence_quality": "low", "fulltext_required": "yes", "comment": "", "relevant_sources": ["19714604", "28118533", "22127708", "21296403", "29295825"]}, {"question_id": 127, "question": "Is the risk of one or more serious infections higher, lower, or the same when comparing belimumab to placebo?", "answer": "no difference", "evidence_quality": "moderate", "fulltext_required": "yes", "comment": "", "relevant_sources": ["19714604", "28118533", "22127708", "21296403"]}], "sources": [{"article_id": "29295825", "content": "Intravenous belimumab plus standard of care (SoC) is approved in the USA and Europe for treatment of active, autoantibody-positive systemic lupus erythematosus (SLE).\nThis phase III, multicentre, randomised, double-blind, placebo-controlled study (BEL113750; NCT01345253) was conducted in 49 centres across China, Japan and South Korea (May 2011\nThe modified intent-to-treat population included 677 patients (belimumab n=451, placebo n=226). At Week 52, the SRI4 response rate was higher with belimumab versus placebo (53.8% vs 40.1%; OR: 1.99 (95% CI: 1.40, 2.82; P=0.0001)). The percentages of patients with a \u22654\u2009point reduction in SELENA-SLEDAI and an SRI7 response were significantly greater for belimumab versus placebo. Patients in the belimumab group had a 50% lower risk of experiencing a severe flare than those receiving placebo (P=0.0004). In patients with baseline prednisone dose >7.5\u2009mg/day, there was a significant reduction in steroid use favouring belimumab (P=0.0228). The incidence of adverse events was similar between groups.\nIn patients with SLE from North East Asia, belimumab significantly improved disease activity, while reducing prednisone use, with no new safety issues.", "title": "A pivotal phase III, randomised, placebo-controlled study of belimumab in patients with systemic lupus erythematosus located in China, Japan and South Korea.", "date": "2018-01-04"}, {"article_id": "28118533", "content": "To assess the efficacy and safety of subcutaneous (SC) belimumab in patients with systemic lupus erythematosus (SLE).\nPatients with moderate-to-severe SLE (score of \u22658 on the Safety of Estrogens in Lupus Erythematosus National Assessment [SELENA] version of the SLE Disease Activity Index [SLEDAI]) were randomized 2:1 to receive weekly SC belimumab 200 mg or placebo by prefilled syringe in addition to standard SLE therapy for 52 weeks. The primary end point was the SLE Responder Index (SRI4) at week 52. Secondary end points were reduction in the corticosteroid dosage and time to severe flare. Safety was assessed according to the adverse events (AEs) reported and the laboratory test results.\nOf 839 patients randomized, 836 (556 in the belimumab group and 280 in the placebo group) received treatment. A total of 159 patients withdrew before the end of the study. At entry, mean SELENA-SLEDAI scores were 10.5 in the belimumab group and 10.3 in the placebo group. More patients who received belimumab were SRI4 responders than those who received placebo (61.4% versus 48.4%; odds ratio [OR] 1.68 [95% confidence interval (95% CI) 1.25-2.25]; P\u2009=\u20090.0006). In the belimumab group, both time to and risk of severe flare were improved (median 171.0 days versus 118.0 days; hazard ratio 0.51 [95% CI 0.35-0.74]; P\u2009=\u20090.0004), and more patients were able to reduce their corticosteroid dosage by \u226525% (to \u22647.5 mg/day) during weeks 40-52 (18.2% versus 11.9%; OR 1.65 [95% CI 0.95-2.84]; P\u2009=\u20090.0732), compared with placebo. AE incidence was comparable between treatment groups; serious AEs were reported by 10.8% of patients taking belimumab and 15.7% of those taking placebo. A worsening of IgG hypoglobulinemia by \u22652 grades occurred in 0.9% of patients taking belimumab and 1.4% of those taking placebo.\nIn patients with moderate-to-severe SLE, weekly SC doses of belimumab 200 mg plus standard SLE therapy significantly improved their SRI4 response, decreased severe disease flares as compared with placebo, and had a safety profile similar to placebo plus standard SLE therapy.", "title": "Efficacy and Safety of Subcutaneous Belimumab in Systemic Lupus Erythematosus: A Fifty-Two-Week Randomized, Double-Blind, Placebo-Controlled Study.", "date": "2017-01-25"}, {"article_id": "22127708", "content": "To assess the efficacy/safety of the B lymphocyte stimulator inhibitor belimumab plus standard therapy compared with placebo plus standard therapy in active systemic lupus erythematosus (SLE).\nIn a phase III, multicenter, randomized, placebo-controlled trial, 819 antinuclear antibody-positive or anti-double-stranded DNA-positive SLE patients with scores \u22656 on the Safety of Estrogens in Lupus Erythematosus National Assessment (SELENA) version of the SLE Disease Activity Index (SLEDAI) were randomized in a 1:1:1 ratio to receive 1 mg/kg belimumab, 10 mg/kg belimumab, or placebo intravenously on days 0, 14, and 28 and then every 28 days for 72 weeks. The primary efficacy end point was the SLE Responder Index (SRI) response rate at week 52 (an SRI response was defined as a \u22654-point reduction in SELENA-SLEDAI score, no new British Isles Lupus Assessment Group [BILAG] A organ domain score and no more than 1 new BILAG B score, and no worsening in physician's global assessment score versus baseline).\nBelimumab at 10 mg/kg plus standard therapy met the primary efficacy end point, generating a significantly greater SRI response at week 52 compared with placebo (43.2% versus 33.5%; P = 0.017). The rate with 1 mg/kg belimumab was 40.6% (P = 0.089). Response rates at week 76 were 32.4%, 39.1%, and 38.5% with placebo, 1 mg/kg belimumab, and 10 mg/kg belimumab, respectively. In post hoc sensitivity analyses evaluating higher SELENA-SLEDAI score thresholds, 10 mg/kg belimumab achieved better discrimination at weeks 52 and 76. Risk of severe flares over 76 weeks (based on the modified SLE Flare Index) was reduced with 1 mg/kg belimumab (34%) (P = 0.023) and 10 mg/kg belimumab (23%) (P = 0.13). Serious and severe adverse events, including infections, laboratory abnormalities, malignancies, and deaths, were comparable across groups.\nBelimumab plus standard therapy significantly improved SRI response rate, reduced SLE disease activity and severe flares, and was generally well tolerated in SLE.", "title": "A phase III, randomized, placebo-controlled study of belimumab, a monoclonal antibody that inhibits B lymphocyte stimulator, in patients with systemic lupus erythematosus.", "date": "2011-12-01"}, {"article_id": "19714604", "content": "To assess the safety, tolerability, biologic activity, and efficacy of belimumab in combination with standard of care therapy (SOC) in patients with active systemic lupus erythematosus (SLE).\nPatients with a Safety of Estrogens in Lupus Erythematosus: National Assessment (SELENA) version of the Systemic Lupus Erythematosus Disease Activity Index (SLEDAI) score >/=4 (n = 449) were randomly assigned to belimumab (1, 4, or 10 mg/kg) or placebo in a 52-week study. Coprimary end points were the percent change in the SELENA-SLEDAI score at week 24 and the time to first SLE flare.\nSignificant differences between the treatment and placebo groups were not attained for either primary end point, and no dose response was observed. Reductions in SELENA-SLEDAI scores from baseline were 19.5% in the combined belimumab group versus 17.2% in the placebo group. The median time to first SLE flare was 67 days in the combined belimumab group versus 83 days in the placebo group. However, the median time to first SLE flare during weeks 24-52 was significantly longer with belimumab treatment (154 versus 108 days; P = 0.0361). In the subgroup (71.5%) of serologically active patients (antinuclear antibody titer >/=1:80 and/or anti-double-stranded DNA [anti-dsDNA] >/=30 IU/ml), belimumab treatment resulted in significantly better responses at week 52 than placebo for SELENA-SLEDAI score (-28.8% versus -14.2%; P = 0.0435), physician's global assessment (-32.7% versus -10.7%; P = 0.0011), and Short Form 36 physical component score (+3.0 versus +1.2 points; P = 0.0410). Treatment with belimumab resulted in a 63-71% reduction of naive, activated, and plasmacytoid CD20+ B cells, and a 29.4% reduction in anti-dsDNA titers (P = 0.0017) by week 52. The rates of adverse events and serious adverse events were similar in the belimumab and placebo groups.\nBelimumab was biologically active and well tolerated. The effect of belimumab on the reduction of SLE disease activity or flares was not significant. However, serologically active SLE patients responded significantly better to belimumab therapy plus SOC than to SOC alone.", "title": "A phase II, randomized, double-blind, placebo-controlled, dose-ranging study of belimumab in patients with active systemic lupus erythematosus.", "date": "2009-08-29"}, {"article_id": "18786258", "content": "This trial evaluated the safety, biologic activity, and pharmacokinetics of belimumab, a fully human monoclonal antibody that inhibits the biologic activity of the soluble form of the essential B-cell survival factor B-lymphocyte stimulator (BLyS) in patients with systemic lupus erythematosus (SLE).\nSeventy patients with mild-to-moderate SLE were enrolled in a phase I, double-blind, randomized study and treated with placebo (n = 13) or belimumab (n = 57) at four different doses (1.0, 4.0, 10, and 20 mg/kg) as a single infusion or two infusions 21 days apart. Patients were followed for 84 to 105 days to assess adverse events, pharmacokinetics, peripheral blood B-cell counts, serology, and SLE disease activity. Data from the study were summarized using descriptive statistics. chi2 type tests were used to analyze discrete variables. The Kruskal-Wallis test, the Wilcoxon test, and the analysis of covariance were used to analyze the continuous variables, as appropriate. The analysis was performed on all randomized patients who received study agent.\nThe incidences of adverse events and laboratory abnormalities were similar among the belimumab and placebo groups. Belimumab pharmacokinetics were linear across the 1.0 to 20 mg/kg dose range. Long terminal elimination half-life (8.5 to 14.1 days), slow clearance (7 ml/day per kg), and small volume of distribution (69 to 112 ml/kg) were consistent with a fully human antibody. Significant reductions in median percentages of CD20+ B cells were observed in patients treated with a single dose of belimumab versus placebo (day 42: P = 0.0042; and day 84: P = 0.0036) and in patients treated with two doses of belimumab versus placebo (day 105: P = 0.0305). SLE disease activity did not change after one or two doses of belimumab.\nBelimumab was well tolerated and reduced peripheral B-cell levels in SLE patients. These data support further studies of belimumab in autoimmune disorders.", "title": "Biologic activity and safety of belimumab, a neutralizing anti-B-lymphocyte stimulator (BLyS) monoclonal antibody: a phase I trial in patients with systemic lupus erythematosus.", "date": "2008-09-13"}, {"article_id": "21296403", "content": "Systemic lupus erythematosus is a heterogeneous autoimmune disease that is associated with B-cell hyperactivity, autoantibodies, and increased concentrations of B-lymphocyte stimulator (BLyS). The efficacy and safety of the fully human monoclonal antibody belimumab (BLyS-specific inhibitor) was assessed in patients with active systemic lupus erythematosus.\nPatients (aged \u226518 years) who were seropositive with scores of at least 6 on the Safety of Estrogens in Lupus Erythematosus National Assessment-Systemic Lupus Erythematosus Disease Activity Index (SELENA-SLEDAI) were enrolled in a multicentre phase 3 study, which was done in Latin America, Asia-Pacific, and eastern Europe. Patients were randomly assigned by use of a central interactive voice response system in a 1:1:1 ratio to belimumab 1 mg/kg or 10 mg/kg, or placebo by intravenous infusion in 1 h on days 0, 14, and 28, and then every 28 days until 48 weeks, with standard of care. Patients, investigators, study coordinators, and sponsors were masked to treatment assignment. Primary efficacy endpoint was improvement in the Systemic Lupus Erythematosus Responder Index (SRI) at week 52 (reduction \u22654 points in SELENA-SLEDAI score; no new British Isles Lupus Assessment Group [BILAG] A organ domain score and no more than 1 new B organ domain score; and no worsening [<0\u00b73 increase] in Physician's Global Assessment [PGA] score) versus baseline. Method of analysis was by modified intention to treat. This trial is registered with ClinicalTrials.gov, number NCT00424476.\n867 patients were randomly assigned to belimumab 1 mg/kg (n=289) or 10 mg/kg (n=290), or placebo (n=288). 865 were treated and analysed in the belimumab (1 mg/kg, n=288; 10 mg/kg, n=290) and placebo groups (n=287). Significantly higher SRI rates were noted with belimumab 1 mg/kg (148 [51%], odds ratio 1\u00b755 [95% CI 1\u00b710-2\u00b719]; p=0\u00b70129) and 10 mg/kg (167 [58%], 1\u00b783 [1\u00b730-2\u00b759]; p=0\u00b70006) than with placebo (125 [44%]) at week 52. More patients had their SELENA-SLEDAI score reduced by at least 4 points during 52 weeks with belimumab 1 mg/kg (153 [53%], 1\u00b751 [1\u00b707-2\u00b714]; p=0\u00b70189) and 10 mg/kg (169 [58%], 1\u00b771 [1\u00b721-2\u00b741]; p=0\u00b70024) than with placebo (132 [46%]). More patients given belimumab 1 mg/kg (226 [78%], 1\u00b738 [0\u00b793-2\u00b704]; p=0\u00b71064) and 10 mg/kg (236 [81%], 1\u00b762 [1\u00b709-2\u00b742]; p=0\u00b70181) had no new BILAG A or no more than 1 new B flare than did those in the placebo group (210 [73%]). No worsening in PGA score was noted in more patients with belimumab 1 mg/kg (227 [79%], 1\u00b768 [1\u00b715-2\u00b747]; p=0\u00b70078) and 10 mg/kg (231 [80%], 1\u00b774 [1\u00b718-2\u00b755]; p=0\u00b70048) than with placebo (199 [69%]). Rates of adverse events were similar in the groups given belimumab 1 mg/kg and 10 mg/kg, and placebo: serious infection was reported in 22 (8%), 13 (4%), and 17 (6%) patients, respectively, and severe or serious hypersensitivity reactions on an infusion day were reported in two (<1%), two (<1%), and no patients, respectively. No malignant diseases were reported.\nBelimumab has the potential to be the first targeted biological treatment that is approved specifically for systemic lupus erythematosus, providing a new option for the management of this important prototypic autoimmune disease.\nHuman Genome Sciences and GlaxoSmithKline.", "title": "Efficacy and safety of belimumab in patients with active systemic lupus erythematosus: a randomised, placebo-controlled, phase 3 trial.", "date": "2011-02-08"}]}
{"original_review": "24477672", "question_data": [{"question_id": 128, "question": "Is the incidence of critical illness polyneuropathy/myopathy (CIP/CIM) higher, lower, or the same when comparing intensive insulin therapy (IIT) to conventional insulin therapy (CIT)?", "answer": "lower", "evidence_quality": "moderate", "fulltext_required": "no", "comment": "", "relevant_sources": ["17138955", "15851721"]}, {"question_id": 129, "question": "Is duration of mechanical ventilation higher, lower, or the same when comparing intensive insulin therapy (IIT) to conventional insulin therapy (CIT)?", "answer": "lower", "evidence_quality": "high", "fulltext_required": "no", "comment": "", "relevant_sources": ["17138955", "15851721"]}, {"question_id": 130, "question": "Is mortality rate at 180 days higher, lower, or the same when comparing corticosteroids to placebo?", "answer": "no difference", "evidence_quality": "high", "fulltext_required": "no", "comment": "", "relevant_sources": ["16625008"]}, {"question_id": 131, "question": "Is the incidence of critical illness polyneuropathy/myopathy (CIP/CIM) higher, lower, or the same when comparing electrical muscle stimulation (EMS) to no stimulation?", "answer": "no difference", "evidence_quality": "very low", "fulltext_required": "no", "comment": "fulltext available but not needed; cochrane authors disagree with assessment of the original paper authors", "relevant_sources": ["20426834"]}, {"question_id": 237, "question": "Is duration of ICU stay higher, lower, or the same when comparing early physical therapy to control?", "answer": "no difference", "evidence_quality": "", "fulltext_required": "yes", "comment": "", "relevant_sources": ["19446324"]}, {"question_id": 238, "question": "Is duration of mechanical ventilation higher, lower, or the same when comparing early physical therapy to control?", "answer": "lower", "evidence_quality": "", "fulltext_required": "no", "comment": "", "relevant_sources": ["19446324"]}, {"question_id": 239, "question": "Is mortality rate at 30 days higher, lower, or the same when comparing early physical therapy to control?", "answer": "insufficient data", "evidence_quality": "", "fulltext_required": "no", "comment": "", "relevant_sources": ["19446324"]}, {"question_id": 240, "question": "Is mortality rate at 180 days higher, lower, or the same when comparing early physical therapy to control?", "answer": "insufficient data", "evidence_quality": "", "fulltext_required": "no", "comment": "", "relevant_sources": ["19446324"]}, {"question_id": 241, "question": "Is mortality rate at 180 days higher, lower, or the same when comparing electrical muscle stimulation (EMS) to no stimulation?", "answer": "insufficient data", "evidence_quality": "", "fulltext_required": "no", "comment": "", "relevant_sources": ["20426834"]}, {"question_id": 242, "question": "Is mortality rate at 30 days higher, lower, or the same when comparing electrical muscle stimulation (EMS) to no stimulation?", "answer": "insufficient data", "evidence_quality": "", "fulltext_required": "no", "comment": "", "relevant_sources": ["20426834"]}], "sources": [{"article_id": "15851721", "content": "To investigate the effectiveness of maintaining blood glucose levels below 6.1 mmol/L with insulin as prevention of secondary injury to the central and peripheral nervous systems of intensive care patients.\nThe authors studied the effect of intensive insulin therapy on critical illness polyneuropathy (CIPNP), assessed by weekly EMG screening, and its impact on mechanical ventilation dependency, as a prospectively planned subanalysis of a large randomized, controlled trial of 1,548 intensive care patients. In the 63 patients admitted with isolated brain injury, the authors studied the impact of insulin therapy on intracranial pressure, diabetes insipidus, seizures, and long-term rehabilitation at 6 and 12 months follow-up.\nIntensive insulin therapy reduced ventilation dependency (p = 0.0007; Mantel-Cox log rank test) and the risk of CIPNP (p < 0.0001). The risk of CIPNP among the 405 long-stay (> or =7 days in intensive care unit) patients was lowered by 49% (p < 0.0001). Of all metabolic and clinical effects of insulin therapy, and corrected for known risk factors, the level of glycemic control independently explained this benefit (OR for CIPNP 1.26 [1.09 to 1.46] per mmol blood glucose, p = 0.002). In turn, prevention of CIPNP explained the ability of intensive insulin therapy to reduce the risk of prolonged mechanical ventilation (OR 3.75 [1.49 to 9.39], p = 0.005). In isolated brain injury patients, intensive insulin therapy reduced mean (p = 0.003) and maximal (p < 0.0001) intracranial pressure while identical cerebral perfusion pressures were obtained with eightfold less vasopressors (p = 0.01). Seizures (p < 0.0001) and diabetes insipidus (p = 0.06) occurred less frequently. At 12 months follow-up, more brain-injured survivors in the intensive insulin group were able to care for most of their own needs (p = 0.05).\nPreventing even moderate hyperglycemia with insulin during intensive care protected the central and peripheral nervous systems, with clinical consequences such as shortening of intensive care dependency and possibly better long-term rehabilitation.", "title": "Insulin therapy protects the central and peripheral nervous system of intensive care patients.", "date": "2005-04-27"}, {"article_id": "16625008", "content": "Persistent acute respiratory distress syndrome (ARDS) is characterized by excessive fibroproliferation, ongoing inflammation, prolonged mechanical ventilation, and a substantial risk of death. Because previous reports suggested that corticosteroids may improve survival, we performed a multicenter, randomized controlled trial of corticosteroids in patients with persistent ARDS.\nWe randomly assigned 180 patients with ARDS of at least seven days' duration to receive either methylprednisolone or placebo in a double-blind fashion. The primary end point was mortality at 60 days. Secondary end points included the number of ventilator-free days and organ-failure-free days, biochemical markers of inflammation and fibroproliferation, and infectious complications.\nAt 60 days, the hospital mortality rate was 28.6 percent in the placebo group (95 percent confidence interval, 20.3 to 38.6 percent) and 29.2 percent in the methylprednisolone group (95 percent confidence interval, 20.8 to 39.4 percent; P=1.0); at 180 days, the rates were 31.9 percent (95 percent confidence interval, 23.2 to 42.0 percent) and 31.5 percent (95 percent confidence interval, 22.8 to 41.7 percent; P=1.0), respectively. Methylprednisolone was associated with significantly increased 60- and 180-day mortality rates among patients enrolled at least 14 days after the onset of ARDS. Methylprednisolone increased the number of ventilator-free and shock-free days during the first 28 days in association with an improvement in oxygenation, respiratory-system compliance, and blood pressure with fewer days of vasopressor therapy. As compared with placebo, methylprednisolone did not increase the rate of infectious complications but was associated with a higher rate of neuromuscular weakness.\nThese results do not support the routine use of methylprednisolone for persistent ARDS despite the improvement in cardiopulmonary physiology. In addition, starting methylprednisolone therapy more than two weeks after the onset of ARDS may increase the risk of death. (ClinicalTrials.gov number, NCT00295269.).", "title": "Efficacy and safety of corticosteroids for persistent acute respiratory distress syndrome.", "date": "2006-04-21"}, {"article_id": "17138955", "content": "Critical illness polyneuropathy/myopathy causes limb and respiratory muscle weakness, prolongs mechanical ventilation, and extends hospitalization of intensive care patients. Besides controlling risk factors, no specific prevention or treatment exists. Recently, intensive insulin therapy prevented critical illness polyneuropathy in a surgical intensive care unit.\nTo investigate the impact of intensive insulin therapy on polyneuropathy/myopathy and treatment with prolonged mechanical ventilation in medical patients in the intensive care unit for at least 7 days.\nThis was a prospectively planned subanalysis of a randomized controlled trial evaluating the effect of intensive insulin versus conventional therapy on morbidity and mortality in critically ill medical patients. All patients who were still in intensive care on Day 7 were screened weekly by electroneuromyography. The effect of intensive insulin therapy on critical illness polyneuropathy/myopathy and the relationship with duration of mechanical ventilation were assessed.\nIndependent of risk factors, intensive insulin therapy reduced incidence of critical illness polyneuropathy/myopathy (107/212 [50.5%] to 81/208 [38.9%], p = 0.02). Treatment with prolonged (> or = 14 d) mechanical ventilation was reduced from 99 of 212 (46.7%) to 72 of 208 (34.6%) (p = 0.01). This was statistically only partially explained by prevention of critical illness polyneuropathy/myopathy.\nIn a subset of medical patients in the intensive care unit for at least 7 days, enrolled in a randomized controlled trial of intensive insulin therapy, those assigned to intensive insulin therapy had a reduced incidence of critical illness polyneuropathy/myopathy and were treated with prolonged mechanical ventilation less frequently.", "title": "Impact of intensive insulin therapy on neuromuscular complications and ventilator dependency in the medical intensive care unit.", "date": "2006-12-02"}, {"article_id": "19446324", "content": "Long-term complications of critical illness include intensive care unit (ICU)-acquired weakness and neuropsychiatric disease. Immobilisation secondary to sedation might potentiate these problems. We assessed the efficacy of combining daily interruption of sedation with physical and occupational therapy on functional outcomes in patients receiving mechanical ventilation in intensive care.\nSedated adults (>/=18 years of age) in the ICU who had been on mechanical ventilation for less than 72 h, were expected to continue for at least 24 h, and who met criteria for baseline functional independence were eligible for enrolment in this randomised controlled trial at two university hospitals. We randomly assigned 104 patients by computer-generated, permuted block randomisation to early exercise and mobilisation (physical and occupational therapy) during periods of daily interruption of sedation (intervention; n=49) or to daily interruption of sedation with therapy as ordered by the primary care team (control; n=55). The primary endpoint-the number of patients returning to independent functional status at hospital discharge-was defined as the ability to perform six activities of daily living and the ability to walk independently. Therapists who undertook patient assessments were blinded to treatment assignment. Secondary endpoints included duration of delirium and ventilator-free days during the first 28 days of hospital stay. Analysis was by intention to treat. This trial is registered with ClinicalTrials.gov, number NCT00322010.\nAll 104 patients were included in the analysis. Return to independent functional status at hospital discharge occurred in 29 (59%) patients in the intervention group compared with 19 (35%) patients in the control group (p=0.02; odds ratio 2.7 [95% CI 1.2-6.1]). Patients in the intervention group had shorter duration of delirium (median 2.0 days, IQR 0.0-6.0 vs 4.0 days, 2.0-8.0; p=0.02), and more ventilator-free days (23.5 days, 7.4-25.6 vs 21.1 days, 0.0-23.8; p=0.05) during the 28-day follow-up period than did controls. There was one serious adverse event in 498 therapy sessions (desaturation less than 80%). Discontinuation of therapy as a result of patient instability occurred in 19 (4%) of all sessions, most commonly for perceived patient-ventilator asynchrony.\nA strategy for whole-body rehabilitation-consisting of interruption of sedation and physical and occupational therapy in the earliest days of critical illness-was safe and well tolerated, and resulted in better functional outcomes at hospital discharge, a shorter duration of delirium, and more ventilator-free days compared with standard care.\nNone.", "title": "Early physical and occupational therapy in mechanically ventilated, critically ill patients: a randomised controlled trial.", "date": "2009-05-19"}, {"article_id": "20426834", "content": "Critical illness polyneuromyopathy (CIPNM) is a common complication of critical illness presenting with muscle weakness and is associated with increased duration of mechanical ventilation and weaning period. No preventive tool and no specific treatment have been proposed so far for CIPNM. Electrical muscle stimulation (EMS) has been shown to be beneficial in patients with severe chronic heart failure and chronic obstructive pulmonary disease. Aim of our study was to assess the efficacy of EMS in preventing CIPNM in critically ill patients.\nOne hundred and forty consecutive critically ill patients with an APACHE II score >or= 13 were randomly assigned after stratification to the EMS group (n = 68) (age:61 +/- 19 years) (APACHE II:18 +/- 4, SOFA:9 +/- 3) or to the control group (n = 72) (age:58 +/- 18 years) (APACHE II:18 +/- 5, SOFA:9 +/- 3). Patients of the EMS group received daily EMS sessions. CIPNM was diagnosed clinically with the medical research council (MRC) scale for muscle strength (maximum score 60, <48/60 cut off for diagnosis) by two unblinded independent investigators. Duration of weaning from mechanical ventilation and intensive care unit (ICU) stay were recorded.\nFifty two patients could be finally evaluated with MRC; 24 in the EMS group and 28 in the control group. CIPNM was diagnosed in 3 patients in the EMS group as compared to 11 patients in the control group (OR = 0.22; CI: 0.05 to 0.92, P = 0.04). The MRC score was significantly higher in patients of the EMS group as compared to the control group [58 (33 to 60) vs. 52 (2 to 60) respectively, median (range), P = 0.04). The weaning period was statistically significantly shorter in patients of the EMS group vs. the control group [1 (0 to 10) days vs. 3 (0 to 44) days, respectively, median (range), P = 0.003].\nThis study suggests that daily EMS sessions prevent the development of CIPNM in critically ill patients and also result in shorter duration of weaning. Further studies should evaluate which patients benefit more from EMS and explore the EMS characteristics most appropriate for preventing CIPNM.\nClinicalTrials.gov NCT00882830.", "title": "Electrical muscle stimulation prevents critical illness polyneuromyopathy: a randomized parallel intervention trial.", "date": "2010-04-30"}]}
{"original_review": "26689943", "question_data": [{"question_id": 132, "question": "Is the rate of all\u2010cause hospital admission higher, lower, or the same when comparing nurse-led titration (NLT) to usual care?", "answer": "lower", "evidence_quality": "high", "fulltext_required": "no", "comment": "12756157 omitted with 12% weight, 25248944 requires fulltext, ", "relevant_sources": ["16908918", "25248944", "23150980"]}], "sources": [{"article_id": "23150980", "content": "Many older people in long-term care do not receive evidence-based diagnosis or management for heart failure; it is not known whether this can be achieved for this population. We initiated an onsite heart failure service, compared with 'usual care' with the aim of establishing the feasibility of accurate diagnosis and appropriate management.\nA pilot randomised controlled trial which randomised residents from 33 care facilities in North-East England with left ventricular systolic dysfunction (LVSD) to usual care or an onsite heart failure service. The primary outcome was the optimum prescription of angiotensin-converting enzyme inhibitors and beta-adrenergic antagonists at 6 months.\nOf 399 echocardiographically-screened residents aged 65-100 years, 30 subjects with LVSD were eligible; 28 (93%) consented and were randomised (HF service: 16; routine care: 12). Groups were similar at baseline; six month follow-up was completed for 25 patients (89%); 3 (11%) patients died. Results for the primary outcome were not statistically significant but there was a consistent pattern of increased drug use and titration to optimum dose in the intervention group (21% compared to 0% receiving routine care, p=0.250). Hospitalisation rates, quality of life and mortality at 6 months were similar between groups.\nThis study demonstrated the feasibility of an on-site heart failure service for older long-term care populations. Optimisation of medication appeared possible without adversely affecting quality of life; this questions clinicians' concerns about adverse effects in this group. This has international implications for managing such patients. These methods should be replicated in a large-scale study to quantify the scale of benefit.\nISRCTN19781227 http://www.controlled-trials.com/ISRCTN19781227", "title": "Feasibility of evidence-based diagnosis and management of heart failure in older people in care: a pilot randomised controlled trial.", "date": "2012-11-16"}, {"article_id": "17065182", "content": "To determine whether an intensive intervention at a heart failure (HF) clinic by a combination of a clinician and a cardiovascular nurse, both trained in HF, reduces the incidence of hospitalisation for worsening HF and/or all-cause mortality (primary end point) and improves functional status (including left ventricular ejection fraction, New York Heart Association (NYHA) class and quality of life) in patients with NYHA class III or IV.\nTwo regional teaching hospitals in The Netherlands.\n240 patients were randomly allocated to the 1-year intervention (n = 118) or usual care (n = 122). The intervention consisted of 9 scheduled patient contacts-at day 3 by telephone, and at weeks 1, 3, 5, 7 and at months 3, 6, 9 and 12 by a visit-to a combined, intensive physician-and-nurse-directed HF outpatient clinic, starting within a week after hospital discharge from the hospital or referral from the outpatient clinic. Verbal and written comprehensive education, optimisation of treatment, easy access to the clinic, recommendations for exercise and rest, and advice for symptom monitoring and self-care were provided. Usual care included outpatient visits initialized by individual cardiologists in the cardiology departments involved and applying the guidelines of the European Society of Cardiology.\nDuring the 12-month study period, the number of admissions for worsening HF and/or all-cause deaths in the intervention group was lower than in the control group (23 vs 47; relative risk (RR) 0.49; 95% confidence interval (CI) 0.30 to 0.81; p = 0.001). There was an improvement in left ventricular ejection fraction (LVEF) in the intervention group (plus 2.6%) compared with the usual care group (minus 3.1%; p = 0.004). Patients in the intervention group were hospitalised for a total of 359 days compared with 644 days for those in the usual care group. Beneficial effects were also observed on NYHA classification, prescription of spironolactone, maximally reached dose of beta-blockers, quality of life, self-care behaviour and healthcare costs.\nA heart failure clinic involving an intensive intervention by both a clinician and a cardiovascular nurse substantially reduces hospitalisations for worsening HF and/or all-cause mortality and improves functional status, while decreasing healthcare costs, even in a country with a primary-care-based healthcare system.", "title": "Added value of a physician-and-nurse-directed heart failure clinic: results from the Deventer-Alkmaar heart failure study.", "date": "2006-10-27"}, {"article_id": "16908918", "content": "Despite therapies proven effective for heart failure with systolic dysfunction, the condition continues to cause substantial hospitalization, disability, and death, especially among African- American and other nonwhite populations.\nTo compare the effects of a nurse-led intervention focused on specific management problems versus usual care among ethnically diverse patients with systolic dysfunction in ambulatory care practices.\nRandomized effectiveness trial conducted from September 2000 to September 2002.\nThe 4 hospitals in Harlem, New York.\n406 adults (45.8% were non-Hispanic black adults, 32.5% were Hispanic adults, 46.3% were women, and 36.7% were > or =65 years of age) who met eligibility criteria: systolic dysfunction, English- or Spanish-language speakers, community-dwelling patients, and ambulatory care practice patients.\nDuring a 12-month intervention, bilingual nurses counseled patients on diet, medication adherence, and self-management of symptoms through an initial visit and regularly scheduled follow-up telephone calls and facilitated evidence-based changes to medications in discussions with patients' clinicians.\nHospitalizations (in 406 of 406 patients during follow-up) and self-reported functioning (in 286 of 406 patients during follow-up) at 12 months.\nAt 12 months, nurse management patients had had fewer hospitalizations (143 hospitalizations vs. 180 hospitalizations; adjusted difference, -0.13 hospitalization/person-year [95% CI, -0.25 to -0.001 hospitalization/person-year]) than usual care patients. They also had better functioning: The Short Form-12 physical component score was 39.9 versus 36.3, respectively (difference, 3.6 [CI, 1.2 to 6.1]), and the Minnesota Living with Heart Failure Questionnaire score was 38.6 versus 47.3, respectively (difference, -8.8 [CI, -15.3 to -2.2]). Through 12 months, 22 deaths occurred in each group and percentages of patients who were hospitalized at least once were similar in each group (30.5% of nurse management patients vs. 36.5% of control patients; adjusted difference, -7.1 percentage points [CI, -16.9 to 2.6 percentage points]).\nThree nurses at 4 hospitals delivered interventions in this modest-sized trial, and 75% of the participants were from 1 site. It is not clear which aspects of the complex intervention accounted for the results.\nNurse management can improve functioning and modestly lower hospitalizations in ethnically diverse ambulatory care patients who have heart failure with systolic dysfunction. Sustaining improved functioning may require continuing nurse contact.", "title": "Effects of nurse management on the quality of heart failure care in minority communities: a randomized trial.", "date": "2006-08-16"}, {"article_id": "12788301", "content": "The aim of this trial was to prospectively evaluate the effect of follow-up at a nurse-led heart failure clinic on mortality, morbidity and self-care behaviour for patients hospitalised due to heart failure for 12 months after discharge.\nA total of 106 patients were randomly assigned to either follow-up at a nurse-led heart failure clinic or to usual care. The nurse-led heart failure clinic was staffed by specially educated and experienced cardiac nurses, delegated the responsibility for making protocol-led changes in medications. The first follow-up visit was 2-3 weeks after discharge. During the visit the nurse evaluated the heart failure status and the treatment, gave education about heart failure and social support to the patient and his family.\nThere were fewer patients with events (death or admission) after 12 months in the intervention group compared to the control group (29 vs 40, p=0.03) and fewer deaths after 12 months (7 vs 20, p=0.005). The intervention group had fewer admissions (33 vs 56, p=0.047) and days in hospital (350 vs 592, p=0.045) during the first 3 months. After 12 months the intervention was associated with a 55% decrease in admissions/patient/month (0.18 vs 0.40, p=0.06) and fewer days in hospital/patient/month (1.4 vs 3.9, p=0.02). The intervention group had significantly higher self-care scores at 3 and 12 months compared to the control group (p=0.02 and p=0.01).\nFollow up after hospitalisation at a nurse-led heart failure clinic can improve survival and self-care behaviour in patients with heart failure as well as reduce the number of events, readmissions and days in hospital.", "title": "Nurse-led heart failure clinics improve survival and self-care behaviour in patients with heart failure: results from a prospective, randomised trial.", "date": "2003-06-06"}, {"article_id": "25248944", "content": "Beta-adrenergic blockade has been shown to improve left ventricular function, reduce hospital admissions and improve survival in chronic heart failure with reduced ejection fraction (HFrEF), with mortality reduction starting early after beta-adrenergic receptor blocker initiation and being dose-related. The aim of this pilot study was to determine the effectiveness of a nurse-led titration clinic in improving the time required for patients to reach optimal doses of the beta-adrenergic receptor blocking agents.\nWe conducted a prospective pilot randomized controlled trial. Twenty eight patients with CHF were randomized to optimisation of beta-adrenergic receptor blocker therapy over six months by either a nurse-led titration (NLT) clinic, led by a nurse specialist with the support of a cardiologist in a CHF clinic, or by their primary care physician (usual care (UC)). The primary endpoint was time to maximal beta-adrenergic receptor blocker dose. The secondary end-point was the proportion of patients reaching the target dose of beta-adrenergic receptor blocker by six months.\nThe patients were predominantly men (72%), age 67 \u00b1 16 years; New York Heart Association (NYHA) functional class I (32%), II (44%) and III (20%); baseline left ventricular ejection fraction 33 \u00b1 10%, and a low mean Charlson co-morbidity score of 2.5 \u00b1 1.4. The time to maximum dose was shorter in the NLT group compared to the UC group (90 \u00b1 14 vs 166 \u00b1 8 days, p < 0.0005). At six months, in the NLT group there were nine patients (82%) on high dose and one patient (9%) on low dose beta-adrenergic receptor blocker compared to the UC group with five (42%) patients reaching maximum dose and five (42%) patients on low dose (p = 0.04). The patients allocated to the NLT group also had significantly less worsening of depression between baseline and six months (p = 0.006).\nA NLT clinic improves optimisation of beta-adrenergic receptor blocker therapy through increasing the proportion of patients reaching maximal dose and facilitating rapid up-titration of beta-adrenergic receptor blocker agents in patients with chronic HFrEF.\nAustralian Clinical Trials Registry (ACTRN012606000383561).", "title": "A nurse-led up-titration clinic improves chronic heart failure optimization of beta-adrenergic receptor blocking therapy--a randomized controlled trial.", "date": "2014-09-25"}, {"article_id": "25727879", "content": "Heart failure (HF) pharmacotherapy is often not prescribed according to guidelines. This longitudinal study investigated prescription rates and dosages of angiotensin-converting enzyme inhibitors/angiotensin receptor blockers (ACEi/ARB), beta-blockers, and mineralocorticoid receptor antagonists (MRA), and concomitant changes of symptoms, echocardiographic parameters of left ventricular (LV) function and morphology and results of the Short Form-36 (SF-36) Health Survey in participants of the Interdisciplinary Network Heart Failure (INH) programme.\nThe INH study evaluated a nurse-coordinated management, HeartNetCare-HF(TM) (HNC), against Usual Care (UC) in patients hospitalized for decompensated HF [LV ejection fraction (LVEF) \u226440% before discharge). A total of 706 subjects surviving >18 months (363 UC, 343 HNC) were examined 6-monthly. At baseline, 92% received ACEi/ARB, (HNC/UC 91/93%, P = 0.28), 86% received beta-blockers (86/86%, P = 0.83), and 44% received MRA (42/47%, P = 0.07). After 18 months, beta-blocker use had increased only in HNC (+7.6%, P < 0.001). Guideline-recommended target doses were achieved more frequently in HNC for ACEi/ARB (HNC/UC: 50/25%, P < 0.001) and beta-blockers (39/15%, P < 0.001). The following variables were more improved and/or better in subjects undergoing HNC compared with UC: LVEF (47 \u00b1 12 vs. 44 \u00b1 12%, P = 0.004, change +17/+14%, P = 0.010), LV end-diastolic diameter (59 \u00b1 9 vs. 61 \u00b1 9.6 mm, P = 0.024, change -2.3/-1.4 mm, P = 0.13), New York Heart Association class (1.9 \u00b1 0.7 vs. 2.1 \u00b1 0.7, P = 0.001, change -0.44/-0.25, P = 0.002) and SF-36 physical component summary score (41.6 \u00b1 11.2 vs. 38.5 \u00b1 11.8, P = 0.004, change +3.3 vs. +1.1 score points, P < 0.02).\nPrescription rates and dosages of ACEi/ARB and beta-blockers improved more in HNC than UC patients. Concomitantly, participation in HNC was associated with significantly better clinical outcomes and more favourable echocardiographic changes after 18 months.", "title": "Nurse-coordinated collaborative disease management improves the quality of guideline-recommended heart failure therapy, patient-reported outcomes, and left ventricular remodelling.", "date": "2015-03-03"}, {"article_id": "12756157", "content": "The dissemination of clinical practice guidelines often has not been accompanied by desired improvements in guideline adherence. This study evaluated interventions for implementing a new practice guideline advocating the use of beta-blockers for heart failure patients.\nThis was a randomized controlled trial involving heart failure patients (n=169) with an ejection fraction < or =45% and no contraindications to beta-blockers. Patients' primary providers were randomized in a stratified design to 1 of 3 interventions: (1) control: provider education; (2) provider and patient notification: computerized provider reminders and patient letters advocating beta-blockers; and (3) nurse facilitator: supervised nurse to initiate and titrate beta-blockers. The primary outcome, the proportion of patients who were initiated or uptitrated and maintained on beta-blockers, analyzed by intention to treat, was achieved in 67% (36 of 54) of patients in the nurse facilitator group compared with 16% (10 of 64) in the provider/patient notification and 27% (14 of 51) in the control groups (P<0.001 for the comparisons between the nurse facilitator group and both other groups). The proportion of patients on target beta-blocker doses at the study end (median follow-up, 12 months) was also highest in the nurse facilitator group (43%) compared with the control (10%) and provider/patient notification groups (2%) (P<0.001). There were no differences in adverse events among groups.\nThe use of a nurse facilitator was a successful approach for implementing a beta-blocker guideline in heart failure patients. The use of provider education, clinical reminders, and patient education was of limited value in this setting.", "title": "Improving guideline adherence: a randomized trial evaluating strategies to increase beta-blocker use in heart failure.", "date": "2003-05-21"}]}
{"original_review": "37497816", "question_data": [{"question_id": 133, "question": "Is time to complete ulcer healing higher, lower, or the same when comparing combined endovenous ablation and compression to compression therapy without or with deferred endovenous treatment?", "answer": "lower", "evidence_quality": "high", "fulltext_required": "no", "comment": "", "relevant_sources": ["29688123", "32928070"]}, {"question_id": 134, "question": "Is ulcer recurrence higher, lower, or the same when comparing combined endovenous ablation and compression to compression therapy without or with deferred endovenous treatment?", "answer": "uncertain effect", "evidence_quality": "low", "fulltext_required": "yes", "comment": "", "relevant_sources": ["32965493", "32928070"]}], "sources": [{"article_id": "30741425", "content": "Treatment of superficial venous reflux in addition to compression therapy accelerates venous leg ulcer healing and reduces ulcer recurrence. The aim of this study was to evaluate the costs and cost-effectiveness of early versus delayed endovenous treatment of patients with venous leg ulcers.\nThis was a within-trial cost-utility analysis with a 1-year time horizon using data from the EVRA (Early Venous Reflux Ablation) trial. The study compared early versus deferred endovenous ablation for superficial venous truncal reflux in patients with a venous leg ulcer. The outcome measure was the cost per quality-adjusted life-year (QALY) over 1 year. Sensitivity analyses were conducted with alternative methods of handling missing data, alternative preference weights for health-related quality of life, and per protocol.\nAfter early intervention, the mean(s.e.m.) cost was higher (difference in cost per patient \u00a3163(318) (\u20ac184(358))) and early intervention was associated with more QALYs at 1 year (mean(s.e.m.) difference 0\u00b7041(0\u00b7017)). The incremental cost-effectiveness ratio (ICER) was \u00a33976 (\u20ac4482) per QALY. There was an 89 per cent probability that early venous intervention is cost-effective at a threshold of \u00a320 000 (\u20ac22 546)/QALY. Sensitivity analyses produced similar results, confirming that early treatment of superficial reflux is highly likely to be cost-effective.\nEarly treatment of superficial reflux is highly likely to be cost-effective in patients with venous leg ulcers over 1 year. Registration number: ISRCTN02335796 (http://www.isrctn.com).", "title": "Cost-effectiveness analysis of a randomized clinical trial of early versus deferred endovenous ablation of superficial venous reflux in patients with venous ulceration.", "date": "2019-02-12"}, {"article_id": "29688123", "content": "Venous disease is the most common cause of leg ulceration. Although compression therapy improves venous ulcer healing, it does not treat the underlying causes of venous hypertension. Treatment of superficial venous reflux has been shown to reduce the rate of ulcer recurrence, but the effect of early endovenous ablation of superficial venous reflux on ulcer healing remains unclear.\nIn a trial conducted at 20 centers in the United Kingdom, we randomly assigned 450 patients with venous leg ulcers to receive compression therapy and undergo early endovenous ablation of superficial venous reflux within 2 weeks after randomization (early-intervention group) or to receive compression therapy alone, with consideration of endovenous ablation deferred until after the ulcer was healed or until 6 months after randomization if the ulcer was unhealed (deferred-intervention group). The primary outcome was the time to ulcer healing. Secondary outcomes were the rate of ulcer healing at 24 weeks, the rate of ulcer recurrence, the length of time free from ulcers (ulcer-free time) during the first year after randomization, and patient-reported health-related quality of life.\nPatient and clinical characteristics at baseline were similar in the two treatment groups. The time to ulcer healing was shorter in the early-intervention group than in the deferred-intervention group; more patients had healed ulcers with early intervention (hazard ratio for ulcer healing, 1.38; 95% confidence interval [CI], 1.13 to 1.68; P=0.001). The median time to ulcer healing was 56 days (95% CI, 49 to 66) in the early-intervention group and 82 days (95% CI, 69 to 92) in the deferred-intervention group. The rate of ulcer healing at 24 weeks was 85.6% in the early-intervention group and 76.3% in the deferred-intervention group. The median ulcer-free time during the first year after trial enrollment was 306 days (interquartile range, 240 to 328) in the early-intervention group and 278 days (interquartile range, 175 to 324) in the deferred-intervention group (P=0.002). The most common procedural complications of endovenous ablation were pain and deep-vein thrombosis.\nEarly endovenous ablation of superficial venous reflux resulted in faster healing of venous leg ulcers and more time free from ulcers than deferred endovenous ablation. (Funded by the National Institute for Health Research Health Technology Assessment Program; EVRA Current Controlled Trials number, ISRCTN02335796 .).", "title": "A Randomized Trial of Early Endovenous Ablation in Venous Ulceration.", "date": "2018-04-25"}, {"article_id": "32965493", "content": "One-year outcomes from the Early Venous Reflux Ablation (EVRA) randomized trial showed accelerated venous leg ulcer healing and greater ulcer-free time for participants who are treated with early endovenous ablation of lower extremity superficial reflux.\nTo evaluate the clinical and cost-effectiveness of early endovenous ablation of superficial venous reflux in patients with venous leg ulceration.\nBetween October 24, 2013, and September 27, 2016, the EVRA randomized clinical trial enrolled 450 participants (450 legs) with venous leg ulceration of less than 6 months' duration and superficial venous reflux. Initially, 6555 patients were assessed for eligibility, and 6105 were excluded for reasons including ulcer duration greater than 6 months, healed ulcer by the time of randomization, deep venous occlusive disease, and insufficient superficial venous reflux to warrant ablation therapy, among others. A total of 426 of 450 participants (94.7%) from the vascular surgery departments of 20 hospitals in the United Kingdom were included in the analysis for ulcer recurrence. Surgeons, participants, and follow-up assessors were not blinded to the treatment group. Data were analyzed from August 11 to November 4, 2019.\nPatients were randomly assigned to receive compression therapy with early endovenous ablation within 2 weeks of randomization (early intervention, n\u2009=\u2009224) or compression with deferred endovenous treatment of superficial venous reflux (deferred intervention, n\u2009=\u2009226). Endovenous modality and strategy were left to the preference of the treating clinical team.\nThe primary outcome for the extended phase was time to first ulcer recurrence. Secondary outcomes included ulcer recurrence rate and cost-effectiveness.\nThe early-intervention group consisted of 224 participants (mean [SD] age, 67.0 [15.5] years; 127 men [56.7%]; 206 White participants [92%]). The deferred-intervention group consisted of 226 participants (mean [SD] age, 68.9 [14.0] years; 120 men [53.1%]; 208 White participants [92%]). Of the 426 participants whose leg ulcer had healed, 121 (28.4%) experienced at least 1 recurrence during follow-up. There was no clear difference in time to first ulcer recurrence between the 2 groups (hazard ratio, 0.82; 95% CI, 0.57-1.17; P\u2009=\u2009.28). Ulcers recurred at a lower rate of 0.11 per person-year in the early-intervention group compared with 0.16 per person-year in the deferred-intervention group (incidence rate ratio, 0.658; 95% CI, 0.480-0.898; P\u2009=\u2009.003). Time to ulcer healing was shorter in the early-intervention group for primary ulcers (hazard ratio, 1.36; 95% CI, 1.12-1.64; P\u2009=\u2009.002). At 3 years, early intervention was 91.6% likely to be cost-effective at a willingness to pay of \u00a320\u202f000 ($26\u202f283) per quality-adjusted life year and 90.8% likely at a threshold of \u00a335\u202f000 ($45\u202f995) per quality-adjusted life year.\nEarly endovenous ablation of superficial venous reflux was highly likely to be cost-effective over a 3-year horizon compared with deferred intervention. Early intervention accelerated the healing of venous leg ulcers and reduced the overall incidence of ulcer recurrence.\nClinicalTrials.gov identifier: ISRCTN02335796.", "title": "Long-term Clinical and Cost-effectiveness of Early Endovenous Ablation in Venous Ulceration: A Randomized Clinical Trial.", "date": "2020-09-24"}, {"article_id": "32928070", "content": "To investigate whether radiofrequency endovenous ablation (RFA) of saphenous and perforating veins increases venous leg ulcer (VLU) healing rates and prevents ulcer recurrence.\nThis prospective, open-label, randomized, controlled, single-center trial recruited 56 patients with VLU divided into: compression alone (CR, N\u2009=\u200929) and RFA plus compression (RF, N\u2009=\u200927). Primary endpoints were ulcer recurrence rate at 12\u2009months; and ulcer healing rates at 6, 12, and 24\u2009weeks. Secondary endpoints were ulcer healing velocity; and Venous Clinical Severity Score (VCSS).\nRecurrence was lower in the RF group (p\u2009<\u2009.001), as well as mean VCSS after treatment (p\u2009=\u2009.001). There were no significant between-group differences in healing rates. Healing velocity was faster in the RF group (p\u2009=\u20090.049). In the RF group, 2 participants had type 1 endovenous heat-induced thrombosis (EHIT).\nRFA plus compression is an excellent treatment for VLU because of its safety, effectiveness, and impact on ulcer recurrence reduction and clinical outcome.", "title": "A randomized clinical trial of the effects of saphenous and perforating veins radiofrequency ablation on venous ulcer healing (VUERT trial).", "date": "2020-09-16"}]}
{"original_review": "34002371", "question_data": [{"question_id": 135, "question": "Is the risk of stroke higher, lower, or the same when comparing rivaroxaban to placebo?", "answer": "lower", "evidence_quality": "moderate", "fulltext_required": "yes", "comment": "", "relevant_sources": ["31461239"]}], "sources": [{"article_id": "31461239", "content": "Stroke is often a devastating event among patients with heart failure with reduced ejection (HFrEF). In COMMANDER HF, rivaroxaban 2.5\u2009mg b.i.d. did not reduce the composite of first occurrence of death, stroke, or myocardial infarction compared with placebo in patients with HFrEF, coronary artery disease (CAD), and sinus rhythm. We now examine the incidence, timing, type, severity, and predictors of stroke or a transient ischaemic attack (TIA), and seek to establish the net clinical benefit of treatment with low-dose rivaroxaban.\nIn this double-blind, randomized trial, 5022 patients who had HFrEF(\u226440%), elevated natriuretic peptides, CAD, and who were in sinus rhythm were treated with rivaroxaban 2.5\u2009mg b.i.d. or placebo in addition to antiplatelet therapy, after an episode of worsening HF. The primary neurological outcome for this post hoc analysis was time to first event of any stroke or TIA. Over a median follow-up of 20.5 (25th-75th percentiles 20.0-20.9) months, 150 all-cause stroke (127) or TIA (23) events occurred (ischaemic stroke in 82% and haemorrhagic stroke in 11% of stroke events). Overall, 47.5% of first-time strokes were either disabling (16.5%) or fatal (31%). Prior stroke, low body mass index, geographic region, and the CHA2DS2-VASc score were predictors of stroke/TIA. Rivaroxaban significantly reduced the primary neurological endpoint of all-cause stroke or TIA compared with placebo by 32% (1.29 events vs. 1.90 events per 100 patient-years), adjusted for the time from index HF event to randomization and stratified by geographic region (adjusted hazard ratio 0.68, 95% confidence interval 0.49-0.94), with a number needed to treat of 164 patients per year to prevent one stroke/TIA event. The principal safety endpoint of fatal bleeding or bleeding into a critical space, occurred at a similar rate on rivaroxaban and placebo (0.44 events vs. 0.55 events per 100 patient-years).\nPatients with HFrEF and CAD are at risk for stroke or TIA in the period following an episode of worsening heart failure in the absence of atrial fibrillation. Most strokes are of ischaemic origin and nearly half are either disabling or fatal. Rivaroxaban at a dose of 2.5\u2009mg b.i.d. reduced rates of stroke or TIA compared with placebo in this population.\nCOMMANDER HF (A Study to Assess the Effectiveness and Safety of Rivaroxaban in Reducing the Risk of Death, Myocardial Infarction, or Stroke in Participants with Heart Failure and Coronary Artery Disease Following an Episode of Decompensated Heart Failure); ClinicalTrials.gov NCT01877915.", "title": "A comprehensive analysis of the effects of rivaroxaban on stroke or transient ischaemic attack in patients with heart failure, coronary artery disease, and sinus rhythm: the COMMANDER HF trial.", "date": "2019-08-29"}, {"article_id": "30146935", "content": "Heart failure is associated with activation of thrombin-related pathways, which predicts a poor prognosis. We hypothesized that treatment with rivaroxaban, a factor Xa inhibitor, could reduce thrombin generation and improve outcomes for patients with worsening chronic heart failure and underlying coronary artery disease.\nIn this double-blind, randomized trial, 5022 patients who had chronic heart failure, a left ventricular ejection fraction of 40% or less, coronary artery disease, and elevated plasma concentrations of natriuretic peptides and who did not have atrial fibrillation were randomly assigned to receive rivaroxaban at a dose of 2.5 mg twice daily or placebo in addition to standard care after treatment for an episode of worsening heart failure. The primary efficacy outcome was the composite of death from any cause, myocardial infarction, or stroke. The principal safety outcome was fatal bleeding or bleeding into a critical space with a potential for causing permanent disability.\nOver a median follow-up period of 21.1 months, the primary end point occurred in 626 (25.0%) of 2507 patients assigned to rivaroxaban and in 658 (26.2%) of 2515 patients assigned to placebo (hazard ratio, 0.94; 95% confidence interval [CI], 0.84 to 1.05; P=0.27). No significant difference in all-cause mortality was noted between the rivaroxaban group and the placebo group (21.8% and 22.1%, respectively; hazard ratio, 0.98; 95% CI, 0.87 to 1.10). The principal safety outcome occurred in 18 patients who took rivaroxaban and in 23 who took placebo (hazard ratio, 0.80; 95% CI, 0.43 to 1.49; P=0.48).\nRivaroxaban at a dose of 2.5 mg twice daily was not associated with a significantly lower rate of death, myocardial infarction, or stroke than placebo among patients with worsening chronic heart failure, reduced left ventricular ejection fraction, coronary artery disease, and no atrial fibrillation. (Funded by Janssen Research and Development; COMMANDER HF ClinicalTrials.gov number, NCT01877915 .).", "title": "Rivaroxaban in Patients with Heart Failure, Sinus Rhythm, and Coronary Disease.", "date": "2018-08-28"}, {"article_id": "16737850", "content": "It is not clear if long-term antithrombotic treatment has a beneficial effect on the incidence of thromboembolism in chronic heart failure (CHF). The HELAS study (Heart failure Long-term Antithrombotic Study) is a multicentre, randomised, double-blind, placebo-controlled trial to evaluate antithrombotic treatment in patients with CHF.\n197HF patients (EF <35%) were enrolled. Patients with Ischaemic Heart Disease were randomised to receive either aspirin 325mg or warfarin. Patients with Dilated Cardiomyopathy (DCM) were randomised to receive either warfarin or placebo.\nAnalysis of the data from 312 patient years showed an incidence of 2.2 embolic events per 100 patient years, with no significant difference between groups. The incidence of myocardial infarction, hospitalisation, exacerbation of heart failure, death and haemorrhage were not different between the groups. No peripheral or pulmonary emboli were reported. Echocardiographic follow-up for 2years showed an overall increase in left ventricular ejection fraction from 28.2+/-6 to 30.3+/-7 p<0.05, which was most obvious in patients with DCM taking warfarin (EF 26.8+/-5.3 at baseline, 30.7+/-10 at 2 years, p<0.05).\n(1) Overall embolic events are rare in heart failure regardless of treatment. (2) Treatment does not seem to affect outcome.", "title": "Efficacy of antithrombotic therapy in chronic heart failure: the HELAS study.", "date": "2006-06-02"}, {"article_id": "15215806", "content": "Heart failure is commonly associated with vascular disease and a high rate of athero-thrombotic events, but the risks and benefits of antithrombotic therapy are unknown.\nThe current study was an open-label, randomized, controlled trial comparing no antithrombotic therapy, aspirin (300 mg/day), and warfarin (target international normalized ratio 2.5) in patients with heart failure and left ventricular systolic dysfunction requiring diuretic therapy. The primary objective was to demonstrate the feasibility and inform the design of a larger outcome study. The primary clinical outcome was death, nonfatal myocardial infarction, or nonfatal stroke.\nTwo hundred seventy-nine patients were randomized and 627 patient-years exposure were accumulated over a mean follow-up time of 27 +/- 1 months. Twenty-six (26%), 29 (32%), and 23 (26%) patients randomized to no antithrombotic treatment, aspirin, and warfarin, respectively, reached the primary outcome (ns). There were trends to a worse outcome among those randomized to aspirin for a number of secondary outcomes. Significantly (P =.044) more patients randomized to aspirin were hospitalized for cardiovascular reasons, especially worsening heart failure.\nThe Warfarin/Aspirin Study in Heart failure (WASH) provides no evidence that aspirin is effective or safe in patients with heart failure. The benefits of warfarin for patients with heart failure in sinus rhythm have not been established. Antithrombotic therapy in patients with heart failure is not evidence based but commonly contributes to polypharmacy.", "title": "The Warfarin/Aspirin Study in Heart failure (WASH): a randomized trial comparing antithrombotic strategies for patients with heart failure.", "date": "2004-06-25"}]}
{"original_review": "29446439", "question_data": [{"question_id": 136, "question": "Is the proportion of children wearing spectacles higher, lower, or the same when comparing vision screening with free spectacles to vision screening with spectacles by prescription only?", "answer": "higher", "evidence_quality": "high", "fulltext_required": "no", "comment": "fulltext for 25249453 is available but not needed", "relevant_sources": ["18156372", "25249453"]}, {"question_id": 137, "question": "Is educational attainment higher, lower, or the same when comparing vision screening with free spectacles to vision screening with spectacles by prescription only?", "answer": "higher", "evidence_quality": "low", "fulltext_required": "yes", "comment": "", "relevant_sources": ["25249453"]}], "sources": [{"article_id": "26787016", "content": "Uncorrected refractive errors are the commonest cause of visual impairment in children, with myopia being the most frequent type. Myopia usually starts around 9\u00a0years of age and progresses throughout adolescence. Hyperopia usually affects younger children, and astigmatism affects all age groups. Many children have a combination of myopia and astigmatism. To correct refractive errors, the type and degree of refractive error are measured and appropriate corrective lenses prescribed and dispensed in the spectacle frame of choice. Custom spectacles (that is, with the correction specifically required for that individual) are required if astigmatism is present, and/or the refractive error differs between eyes. Spectacles without astigmatic correction and where the refractive error is the same in both eyes are straightforward to dispense. These are known as 'ready-made' spectacles. High-quality spectacles of this type can be produced in high volume at an extremely low cost. Although spectacle correction improves visual function, a high proportion of children do not wear their spectacles for a variety of reasons. The aim of this study is to compare spectacle wear at 3-4 months amongst school children aged 11 to 15\u00a0years who have significant, simple uncorrected refractive error randomised to ready-made or custom spectacles of equivalent quality, and to evaluate cost savings to programmes. The study will take place in urban and semi-urban government schools in Bangalore, India. The hypothesis is that similar proportions of children randomised to ready-made or custom spectacles will be wearing their spectacles at 3-4 months.\nThe trial is a randomised, non-inferiority, double masked clinical trial of children with simple uncorrected refractive errors. After screening, children will be randomised to ready-made or custom spectacles. Children will choose their preferred frame design. After 3-4 months the children will be followed up to assess spectacle wear.\nReady-made spectacles have benefits for providers as well as parents and children, as a wide range of prescriptions and frame types can be taken to schools and dispensed immediately. In contrast, custom spectacles have to be individually made up in optical laboratories, and taken back to the school and given to the correct child.\nISRCTN14715120 (Controlled-Trials.com) Date registered: 04 February 2015.", "title": "Spectacle wearing in children randomised to ready-made or custom spectacles, and potential cost savings to programmes: study protocol for a randomised controlled trial.", "date": "2016-01-21"}, {"article_id": "27321197", "content": "To study, for the first time, the effect of wearing ready-made glasses and glasses with power determined by self-refraction on children's quality of life.\nThis is a randomized, double-masked non-inferiority trial. Children in grades 7 and 8 (age 12-15\u00a0years) in nine Chinese secondary schools, with presenting visual acuity (VA) \u22646/12 improved with refraction to \u22656/7.5 bilaterally, refractive error \u2264-1.0 D and <2.0 D of anisometropia and astigmatism bilaterally, were randomized to receive ready-made spectacles (RM) or identical-appearing spectacles with power determined by: subjective cycloplegic retinoscopy by a university optometrist (U), a rural refractionist (R) or non-cycloplegic self-refraction (SR). Main study outcome was global score on the National Eye Institute Refractive Error Quality of Life-42 (NEI-RQL-42) after 2\u00a0months of wearing study glasses, comparing other groups with the U group, adjusting for baseline score.\nOnly one child (0.18%) was excluded for anisometropia or astigmatism. A total of 426 eligible subjects (mean age 14.2\u00a0years, 84.5% without glasses at baseline) were allocated to U [103 (24.2%)], RM [113 (26.5%)], R [108 (25.4%)] and SR [102 (23.9%)] groups, respectively. Baseline and endline score data were available for 398 (93.4%) of subjects. In multiple regression models adjusting for baseline score, older age (p\u00a0=\u00a00.003) and baseline spectacle wear (p\u00a0=\u00a00.016), but not study group assignment, were significantly associated with lower final score.\nQuality of life wearing ready-mades or glasses based on self-refraction did not differ from that with cycloplegic refraction by an experienced optometrist in this non-inferiority trial.", "title": "Self-refraction, ready-made glasses and quality of life among rural myopic Chinese children: a non-inferiority randomized trial.", "date": "2016-06-21"}, {"article_id": "19592103", "content": "We sought to evaluate visual performance and satisfaction with ready-made spectacles (RMS) in Chinese school-aged children with uncorrected refractive error.\nRandomized, double-blind, clinical trial.\nJunior high school students from urban Guangzhou, China, aged approximately 12 to 15 years with > or =1 diopter (D) of uncorrected spherical equivalent (SE) refractive error. Students were excluded with > or =2.00 D astigmatism, > or =2 D myopic anisometropia, and > or =1 D hyperopic anisometropia and ocular disease affecting vision.\nRefractive error was determined by cycloplegic subjective refraction. Students were randomly assigned to receive RMS or custom spectacles (CS) and assessed after 1 month of use. We required 175 students to complete in each arm to be able to measure a 15% difference in compliance.\nCompliance with spectacles lens wear, patterns of use, vision, symptoms, and perceived value.\nScreening identified 965 of 4607 (20.9%) students with reduced distance vision; 212 of the 965 (22.0%) refused evaluation and 187 of the 965 (20.8%) had <1 D of SE refractive error. Sixty-one (6.3%) were referred for further evaluation and the remaining 495 (51.3%) participated. Social, demographic, and ocular parameters were similar in the 2 groups. Average SE refractive error was -2.57+/-1.31 (mean value +/- standard deviation [SD]). Spectacle vision (Snellen acuity, mean +/- SD) was worse with RMS in the eye with lower SE (20/25(-0.5)+/-0.9 lines vs 20/25(+1)+/-0.7 lines; P = 0.004) and higher SE (20/25(-2)+/-1.2 lines vs 20/25(+1)+/-0.8; P<0.001). There were no differences (P>0.05) in the rate of use (94.3% vs 92.2%), wearing to the 1-month visit (46.9% vs 51.5%), planned use (93.3% vs 93.7%), value (89.5% vs 91.7% \"moderate or high value or most valued possession\"), or symptoms (blur, 21.1% vs 19.4% [P = 0.8] and other symptoms [P>0.2]).\nAlthough visual acuity was better with CS, no difference was found in acceptability in this population of students with predominantly simple myopic refractive error. This study supports the use of RMS in a school-based refractive services program, saving costs and improving the logistics of service delivery.", "title": "A randomized, clinical trial evaluating ready-made and custom spectacles delivered via a school-based screening program in China.", "date": "2009-07-14"}, {"article_id": "25249453", "content": "To assess the effect of provision of free glasses on academic performance in rural Chinese children with myopia.\nCluster randomized, investigator masked, controlled trial.\n252 primary schools in two prefectures in western China, 2012-13.\n3177 of 19,934 children in fourth and fifth grades (mean age 10.5 years) with visual acuity <6/12 in either eye without glasses correctable to >6/12 with glasses. 3052 (96.0%) completed the study.\nChildren were randomized by school (84 schools per arm) to one of three interventions at the beginning of the school year: prescription for glasses only (control group), vouchers for free glasses at a local facility, or free glasses provided in class.\nSpectacle wear at endline examination and end of year score on a specially designed mathematics test, adjusted for baseline score and expressed in standard deviations.\nAmong 3177 eligible children, 1036 (32.6%) were randomized to control, 988 (31.1%) to vouchers, and 1153 (36.3%) to free glasses in class. All eligible children would benefit from glasses, but only 15% wore them at baseline. At closeout glasses wear was 41% (observed) and 68% (self reported) in the free glasses group, and 26% (observed) and 37% (self reported) in the controls. Effect on test score was 0.11 SD (95% confidence interval 0.01 to 0.21) when the free glasses group was compared with the control group. The adjusted effect of providing free glasses (0.10, 0.002 to 0.19) was greater than parental education (0.03, -0.04 to 0.09) or family wealth (0.01, -0.06 to 0.08). This difference between groups was significant, but was smaller than the prespecified 0.20 SD difference that the study was powered to detect.\nThe provision of free glasses to Chinese children with myopia improves children's performance on mathematics testing to a statistically significant degree, despite imperfect compliance, although the observed difference between groups was smaller than the study was originally designed to detect. Myopia is common and rarely corrected in this setting.Trial Registration Current Controlled Trials ISRCTN03252665.", "title": "Effect of providing free glasses on children's educational outcomes in China: cluster randomized controlled trial.", "date": "2014-09-25"}, {"article_id": "21889800", "content": "To test an educational intervention promoting the purchase of spectacles among Chinese children.\nRandomized, controlled trial.\nChildren in years 1 and 2 of all 20 junior and senior high schools (ages 12-17 years) in 3 rural townships in Guangdong, China.\nChildren underwent visual acuity (VA) testing, and parents of participants with presenting VA worse than 6/12 in either eye improving by more than 2 lines with cycloplegic refraction were recommended to purchase glasses. Children at 10 randomly selected schools received a lecture, video, and classroom demonstration promoting spectacle purchase.\nSelf-reported purchase of spectacles (primary outcome) and observed wear or possession of newly purchased glasses (secondary outcome) at follow-up examinations (mean, 219 \u00b1 87 days after the baseline visit).\nAmong 15 404 eligible children, examinations were completed for 6379 (74.6%) at intervention schools and 5044 (73.6%) at control schools. Spectacles were recommended for 2236 (35.1%) children at intervention schools and for 2212 (43.9%) at control schools. Of these, 417 (25.7%) intervention schools children and 537 (34.0%, P = 0.45) control schools children reported buying glasses. Predictors of purchase in regression models included female gender (P = 0.02), worse uncorrected VA (P < 0.001), and higher absolute value of refractive error (P = 0.001). Neither the rate of self-reported purchase of glasses or observed wear or possession of newly purchased glasses differed between control schools and intervention schools in mixed-effect logistic regression models. Among children not purchasing glasses, 21.7% had better-eye VA of worse than 6/18.\nAn intervention based on extensive pilot testing and focus groups in the area failed to promote spectacle purchase or wear. The high burden of remaining uncorrected poor vision underscores the need to develop better interventions.\nThe author(s) have no proprietary or commercial interest in any materials discussed in this article.", "title": "Randomized, controlled trial of an educational intervention to promote spectacle use in rural China: the see well to learn well study.", "date": "2011-09-06"}, {"article_id": "26275472", "content": "To study the effect of free glasses combined with teacher incentives on in-school glasses wear among Chinese urban migrant children.\nCluster-randomized controlled trial.\nChildren with visual acuity (VA) \u22646/12 in either eye owing to refractive error in 94 randomly chosen primary schools underwent randomization by school to receive free glasses, education on their use, and a teacher incentive (Intervention), or glasses prescriptions only (Control). Intervention group teachers received a tablet computer if \u226580% of children given glasses wore them during unannounced visits 6 weeks and 6 months (main outcome) after intervention.\nAmong 4376 children, 728 (16.7%, mean age 10.9 years, 51.0% boys) met enrollment criteria and were randomly allocated, 358 (49.2%, 47 schools) to Intervention and 370 (50.8%, 47 schools) to Control. Among these, 693 children (95.2%) completed the study and underwent analysis. Spectacle wear was significantly higher at 6 months among Intervention children (Observed [main outcome]: 68.3% vs 23.9%, adjusted odds ratio [OR] = 11.5, 95% confidence interval [CI] 5.91-22.5, P < .001; Self-reported: 90.6% vs 32.1%, OR = 43.7, 95% CI = 21.7-88.5, P < .001). Other predictors of observed wear at 6 months included baseline spectacle wear (P < .001), uncorrected VA <6/18 (P = .01), and parental spectacle wear (P = .02). The 6-month observed wear rate was only 41% among similar-aged children provided free glasses in our previous trial without teacher incentives.\nFree spectacles and teacher incentives maintain classroom wear in the large majority of children needing glasses over a school year. Low wear among Control children demonstrates the need for interventions.", "title": "Impact of Free Glasses and a Teacher Incentive on Children's Use of Eyeglasses: A Cluster-Randomized Controlled Trial.", "date": "2015-08-16"}, {"article_id": "18156372", "content": "To compare whether free spectacles or only a prescription for spectacles influences wearing rates among Tanzanian students with un/undercorrected refractive error (RE).\nCluster randomised trial.\n37 secondary schools in Dar es Salaam, Tanzania.\nDistance visual acuity was measured in 6,904 year-1 students (90.2% response rate; median age 14 years; range 11-25 years) using a Snellen E-chart. 135 had RE requiring correction.\nSchools were randomly allocated to free spectacles (arm A) or prescription only (arm B).\nSpectacle use at 3 months.\nThe prevalence of un/undercorrected RE was 1.8% (95% CI: 1.5 to 2.2%). At 3 months, 27/58 (47%) students in arm A were wearing spectacles or had them at school compared with 13/50 (26%) in arm B (adjusted OR 2.4, 95% CI 1.0 to 6.7). Free spectacles and myopia were independently associated with spectacle use.\nThe low prevalence of un/undercorrected RE and poor uptake of spectacles, even when provided free, raises doubts about the value of vision-screening programmes in Tanzanian secondary schools. Policy decisions on school vision screening in middle- and low-income countries should take account of the cost-effectiveness as well as competing demands for scarce resources.", "title": "Two strategies for correcting refractive errors in school students in Tanzania: randomised comparison, with implications for screening programmes.", "date": "2007-12-25"}, {"article_id": "26284747", "content": "To study safety of children's glasses in rural China, where fear that glasses harm vision is an important barrier for families and policy makers.\nExploratory analysis from a cluster-randomized, investigator-masked, controlled trial.\nAmong primary schools (n\u00a0= 252) in western China, children were randomized by school to 1 of 3 interventions: free glasses provided in class, vouchers for free glasses at a local facility, or glasses prescriptions only (Control group). The main outcome of this analysis is uncorrected visual acuity after 8\u00a0months, adjusted for baseline acuity.\nAmong 19 934 children randomly selected for screening, 5852 myopic (spherical equivalent refractive error \u2264-0.5 diopters) eyes of 3001 children (14.7%, mean age 10.5 years) had VA \u22646/12 without glasses correctable to >6/12 with glasses, and were eligible. Among these, 1903 (32.5%), 1798 (30.7%), and 2151 (36.8%) were randomized to Control, Voucher, and Free Glasses, respectively. Intention-to-treat analyses were performed on all 1831 (96.2%), 1699 (94.5%), and 2007 (93.3%) eyes of children with follow-up in Control, Voucher, and Free Glasses groups. Final visual acuity for eyes of children in the treatment groups (Free Glasses and Voucher) was significantly better than for Control children, adjusting only for baseline visual acuity (difference of 0.023 logMAR units [0.23 vision chart lines, 95% CI: 0.03, 0.43]) or for other baseline factors as well (0.025 logMAR units [0.25 lines, 95% CI 0.04, 0.45]).\nWe found no evidence that spectacles promote decline in uncorrected vision with aging among children.", "title": "Safety of Spectacles for Children's Vision: A Cluster-Randomized Controlled Trial.", "date": "2015-08-19"}, {"article_id": "28426857", "content": "Uncorrected refractive errors are the most common cause of visual impairment in children despite correction being highly cost-effective.\nTo determine whether less expensive ready-made spectacles produce rates of spectacle wear at 3 to 4 months comparable to those of more expensive custom-made spectacles among eligible school-aged children.\nThis noninferiority, double-masked, randomized clinical trial recruited children aged 11 to 15 years from January 12 through July 31, 2015, from government schools in urban and periurban areas surrounding Bangalore, India. Follow-up occurred from August 1 through September 31, 2015. Participants met the following eligibility criteria for ready-made spectacles: failed vision screening at the 6/9 level in each eye; refraction was indicated; acuity improved with correction by 2 or more lines in the better-seeing eye; the corrected acuity with the spherical equivalent was not more than 1 line less than with full correction; anisometropia measured less than 1.0 diopter; and an appropriate frame was available.\nEligible children were randomized to ready-made or custom-made spectacles.\nProportion of children wearing their spectacles at unannounced visits 3 to 4 months after the intervention.\nOf 23\u202f345 children aged 11 to 15 years who underwent screening, 694 had visual acuity of less than 6/9 in both eyes, and 535 underwent assessment for eligibility. A total of 460 children (227 female [49.3%] and 233 male [50.7%]; mean [SD] age, 13.4 [1.3] years) were eligible for ready-made spectacles (2.0% undergoing screening and 86.0% undergoing assessment) and were randomized to ready-made (n\u2009=\u2009232) or custom-made (n\u2009=\u2009228) spectacles. Follow-up rates at 3 to 4 months were similar (184 [79.3%] in the ready-made group and 178 [78.1%] in the custom-made group). Rates of spectacle wear in the 2 arms were similar among 139 of 184 children (75.5%) in the ready-made arm and 131 of 178 children (73.6%) in the custom-made arm (risk difference, 1.8%; 95% CI, -7.1% to 10.8%).\nMost children were eligible for ready-made spectacles, and the proportion wearing ready-made spectacles was not inferior to the proportion wearing custom-made spectacles at 3 to 4 months. These findings suggest that ready-made spectacles could substantially reduce costs for school-based eye health programs in India without compromising spectacle wear, at least in the short term.\nisrctn.com Identifier: ISRCTN14715120.", "title": "Spectacle Wear Among Children in a School-Based Program for Ready-Made vs Custom-Made Spectacles in India: A Randomized Clinical Trial.", "date": "2017-04-21"}]}
{"original_review": "32352165", "question_data": [{"question_id": 138, "question": "Is the risk of major bleeding higher, lower, or the same when comparing aspirin to placebo?", "answer": "higher", "evidence_quality": "high", "fulltext_required": "no", "comment": "", "relevant_sources": ["30221596"]}, {"question_id": 139, "question": "Is the risk of dementia higher, lower, or the same when comparing aspirin to placebo?", "answer": "no difference", "evidence_quality": "high", "fulltext_required": "no", "comment": "", "relevant_sources": ["30221596"]}, {"question_id": 140, "question": "Is the risk of mortality higher, lower, or the same when comparing aspirin to placebo?", "answer": "higher", "evidence_quality": "high", "fulltext_required": "no", "comment": "cochrane authors interpret results differently than the original paper authors", "relevant_sources": ["30221596"]}, {"question_id": 141, "question": "Is the incidence of myocardial infarction higher, lower, or the same when comparing NSAIDs to placebo?", "answer": "no difference", "evidence_quality": "moderate", "fulltext_required": "yes", "comment": "", "relevant_sources": ["17111043"]}, {"question_id": 142, "question": "Is the risk of mortality higher, lower, or the same when comparing NSAIDs to placebo?", "answer": "no difference", "evidence_quality": "moderate", "fulltext_required": "yes", "comment": "", "relevant_sources": ["17111043"]}, {"question_id": 143, "question": "Is the incidence of Alzheimer's disease higher, lower, or the same when comparing NSAIDs to placebo?", "answer": "no difference", "evidence_quality": "moderate", "fulltext_required": "yes", "comment": "", "relevant_sources": ["17111043"]}, {"question_id": 144, "question": "Is the risk of stroke higher, lower, or the same when comparing NSAIDs to placebo?", "answer": "no difference", "evidence_quality": "moderate", "fulltext_required": "yes", "comment": "", "relevant_sources": ["17111043"]}, {"question_id": 145, "question": "Is the incidence of Alzheimer's disease in patients with mild cognitive impairment higher, lower, or the same when comparing rofecoxib to placebo?", "answer": "higher", "evidence_quality": "moderate", "fulltext_required": "no", "comment": "", "relevant_sources": ["15742005"]}], "sources": [{"article_id": "15742005", "content": "Inflammatory mechanisms have been implicated in Alzheimer's disease (AD) and might be mediated via the COX-2 enzyme. Previous studies with the selective COX-2 inhibitors, rofecoxib and celecoxib, have shown that they do not alter the progression of AD. We conducted a double-blind study to investigate whether rofecoxib could delay a diagnosis of AD in patients with mild cognitive impairment (MCI), a group with an expected annual AD diagnosis rate of 10-15%. MCI patients > or =65 years were randomized to rofecoxib 25 mg (N=725) or placebo (N=732) daily for up to 4 years. The primary end point was the percentage of patients with a clinical diagnosis of AD. The estimated annual AD diagnosis rate was lower than the anticipated 10-15%: 6.4% in the rofecoxib group vs 4.5% in the placebo group (rofecoxib : placebo hazard ratio=1.46 (95% CI: 1.09, 1.94), p=0.011). Analyses of secondary end points, including measures of cognition (eg the cognitive subscale of the AD Assessment Scale (ADAS-Cog)) and global function (eg the Clinical Dementia Rating (CDR)), did not demonstrate differences between treatment groups. There was also no consistent evidence that rofecoxib differed from placebo in post hoc analyses comparing ADAS-Cog and CDR-sum of boxes scores in overlapping subgroups of patients who had Mini Mental State Exam scores of 24-26 in the present MCI study and in a previous AD treatment study with a similar design. The results from this MCI study did not support the hypothesis that rofecoxib would delay a diagnosis of AD. In conjunction with the lack of effects observed in previous AD studies, the findings suggest that inhibition of COX-2 is not a useful therapeutic approach in AD.", "title": "A randomized, double-blind, study of rofecoxib in patients with mild cognitive impairment.", "date": "2005-03-03"}, {"article_id": "17460158", "content": "To evaluate the efficacy and safety of naproxen and celecoxib for the primary prevention of Alzheimer disease (AD).\nRandomized, placebo-controlled, double-masked clinical trial conducted at six US dementia research clinics. Volunteers aged 70+ years, with cognitive screening scores above designated cut-offs and a family history of AD, were randomly assigned to celecoxib 200 mg BID, naproxen sodium 220 mg BID, or placebo. Enrollment began in early 2001. The main outcome measure was diagnosis of AD after randomization.\nOn December 17, 2004, treatments were suspended. Events while on treatment yielded hazard ratios vs placebo of 1.99 (95% CI 0.80 to 4.97; p = 0.14) for celecoxib and 2.35 (0.95 to 5.77; p = 0.06) for naproxen. Imperfect screening measures led to enrollment of 7 individuals with dementia and 46 others with milder cognitive syndromes. Their (prevalent) illness was detected at enrollment and diagnosed within 6 months following randomization. Secondary analyses that excluded the 7 cases of prevalent dementia showed increased hazard ratios for AD with both treatments. Neither treatment produced a notable effect on the incidence of milder cognitive syndromes.\nThese results do not support the hypothesis that celecoxib or naproxen prevent Alzheimer dementia, at least within the early years after initiation of treatment. Masked long-term follow-up of these participants will be essential.", "title": "Naproxen and celecoxib do not prevent AD in early results from a randomized controlled trial.", "date": "2007-04-27"}, {"article_id": "19038899", "content": "Because anti-inflammatory drugs may delay cognitive decline and influence brain metabolism in normal aging, the authors determined the effects of the cyclooxygenase-2 inhibitor, celecoxib, on cognitive performance and regional cerebral glucose metabolism in nondemented volunteers with mild age-related memory decline.\nRandomized, double-blind, placebo-controlled, parallel group trial with 18-months of exposure to study medication.\nUniversity research institute.\nEighty-eight subjects, aged 40-81 years (mean: 58.7, SD: 8.9 years) with mild self-reported memory complaints but normal memory performance scores were recruited from community physician referrals, media coverage, and advertising. Forty subjects completed the study.\nDaily celecoxib dose of 200 or 400 mg, or placebo.\nStandardized neuropsychological test battery and statistical parametric mapping (SPM) of FDG-PET scans performed during mental rest.\nMeasures of cognition showed significant between-group differences in executive functioning (F [1, 30] = 5.06, p = 0.03) and language/semantic memory (F [1, 31] = 6.19, p = 0.02), favoring the celecoxib group compared with the placebo group. Concomitantly, FDG-PET scans demonstrated bilateral metabolic increases in prefrontal cortex in the celecoxib group in the vicinity of Brodmann's areas 9 and 10, but not in the placebo group. SPM analyses of the PET data pooled by treatment arm corresponded to a 6% increase in activity over pretreatment levels (p <0.01, after adjustment for multiple comparisons).\nThese results suggest that daily celecoxib use may improve cognitive performance and increase regional brain metabolism in people with age-associated memory decline.", "title": "Cognitive and cerebral metabolic effects of celecoxib versus placebo in people with age-related memory loss: randomized controlled study.", "date": "2008-11-29"}, {"article_id": "30221596", "content": "Information on the use of aspirin to increase healthy independent life span in older persons is limited. Whether 5 years of daily low-dose aspirin therapy would extend disability-free life in healthy seniors is unclear.\nFrom 2010 through 2014, we enrolled community-dwelling persons in Australia and the United States who were 70 years of age or older (or \u226565 years of age among blacks and Hispanics in the United States) and did not have cardiovascular disease, dementia, or physical disability. Participants were randomly assigned to receive 100 mg per day of enteric-coated aspirin or placebo orally. The primary end point was a composite of death, dementia, or persistent physical disability. Secondary end points reported in this article included the individual components of the primary end point and major hemorrhage.\nA total of 19,114 persons with a median age of 74 years were enrolled, of whom 9525 were randomly assigned to receive aspirin and 9589 to receive placebo. A total of 56.4% of the participants were women, 8.7% were nonwhite, and 11.0% reported previous regular aspirin use. The trial was terminated at a median of 4.7 years of follow-up after a determination was made that there would be no benefit with continued aspirin use with regard to the primary end point. The rate of the composite of death, dementia, or persistent physical disability was 21.5 events per 1000 person-years in the aspirin group and 21.2 per 1000 person-years in the placebo group (hazard ratio, 1.01; 95% confidence interval [CI], 0.92 to 1.11; P=0.79). The rate of adherence to the assigned intervention was 62.1% in the aspirin group and 64.1% in the placebo group in the final year of trial participation. Differences between the aspirin group and the placebo group were not substantial with regard to the secondary individual end points of death from any cause (12.7 events per 1000 person-years in the aspirin group and 11.1 events per 1000 person-years in the placebo group), dementia, or persistent physical disability. The rate of major hemorrhage was higher in the aspirin group than in the placebo group (3.8% vs. 2.8%; hazard ratio, 1.38; 95% CI, 1.18 to 1.62; P<0.001).\nAspirin use in healthy elderly persons did not prolong disability-free survival over a period of 5 years but led to a higher rate of major hemorrhage than placebo. (Funded by the National Institute on Aging and others; ASPREE ClinicalTrials.gov number, NCT01038583 .).", "title": "Effect of Aspirin on Disability-free Survival in the Healthy Elderly.", "date": "2018-09-18"}, {"article_id": "17111043", "content": "The Alzheimer's Disease Anti-inflammatory Prevention Trial (ADAPT) was designed to evaluate the conventional NSAID naproxen sodium and the selective COX-2 inhibitor celecoxib for primary prevention of Alzheimer's dementia (AD). On 17 December 2004, after the Adenoma Prevention with Celecoxib (APC) trial reported increased cardiovascular risks with celecoxib, the ADAPT Steering Committee suspended treatment and enrollment. This paper reports on cardiovascular and cerebrovascular events in ADAPT.\nADAPT is a randomized, placebo-controlled, parallel chemoprevention trial with 1-46 mo of follow-up.\nThe trial was conducted at six field sites in the United States: Baltimore, Maryland; Boston, Massachusetts; Rochester, New York; Seattle, Washington; Sun City, Arizona; and Tampa, Florida.\nThe 2,528 participants were aged 70 y and older with a family history of AD.\nStudy treatments were celecoxib (200 mg b.i.d.), naproxen sodium (220 mg b.i.d.), and placebo.\nOutcome measures were deaths, along with nonfatal myocardial infarction (MI), stroke, congestive heart failure (CHF), transient ischemic attack (TIA), and antihypertensive treatment recorded from structured interviews at scheduled intervals. Cox proportional hazards regression was used to analyze these events individually and in several composites.\nCounts (with 3-y incidence) of participants who experienced cardiovascular or cerebrovascular death, MI, stroke, CHF, or TIA in the celecoxib-, naproxen-, and placebo-treated groups were 28/717 (5.54%), 40/713 (8.25%), and 37/1070 (5.68%), respectively. This yielded a hazard ratio (95% confidence interval [CI]) for celecoxib of 1.10 (0.67-1.79) and for naproxen of 1.63 (1.04-2.55). Antihypertensive treatment was initiated in 160/440 (47.43%), 147/427 (45.00%), and 164/644 (34.08%). This yielded hazard ratios (CIs) of 1.56 for celecoxib (1.26-1.94) and 1.40 for naproxen (1.12-1.75).\nFor celecoxib, ADAPT data do not show the same level of risk as those of the APC trial. The data for naproxen, although not definitive, are suggestive of increased cardiovascular and cerebrovascular risk.", "title": "Cardiovascular and cerebrovascular events in the randomized, controlled Alzheimer's Disease Anti-Inflammatory Prevention Trial (ADAPT).", "date": "2006-11-18"}]}
{"original_review": "31032883", "question_data": [{"question_id": 146, "question": "Is the overall risk of developing breast cancer higher, lower, or the same when comparing tamoxifen to placebo?", "answer": "lower", "evidence_quality": "moderate", "fulltext_required": "no", "comment": "17312305 omitted with 13.9% weight", "relevant_sources": ["16288118", "25497694"]}, {"question_id": 147, "question": "Is the risk of developing invasive breast cancer higher, lower, or the same when comparing tamoxifen to placebo?", "answer": "lower", "evidence_quality": "moderate", "fulltext_required": "no", "comment": "", "relevant_sources": ["16288118", "25497694", "17312305"]}, {"question_id": 148, "question": "Is the overall risk of breast cancer higher, lower, or the same when comparing aromatase inhibitors (AIs) to placebo?", "answer": "lower", "evidence_quality": "high", "fulltext_required": "no", "comment": "", "relevant_sources": ["21639806", "24333009"]}, {"question_id": 149, "question": "Is the overall risk of developing breast cancer higher, lower, or the same when comparing raloxifene to tamoxifen?", "answer": "higher", "evidence_quality": "moderate", "fulltext_required": "no", "comment": "", "relevant_sources": ["20404000"]}, {"question_id": 150, "question": "Is the risk of developing invasive breast cancer higher, lower, or the same when comparing raloxifene to tamoxifen?", "answer": "higher", "evidence_quality": "moderate", "fulltext_required": "no", "comment": "", "relevant_sources": ["20404000"]}], "sources": [{"article_id": "20404000", "content": "The selective estrogen-receptor modulator (SERM) tamoxifen became the first U.S. Food and Drug Administration (FDA)-approved agent for reducing breast cancer risk but did not gain wide acceptance for prevention, largely because it increased endometrial cancer and thromboembolic events. The FDA approved the SERM raloxifene for breast cancer risk reduction following its demonstrated effectiveness in preventing invasive breast cancer in the Study of Tamoxifen and Raloxifene (STAR). Raloxifene caused less toxicity (versus tamoxifen), including reduced thromboembolic events and endometrial cancer. In this report, we present an updated analysis with an 81-month median follow-up. STAR women were randomly assigned to receive either tamoxifen (20 mg/d) or raloxifene (60 mg/d) for 5 years. The risk ratio (RR; raloxifene:tamoxifen) for invasive breast cancer was 1.24 (95% confidence interval [CI], 1.05-1.47) and for noninvasive disease, 1.22 (95% CI, 0.95-1.59). Compared with initial results, the RRs widened for invasive and narrowed for noninvasive breast cancer. Toxicity RRs (raloxifene:tamoxifen) were 0.55 (95% CI, 0.36-0.83; P = 0.003) for endometrial cancer (this difference was not significant in the initial results), 0.19 (95% CI, 0.12-0.29) for uterine hyperplasia, and 0.75 (95% CI, 0.60-0.93) for thromboembolic events. There were no significant mortality differences. Long-term raloxifene retained 76% of the effectiveness of tamoxifen in preventing invasive disease and grew closer over time to tamoxifen in preventing noninvasive disease, with far less toxicity (e.g., highly significantly less endometrial cancer). These results have important public health implications and clarify that both raloxifene and tamoxifen are good preventive choices for postmenopausal women with elevated risk for breast cancer.", "title": "Update of the National Surgical Adjuvant Breast and Bowel Project Study of Tamoxifen and Raloxifene (STAR) P-2 Trial: Preventing breast cancer.", "date": "2010-04-21"}, {"article_id": "17312305", "content": "Several clinical trials have reported an early reduction in breast cancer incidence in healthy women using tamoxifen to reduce their risk of breast cancer but have not reported longer follow-up data for the evaluation of breast cancer prevention. We report the blinded 20-year follow-up (median follow-up = 13 years) of the Royal Marsden trial to identify any long-term prevention of breast cancer associated with tamoxifen treatment.\nWe randomly assigned 2494 healthy women to oral tamoxifen (20 mg/day) or placebo for 8 years. The primary outcome was occurrence of invasive breast cancer. A secondary planned analysis of estrogen receptor (ER)-positive invasive breast cancer was also done. Survival was assessed by use of a Cox proportional hazards model in both univariate and multivariable analyses. The durability of the treatment effect was assessed by use of a Cox regression analysis. All statistical tests were two-sided.\nAmong the 2471 eligible participants (1238 participants in the tamoxifen arm and 1233 participants in the placebo arm), 186 developed invasive breast cancer (82 on tamoxifen and 104 on placebo; hazard ratio [HR] = 0.78, 95% confidence interval [CI] = 0.58 to 1.04; P = .1). Of these 186 cancers, 139 were ER positive (53 on tamoxifen and 86 on placebo; HR = 0.61, 95% CI = 0.43 to 0.86; P = .005). The risk of ER-positive breast cancer was not statistically significantly lower in the tamoxifen arm than in the placebo arm during the 8-year treatment period (30 cancers in the tamoxifen arm and 39 in the placebo arm; HR = 0.77, 95% CI = 0.48 to 1.23; P = .3) but was statistically significantly lower in the posttreatment period (23 in the tamoxifen arm and 47 in the placebo arm; HR = 0.48, 95% CI = 0.29 to 0.79; P = .004). Fifty-four participants in each arm have died from any cause (HR = 0.99, 95% CI = 0.68 to 1.44; P = .95). The adverse event profiles for both arms were similar to those previously reported and occurred predominantly during the treatment period.\nA statistically significant reduction in the incidence of ER-positive breast cancer was observed in the tamoxifen arm that occurred predominantly during the post treatment follow-up, indicating long-term prevention of estrogen-dependent breast cancer by tamoxifen.", "title": "Twenty-year follow-up of the Royal Marsden randomized, double-blinded tamoxifen breast cancer prevention trial.", "date": "2007-02-22"}, {"article_id": "16288118", "content": "Initial findings from the National Surgical Adjuvant Breast and Bowel Project Breast Cancer Prevention Trial (P-1) demonstrated that tamoxifen reduced the risk of estrogen receptor-positive tumors and osteoporotic fractures in women at increased risk for breast cancer. Side effects of varying clinical significance were observed. The trial was unblinded because of the positive results, and follow-up continued. This report updates our initial findings.\nWomen (n = 13,388) were randomly assigned to receive placebo or tamoxifen for 5 years. Rates of breast cancer and other events were compared by the use of risk ratios (RRs) and 95% confidence intervals (CIs). Estimates of the net benefit from 5 years of tamoxifen therapy were compared by age, race, and categories of predicted breast cancer risk. Statistical tests were two-sided.\nAfter 7 years of follow-up, the cumulative rate of invasive breast cancer was reduced from 42.5 per 1000 women in the placebo group to 24.8 per 1000 women in the tamoxifen group (RR = 0.57, 95% CI = 0.46 to 0.70) and the cumulative rate of noninvasive breast cancer was reduced from 15.8 per 1000 women in the placebo group to 10.2 per 1000 women in the tamoxifen group (RR = 0.63, 95% CI = 0.45 to 0.89). These reductions were similar to those seen in the initial report. Tamoxifen led to a 32% reduction in osteoporotic fractures (RR = 0.68, 95% CI = 0.51 to 0.92). Relative risks of stroke, deep-vein thrombosis, and cataracts (which increased with tamoxifen) and of ischemic heart disease and death (which were not changed with tamoxifen) were also similar to those initially reported. Risks of pulmonary embolism were approximately 11% lower than in the original report, and risks of endometrial cancer were about 29% higher, but these differences were not statistically significant. The net benefit achieved with tamoxifen varied according to age, race, and level of breast cancer risk.\nDespite the potential bias caused by the unblinding of the P-1 trial, the magnitudes of all beneficial and undesirable treatment effects of tamoxifen were similar to those initially reported, with notable reductions in breast cancer and increased risks of thromboembolic events and endometrial cancer. Readily identifiable subsets of individuals comprising 2.5 million women could derive a net benefit from the drug.", "title": "Tamoxifen for the prevention of breast cancer: current status of the National Surgical Adjuvant Breast and Bowel Project P-1 study.", "date": "2005-11-17"}, {"article_id": "25497694", "content": "Four previously published randomised clinical trials have shown that tamoxifen can reduce the risk of breast cancer in healthy women at increased risk of breast cancer in the first 10 years of follow-up. We report the long-term follow-up of the IBIS-I trial, in which the participants and investigators remain largely masked to treatment allocation.\nIn the IBIS-I randomised controlled trial, premenopausal and postmenopausal women 35-70 years of age deemed to be at an increased risk of developing breast cancer were randomly assigned (1:1) to receive oral tamoxifen 20 mg daily or matching placebo for 5 years. Patients were randomly assigned to the two treatment groups by telephone or fax according to a block randomisation schedule (permuted block sizes of six or ten). Patients and investigators were masked to treatment assignment by use of central randomisation and coded drug supply. The primary endpoint was the occurrence of breast cancer (invasive breast cancer and ductal carcinoma in situ), analysed by intention to treat. Cox proportional hazard models were used to assess breast cancer occurrence and mortality. The trial is closed to recruitment and active treatment is completed, but long-term follow-up is ongoing. This trial is registered with controlledtrials.com, number ISRCTN91879928.\nBetween April 14, 1992, and March 30, 2001, 7154 eligible women recruited from genetics clinics and breast care clinics in eight countries were enrolled into the IBIS-I trial and were randomly allocated to the two treatment groups: 3579 to tamoxifen and 3575 to placebo. After a median follow up of 16.0 years (IQR 14.1-17.6), 601 breast cancers have been reported (251 [7.0%] in 3579 patients in the tamoxifen group vs 350 [9.8%] in 3575 women in the placebo group; hazard ratio [HR] 0.71 [95% CI 0.60-0.83], p<0.0001). The risk of developing breast cancer was similar between years 0-10 (226 [6.3%] in 3575 women in the placebo group vs 163 [4.6%] in 3579 women in the tamoxifen group; hazard ratio [HR] 0.72 [95% CI 0.59-0.88], p=0.001) and after 10 years (124 [3.8%] in 3295 women vs 88 [2.6%] in 3343, respectively; HR 0.69 [0.53-0.91], p=0.009). The greatest reduction in risk was seen in invasive oestrogen receptor-positive breast cancer (HR 0.66 [95% CI 0.54-0.81], p<0.0001) and ductal carcinoma in situ (0.65 [0.43-1.00], p=0.05), but no effect was noted for invasive oestrogen receptor-negative breast cancer (HR 1.05 [95% CI 0.71-1.57], p=0.8).\nThese results show that tamoxifen offers a very long period of protection after treatment cessation, and thus substantially improves the benefit-to-harm ratio of the drug for breast cancer prevention.\nCancer Research UK (UK) and the National Health and Medical Research Council (Australia).", "title": "Tamoxifen for prevention of breast cancer: extended long-term follow-up of the IBIS-I breast cancer prevention trial.", "date": "2014-12-17"}, {"article_id": "21639806", "content": "Tamoxifen and raloxifene have limited patient acceptance for primary prevention of breast cancer. Aromatase inhibitors prevent more contralateral breast cancers and cause fewer side effects than tamoxifen in patients with early-stage breast cancer.\nIn a randomized, placebo-controlled, double-blind trial of exemestane designed to detect a 65% relative reduction in invasive breast cancer, eligible postmenopausal women 35 years of age or older had at least one of the following risk factors: 60 years of age or older; Gail 5-year risk score greater than 1.66% (chances in 100 of invasive breast cancer developing within 5 years); prior atypical ductal or lobular hyperplasia or lobular carcinoma in situ; or ductal carcinoma in situ with mastectomy. Toxic effects and health-related and menopause-specific qualities of life were measured.\nA total of 4560 women for whom the median age was 62.5 years and the median Gail risk score was 2.3% were randomly assigned to either exemestane or placebo. At a median follow-up of 35 months, 11 invasive breast cancers were detected in those given exemestane and in 32 of those given placebo, with a 65% relative reduction in the annual incidence of invasive breast cancer (0.19% vs. 0.55%; hazard ratio, 0.35; 95% confidence interval [CI], 0.18 to 0.70; P=0.002). The annual incidence of invasive plus noninvasive (ductal carcinoma in situ) breast cancers was 0.35% on exemestane and 0.77% on placebo (hazard ratio, 0.47; 95% CI, 0.27 to 0.79; P=0.004). Adverse events occurred in 88% of the exemestane group and 85% of the placebo group (P=0.003), with no significant differences between the two groups in terms of skeletal fractures, cardiovascular events, other cancers, or treatment-related deaths. Minimal quality-of-life differences were observed.\nExemestane significantly reduced invasive breast cancers in postmenopausal women who were at moderately increased risk for breast cancer. During a median follow-up period of 3 years, exemestane was associated with no serious toxic effects and only minimal changes in health-related quality of life. (Funded by Pfizer and others; NCIC CTG MAP.3 ClinicalTrials.gov number, NCT00083174.).", "title": "Exemestane for breast-cancer prevention in postmenopausal women.", "date": "2011-06-07"}, {"article_id": "24333009", "content": "Aromatase inhibitors effectively prevent breast cancer recurrence and development of new contralateral tumours in postmenopausal women. We assessed the efficacy and safety of the aromatase inhibitor anastrozole for prevention of breast cancer in postmenopausal women who are at high risk of the disease.\nBetween Feb 2, 2003, and Jan 31, 2012, we recruited postmenopausal women aged 40-70 years from 18 countries into an international, double-blind, randomised placebo-controlled trial. To be eligible, women had to be at increased risk of breast cancer (judged on the basis of specific criteria). Eligible women were randomly assigned (1:1) by central computer allocation to receive 1 mg oral anastrozole or matching placebo every day for 5 years. Randomisation was stratified by country and was done with blocks (size six, eight, or ten). All trial personnel, participants, and clinicians were masked to treatment allocation; only the trial statistician was unmasked. The primary endpoint was histologically confirmed breast cancer (invasive cancers or non-invasive ductal carcinoma in situ). Analyses were done by intention to treat. This trial is registered, number ISRCTN31488319.\n1920 women were randomly assigned to receive anastrozole and 1944 to placebo. After a median follow-up of 5\u00b70 years (IQR 3\u00b70-7\u00b71), 40 women in the anastrozole group (2%) and 85 in the placebo group (4%) had developed breast cancer (hazard ratio 0\u00b747, 95% CI 0\u00b732-0\u00b768, p<0\u00b70001). The predicted cumulative incidence of all breast cancers after 7 years was 5\u00b76% in the placebo group and 2\u00b78% in the anastrozole group. 18 deaths were reported in the anastrozole group and 17 in the placebo group, and no specific causes were more common in one group than the other (p=0\u00b7836).\nAnastrozole effectively reduces incidence of breast cancer in high-risk postmenopausal women. This finding, along with the fact that most of the side-effects associated with oestrogen deprivation were not attributable to treatment, provides support for the use of anastrozole in postmenopausal women at high risk of breast cancer.\nCancer Research UK, the National Health and Medical Research Council Australia, Sanofi-Aventis, and AstraZeneca.", "title": "Anastrozole for prevention of breast cancer in high-risk postmenopausal women (IBIS-II): an international, double-blind, randomised placebo-controlled trial.", "date": "2013-12-18"}]}
{"original_review": "34850380", "question_data": [{"question_id": 151, "question": "Is the likelihood of good functional outcome higher, lower, or the same when comparing endovascular thrombectomy to control?", "answer": "higher", "evidence_quality": "high", "fulltext_required": "no", "comment": "", "relevant_sources": ["29129157", "29364767"]}, {"question_id": 152, "question": "Is the risk of death at 90 days higher, lower, or the same when comparing endovascular thrombectomy to control?", "answer": "no difference", "evidence_quality": "high", "fulltext_required": "no", "comment": "", "relevant_sources": ["29129157", "29364767"]}, {"question_id": 153, "question": "Is the likelihood of good functional outcome higher, lower, or the same when comparing intravenous thrombolytic treatment to control?", "answer": "higher", "evidence_quality": "high", "fulltext_required": "no", "comment": "fulltext for 32248771 available but not needed for 75% weight", "relevant_sources": ["30947642", "31067369", "21808985", "32248771", "29766770"]}, {"question_id": 154, "question": "Is the risk of death at 90 days higher, lower, or the same when comparing intravenous thrombolytic treatment to control?", "answer": "no difference", "evidence_quality": "high", "fulltext_required": "no", "comment": "fulltext for 32248771 available but not needed for 75% weight; the death count is also different than what is reported in the meta review? but conclusions are the same", "relevant_sources": ["30947642", "31067369", "21808985", "32248771", "29766770"]}], "sources": [{"article_id": "29129157", "content": "The effect of endovascular thrombectomy that is performed more than 6 hours after the onset of ischemic stroke is uncertain. Patients with a clinical deficit that is disproportionately severe relative to the infarct volume may benefit from late thrombectomy.\nWe enrolled patients with occlusion of the intracranial internal carotid artery or proximal middle cerebral artery who had last been known to be well 6 to 24 hours earlier and who had a mismatch between the severity of the clinical deficit and the infarct volume, with mismatch criteria defined according to age (<80 years or \u226580 years). Patients were randomly assigned to thrombectomy plus standard care (the thrombectomy group) or to standard care alone (the control group). The coprimary end points were the mean score for disability on the utility-weighted modified Rankin scale (which ranges from 0 [death] to 10 [no symptoms or disability]) and the rate of functional independence (a score of 0, 1, or 2 on the modified Rankin scale, which ranges from 0 to 6, with higher scores indicating more severe disability) at 90 days.\nA total of 206 patients were enrolled; 107 were assigned to the thrombectomy group and 99 to the control group. At 31 months, enrollment in the trial was stopped because of the results of a prespecified interim analysis. The mean score on the utility-weighted modified Rankin scale at 90 days was 5.5 in the thrombectomy group as compared with 3.4 in the control group (adjusted difference [Bayesian analysis], 2.0 points; 95% credible interval, 1.1 to 3.0; posterior probability of superiority, >0.999), and the rate of functional independence at 90 days was 49% in the thrombectomy group as compared with 13% in the control group (adjusted difference, 33 percentage points; 95% credible interval, 24 to 44; posterior probability of superiority, >0.999). The rate of symptomatic intracranial hemorrhage did not differ significantly between the two groups (6% in the thrombectomy group and 3% in the control group, P=0.50), nor did 90-day mortality (19% and 18%, respectively; P=1.00).\nAmong patients with acute stroke who had last been known to be well 6 to 24 hours earlier and who had a mismatch between clinical deficit and infarct, outcomes for disability at 90 days were better with thrombectomy plus standard care than with standard care alone. (Funded by Stryker Neurovascular; DAWN ClinicalTrials.gov number, NCT02142283 .).", "title": "Thrombectomy 6 to 24 Hours after Stroke with a Mismatch between Deficit and Infarct.", "date": "2017-11-14"}, {"article_id": "29364767", "content": "Thrombectomy is currently recommended for eligible patients with stroke who are treated within 6 hours after the onset of symptoms.\nWe conducted a multicenter, randomized, open-label trial, with blinded outcome assessment, of thrombectomy in patients 6 to 16 hours after they were last known to be well and who had remaining ischemic brain tissue that was not yet infarcted. Patients with proximal middle-cerebral-artery or internal-carotid-artery occlusion, an initial infarct size of less than 70 ml, and a ratio of the volume of ischemic tissue on perfusion imaging to infarct volume of 1.8 or more were randomly assigned to endovascular therapy (thrombectomy) plus standard medical therapy (endovascular-therapy group) or standard medical therapy alone (medical-therapy group). The primary outcome was the ordinal score on the modified Rankin scale (range, 0 to 6, with higher scores indicating greater disability) at day 90.\nThe trial was conducted at 38 U.S. centers and terminated early for efficacy after 182 patients had undergone randomization (92 to the endovascular-therapy group and 90 to the medical-therapy group). Endovascular therapy plus medical therapy, as compared with medical therapy alone, was associated with a favorable shift in the distribution of functional outcomes on the modified Rankin scale at 90 days (odds ratio, 2.77; P<0.001) and a higher percentage of patients who were functionally independent, defined as a score on the modified Rankin scale of 0 to 2 (45% vs. 17%, P<0.001). The 90-day mortality rate was 14% in the endovascular-therapy group and 26% in the medical-therapy group (P=0.05), and there was no significant between-group difference in the frequency of symptomatic intracranial hemorrhage (7% and 4%, respectively; P=0.75) or of serious adverse events (43% and 53%, respectively; P=0.18).\nEndovascular thrombectomy for ischemic stroke 6 to 16 hours after a patient was last known to be well plus standard medical therapy resulted in better functional outcomes than standard medical therapy alone among patients with proximal middle-cerebral-artery or internal-carotid-artery occlusion and a region of tissue that was ischemic but not yet infarcted. (Funded by the National Institute of Neurological Disorders and Stroke; DEFUSE 3 ClinicalTrials.gov number, NCT02586415 .).", "title": "Thrombectomy for Stroke at 6 to 16 Hours with Selection by Perfusion Imaging.", "date": "2018-01-25"}, {"article_id": "21808985", "content": "Patients with unknown stroke onset are generally excluded from acute recanalisation treatments. We designed a pilot study to assess feasibility of a trial of perfusion computed tomography (PCT)-guided thrombolysis in patients with ischemic tissue at risk of infarction and unknown stroke onset.\nPatients with a supratentorial stroke of unknown onset in the middle cerebral artery territory and significant volume of at-risk tissue on PCT were randomized to intravenous thrombolysis with alteplase (0.9\u00a0mg/kg) or placebo. Feasibility endpoints were randomization and blinded treatment of patients within 2\u00a0h after hospital arrival, and the correct application (estimation) of the perfusion imaging criteria.\nAt baseline, there was a trend towards older age [69.5 (57-78) vs. 49 (44-78)\u2009years] in the thrombolysis group (n\u2009=\u20096) compared to placebo (n\u2009=\u20096). Regarding feasibility, hospital arrival to treatment delay was above the allowed 2\u00a0h in three patients (25%). There were two protocol violations (17%) regarding PCT, both underestimating the predicted infarct in patients randomized in the placebo group. No symptomatic hemorrhage or death occurred during the first 7\u00a0days. Three of the four (75%) and one of the five (20%) patients were recanalized in the thrombolysis and placebo group respectively. The volume of non-infarcted at-risk tissue was 84 (44-206)\u2009cm(3) in the treatment arm and 29 (8-105)\u2009cm(3) in the placebo arm.\nThis pilot study shows that a randomized PCT-guided thrombolysis trial in patients with stroke of unknown onset may be feasible if issues such as treatment delays and reliable identification of tissue at risk of infarction tissue are resolved. Safety and efficiency of such an approach need to be established.", "title": "Perfusion-CT guided intravenous thrombolysis in patients with unknown-onset stroke: a randomized, double-blind, placebo-controlled, pilot feasibility trial.", "date": "2011-08-03"}, {"article_id": "31067369", "content": "The time to initiate intravenous thrombolysis for acute ischemic stroke is generally limited to within 4.5 hours after the onset of symptoms. Some trials have suggested that the treatment window may be extended in patients who are shown to have ischemic but not yet infarcted brain tissue on imaging.\nWe conducted a multicenter, randomized, placebo-controlled trial involving patients with ischemic stroke who had hypoperfused but salvageable regions of brain detected on automated perfusion imaging. The patients were randomly assigned to receive intravenous alteplase or placebo between 4.5 and 9.0 hours after the onset of stroke or on awakening with stroke (if within 9 hours from the midpoint of sleep). The primary outcome was a score of 0 or 1 on the modified Rankin scale, on which scores range from 0 (no symptoms) to 6 (death), at 90 days. The risk ratio for the primary outcome was adjusted for age and clinical severity at baseline.\nAfter 225 of the planned 310 patients had been enrolled, the trial was terminated because of a loss of equipoise after the publication of positive results from a previous trial. A total of 113 patients were randomly assigned to the alteplase group and 112 to the placebo group. The primary outcome occurred in 40 patients (35.4%) in the alteplase group and in 33 patients (29.5%) in the placebo group (adjusted risk ratio, 1.44; 95% confidence interval [CI], 1.01 to 2.06; P\u2009=\u20090.04). Symptomatic intracerebral hemorrhage occurred in 7 patients (6.2%) in the alteplase group and in 1 patient (0.9%) in the placebo group (adjusted risk ratio, 7.22; 95% CI, 0.97 to 53.5; P\u2009=\u20090.05). A secondary ordinal analysis of the distribution of scores on the modified Rankin scale did not show a significant between-group difference in functional improvement at 90 days.\nAmong the patients in this trial who had ischemic stroke and salvageable brain tissue, the use of alteplase between 4.5 and 9.0 hours after stroke onset or at the time the patient awoke with stroke symptoms resulted in a higher percentage of patients with no or minor neurologic deficits than the use of placebo. There were more cases of symptomatic cerebral hemorrhage in the alteplase group than in the placebo group. (Funded by the Australian National Health and Medical Research Council and others; EXTEND ClinicalTrials.gov numbers, NCT00887328 and NCT01580839.).", "title": "Thrombolysis Guided by Perfusion Imaging up to 9 Hours after Onset of Stroke.", "date": "2019-05-09"}, {"article_id": "29766770", "content": "Under current guidelines, intravenous thrombolysis is used to treat acute stroke only if it can be ascertained that the time since the onset of symptoms was less than 4.5 hours. We sought to determine whether patients with stroke with an unknown time of onset and features suggesting recent cerebral infarction on magnetic resonance imaging (MRI) would benefit from thrombolysis with the use of intravenous alteplase.\nIn a multicenter trial, we randomly assigned patients who had an unknown time of onset of stroke to receive either intravenous alteplase or placebo. All the patients had an ischemic lesion that was visible on MRI diffusion-weighted imaging but no parenchymal hyperintensity on fluid-attenuated inversion recovery (FLAIR), which indicated that the stroke had occurred approximately within the previous 4.5 hours. We excluded patients for whom thrombectomy was planned. The primary end point was favorable outcome, as defined by a score of 0 or 1 on the modified Rankin scale of neurologic disability (which ranges from 0 [no symptoms] to 6 [death]) at 90 days. A secondary outcome was the likelihood that alteplase would lead to lower ordinal scores on the modified Rankin scale than would placebo (shift analysis).\nThe trial was stopped early owing to cessation of funding after the enrollment of 503 of an anticipated 800 patients. Of these patients, 254 were randomly assigned to receive alteplase and 249 to receive placebo. A favorable outcome at 90 days was reported in 131 of 246 patients (53.3%) in the alteplase group and in 102 of 244 patients (41.8%) in the placebo group (adjusted odds ratio, 1.61; 95% confidence interval [CI], 1.09 to 2.36; P=0.02). The median score on the modified Rankin scale at 90 days was 1 in the alteplase group and 2 in the placebo group (adjusted common odds ratio, 1.62; 95% CI, 1.17 to 2.23; P=0.003). There were 10 deaths (4.1%) in the alteplase group and 3 (1.2%) in the placebo group (odds ratio, 3.38; 95% CI, 0.92 to 12.52; P=0.07). The rate of symptomatic intracranial hemorrhage was 2.0% in the alteplase group and 0.4% in the placebo group (odds ratio, 4.95; 95% CI, 0.57 to 42.87; P=0.15).\nIn patients with acute stroke with an unknown time of onset, intravenous alteplase guided by a mismatch between diffusion-weighted imaging and FLAIR in the region of ischemia resulted in a significantly better functional outcome and numerically more intracranial hemorrhages than placebo at 90 days. (Funded by the European Union Seventh Framework Program; WAKE-UP ClinicalTrials.gov number, NCT01525290; and EudraCT number, 2011-005906-32 .).", "title": "MRI-Guided Thrombolysis for Stroke with Unknown Time of Onset.", "date": "2018-05-17"}, {"article_id": "30947642", "content": "Intravenous thrombolysis with alteplase within a time window up to 4.5\u2009h is the only approved pharmacological treatment for acute ischemic stroke. We studied whether acute ischemic stroke patients with penumbral tissue identified on magnetic resonance imaging 4.5-9\u2009h after symptom onset benefit from intravenous thrombolysis compared to placebo.\nAcute ischemic stroke patients with salvageable brain tissue identified on a magnetic resonance imaging were randomly assigned to receive standard dose alteplase or placebo. The primary end point was disability at 90 days assessed by the modified Rankin scale, which has a range of 0-6 (with 0 indicating no symptoms at all and 6 indicating death). Safety end points included death, symptomatic intracranial hemorrhage, and other serious adverse events.\nThe trial was stopped early for slow recruitment after the enrollment of 119 (61 alteplase, 58 placebo) of 264 patients planned. Median time to intravenous thrombolysis was 7\u2009h 42\u2009min. The primary endpoint showed no significant difference in the modified Rankin scale distribution at day 90 (odds ratio alteplase versus placebo, 1.20; 95% CI, 0.63-2.27, P\u2009=\u20090.58). One symptomatic intracranial hemorrhage occurred in the alteplase group. Mortality at 90 days did not differ significantly between the two groups (11.5 and 6.8%, respectively; P\u2009=\u20090.53).\nIntravenous alteplase administered between 4.5 and 9\u2009h after the onset of symptoms in patients with salvageable tissue did not result in a significant benefit over placebo. (Supported by Boehringer Ingelheim, Germany; ISRCTN 71616222).", "title": "Extending the time window for intravenous thrombolysis in acute ischemic stroke using magnetic resonance imaging-based patient selection.", "date": "2019-04-06"}, {"article_id": "32248771", "content": "Background and Purpose- We assessed whether lower-dose alteplase at 0.6 mg/kg is efficacious and safe for acute fluid-attenuated inversion recovery-negative stroke with unknown time of onset. Methods- This was an investigator-initiated, multicenter, randomized, open-label, blinded-end point trial. Patients met the standard indication criteria for intravenous thrombolysis other than a time last-known-well >4.5 hours (eg, wake-up stroke). Patients were randomly assigned (1:1) to receive alteplase at 0.6 mg/kg or standard medical treatment if magnetic resonance imaging showed acute ischemic lesion on diffusion-weighted imaging and no marked corresponding hyperintensity on fluid-attenuated inversion recovery. The primary outcome was a favorable outcome (90-day modified Rankin Scale score of 0-1). Results- Following the early stop and positive results of the WAKE-UP trial (Efficacy and Safety of MRI-Based Thrombolysis in Wake-Up Stroke), this trial was prematurely terminated with 131 of the anticipated 300 patients (55 women; mean age, 74.4\u00b112.2 years). Favorable outcome was comparable between the alteplase group (32/68, 47.1%) and the control group (28/58, 48.3%; relative risk [RR], 0.97 [95% CI, 0.68-1.41]; ", "title": "Thrombolysis With Alteplase at 0.6 mg/kg for Stroke With Unknown Time of Onset: A Randomized Controlled Trial.", "date": "2020-04-07"}]}
{"original_review": "28349529", "question_data": [{"question_id": 155, "question": "Is survival to hospital discharge higher, lower, or the same when comparing untrained bystander CPR with continuous chest compression to untrained bystander CPR with chest compression interrupted with pauses for rescue breathing?", "answer": "higher", "evidence_quality": "high", "fulltext_required": "no", "comment": "interesting one because none of the studies individually found a significant difference", "relevant_sources": ["10824072", "20818863", "20818864"]}, {"question_id": 156, "question": "Is the likelihood of survival to hospital discharge higher, lower, or the same when comparing continuous chest compression CPR with asynchronous rescue breathing to interrupted chest compression plus rescue breathing?", "answer": "lower", "evidence_quality": "moderate", "fulltext_required": "no", "comment": "", "relevant_sources": ["26550795"]}, {"question_id": 157, "question": "Is the likelihood of favorable neurological outcome higher, lower, or the same when comparing continuous chest compression CPR with asynchronous rescue breathing to interrupted chest compression plus rescue breathing?", "answer": "no difference", "evidence_quality": "high", "fulltext_required": "no", "comment": "", "relevant_sources": ["26550795"]}], "sources": [{"article_id": "20818864", "content": "Emergency medical dispatchers give instructions on how to perform cardiopulmonary resuscitation (CPR) over the telephone to callers requesting help for a patient with suspected cardiac arrest, before the arrival of emergency medical services (EMS) personnel. A previous study indicated that instructions to perform CPR consisting of only chest compression result in a treatment efficacy that is similar or even superior to that associated with instructions given to perform standard CPR, which consists of both compression and ventilation. That study, however, was not powered to assess a possible difference in survival. The aim of this prospective, randomized study was to evaluate the possible superiority of compression-only CPR over standard CPR with respect to survival.\nPatients with suspected, witnessed, out-of-hospital cardiac arrest were randomly assigned to undergo either compression-only CPR or standard CPR. The primary end point was 30-day survival.\nData for the primary analysis were collected from February 2005 through January 2009 for a total of 1276 patients. Of these, 620 patients had been assigned to receive compression-only CPR and 656 patients had been assigned to receive standard CPR. The rate of 30-day survival was similar in the two groups: 8.7% (54 of 620 patients) in the group receiving compression-only CPR and 7.0% (46 of 656 patients) in the group receiving standard CPR (absolute difference for compression-only vs. standard CPR, 1.7 percentage points; 95% confidence interval, -1.2 to 4.6; P=0.29).\nThis prospective, randomized study showed no significant difference with respect to survival at 30 days between instructions given by an emergency medical dispatcher, before the arrival of EMS personnel, for compression-only CPR and instructions for standard CPR in patients with suspected, witnessed, out-of-hospital cardiac arrest. (Funded by the Swedish Heart\u2013Lung Foundation and others; Karolinska Clinical Trial Registration number, CT20080012.)", "title": "Compression-only CPR or standard CPR in out-of-hospital cardiac arrest.", "date": "2010-09-08"}, {"article_id": "20818863", "content": "The role of rescue breathing in cardiopulmonary resuscitation (CPR) performed by a layperson is uncertain. We hypothesized that the dispatcher instructions to bystanders to provide chest compression alone would result in improved survival as compared with instructions to provide chest compression plus rescue breathing.\nWe conducted a multicenter, randomized trial of dispatcher instructions to bystanders for performing CPR. The patients were persons 18 years of age or older with out-of-hospital cardiac arrest for whom dispatchers initiated CPR instruction to bystanders. Patients were randomly assigned to receive chest compression alone or chest compression plus rescue breathing. The primary outcome was survival to hospital discharge. Secondary outcomes included a favorable neurologic outcome at discharge.\nOf the 1941 patients who met the inclusion criteria, 981 were randomly assigned to receive chest compression alone and 960 to receive chest compression plus rescue breathing. We observed no significant difference between the two groups in the proportion of patients who survived to hospital discharge (12.5% with chest compression alone and 11.0% with chest compression plus rescue breathing, P=0.31) or in the proportion who survived with a favorable neurologic outcome in the two sites that assessed this secondary outcome (14.4% and 11.5%, respectively; P=0.13). Prespecified subgroup analyses showed a trend toward a higher proportion of patients surviving to hospital discharge with chest compression alone as compared with chest compression plus rescue breathing for patients with a cardiac cause of arrest (15.5% vs. 12.3%, P=0.09) and for those with shockable rhythms (31.9% vs. 25.7%, P=0.09).\nDispatcher instruction consisting of chest compression alone did not increase the survival rate overall, although there was a trend toward better outcomes in key clinical subgroups. The results support a strategy for CPR performed by laypersons that emphasizes chest compression and minimizes the role of rescue breathing. (Funded in part by the Laerdal Foundation for Acute Medicine and the Medic One Foundation; ClinicalTrials.gov number, NCT00219687.)", "title": "CPR with chest compression alone or with rescue breathing.", "date": "2010-09-08"}, {"article_id": "26550795", "content": "During cardiopulmonary resuscitation (CPR) in patients with out-of-hospital cardiac arrest, the interruption of manual chest compressions for rescue breathing reduces blood flow and possibly survival. We assessed whether outcomes after continuous compressions with positive-pressure ventilation differed from those after compressions that were interrupted for ventilations at a ratio of 30 compressions to two ventilations.\nThis cluster-randomized trial with crossover included 114 emergency medical service (EMS) agencies. Adults with non-trauma-related cardiac arrest who were treated by EMS providers received continuous chest compressions (intervention group) or interrupted chest compressions (control group). The primary outcome was the rate of survival to hospital discharge. Secondary outcomes included the modified Rankin scale score (on a scale from 0 to 6, with a score of \u22643 indicating favorable neurologic function). CPR process was measured to assess compliance.\nOf 23,711 patients included in the primary analysis, 12,653 were assigned to the intervention group and 11,058 to the control group. A total of 1129 of 12,613 patients with available data (9.0%) in the intervention group and 1072 of 11,035 with available data (9.7%) in the control group survived until discharge (difference, -0.7 percentage points; 95% confidence interval [CI], -1.5 to 0.1; P=0.07); 7.0% of the patients in the intervention group and 7.7% of those in the control group survived with favorable neurologic function at discharge (difference, -0.6 percentage points; 95% CI, -1.4 to 0.1, P=0.09). Hospital-free survival was significantly shorter in the intervention group than in the control group (mean difference, -0.2 days; 95% CI, -0.3 to -0.1; P=0.004).\nIn patients with out-of-hospital cardiac arrest, continuous chest compressions during CPR performed by EMS providers did not result in significantly higher rates of survival or favorable neurologic function than did interrupted chest compressions. (Funded by the National Heart, Lung, and Blood Institute and others; ROC CCC ClinicalTrials.gov number, NCT01372748.).", "title": "Trial of Continuous or Interrupted Chest Compressions during CPR.", "date": "2015-11-10"}, {"article_id": "10824072", "content": "Despite extensive training of citizens of Seattle in cardiopulmonary resuscitation (CPR), bystanders do not perform CPR in almost half of witnessed cardiac arrests. Instructions in chest compression plus mouth-to-mouth ventilation given by dispatchers over the telephone can require 2.4 minutes. In experimental studies, chest compression alone is associated with survival rates similar to those with chest compression plus mouth-to-mouth ventilation. We conducted a randomized study to compare CPR by chest compression alone with CPR by chest compression plus mouth-to-mouth ventilation.\nThe setting of the trial was an urban, fire-department-based, emergency-medical-care system with central dispatching. In a randomized manner, telephone dispatchers gave bystanders at the scene of apparent cardiac arrest instructions in either chest compression alone or chest compression plus mouth-to-mouth ventilation. The primary end point was survival to hospital discharge.\nData were analyzed for 241 patients randomly assigned to receive chest compression alone and 279 assigned to chest compression plus mouth-to-mouth ventilation. Complete instructions were delivered in 62 percent of episodes for the group receiving chest compression plus mouth-to-mouth ventilation and 81 percent of episodes for the group receiving chest compression alone (P=0.005). Instructions for compression required 1.4 minutes less to complete than instructions for compression plus mouth-to-mouth ventilation. Survival to hospital discharge was better among patients assigned to chest compression alone than among those assigned to chest compression plus mouth-to-mouth ventilation (14.6 percent vs. 10.4 percent), but the difference was not statistically significant (P=0.18).\nThe outcome after CPR with chest compression alone is similar to that after chest compression with mouth-to-mouth ventilation, and chest compression alone may be the preferred approach for bystanders inexperienced in CPR.", "title": "Cardiopulmonary resuscitation by chest compression alone or with mouth-to-mouth ventilation.", "date": "2000-05-29"}]}
{"original_review": "24809657", "question_data": [{"question_id": 158, "question": "Is pain relief higher, lower, or the same when comparing 120 mg etoricoxib to placebo?", "answer": "higher", "evidence_quality": "high", "fulltext_required": "no", "comment": "", "relevant_sources": ["15333415", "21188849", "15100590", "15220011", "15881486", "16192529"]}], "sources": [{"article_id": "16192529", "content": "In this randomized, double-blind, placebo-controlled, multicenter study we assessed the analgesic effect of etoricoxib (a new cyclooxygenase-2 inhibitor) in patients having had knee or hip replacement surgery. A total of 228 patients with moderate or severe pain were randomly allocated within 72 h after surgery to receive etoricoxib 120 mg, controlled-release naproxen sodium 1100 mg, or placebo (1:1:1) on day 1 followed by etoricoxib and placebo (1:2) on days 2 to 7. Patients reported pain scores, rescue (opioid-combination) medication use, and the response to study drug. On day 1, etoricoxib provided an analgesic effect superior to placebo and similar to controlled-release naproxen sodium as demonstrated by the total pain relief score over 8 h, the primary end-point; least-squares mean scores were 11.0, 11.5, and 5.6, respectively (P < 0.001 versus placebo). Similarly, a larger percentage of patients receiving etoricoxib and naproxen sodium than those receiving placebo reported good to excellent responses to study drug: 53%, 60%, and 26% respectively. On days 2-7, etoricoxib demonstrated a significant reduction of rescue medication use, 35% (P < 0.001 versus placebo). The clinical relevance of the decrease was confirmed by Patient's Global Evaluation (P < 0.05 versus placebo). Patients receiving etoricoxib also experienced significantly less \"worst\" and \"average\" pain than did those on placebo. Etoricoxib was generally well tolerated in this study; the incidence of adverse experiences was infrequent and similar across treatment groups. In summary, etoricoxib provided analgesia that was similar to controlled-release naproxen sodium on day 1 and superior to placebo with reduced supplemental opioid use over 7 days.\nIn a postsurgery setting (knee and hip replacements), etoricoxib 120 mg provided analgesia superior to placebo and similar to controlled-release naproxen sodium 1100 mg. Patients receiving etoricoxib suffered less pain and took less opioid rescue medication compared with patients on placebo.", "title": "Etoricoxib provides analgesic efficacy to patients after knee or hip replacement surgery: a randomized, double-blind, placebo-controlled study.", "date": "2005-09-30"}, {"article_id": "15220011", "content": "Patients experiencing acute pain after surgery, including dental surgery, often require analgesia. Ideally, the chosen analgesic should have a rapid onset and sustained effect. Etoricoxib is a new cyclooxygenase-2-selective inhibitor that has demonstrated analgesic efficacy in the treatment of acute pain with a rapid onset and long-lasting pain relief.\nThe goal of this study was to determine the analgesic effect of single oral doses of etoricoxib 60, 120, 180, and 240 mg compared with placebo in the treatment of pain after dental surgery. Ibuprofen was used as an active control.\nThis was a randomized, double-blind, parallel-group, single-dose, placebo- and active comparator-controlled study performed at a single center. It consisted of 3 visits (prestudy, treatment, and poststudy). Eligible patients were aged > or =16 years with moderate or severe pain after surgical extraction of > or =2 third molars, of which > or =1 was an impacted mandibular molar. Patients were assessed over 24 hours and reported pain intensity and pain relied at 14 predefined time points. Plasma samples for a pharmacokinetic/pharmacodynamic analysis were collected from a subset of patients at baseline and the 14 predefined time points. The end points included total pain relief over 8 hours (TOPAR8, the primary end point), sum of pain intensity difference over 8 hours, patient's global evaluation of treatment, median time to onset of pain relief (2-stopwatch method), peak pain relief, and duration of analgesic effect (median time to use of rescue medication). Adverse events were collected up to 14 days postdose.\nThree hundred ninety-eight (63.1% women, 36.9% men; mean age, 21.1 years; 72.1% white, 27.9% other; mean number of third molars removed, 3.5; 65.2% experiencing moderate pain) were randomly allocated to receive etoricoxib 60 mg (n = 75), etoricoxib 120 mg (n = 76), etoricoxib 180 mg (n = 74), etoricoxib 240 mg (n = 76), ibuprofen 400 mg (n = 48), and placebo (n = 49). All active treatments had significantly greater overall analgesic effect (TOPAR8) compared with placebo (P < or 0.001). Patients who received etoricoxib 120 and 180 mg had significantly higher TOPAR8 scores than those who received etoricoxib 60 mg ( P < = 0.001) and ibuprofen (P < 0.05 etoricoxib 120 mg; P < or = 0.001 etoricoxib 180 mg). Least-squares mean TOPAR8 scores for etoricoxib 60, 120, 180, and 240 mg, ibuprofen, and placebo were 16.0, 22.0, 23.5, 20.7, 18.6, and 5.2, respectively. The median time to onset of analgesia was 24 minutes for etoricoxib 120, 180, and 240 mg, and 30 minutes for etoricoxib 60 mg and ibuprofen. There were no significant differences in the onset of analgesia between etoricoxib 120, 180, and 240 mg and ibuprofen. The duration of analgesic effect was >24 hours for etoricoxib 120, 180, and 240 mg, and 12.1 hours for etoricoxib 60 mg. The duration of effect was significantly longer with all 4 etoricoxib doses compared with ibuprofen (10.1 hours; P < 0.05 etoricoxib 60 mg; < or = 0.001etoricoxib 120, 180, and 240 mg) and compared with placebo (2.1 hours; P < = 0.001). In the pharmacokinetic/pharmacodynamic analysis (n approximately 120), there was a linear relationship between plasma etoricoxib concentrations and pain relief scores up to the maximum observed concentration, followed by a decline in plasma concentrations with persistent analgesia. The most common adverse events were postextraction alveolitis and nausea.\nIn this dose-ranging study, etoricoxib 120 mg was determined to be the minimum dose that had maximal efficacy in patients with moderate to severe acute pain associated with dental surgery. Both etoricoxib and ibuprofen were generally well tolerated.", "title": "Etoricoxib in acute pain associated with dental surgery: a randomized, double-blind, placebo- and active comparator-controlled dose-ranging study.", "date": "2004-06-29"}, {"article_id": "15333415", "content": "Our objective in this study was to compare the analgesic effects of etoricoxib and oxycodone/acetaminophen in a postoperative dental pain model. Patients experiencing moderate to severe pain after extraction of two or more third molars were randomized to single doses of etoricoxib 120 mg (n = 100), oxycodone/acetaminophen 10/650 mg (n = 100), or placebo (n = 25). The primary end-point was total pain relief over 6 h. Other end-points included patient global assessment of response to therapy; onset, peak, and duration of effect; and rescue opioid analgesic use. Active treatments were statistically significantly superior to placebo for all efficacy measures. Total pain relief over 6 h for etoricoxib was significantly more than for oxycodone/acetaminophen (P < 0.001). Patient global assessment of response to therapy at 6 and 24 h was superior for etoricoxib. Both drugs achieved rapid onset, although the time was faster for oxycodone/acetaminophen by 5 min. The peak effect was similar for both drugs. Compared with oxycodone/acetaminophen patients, etoricoxib patients experienced a longer analgesic duration, had a smaller percentage requiring rescue opioids during 6 and 24 h, and required less rescue analgesia during 6 and 24 h. Oxycodone/acetaminophen treatment resulted in more frequent adverse events (AEs), drug-related AEs, nausea, and vomiting compared with etoricoxib treatment. In conclusion, etoricoxib 120 mg provided superior overall efficacy compared with oxycodone/acetaminophen 10/650 mg and was associated with significantly fewer AEs.", "title": "The analgesic efficacy of etoricoxib compared with oxycodone/acetaminophen in an acute postoperative pain model: a randomized, double-blind clinical trial.", "date": "2004-08-31"}, {"article_id": "21188849", "content": "This study was conducted to evaluate the dose range of etoricoxib in acute pain using the postoperative dental pain model further.\nThis double-blind, randomized controlled study evaluated etoricoxib (90 and 120 mg), ibuprofen (600 mg), and acetaminophen (600 mg/codeine) (60 mg, (A/C)) in patients aged \u2265 18 years with moderate or severe pain after surgical extraction of \u2265 2 third molars (\u2265 1 impacted). The patients reported pain intensity and pain relief over 24 hours. The primary efficacy endpoint was total pain relief over 6 hours (TOPAR6). Adverse events were evaluated throughout the study.\nThere were 588 patients randomized to placebo (n=46),etoricoxib (90 mg (n=191)), etoricoxib (120 mg (n=97)), ibuprofen(2400 mg (n=192)), and A/C (n=62). The overall analgesic effect (TOPAR6) of etoricoxib (90, 120 mg) was significantly greater than that of placebo (P \u2264 0.001), and not inferior to that of ibuprofen; no discernible difference was observed between etoricoxib 90 and 120 mg. Both etoricoxib doses were superior to A/C (P \u2264 0.001). Etoricoxib (90 and 120 mg) and ibuprofen(2400 mg) were generally well tolerated and had a similar incidence of adverse events (AEs). A/C was associated with significantly more AEs that led to discontinuation (ie, nausea and vomiting).\nEtoricoxib (90 and 120 mg) showed similar efficacy in the postoperative dental pain model, which was noninferior to ibuprofen and superior to A/C. A higher number of tooth extractions or a higher mean impaction score may have led to a greater separation in efficacy between the 2 etoricoxib doses.", "title": "Evaluation of the dose range of etoricoxib in an acute pain setting using the postoperative dental pain model.", "date": "2010-12-29"}, {"article_id": "15881486", "content": "To compare the analgesic effect of single doses of etoricoxib 120 mg, oxycodone/ acetaminophen 10 mg/650 mg and codeine/ acetaminophen 60 mg/600 mg in acute pain using the dental impaction model.\nIn this randomized, double-blind, placebo-controlled, parallel-group study, patients reported pain intensity and pain relief (16 times) and global scores (twice) during a 24-h period. The primary endpoint was the overall analgesic effect, total pain relief over 6 h (TOPAR6). Other endpoints were patient global evaluation, time to onset (2-stopwatch method), duration of analgesic effect (median time to and amount of rescue medication use). Tolerability was evaluated by overall and opioid-related (nausea and vomiting) adverse experiences.\n302 patients (mean age 23; 63% women; 63 % White) were randomized to etoricoxib 120 mg, oxycodone/acetaminophen 10 mg/650 mg, codeine/acetaminophen 60 mg/600 mg, and placebo (2:2:1:1). Etoricoxib demonstrated significantly greater overall analgesic efficacy (TOPAR6) (13.2 units) versus oxycodone/acetaminophen (10.2 units); and codeine/acetaminophen (6.0 units); p < 0.001 for all. All active treatments were superior to placebo. Median time to onset was significantly (p < 0.001) shorter for oxycodone/acetaminophen (20 min) and numerically but not significantly shorter (p = 0.259) for codeine/acetaminophen (26 min) compared with etoricoxib (40 min). Etoricoxib (24 h) had a significantly longer lasting analgesic effect than oxycodone/acetaminophen (5.3 h), codeine/acetaminophen (2.7 h), and placebo (1.7 h) (p < 0.001 for all). Etoricoxib patients experienced fewer clinical adverse experiences than patients on oxycodone/acetaminophen and codeine/acetaminophen, specifically, significantly (p < 0.05) fewer episodes of nausea.\nEtoricoxib 120 mg provided superior overall analgesic effect with a smaller percentage of patients experiencing nausea versus both oxycodone/acetaminophen 10 mg/650 mg and codeine/acetaminophen 60 mg/600 mg.", "title": "The analgesic effect of etoricoxib relative to that of cetaminophen analgesics: a randomized, controlled single-dose study in acute dental impaction pain.", "date": "2005-05-11"}, {"article_id": "15100590", "content": "To compare the overall analgesic effect, including time to onset, peak and duration of effect for etoricoxib 120 mg, a new COX-2 selective inhibitor, in patients with acute pain to that of placebo. Naproxen sodium 550 mg and acetaminophen/codeine 600/60 mg were the active comparators.\nA total of 201 patients with moderate to severe pain following surgical extraction of > or = 2 third molars, of which at least the mandibular tooth was impacted, were randomly allocated to receive single oral doses of placebo (n = 50), etoricoxib 120 mg (n = 50), naproxen sodium 550 mg (n = 51), or acetaminophen/codeine 600/60 mg (n = 50). The endpoints included total pain relief over 8 hours (TOPAR8, primary end point), sum of pain intensity difference over 8 hours, patient's global evaluation, onset, peak, and duration of analgesia.\nEtoricoxib 120 mg had a significantly greater least squares (LS) mean TOPAR8 score than placebo (20.9 vs 5.4; P < 0.001) and acetaminophen/codeine 600/60 mg (20.9 vs 11.5; P < 0.001), and a similar LS mean TOPAR8 score to naproxen sodium 550 mg (20.9 vs 21.3). All three active treatments had rapid onset of analgesia, median time approximately 30 minutes. The duration of analgesic effect, defined as median time to rescue medication use, was >24 hours for etoricoxib, 20.8 hours for naproxen sodium, 3.6 hours for acetaminophen/codeine, and 1.6 hours for placebo.\nEtoricoxib is a new COX-2 selective inhibitor under development for treatment of osteoarthritis, rheumatoid arthritis, and acute pain. In this study, etoricoxib 120 mg provided rapid and long-lasting pain relief to patients with moderate-to-severe postdental surgery pain. Etoricoxib was generally well tolerated.", "title": "A randomized, double-blind, parallel-group study comparing the analgesic effect of etoricoxib to placebo, naproxen sodium, and acetaminophen with codeine using the dental impaction pain model.", "date": "2004-04-22"}]}
{"original_review": "28829911", "question_data": [{"question_id": 159, "question": "Is overall survival higher, lower, or the same when comparing chemoradiotherapy plus esophagectomy to chemoradiotherapy alone?", "answer": "no difference", "evidence_quality": "high", "fulltext_required": "no", "comment": "", "relevant_sources": ["17401004", "15800321"]}, {"question_id": 160, "question": "Is the risk of treatment\u2010related mortality higher, lower, or the same when comparing chemoradiotherapy plus esophagectomy to chemoradiotherapy alone?", "answer": "higher", "evidence_quality": "low", "fulltext_required": "no", "comment": "", "relevant_sources": ["17401004", "15800321"]}, {"question_id": 161, "question": "Is short-term quality of life higher, lower, or the same when comparing chemoradiotherapy plus esophagectomy to chemoradiotherapy alone?", "answer": "lower", "evidence_quality": "very low", "fulltext_required": "no", "comment": "", "relevant_sources": ["16524973"]}], "sources": [{"article_id": "17971585", "content": "Chemoradiotherapy (CRT) is an alternative to surgery for resectable locally advanced esophageal carcinoma (RLA-EC). We investigated the heterogeneity of the treatment benefits across subgroups of patients, defined according to the radiation scheme.\nBetween February 1993 and December 2000, 451 patients were enrolled. The following two schemes were allowed: protracted radiotherapy (P-RT), which scheduled 46 Gy over 4.5 weeks or split-course radiotherapy (SC-RT) with two 1-week courses of 15 Gy. Two courses of cisplatin and fluorouracil were delivered concomitantly. In case of exclusive CRT, a further course of 20 Gy over 2 weeks in the P-RT group and one 1-week course of 15 Gy in the SC-RT group were delivered with three courses of chemotherapy. SC-RT and P-RT were administered to 285 patients (64%) and 161 patients (36%), respectively.\nFor P-RT versus SC-RT, the response rate to induction CRT was 67% v 68%, respectively (P = .09), and 2-year local relapse-free survival rate was 76.7% v 56.8%, respectively (P = .002). Shorter tumor length and P-RT were associated with better local control in multivariate analysis (P = .002 for both). After a median follow-up time of 47.4 months, 2-year overall survival rate was 37.1% for P-RT compared with 30.5% for SC-RT (P = .25). Independent prognostic factors on survival were tumor diameter (P = .02), weight loss of 10% or less (P = .05), and response to induction CRT (P = .002).\nPatients with RLA-EC treated with P-RT had better local control than patients treated with SC-RT. Response to induction CRT is a determinant prognostic factor on survival.", "title": "Phase III trial of protracted compared with split-course chemoradiation for esophageal carcinoma: Federation Francophone de Cancerologie Digestive 9102.", "date": "2007-11-01"}, {"article_id": "17401004", "content": "Uncontrolled studies suggest that chemoradiation has similar efficacy as surgery for esophageal cancer. Therefore, a randomized trial was carried out to compare, in responders only, chemoradiation alone with chemoradiation followed by surgery in patients with locally advanced tumors.\nEligible patients had operable T3N0-1M0 thoracic esophageal cancer. Patients received two cycles of fluorouracil (FU) and cisplatin (days 1 to 5 and 22 to 26) and either conventional (46 Gy in 4.5 weeks) or split-course (15 Gy, days 1 to 5 and 22 to 26) concomitant radiotherapy. Patients with response and no contraindication to either treatment were randomly assigned to surgery (arm A) or continuation of chemoradiation (arm B; three cycles of FU/cisplatin and either conventional [20 Gy] or split-course [15 Gy] radiotherapy). Chemoradiation was considered equivalent to surgery if the difference in 2-year survival rate was less than 10%.\nOf 444 eligible patients, 259 were randomly assigned; 230 patients (88.8%) had epidermoid cancer, and 29 (11.2%) had glandular carcinoma. Two-year survival rate was 34% in arm A versus 40% in arm B (hazard ratio for arm B v arm A = 0.90; adjusted P = .44). Median survival time was 17.7 months in arm A compared with 19.3 months in arm B. Two-year local control rate was 66.4% in arm A compared with 57.0% in arm B, and stents were less required in the surgery arm (5% in arm A v 32% in arm B; P < .001). The 3-month mortality rate was 9.3% in arm A compared with 0.8% in arm B (P = .002). Cumulative hospital stay was 68 days in arm A compared with 52 days in arm B (P = .02).\nOur data suggest that, in patients with locally advanced thoracic esophageal cancers, especially epidermoid, who respond to chemoradiation, there is no benefit for the addition of surgery after chemoradiation compared with the continuation of additional chemoradiation.", "title": "Chemoradiation followed by surgery compared with chemoradiation alone in squamous cancer of the esophagus: FFCD 9102.", "date": "2007-04-03"}, {"article_id": "16524973", "content": "The aim of the study was to compare the longitudinal quality of life (QoL) between chemoradiation with or without surgery in patients with locally advanced squamous resectable esophageal cancer included in a randomized multicenter phase III trial (FFCD 9102).\nAll patients with locally advanced resectable (T3-4 N0-1 M0) epidermoid or glandular esophageal cancer (n = 451) received induction chemoradiation. Responders (n = 259) were randomized between surgery (arm A) and continuation of chemoradiation (arm B). The Spitzer QoL Index was scored (0-10) at inclusion and at each follow-up, every 3 months during 2 years. QoL at baseline and longitudinal changes were respectively compared with univariate ANOVA and mixed-model analysis of variance for repeated measurements. The time interval between the follow-up was assessed and the same analyses were performed among survivors with 2 years of follow-up.\nThe squamous histology was predominant in both arms. The mean QoL score decreased between baseline and the first follow-up and between the first and the second follow-ups. QoL scores at the first follow-up were comparatively worse in arm A than in arm B (7.52 versus 8.45, P < 0.01), whereas the longitudinal QoL study showed no difference between treatments (adjusted P = 0.26). Furthermore, the longitudinal QoL was not different (adjusted P = 0.23) among survivors with 2 years of follow-up.\nAmong patients responding to induction chemoradiation, surgery and continuation of chemoradiation had the same impact on QoL in patients with locally advanced, resectable esophageal cancer although a significantly greater decrease in the Spitzer Index was observed in the postoperative period.", "title": "A comparative longitudinal quality of life study using the Spitzer quality of life index in a randomized multicenter phase III trial (FFCD 9102): chemoradiation followed by surgery compared with chemoradiation alone in locally advanced squamous resectable thoracic esophageal cancer.", "date": "2006-03-10"}, {"article_id": "15800321", "content": "Combined chemoradiotherapy with and without surgery are widely accepted alternatives for the curative treatment of patients with locally advanced esophageal cancer. The value of adding surgery to chemotherapy and radiotherapy is unknown.\nPatients with locally advanced squamous cell carcinoma (SCC) of the esophagus were randomly allocated to either induction chemotherapy followed by chemoradiotherapy (40 Gy) followed by surgery (arm A), or the same induction chemotherapy followed by chemoradiotherapy (at least 65 Gy) without surgery (arm B). Primary outcome was overall survival time.\nThe median observation time was 6 years. The analysis of 172 eligible, randomized patients (86 patients per arm) showed overall survival to be equivalent between the two treatment groups (log-rank test for equivalence, P < .05). Local progression-free survival was better in the surgery group (2-year progression-free survival, 64.3%; 95% CI, 52.1% to 76.5%) than in the chemoradiotherapy group (2-year progression-free survival, 40.7%; 95% CI, 28.9% to 52.5%; hazard ratio [HR] for arm B v arm A, 2.1; 95% CI, 1.3 to 3.5; P = .003). Treatment-related mortality was significantly increased in the surgery group than in the chemoradiotherapy group (12.8% v 3.5%, respectively; P = .03). Cox regression analysis revealed clinical tumor response to induction chemotherapy to be the single independent prognostic factor for overall survival (HR, 0.30; 95% CI, 0.19 to 0.47; P < .0001).\nAdding surgery to chemoradiotherapy improves local tumor control but does not increase survival of patients with locally advanced esophageal SCC. Tumor response to induction chemotherapy identifies a favorable prognostic group within these high-risk patients, regardless of the treatment group.", "title": "Chemoradiation with and without surgery in patients with locally advanced squamous cell carcinoma of the esophagus.", "date": "2005-04-01"}, {"article_id": "18372134", "content": "Sitagliptin, an oral dipeptidyl peptidase-4 inhibitor, lowers blood glucose when administered as monotherapy or in combination with other antihyperglycemic agents. TECOS will evaluate the effects of adding sitagliptin to usual diabetes care on cardiovascular outcomes and clinical safety. TECOS is a pragmatic, academically run, multinational, randomized, double-blind, placebo-controlled, event-driven trial recruiting approximately 14,000 patients in 38 countries who have type 2 diabetes (T2DM), are at least 50 years old, have cardiovascular disease, and have an hemoglobin A1c value between 6.5% and 8.0%. Eligible participants will be receiving stable mono- or dual therapy with metformin, sulfonylurea, or pioglitazone, or insulin alone or in combination with metformin. Randomization is 1:1 to double-blind sitagliptin or matching placebo, in addition to existing therapy in a usual care setting. Follow-up occurs at 4-month intervals in year 1 and then twice yearly until 1300 confirmed primary end points have occurred. Glycemic equipoise between randomized groups is a desired aim. The primary composite cardiovascular endpoint is time to the first occurrence of cardiovascular death, nonfatal myocardial infarction, nonfatal stroke, or hospitalization for unstable angina, with cardiovascular events adjudicated by an independent committee blinded to study therapy. TECOS is a pragmatic-design cardiovascular outcome trial assessing the cardiovascular effects of sitagliptin when added to usual T2DM management.", "title": "Rationale, design, and organization of a randomized, controlled Trial Evaluating Cardiovascular Outcomes with Sitagliptin (TECOS) in patients with type 2 diabetes and established cardiovascular disease.", "date": "2013-11-26"}, {"article_id": "26163097", "content": "Two randomised trials concerning thoracic oesophageal cancer concluded that for squamous cell carcinoma, chemoradiation alone leads to the same overall survival (OS) as chemoradiation followed by surgery. One of these trials, FFCD 9102, randomised only fit, compliant and operable responders to induction chemoradiation between continuation of chemoradiation and surgery. In the present analysis, the outcome in the patients not eligible for randomisation was calculated to determine if attempt of surgery should be recommended.\nEligible patients had operable T3-N0/N1-M0 thoracic oesophageal cancer. After initial chemoradiation, patients with no clinical response, or with contraindication to follow any attributed treatment, were not randomised. OS was studied first in the whole population of not randomised patients, and then specifically in clinical non-responders. The impact of surgery on OS was studied in these two populations.\nOf the 451 registered patients in the trial, 192 were not randomised. Among them, 111 were clinical non-responders. Median OS was significantly shorter for non-randomised patients (11.5 months) than for randomised patients (18.9 months; p=0.0024). However, for the 112 non-randomised patients who underwent surgery, median OS was not different from that in randomised patients: 17.3 versus 18.9 months (p=0.58). Concerning clinical non-responders, median OS was longer for those who underwent surgery compared to non-operated patients: 17.0 versus 5.5 months (hazard ratio (HR)=0.39 [0.25-0.61]; p<0.0001), and again was not different from that in responding, randomised patients (p=0.40).\nIn patients with locally advanced thoracic oesophageal cancer, overall survival did not differ between responders to induction chemoradiation and patients having surgery after clinical failure of chemoradiation. Surgery should therefore be considered in those patients who are still operable.", "title": "Early surgery for failure after chemoradiation in operable thoracic oesophageal cancer. Analysis of the non-randomised patients in FFCD 9102 phase III trial: Chemoradiation followed by surgery versus chemoradiation alone.", "date": "2015-07-15"}]}
{"original_review": "25739381", "question_data": [{"question_id": 162, "question": "Is neonatal mortality higher, lower, or the same when comparing community-led chlorhexidine skin cleansing to community-led usual skin care?", "answer": "no difference", "evidence_quality": "high", "fulltext_required": "no", "comment": "", "relevant_sources": ["17210728"]}, {"question_id": 163, "question": "Is neonatal mortality higher, lower, or the same when comparing community-led chlorhexidine cord cleansing to community-led dry cord care?", "answer": "lower", "evidence_quality": "high", "fulltext_required": "no", "comment": "", "relevant_sources": ["22322124", "16546539", "22322126"]}, {"question_id": 164, "question": "Is the risk of omphalitis/infections higher, lower, or the same when comparing community-led chlorhexidine cord cleansing to community-led dry cord care?", "answer": "lower", "evidence_quality": "high", "fulltext_required": "no", "comment": "", "relevant_sources": ["22322124", "16546539", "22322126"]}, {"question_id": 225, "question": "Is the risk of omphalitis/infections higher, lower, or the same when comparing community-led chlorhexidine skin cleansing to community-led usual skin care?", "answer": "insufficient data", "evidence_quality": "", "fulltext_required": "no", "comment": "", "relevant_sources": ["17210728"]}], "sources": [{"article_id": "21247573", "content": "To determine the safety, acceptability, and antimicrobial effect of 1% chlorhexidine (CHX) vaginal washing of women in labor and their neonates.\nRandomized controlled trial of 1% CHX vaginal and neonatal washing compared with no washing (usual care [UC]). The study included 502 women (334 CHX, 168 UC) who delivered 508 liveborn neonates (335 CHX, 173 UC). Main outcome measures were the incidence of maternal adverse effects, the incidence of neonatal skin rash, the axillary temparature before and after neonatal wiping, and vaginal culture results.\nMaternal demographics did not differ between the groups. No case of maternal rash occurred; 4% of women experienced vaginal burning. An axillary temperature drop of more than 1 \u00b0C after CHX cleansing occurred in 8 neonates; 2 neonates had a minor rash. In the subset of women with positive vaginal cultures as baseline, 1% CHX eliminated culture growth in 56% after 1 wash, and in 86% after 2 washes.\nUse of 1% CHX is safe for neonates, well tolerated by laboring mothers, and effective in treating vaginal infections during labor. A randomized controlled trial using 1% CHX and powered for a reduction in neonatal septic mortality is justified based on these data.", "title": "Randomized study of vaginal and neonatal cleansing with 1% chlorhexidine.", "date": "2011-01-21"}, {"article_id": "19846212", "content": "About 500,000 sepsis-related deaths per year arise in the first 3 days of life. On the basis of results from non-randomised studies, use of vaginal chlorhexidine wipes during labour has been proposed as an intervention for the prevention of early-onset neonatal sepsis in developing countries. We therefore assessed the efficacy of chlorhexidine in early-onset neonatal sepsis and vertical transmission of group B streptococcus.\nIn a trial in Soweto, South Africa, 8011 women (aged 12-51 years) were randomly assigned in a 1:1 ratio to chlorhexidine vaginal wipes or external genitalia water wipes during active labour, and their 8129 newborn babies were assigned to full-body (intervention group) or foot (control group) washes with chlorhexidine at birth, respectively. In a subset of mothers (n=5144), we gathered maternal lower vaginal swabs and neonatal skin swabs after delivery to assess colonisation with potentially pathogenic bacteria. Primary outcomes were neonatal sepsis in the first 3 days of life and vertical transmission of group B streptococcus. Analysis was by intention to treat. The trial is registered with ClinicalTrials.gov, number NCT00136370.\nRates of neonatal sepsis did not differ between the groups (chlorhexidine 141 [3%] of 4072 vs control 148 [4%] of 4057; p=0.6518). Rates of colonisation with group B streptococcus in newborn babies born to mothers in the chlorhexidine (217 [54%] of 401) and control groups (234 [55%] of 429] did not differ (efficacy -0.05%, 95% CI -9.5 to 7.9).\nBecause chlorhexidine intravaginal and neonatal wipes did not prevent neonatal sepsis or the vertical acquisition of potentially pathogenic bacteria among neonates, we need other interventions to reduce childhood mortality.\nUS Agency for International Development, National Vaccine Program Office and Centers for Disease Control's Antimicrobial Resistance Working Group, and Bill & Melinda Gates Foundation.", "title": "Chlorhexidine maternal-vaginal and neonate body wipes in sepsis and vertical transmission of pathogenic bacteria in South Africa: a randomised, controlled trial.", "date": "2009-10-23"}, {"article_id": "16546539", "content": "Omphalitis contributes to neonatal morbidity and mortality in developing countries. Umbilical cord cleansing with antiseptics might reduce infection and mortality risk, but has not been rigorously investigated.\nIn our community-based, cluster-randomised trial, 413 communities in Sarlahi, Nepal, were randomly assigned to one of three cord-care regimens. 4934 infants were assigned to 4.0% chlorhexidine, 5107 to cleansing with soap and water, and 5082 to dry cord care. In intervention clusters, the newborn cord was cleansed in the home on days 1-4, 6, 8, and 10. In all clusters, the cord was examined for signs of infection (pus, redness, or swelling) on these visits and in follow-up visits on days 12, 14, 21, and 28. Incidence of omphalitis was defined under three sign-based algorithms, with increasing severity. Infant vital status was recorded for 28 completed days. The primary outcomes were incidence of neonatal omphalitis and neonatal mortality. Analysis was by intention-to-treat. This trial is registered with , number NCT00109616.\nFrequency of omphalitis by all three definitions was reduced significantly in the chlorhexidine group. Severe omphalitis in chlorhexidine clusters was reduced by 75% (incidence rate ratio 0.25, 95% CI 0.12-0.53; 13 infections/4839 neonatal periods) compared with dry cord-care clusters (52/4930). Neonatal mortality was 24% lower in the chlorhexidine group (relative risk 0.76 [95% CI 0.55-1.04]) than in the dry cord care group. In infants enrolled within the first 24 h, mortality was significantly reduced by 34% in the chlorhexidine group (0.66 [0.46-0.95]). Soap and water did not reduce infection or mortality risk.\nRecommendations for dry cord care should be reconsidered on the basis of these findings that early antisepsis with chlorhexidine of the umbilical cord reduces local cord infections and overall neonatal mortality.", "title": "Topical applications of chlorhexidine to the umbilical cord for prevention of omphalitis and neonatal mortality in southern Nepal: a community-based, cluster-randomised trial.", "date": "2006-03-21"}, {"article_id": "17210728", "content": "Hospital-based data from Africa suggest that newborn skin-cleansing with chlorhexidine may reduce neonatal mortality. Evaluation of this intervention in the communities where most births occur in the home has not been done. Our objective was to assess the efficacy of a 1-time skin-cleansing of newborn infants with 0.25% chlorhexidine on neonatal mortality.\nThe design was a community-based, placebo-controlled, cluster-randomized trial in Sarlahi District in southern Nepal. Newborn infants were cleansed with infant wipes that contained 0.25% chlorhexidine or placebo solution as soon as possible after delivery in the home (median: 5.8 hours). The primary outcome was all-cause mortality by 28 days. After the completion of the randomized phase, all newborns in study clusters were converted to chlorhexidine treatment for the subsequent 9 months.\nA total of 17,530 live births occurred in the enrolled sectors, 8650 and 8880 in the chlorhexidine and placebo groups, respectively. Baseline characteristics were similar in the treatment groups. Intention-to-treat analysis among all live births showed no impact of the intervention on neonatal mortality. Among live-born infants who actually received their assigned treatment (98.7%), there was a nonsignificant 11% lower neonatal mortality rate among those who were treated with chlorhexidine compared with placebo. Low birth weight infants had a statistically significant 28% reduction in neonatal mortality; there was no significant difference among infants who were born weighing > or = 2500 g. After conversion to active treatment in the placebo clusters, there was a 37% reduction in mortality among low birth weight infants in the placebo clusters versus no change in the chlorhexidine clusters.\nNewborn skin-wiping with chlorhexidine solution once, soon after birth, reduced neonatal mortality only among low birth weight infants. Evidence from additional trials is needed to determine whether this inexpensive and simple intervention could improve survival significantly among low birth weight infants in settings where home delivery is common and hygiene practices are poor.", "title": "Impact of newborn skin-cleansing with chlorhexidine on neonatal mortality in southern Nepal: a community-based, cluster-randomized trial.", "date": "2007-01-11"}, {"article_id": "18979599", "content": "A masked randomized clinical trial was conducted in 93 neonates who received the first bath with chlorhexidine (experimental) (n =44) or neutral liquid soap (control) (n =49). Three samples were collected for culture from the neonates' right axilla skin before bath, 30 min and 24 h after bath. Immediately before bath, Staphylococcus aureus colonization prevalence was 10.2% (n = 5) in control and 4.5% (n =2) in the experimental group (p =0.74). 30 min after bath, S. aureus prevalence was 20.4% (n = 10) in control and 2.3% (n = 1) in the experimental group (p =0.017). 24 h after bath, S. aureus prevalence was 36.7% (n = 18) in control and 13.6% (n =6) in the experimental group (p =0.021). There was no occurrence of sepsis in the first month in both groups. In conclusion, a first bath with chlorhexidine reduced S. aureus colonization on the newborn's skin in a 24-h period.", "title": "Effect of the first bath with chlorhexidine on skin colonization with Staphylococcus aureus in normal healthy term newborns.", "date": "2008-11-04"}, {"article_id": "19202343", "content": "Best practice for umbilical cord care (UC) still remains controversial in developed countries with aseptic perinatal care. A bicenter randomized clinical trial was performed to evaluate the efficacy of chlorhexidine (CX) powder versus dry cord care (DC) for UC.\nAll neonates of two neonatal care units were invited to take part in the study. Participants were randomized to either DC or UC with CX powder (0.1%). Primary study outcome was the cord separation time. Secondary outcomes were omphalitis, granuloma of the umbilical ground, adverse events and parents' treatment satisfaction. The outcome parameters were documented at a hospital-located study visit 10-14 days after birth.\n669 neonates were enrolled in the trial. 337 were randomized to receive CX powder for umbilical cord care, 332 to DC. Cord separation time was 7.0 +/- 2.5 days in CX-treated neonates and 7.8 +/- 2.9 days in DC (p < 0.001). There were 9 cases of omphalitis, 2 in the CX group, 7 in the DC group (p = 0.1). No difference in the occurrence of umbilical granuloma between the treatment regimens was detected. Neonates randomized to CX were less likely to have an adverse event (140 in 109 subjects vs. 205 in 149 subjects in the DC neonates; p = 0.001). Half of these adverse events were cord-related. Neonates randomized to DC had nearly twice as many cord-related adverse events as those with CX treatment (CX: 58 in 54 patients vs. DC: 110 in 97 patients; p < 0.001). Parents' treatment satisfaction was significantly higher in the neonates with CX cord care.\nCord-related adverse events in neonatal umbilical cord care remain a clinical issue. Even in an aseptic birth context in a developed country, cord care with CX powder showed a reduction of cord-related adverse events.", "title": "Higher rate of cord-related adverse events in neonates with dry umbilical cord care compared to chlorhexidine powder. Results of a randomized controlled study to compare efficacy and safety of chlorhexidine powder versus dry care in umbilical cord care of the newborn.", "date": "2009-02-10"}, {"article_id": "20502294", "content": "To estimate the effects of chlorhexidine vaginal and baby wipes on fetal and neonatal mortality, respectively, and infection-related morbidity.\nWe performed a placebo-controlled, randomized trial of chlorhexidine vaginal and neonatal wipes to reduce neonatal sepsis and mortality in three hospitals in Pakistan. The primary study outcome was a composite of neonatal sepsis or 7-day perinatal mortality.\nFrom 2005 to 2008, 5,008 laboring women and their neonates were randomly assigned to receive either chlorhexidine wipes (n=2,505) or wipes with a saline placebo (n=2,503). The primary outcome was similar in the chlorhexidine and control groups (3.1% compared with 3.4%; relative risk 0.91, 95% confidence interval 0.67-1.24) as was the composite rate of neonatal sepsis or 28-day perinatal mortality (3.8% compared with 3.9%, relative risk 0.96, 95% confidence interval 0.73-1.27). At day 7, the chlorhexidine group had a lower rate of neonatal skin infection (3.3% compared with 8.2%, P<.001). With the exception of less frequent 7-day hospitalization in the chlorhexidine group, there were no significant differences in maternal outcomes between the groups.\nUsing maternal chlorhexidine vaginal wipes during labor and neonatal chlorhexidine wipes does not reduce maternal and perinatal mortality or neonatal sepsis. The finding of reduced superficial skin infections on day 7 without change in sepsis or mortality suggests that this difference, although statistically significant, may not be of major importance.\nI.", "title": "Chlorhexidine vaginal and infant wipes to reduce perinatal mortality and morbidity: a randomized controlled trial.", "date": "2010-05-27"}, {"article_id": "22322124", "content": "Up to half of neonatal deaths in high mortality settings are due to infections, many of which can originate through the freshly cut umbilical cord stump. We aimed to assess the effectiveness of two cord-cleansing regimens with the promotion of dry cord care in the prevention of neonatal mortality.\nWe did a community-based, parallel cluster-randomised trial in Sylhet, Bangladesh. We divided the study area into 133 clusters, which were randomly assigned to one of the two chlorhexidine cleansing regimens (single cleansing as soon as possible after birth; daily cleansing for 7 days after birth) or promotion of dry cord care. Randomisation was done by use of a computer-generated sequence, stratified by cluster-specific participation in a previous trial. All livebirths were eligible; those visited within 7 days by a local female village health worker trained to deliver the cord care intervention were enrolled. We did not mask study workers and participants to the study interventions. Our primary outcome was neonatal mortality (within 28 days of birth) per 1000 livebirths, which we analysed on an intention-to-treat basis. This trial is registered with ClinicalTrials.gov, number NCT00434408.\nBetween June, 2007, and September, 2009, we enrolled 29\u2008760 newborn babies (10\u2008329, 9423, and 10\u2008008 in the multiple-cleansing, single-cleansing, and dry cord care groups, respectively). Neonatal mortality was lower in the single-cleansing group (22\u00b75 per 1000 livebirths) than it was in the dry cord care group (28\u00b73 per 1000 livebirths; relative risk [RR] 0\u00b780 [95% CI] 0\u00b765-0\u00b798). Neonatal mortality in the multiple-cleansing group (26\u00b76 per 1000 livebirths) was not statistically significantly lower than it was in the dry cord care group (RR 0\u00b794 [0\u00b778-1\u00b714]). Compared with the dry cord care group, we recorded a statistically significant reduction in the occurrence of severe cord infection (redness with pus) in the multiple-cleansing group (risk per 1000 livebirths=4\u00b72 vs risk per 1000 livebirths=1\u00b72; RR 0\u00b735 [0\u00b715-0\u00b781]) but not in the single-cleansing group (risk per 1000 livebirths=3\u00b73; RR 0\u00b777 [0\u00b740-1\u00b748]).\nChlorhexidine cleansing of a neonate's umbilical cord can save lives, but further studies are needed to establish the best frequency with which to deliver the intervention.\nUnited States Agency for International Development and Save the Children's Saving Newborn Lives program, through a grant from the Bill & Melinda Gates Foundation.", "title": "The effect of cord cleansing with chlorhexidine on neonatal mortality in rural Bangladesh: a community-based, cluster-randomised trial.", "date": "2012-02-11"}, {"article_id": "3890463", "content": "In a prospective randomized study different regimens for skin and umbilical disinfection in newborn infants were tested: daily whole body soap wash (control group), daily whole body soap wash and umbilical cleansing with (i) benzine solution, or (ii) 0.05% chlorhexidine, and daily whole body wash and umbilical cleansing with a 4% chlorhexidine detergent solution (Hibiscrub). Bacterial cultures were taken from the nose and umbilical area at discharge. Clinical infections were registered in the nursery, and after discharge until 6 weeks of age. Cultures were taken from infected areas. In the control group a high colonization rate was found for S. aureus (91%), E. coli (39%), and group B streptococci (GBS) (20%). The colonization rates were influenced by the Hibiscrub regimen (colonization rate for S. aureus 59%, E. coli 23%, and GBS 10%), but not by the other regimens. Infections (pemphigus, paronychia, conjunctivitis, umbilical infection) occurred in 12.9% of the infants, of whom 65% got infection after discharge from the nursery. 96% of the infections were caused by S. aureus, and 87% caused by strains colonizing the infants in the nursery. None of the tested regimens reduced the rate of infections during the first 6 weeks of life.", "title": "Bacterial colonization and neonatal infections. Effects of skin and umbilical disinfection in the nursery.", "date": "1985-05-01"}, {"article_id": "17978107", "content": "To assess tolerance and safety of 0.6% chlorhexidine vaginal and neonatal wipes to improve perinatal outcomes in home deliveries in Pakistan and the ability of traditional birth attendants and project staff to perform a randomized trial of this intervention.\nFocus groups of pregnant and nonpregnant women and in-depth interviews of traditional birth attendants explored barriers to the use of chlorhexidine wipes. Then, a study was performed of women delivering at home attended by traditional birth attendants. Consenting women were randomly assigned to receive either 0.6% chlorhexidine or saline vaginal and neonatal wipes. Women and their infants were followed up on postpartum days 7, 14, and 28. Acceptability and tolerance of vaginal and neonatal wipes, as well as maternal and neonatal outcomes, were assessed.\nThe focus groups and interviews indicated that the chlorhexidine intervention would be acceptable to women and their providers. Of the 213 eligible pregnant women approached, 203 (95%) gave informed consent and were enrolled and allocated to groups. Traditional birth attendants had no difficulty administering chlorhexidine vaginal and neonatal wipes in a home setting. Of the 203 births, 103 (51%) of whom received 0.6% chlorhexidine, there were no allergic reactions, vaginal itching, burning, or requests for study termination. Follow-up at 28 days postpartum was more than 95%. Although this study was not powered to show significant differences in neonatal outcomes between treatment groups, the lower rates of some neonatal adverse clinical outcomes in the chlorhexidine group were encouraging.\nUse of 0.6% chlorhexidine vaginal and neonatal wipes for the prevention of neonatal infection is well-tolerated and seems safe. A trial of this intervention by traditional birth attendants in a home-delivery setting is feasible.\nClinicalTrials.gov, www.clinicaltrials.gov, NCT00121394\nI.", "title": "Chlorhexidine vaginal and neonatal wipes in home births in Pakistan: a randomized controlled trial.", "date": "2007-11-06"}, {"article_id": "22322126", "content": "Umbilical cord infection (omphalitis) is a risk factor for neonatal sepsis and mortality in low-resource settings where home deliveries are common. We aimed to assess the effect of umbilical-cord cleansing with 4% chlorhexidine (CHX) solution, with or without handwashing with antiseptic soap, on the incidence of omphalitis and neonatal mortality.\nWe did a two-by-two factorial, cluster-randomised trial in Dadu, a rural area of Sindh province, Pakistan. Clusters were defined as the population covered by a functional traditional birth attendant (TBA), and were randomly allocated to one of four groups (groups A to D) with a computer-generated random number sequence. Implementation and data collection teams were masked to allocation. Liveborn infants delivered by participating TBAs who received birth kits were eligible for enrolment in the study. One intervention comprised birth kits containing 4% CHX solution for application to the cord at birth by TBAs and once daily by family members for up to 14 days along with soap and educational messages promoting handwashing. One intervention was CHX solution only and another was handwashing only. Standard dry cord care was promoted in the control group. The primary outcomes were incidence of neonatal omphalitis and neonatal mortality. The trial is registered with ClinicalTrials.gov, number NCT00682006.\n187 clusters were randomly allocated to one of the four study groups. Of 9741 newborn babies delivered by participating TBAs, factorial analysis indicated a reduction in risk of omphalitis with CHX application (risk ratio [RR]=0\u00b758, 95% CI 0\u00b741-0\u00b782; p=0\u00b7002) but no evidence of an effect of handwashing (RR=0\u00b783, 0\u00b761-1\u00b713; p=0\u00b724). We recorded strong evidence of a reduction in neonatal mortality in neonates who received CHX cleansing (RR=0\u00b762, 95 % CI 0\u00b745-0\u00b785; p=0\u00b7003) but no evidence of an effect of handwashing promotion on neonatal mortality (RR=1\u00b708, 0\u00b779-1\u00b748; p=0\u00b762). We recorded no serious adverse events.\nApplication of 4% CHX to the umbilical cord was effective in reducing the risk of omphalitis and neonatal mortality in rural Pakistan. Provision of CHX in birth kits might be a useful strategy for the prevention of neonatal mortality in high-mortality settings.\nThe United States Agency for International Development.", "title": "Topical application of chlorhexidine to neonatal umbilical cords for prevention of omphalitis and neonatal mortality in a rural district of Pakistan: a community-based, cluster-randomised trial.", "date": "2012-02-11"}, {"article_id": "23407285", "content": "To compare topical application of chlorhexidine for umbilical cord care with conventional dry care for prevention of neonatal sepsis in neonatal intensive care unit (NICU).\nThe study was conducted in the NICU of a teaching hospital in north India between 2010 and 2011. Newborns (\u2265 32 weeks of gestation and weighing \u2265 1500 g) were randomized into chlorhexidine application and dry care groups. Data regarding time of cord separation, umbilical sepsis and neonatal sepsis were recorded.\nOne hundred forty (dry care group 70, chlorhexidine group 70) were enrolled and finally analysed. A significant difference was observed among groups in terms of time to cord separation and incidence of blood culture-proven sepsis though there was no statistical difference noted among the groups with regards to umbilical infection, probable sepsis and meningitis.\nUse of chlorhexidine for umbilical cord care prevents sepsis in the NICU.", "title": "Effect of topical application of chlorhexidine for umbilical cord care in comparison with conventional dry cord care on the risk of neonatal sepsis: a randomized controlled trial.", "date": "2013-02-15"}]}
{"original_review": "29460275", "question_data": [{"question_id": 165, "question": "Is distance visual acuity higher, lower, or the same when comparing vision screening alone to no vision screening?", "answer": "no difference", "evidence_quality": "high", "fulltext_required": "no", "comment": "", "relevant_sources": ["18614568"]}, {"question_id": 166, "question": "Is near visual acuity higher, lower, or the same when comparing vision screening alone to no vision screening?", "answer": "no difference", "evidence_quality": "high", "fulltext_required": "no", "comment": "", "relevant_sources": ["18614568"]}, {"question_id": 167, "question": "Is visually-related quality of life higher, lower, or the same when comparing vision screening alone to no vision screening?", "answer": "no difference", "evidence_quality": "high", "fulltext_required": "no", "comment": "", "relevant_sources": ["18614568"]}, {"question_id": 168, "question": "Is the risk of visual impairment higher, lower, or the same when comparing a detailed health assessment including measurement of visual acuity to a brief health assessment including one question about vision (standard care)?", "answer": "no difference", "evidence_quality": "moderate", "fulltext_required": "no", "comment": "", "relevant_sources": ["14593039"]}, {"question_id": 169, "question": "Is visually-related quality of life higher, lower, or the same when comparing a detailed health assessment including measurement of visual acuity to a brief health assessment including one question about vision (standard care)?", "answer": "no difference", "evidence_quality": "high", "fulltext_required": "no", "comment": "", "relevant_sources": ["14593039"]}], "sources": [{"article_id": "14593039", "content": "To determine the effectiveness of screening for visual impairment in people aged 75 or over as part of a multidimensional screening programme.\nCluster randomised trial.\nGeneral practices in the United Kingdom participating in the MRC trial of assessment and management of older people in the community.\n4340 people aged 75 years or over randomly sampled from 20 general practices, excluding people resident in hospitals or nursing homes.\nVisual acuity testing and referral to eye services for people with visual impairment. Universal screening (assessment and visual acuity testing) was compared with targeted screening, in which only participants with a range of health related problems were offered an assessment that included acuity screening.\nProportion of people with visual acuity less than 6/18 in either eye; mean composite score of 25 item version of the National Eye Institute visual function questionnaire.\nThree to five years after screening, the relative risk of having visual acuity < 6/18 in either eye, comparing universal with targeted screening, was 1.07 (95% confidence interval 0.84 to 1.36; P = 0.58). The mean composite score of the visual function questionnaire was 85.6 in the targeted screening group and 86.0 in the universal group (difference 0.4, 95% confidence interval -1.7 to 2.5, P = 0.69).\nIncluding a vision screening component by a practice nurse in a pragmatic trial of multidimensional screening for older people did not lead to improved visual outcomes.", "title": "Screening older people for impaired vision in primary care: cluster randomised trial.", "date": "2003-11-01"}, {"article_id": "8343668", "content": "To assess the effect of preventive home visits by public health nurses on the state of health of and use of services by elderly people living at home.\nRandomised controlled trial.\nGeneral population of elderly people in one of the southern regions of the Netherlands.\n580 subjects aged between 75 and 84 years randomly allocated to intervention (292) or control (288) group.\nFour visits a year over three years in intervention group. Control group received no home visits.\nSelf rated health, functional state, well being, loneliness, aspects of the mental state (depressive complaints, memory disturbances), and mortality. Use of services and costs.\nVisits had no effect on the health of the subjects. In the group visited no higher scores were seen on health related measures, fewer died (42 (14%) v 50 (17%)), and community care increased slightly. In the control group more were referred to outpatient clinics (166 (66%) v 132 (55%)), and they had a 40% increased risk of admission (incidence rate ratio 1.4; 90% confidence interval 1.2 to 1.6). No differences were found in long term institutional care, and overall expenditure per person in the intervention group exceeded that in the control group by 4%. Additional analyses showed that visits were effective for subjects who initially rated their health as poor.\nPreventive home visits are not beneficial for the general population of elderly people living at home but might be effective when restricted to subjects with poor health.", "title": "Effects of preventive home visits to elderly people.", "date": "1993-07-03"}, {"article_id": "2112022", "content": "A randomized controlled trial was carried out to test the effectiveness of a screening programme carried out by nurses for elderly people aged 75 years and over in a general practice. A total of 151 people were randomly allocated to the test group and 145 to the control group. The test group received a home visit from a nurse at which an assessment lasting 45 minutes was made of: activities of daily living, social functioning, sensory functions, mental and emotional problems, current medical problems, blood pressure, urinalysis, haemoglobin level and compliance with medication. Both groups completed a selection of items from four health indices before and 20 months after the intervention. At follow up, the test group scored significantly better than the control group on a morale scale. However, this trial provided no evidence for better resolution of physical problems or finding activities of daily living easier in the test group compared with the control group. It is suggested that the main benefit of such a screening process is that the special attention and education provided improves adaptation to old age and awareness of the support systems available. The government has proposed an annual review of elderly people in their own home and this study suggests that the objectives of this scheme should be clarified.", "title": "Screening elderly people in primary care: a randomized controlled trial.", "date": "1990-03-01"}, {"article_id": "9217619", "content": "To test the effectiveness of a 10-minute office-staff administered screen to evaluate malnutrition/weight loss, visual impairment, hearing loss, cognitive impairment, urinary incontinence, depression, physical limitations, and reduced leg mobility among older persons seen in office practice. This screen was coupled with clinical summaries to assist the physician in further evaluating and managing the screen-included problems.\nTwenty-six community-based office practices of internists and family physicians in Los Angeles were randomized to intervention or control groups. Two hundred and sixty-one patients aged > or = 70 years and seeing these physicians for a new visit or a physical examination participated in the study. At the enrollment visit intervention group patients were administered the screening measure and their physicians were given the pertinent clinical summaries. Outcome measures were detection of, and intervention for conditions screened, and health status 6 months after the intervention.\nHearing loss was both more commonly detected (40% intervention versus 28% control) and further evaluated (29% versus 16%) by physicians in the intervention group (P < 0.05). No other differences in the frequency of problem detection or intervention were noted between groups. Six months after the intervention no differences were noted in health status between groups.\nA brief measure to screen for common conditions in older persons was associated with more frequent detection and follow-up assessment of hearing loss. Although the measure was well accepted by physicians and their staffs, it did not appear to affect detection and intervention in regard to the other screen-included conditions, or health status at 6 months.", "title": "A randomized trial of office-based screening for common problems in older persons.", "date": "1997-04-01"}, {"article_id": "10934182", "content": "Preventive assessment of prevalent disorders may be considered as an instrument to maintain independence in the elderly. However, the outcomes of studies on these types of screening differ considerably regarding their effects.\nThe aim of the present study was to assess the effects of GPs' screening of the elderly on four highly prevalent disorders with possibilities for treatment: hearing and visual disorders, urinary incontinence and mobility disorders.\nIn an intervention study in 12 general practices, 1121 subjects aged 75 years and over were screened. Randomization was done by practice into an intervention group (576) and a control group (545). In the intervention group, all elderly patients were screened for the four disorders during the first year of the study. When the GP and patient agreed on intervention, usual care was provided by the GP. The patients in the control group were not screened in the first year. In the second year, all patients in both groups were screened for the four disorders.\nFor none of the four disorders was a measurable effect of the screening at the population level found. In the first year, 1013 new disorders were found involving 479 of 576 people. The GPs considered information to be new in 293 cases. In 245 cases (out of 293), the GP discussed the new information with the patient. Of the 89 cases in which the patient agreed with an intervention, improvement was reported in 17 cases.\nImplementing a standardized screening programme for four highly prevalent disorders for elderly people is not recommended. Preventive assessment of the elderly should be applied in ways other than by screening. Preventive care should pay attention to the individual needs of the elderly, should be started before the age of 75 years and should be offered in a flexible way.", "title": "Effects of screening for disorders among the elderly: an intervention study in general practice.", "date": "2000-08-10"}, {"article_id": "18614568", "content": "To assess the effects of vision screening, and subsequent management of visual impairment, on visual acuity and vision-related quality of life among frail older people.\nRandomised controlled trial.\nCommunity in Sydney, Australia.\n616 men and women aged 70 years and over (mean age 81 years) recruited mainly from people attending outpatient aged care services.\nNo vision assessment or intervention\nComprehensive vision and eye examinations conducted by an optometrist. Three hundred subjects were seen by the study optometrist, with 146 judged to need treatment for a vision or eye problem. The optometrist arranged new glasses for 92 subjects; 24 were referred for a home visit by an occupational therapist; 17 were referred for glaucoma management; and 15 were referred for cataract surgery.\nDistance and near visual acuity (logMAR) and composite scores on the 25-item version of the National Eye Institute Visual Function Questionnaire, both assessed at a 12-month follow-up home visit.\nAfter 12 months' follow-up, the mean (logMAR) distance visual acuity was 0.27 in the intervention group and 0.25 in the control group (p = 0.32). The mean (logMAR) near visual acuities were -0.01 in the intervention group and -0.03 in the control group (p = 0.26). The mean composite score on the National Eye Institute Visual Function Questionnaire was 84.3 in the intervention group and 86.4 in the control group (p = 0.49).\nVision screening by an optometrist for frail older people living in the community in Australia does not lead to improvements in vision or vision-related quality of life after 1 year's follow-up.", "title": "Vision screening for frail older people: a randomised trial.", "date": "2008-07-11"}, {"article_id": "7977921", "content": "Because preventing disability and falls in older adults is a national priority, a randomized controlled trial was conducted to test a multicomponent intervention program.\nFrom a random sample of health maintenance organization (HMO) enrollees 65 years and older, 1559 ambulatory seniors were randomized to one of three groups: a nurse assessment visit and follow-up interventions targeting risk factors for disability and falls (group 1, n = 635); a general health promotion nurse visit (group 2, n = 317); and usual care (group 3, n = 607). Data collection consisted of a baseline and two annual follow-up surveys.\nAfter 1 year, group 1 subjects reported a significantly lower incidence of declining functional status and a significantly lower incidence of falls than group 3 subjects. Group 2 subjects had intermediate levels of most outcomes. After 2 years of follow-up, the differences narrowed.\nThe results suggest that a modest, one-time prevention program appeared to confer short-term health benefits on ambulatory HMO enrollees, although benefits diminished by the second year of follow-up. The mechanisms by which the intervention may have improved outcomes require further investigation.", "title": "Preventing disability and falls in older adults: a population-based randomized trial.", "date": "1994-11-01"}, {"article_id": "1392755", "content": "To assess whether intervention by a health visitor could reduce the number of fractures, over a four year period, in those aged 70 and over.\nRandomised, controlled trial; randomisation by household.\nGeneral practice in a market town.\nOf 863 patients aged 70 and over on the practice records, 674 were traced and successfully interviewed; 350 were assigned to the intervention group, 324 as controls.\nThe people in the intervention group were allocated to the care of a health visitor. The approach was four pronged: assessment and correction of nutritional deficiencies, including reducing smoking and alcohol intake; assessment and referral of medical conditions such as heart block or inappropriate medication; assessment and correction of environmental hazards in the home such as poor lighting; assessment and improvement of fitness--for example, exercise classes for the moderately fit. The intervention continued for four years.\nFracture rate over four years.\nThe incidence of fractures was 5% (16/350) in the intervention group and 4% (14/324) in the control group (difference not significant).\nA health visitor visiting a group of people aged 70 and over and using simple preventive measures had no effect on the incidence of fractures.", "title": "Can health visitors prevent fractures in elderly people?", "date": "1992-04-04"}, {"article_id": "16626428", "content": "To assess the need for, and the use of eye care services in older people seeking aged care.\nIn total, 188 people (69.1% of those eligible) aged 65+ years who were assessed for aged care provision at Westmead Hospital, Sydney, were recruited in 2003 and re-examined a year later. At baseline, presenting visual acuity (VA) was randomly assessed in half the participants. People with under-corrected refractive error (pinhole VA improved at least 10 letters in those with presenting VA <6/6), bilateral visual impairment (better eye VA <6/12), or self-reported visual problems, were recommended to have further assessment by eye care professionals. At follow up, information on utilization of eye care services in the past 12 months was collected and VA was assessed in all returned participants.\nOf the 188 baseline participants, 121 (70% of survivors) were revisited a year later. Overall, 90/121 participants (74%) had seen an eye care professional in the previous year. Of the 66 participants who were recommended to see an eye care professional, 42 (64%) were revisited and 37/42 (88%) complied with the recommendation. At revisit, bilateral visual impairment was found in 49/120 (41%). The proportion with bilateral visual impairment was lower in participants whose vision was assessed at baseline (35%) than in those whose vision was not assessed (47%, P = 0.17), and also lower among people who had visited an eye care professional during the previous 12 months (39%) than those who had not (45%, P = 0.57).\nThis pilot study indicates a relatively high need for, and high utilization of eye care services in the subgroup of older people seeking aged care services.", "title": "Eye care service utilization in older people seeking aged care.", "date": "2006-04-22"}, {"article_id": "6229314", "content": "Health visitors were employed specifically to care for two years for a random sample of patients in general practice who were aged over 70. Independent assessments made at the beginning and end of the study showed that the health visitor in an urban practice had some impact on her caseload of patients; she provided more services for them, their mortality was reduced, and their quality of life improved, though the last measure just failed to be statistically significant. The health visitor working in a rural practice had no such effect.", "title": "Effect of health visitors working with elderly patients in general practice: a randomised controlled trial.", "date": "1984-02-04"}]}
{"original_review": "25375291", "question_data": [{"question_id": 170, "question": "Is the risk of job loss higher, lower, or the same when comparing non-pharmacological job loss prevention interventions to usual care?", "answer": "uncertain effect", "evidence_quality": "very low", "fulltext_required": "no", "comment": "", "relevant_sources": ["16208658", "14613285"]}, {"question_id": 171, "question": "Is intermediate term work functioning higher, lower, or the same when comparing non-pharmacological job loss prevention interventions to usual care?", "answer": "higher", "evidence_quality": "very low", "fulltext_required": "no", "comment": "", "relevant_sources": ["19877106"]}, {"question_id": 220, "question": "Is the risk of adverse events higher, lower, or the same when comparing non-pharmacological job loss prevention interventions to usual care?", "answer": "insufficient data", "evidence_quality": "", "fulltext_required": "no", "comment": "", "relevant_sources": ["16208658", "14613285", "19877106"]}], "sources": [{"article_id": "14613285", "content": "Job loss is a major consequence of rheumatic diseases, and clinicians may refer patients to vocational rehabilitation for help. When provided after job loss, the impact of vocational rehabilitation is short term. This randomized controlled trial with 48 months of followup was undertaken to determine the efficacy of vocational rehabilitation provided to persons with rheumatic diseases while they are still employed, but at risk for job loss.\nA total of 242 patients with rheumatic diseases residing in Massachusetts were recruited through their rheumatologists for study. Participants were randomly assigned to the experimental group (n = 122) or the control group (n = 120). Subjects in the experimental group received two 1.5-hour sessions of vocational rehabilitation; those in the control group received print materials about disability employment issues and resources by mail. The main outcome assessed was the time to first job loss. Job losses were defined as permanent disability, premature retirement, or a period of unemployment. All analyses were conducted on an intent-to-treat basis.\nJob loss was delayed in the experimental group compared with the control group (P = 0.03 by log rank test). After adjustment for confounders, participation in the experimental group was found to be protective against job loss (odds ratio 0.58 [95% confidence interval 0.34-0.99], P = 0.05 by pooled logistic regression).\nVocational rehabilitation delivered to patients at risk for job loss, but while they were still employed, delayed job loss. Such an intervention has the potential to reduce the high indirect costs, as well as the personal impact, of rheumatic diseases.", "title": "Reduction of job loss in persons with rheumatic diseases receiving vocational rehabilitation: a randomized controlled trial.", "date": "2003-11-13"}, {"article_id": "16208658", "content": "Work disability is a major consequence of inflammatory rheumatic conditions. Evidence regarding the effectiveness of interventions aimed at the prevention or reduction of work disability in rheumatic diseases is limited. We conducted a randomized controlled trial to investigate the effectiveness of a multidisciplinary job-retention vocational rehabilitation (VR) program in patients with a rheumatic condition who were at risk for job loss.\nA total of 140 patients with a chronic rheumatic condition were randomly assigned to either a multidisciplinary job-retention VR program (n = 74) or usual outpatient care (UC) (n = 66). Patients in the VR group were assessed and guided by a multidisciplinary team, whereas patients in the UC group received care as initiated by their rheumatologist, supplemented with written information. The main outcome measure was the occurrence of job loss (complete work disability or unemployment); additional outcome measures included job satisfaction, pain, functional status, emotional status, and quality of life.\nThere was no difference between the 2 groups regarding the proportion of patients having lost their job at any time point, with 24% and 23% of the patients in the VR and UC groups, respectively, having lost their job after 24 months. Over the total period of 24 months, patients in the VR group had a significantly greater improvement of the fatigue visual analog scale and of emotional status (all P values < 0.05).\nA job-retention VR program did not reduce the risk of job loss but improved fatigue and mental health in patients with chronic rheumatic diseases at risk for job loss.", "title": "Randomized comparison of a multidisciplinary job-retention vocational rehabilitation program with usual outpatient care in patients with chronic arthritis at risk for job loss.", "date": "2005-10-07"}, {"article_id": "19877106", "content": "Work disability is a serious consequence of rheumatoid arthritis (RA). We conducted a 6-month, prospective randomized controlled trial comparing assessments of function, work, coping, and disease activity in employed patients with RA receiving occupational therapy intervention versus usual care.\nEmployed patients with RA with increased perceived work disability risk were identified by the RA Work Instability Scale (WIS; score >or=10). Patients were stratified into medium- (score >or=10 and <17) and high-risk (>or=17) groups, then randomized into occupational therapy or usual care groups. Assessments were conducted at baseline and 6 months. The primary outcome was the Canadian Occupational Performance Measure (COPM), a standardized patient self-report of function. Other outcomes included the disability index (DI) of the Health Assessment Questionnaire (HAQ); Disease Activity Score in 28 joints (DAS28); RA WIS; EuroQol Index; visual analog scales (VAS) for pain, work satisfaction, and work performance; and days missed/month. Independent sample t-tests and Mann-Whitney U tests were used.\nWe recruited 32 employed patients with RA. At baseline the groups were well matched. At 6 months the improvement in the occupational therapy group was significantly greater than that in the usual care group for all functional outcomes (COPM performance P < 0.001, COPM satisfaction P < 0.001, HAQ DI P = 0.02) and most work outcomes (RA WIS [P = 0.04], VAS work satisfaction [P < 0.001], VAS work performance [P = 0.01]). Additionally, Arthritis Helplessness Index (P = 0.02), Arthritis Impact Measurement Scales II pain subscale (P = 0.03), VAS pain (P = 0.007), EuroQol Index (P = 0.02), EuroQol global (P = 0.02), and DAS28 (P = 0.03) scores significantly improved.\nTargeted, comprehensive occupational therapy intervention improves functional and work-related outcomes in employed RA patients at risk of work disability.", "title": "Functional and work outcomes improve in patients with rheumatoid arthritis who receive targeted, comprehensive occupational therapy.", "date": "2009-10-31"}]}
{"original_review": "31425604", "question_data": [{"question_id": 172, "question": "Is time to establish full enteral feeds higher, lower, or the same when comparing monitoring of gastric residual volume and quality to only monitoring of gastric residual quality?", "answer": "uncertain effect", "evidence_quality": "very low", "fulltext_required": "no", "comment": "", "relevant_sources": ["29866595"]}, {"question_id": 173, "question": "Is the risk of necrotizing enterocolitis higher, lower, or the same when comparing monitoring of gastric residual volume and quality to only monitoring of gastric residual quality?", "answer": "uncertain effect", "evidence_quality": "very low", "fulltext_required": "no", "comment": "", "relevant_sources": ["29866595"]}, {"question_id": 174, "question": "Is time taken to establish full enteral feeds higher, lower, or the same when comparing routine monitoring of gastric residual to no monitoring of gastric residual?", "answer": "higher", "evidence_quality": "low", "fulltext_required": "no", "comment": "", "relevant_sources": ["25238118", "25166623"]}, {"question_id": 175, "question": "Is the number of total parenteral nutrition days higher, lower, or the same when comparing routine monitoring of gastric residual to no monitoring of gastric residual?", "answer": "higher", "evidence_quality": "low", "fulltext_required": "no", "comment": "", "relevant_sources": ["25238118", "25166623"]}], "sources": [{"article_id": "29866595", "content": "To evaluate the effect of not relying on prefeeding gastric residual volumes to guide feeding advancement on the time to reach full feeding volumes in preterm infants, compared with routine measurement of gastric residual volumes. We hypothesized that not measuring prefeeding gastric residual volumes can shorten the time to reach full feeds.\nIn this single-center, randomized, controlled trial, we included gavage fed preterm infants with birth weights (BW) 1500-2000\u2009g who were enrolled within 48\u2009hours of birth. Exclusion criteria were major congenital malformations, asphyxia, and BW below the third percentile. In the study group, the gastric residual volume was measured only in the presence of bloody aspirates, vomiting, or an abnormal abdominal examination. In the control group, gastric residual volume was assessed routinely, and feeding advancement was based on the gastric residual volume. The primary outcome was the time to reach feeding volumes of 120\u2009mL/kg per day. Secondary outcomes were time to regain BW, episodes of feeding interruptions, sepsis, and necrotizing enterocolitis.\nEighty-seven infants were enrolled. There were no differences between the study and control groups with respect to time to reach full feeds (6 days [95% CI, 5.5-6.5] vs 5 days [95% CI, 4.5-5.5]; P\u2009=\u2009.82), time to regain BW, episodes of feeding interruptions, or sepsis. Two infants in the control group developed necrotizing enterocolitis.\nAvoiding routine assessment of gastric residual volume before feeding advancement did not shorten the time to reach full feeds in preterm infants with BW between 1500 and 2000 g.\nClinicaltrials.gov: NCT01337622.", "title": "Gastric Residual Volume in Feeding Advancement in Preterm Infants (GRIP Study): A Randomized Trial.", "date": "2018-06-06"}, {"article_id": "25238118", "content": "The aim of the study was to compare prefeed abdominal circumference (AC) and gastric residual volume (GRV) as a measure of feed intolerance in very-low-birth-weight infants (VLBW).\nEighty VLBW infants were randomized to 2 groups; feed intolerance was monitored by measuring either GRV group or prefeed AC group. The primary outcome was time to full enteral feeds (180 mL \u00b7 kg \u00b7 day). Other main outcome measures were feed interruption days, duration of parenteral nutrition, incidence of culture positive sepsis, necrotizing enterocolitis, mortality, and duration of hospital stay.\nThe median (interquartile range) time to achieve full feeds was 10 (9-13) versus 14 (12-17.5) days in AC and GRV groups, respectively (P\u200a<\u200a0.001). Infants in AC group had fewer feed interruption days (0 [0-2] vs 2.0 [1, 5], P\u200a<\u200a0.001) and shorter duration of parenteral nutrition (P\u200a<\u200a0.001). The incidence of culture-positive sepsis in AC and GRV groups was 17.5% and 30 %, respectively (P\u200a=\u200a0.18). Duration of hospital stay and mortality were comparable in both the groups.\nPrefeed AC as a measure of feed intolerance in VLBW infants may shorten the time taken to achieve full feeds.", "title": "Abdominal circumference or gastric residual volume as measure of feed intolerance in VLBW infants.", "date": "2014-09-23"}, {"article_id": "25166623", "content": "Little information exists regarding gastric residual (GR) evaluation prior to feedings in premature infants. The purpose of this study was to compare the amount of feedings at 2 and 3 weeks of age, number of days to full feedings, growth and incidence of complications between infants who underwent RGR (routine evaluation of GR) evaluation versus those who did not.\nSixty-one premature infants were randomized to one of two groups. Group 1 received RGR evaluation prior to feeds and Group 2 did not.\nThere was no difference in amount of feeding at 2 (P=0.66) or 3 (P=0.41) weeks of age, growth, days on parenteral nutrition or complications. Although not statistically significant, infants without RGR evaluation reached feeds of 150\u2009ml\u2009kg(-1) per day 6 days earlier and had 6 fewer days with central venous access.\nRESULTs suggest RGR evaluation may not improve nutritional outcomes in premature infants.", "title": "The value of routine evaluation of gastric residuals in very low birth weight infants.", "date": "2014-08-29"}]}
{"original_review": "34046884", "question_data": [{"question_id": 176, "question": "Is the risk of recurrence higher, lower, or the same when comparing absorbable tacks to nonabsorbable tacks?", "answer": "uncertain effect", "evidence_quality": "very low", "fulltext_required": "no", "comment": "29799075 just states statistical significance but authors downgrade evidence for overall sample size anyway", "relevant_sources": ["29799075", "26885113"]}, {"question_id": 177, "question": "Is the risk of recurrence higher, lower, or the same when comparing nonabsorbable tacks to nonabsorbable sutures?", "answer": "uncertain effect", "evidence_quality": "very low", "fulltext_required": "no", "comment": "", "relevant_sources": ["20652715"]}, {"question_id": 178, "question": "Is early postoperative pain higher, lower, or the same when comparing absorbable tacks to absorbable sutures?", "answer": "uncertain effect", "evidence_quality": "very low", "fulltext_required": "no", "comment": "\"Rated down by one level for risk of bias (performance bias) and by two levels for imprecision (optimal information size threshold not reached and 95% confidence intervals include appreciable benefit and appreciable harm).\"", "relevant_sources": ["31236731"]}, {"question_id": 179, "question": "Is the risk of recurrence higher, lower, or the same when comparing nonabsorbable tacks to fibrin sealant?", "answer": "uncertain effect", "evidence_quality": "very low", "fulltext_required": "no", "comment": "29799075 just states statistical significance but authors downgrade evidence for overall sample size anyway", "relevant_sources": ["29799075", "23657861"]}], "sources": [{"article_id": "20033726", "content": "Persistent, activity-limiting pain after laparoscopic ventral or incisional hernia repair (LVIHR) appears to be related to fixation of the implanted mesh. A randomized study comparing commonly used fixation techniques with respect to postoperative pain and quality of life has not previously been reported.\nA total of 199 patients undergoing non-urgent LVIHR in our unit between August 2005 and July 2008 were randomly assigned to one of three mesh-fixation groups: absorbable sutures (AS) with tacks; double crown (DC), which involved two circles of tacks and no sutures; and nonabsorbable sutures (NS) with tacks. All operations were performed by one of two experienced surgeons, who used a standardized technique and the same type of mesh and mesh-fixation materials. The severity of the patients' pain was assessed preoperatively and at 2 weeks, 6 weeks and 3 months postoperatively by using a visual analogue scale (VAS). Quality of life (QoL) was evaluated by administering a standard health survey before and 3 months after surgery. Results in the three groups were compared.\nThe AS, DC, and NS mesh-fixation groups had similar patient demographic, hernia and operative characteristics. There were no significant differences among the groups in VAS scores at any assessment time or in the change in VAS score from preoperative to postoperative evaluations. The QoL survey data showed a significant difference among groups for only two of the eight health areas analyzed.\nIn this trial, the three mesh-fixation methods were associated with similar postoperative pain and QoL findings. These results suggest that none of the techniques can be considered to have a pain-reduction advantage over the others. Development of new methods for securing the mesh may be required to decrease the rate or severity of pain after LVIHR.", "title": "Mesh-fixation method and pain and quality of life after laparoscopic ventral or incisional hernia repair: a randomized trial of three fixation techniques.", "date": "2009-12-25"}, {"article_id": "22729705", "content": "Technique of mesh fixation in laparoscopic incisional hernia repair is a matter of debate. Literature is lacking in randomized trials comparing various methods of mesh fixation. This study was designed to compare the cost-effectiveness and long-term outcomes following the two methods of mesh fixation.\nA total of 110 patients were randomized to tacker mesh fixation or suture mesh fixation. Patients with nonrecurrent hernias with defect size ranging from 2 to 5\u00a0cm were included. The cost and incremental cost-effectiveness ratio was calculated. SF-36v2 health survey was used for quality-of-life analysis. Patients were followed up at regular intervals, and return to activity and satisfaction scores were recorded.\nDemographic profile and hernia characteristics were comparable between the two groups. Operation time was significantly higher (p\u00a0<\u00a00) and early postoperative pain at 1\u00a0h, 6\u00a0h, and 1\u00a0month was significantly lower in the suture group. There was no significant difference in the incidence of chronic pain and seroma formation over a mean follow-up of 32.2\u00a0months. Cost of procedure was significantly higher in group I (p\u00a0<\u00a00.001). Suture fixation was found to be more cost-effective than tacker fixation. Postoperative quality of life outcomes were similar in the two groups. Among return to activity parameters, time to resumption of daily activities and starting climbing stairs were significantly shorter in the suture group.\nThe suture fixation method is a cost-effective alternative to tacker fixation in patients with small- to medium-sized defects in laparoscopic incisional and ventral hernia repair. Suture fixation is better than tacker fixation in terms of early postoperative pain and return to activity. The two procedures are equally effective regarding the recurrence rates, complications, hospital stay, chronic pain, quality of life determinants, and patient satisfaction.", "title": "Comparison of long-term outcome and quality of life after laparoscopic repair of incisional and ventral hernias with suture fixation with and without tacks: a prospective, randomized, controlled study.", "date": "2012-06-26"}, {"article_id": "26885113", "content": "The aim of this prospective randomized trial was to compare 2 main fixation devices in regard to pain and recurrence in laparoscopic ventral incisional hernia repair (LVIHR). A total of 51 patients were evaluated in this study (n = 25, nonabsorbable tack (NAT) and n = 26, absorbable tack (AT) groups). A visual analogue scale (VAS) was performed on both groups preoperatively and on the postoperative (PO) first day, second week, and sixth month. All patients were followed for recurrence by clinical examination, ultrasonography, and/or abdominal computed tomography. The median follow-up time was 31 months (15-45). The mean age and the mean body mass index (BMI) of the patients were 53.1 \u00b1 11 years and 34 \u00b1 5 kg/m(2), respectively. The median defect size was 60 cm(2) (35-150) and median operation time was 110 minutes (40-360). In 2 patients from AT group and 2 from NAT group (7.8%), recurrence occurred. The 2 groups had similar features regarding demographics, operation time, postoperative hospital stay, morbidity, and VAS scores. The 2 fixation methods were found similar for PO pain and recurrence. In our opinion, the choice of either of these fixation methods during surgery should not be based on the concerns of pain or recurrence. AT may be the preferable option in LVIHR due to the lower cost.", "title": "Prospective randomized trial of mesh fixation with absorbable versus nonabsorbable tacker in laparoscopic ventral incisional hernia repair.", "date": "2016-02-18"}, {"article_id": "23546864", "content": "Although laparoscopic intra-peritoneal mesh repair (LVHR) is a well-established treatment option to repair ventral and incisional hernias, no consensus in the literature can be found on the best method of fixation of the mesh to the abdominal wall.\nBetween December 2004 and July 2008, 76 patients undergoing a LVHR were randomized between mesh fixation using a double row of spiral tackers (DC) (n\u00a0=\u00a033) and mesh fixation with transfascial sutures combined with one row of spiral tackers (S&T) (n\u00a0=\u00a043), in the WoW trial (with or without sutures). Patients were clinically examined and evaluated using a visual analog scale for pain (VAS) in rest and after coughing 4\u00a0h post-operatively, after 4\u00a0weeks and 3\u00a0months after surgery. Primary endpoint of the study was abdominal wall pain, defined as a VAS score of at least 1.0\u00a0cm, at 3\u00a0months post-operative. Quality of life was quantified with the SF-36 questionnaire preoperatively and after 3\u00a0months. Secondary endpoint was the recurrence rate at 24-month follow-up.\nThe DC and S&T group were comparable in age, gender, ASA score, BMI, indication, hernia, and mesh variables. The DC group had a significant shorter operating time compared with the S&T group (74\u00a0vs 96\u00a0min; p\u00a0=\u00a00.014) and a significant lower mean VAS score 4\u00a0h post-operatively (in rest; p\u00a0=\u00a00.028/coughing; p\u00a0=\u00a00.013). At 3\u00a0months, there were significant more patients in the S&T group with VAS score \u22651.0\u00a0cm (31.4 vs 8.3\u00a0%; p\u00a0=\u00a00.036). Clinical follow-up at 24\u00a0months was obtained in 63 patients (82.9\u00a0%). The recurrence rate at 24\u00a0months was 7.9\u00a0% overall (5/63). There were more recurrences in the S&T group (4/36) than in the DC group (1/27), but this difference was not significant (11.1 vs 3.7\u00a0%; p\u00a0=\u00a00.381).\nWe found that double-crown fixation of intra-peritoneal mesh during laparoscopic ventral hernia repair was quicker, was less painful immediately post-operative and after 3\u00a0months, and did not increase the recurrence rate at 24\u00a0months. In hernias at a distance from the bony borders of the abdomen, transfascial sutures can be omitted if a double crown of tackers is placed.", "title": "Randomized clinical trial of mesh fixation with \"double crown\" versus \"sutures and tackers\" in laparoscopic ventral hernia repair.", "date": "2013-04-03"}, {"article_id": "21964681", "content": "The use of tacks for mesh fixation may induce pain after surgery for ventral hernia. The aim of this study was to compare postoperative pain after laparoscopic ventral hernia repair (LVHR) with conventional mesh fixation using titanium tacks versus fibrin sealant (FS).\nThis randomized clinical trial included patients with an umbilical hernia defect ranging from 1\u00b75 to 5 cm at three Danish hernia centres. Participants were assigned randomly to FS or titanium tack fixation. The primary outcome was acute pain, defined as the mean pain score on days 0-2 after surgery, measured on a 0-100-mm visual analogue scale (VAS).\nForty patients were included, of whom 38 were available for intention-to-treat analysis after 1 month. Patients in the FS group reported less pain than those in the tack group on days 0-2, both at rest (median 19 versus 47 mm; P = 0\u00b7025) and during activity (38 versus 60 mm; P = 0\u00b7014). The absolute difference in pain score between groups was 19 (95 per cent confidence interval 3 to 34) and 20 (4 to 35) mm at rest and during activity respectively. Patients in the FS group resumed normal daily activity earlier (after median 7 versus 18 days; P = 0\u00b7027) and reported significantly less discomfort. No recurrences were observed.\nMesh fixation with FS in LVHR was associated with less acute postoperative pain, discomfort and a shorter convalescence than tack fixation. Long-term follow-up is needed to show whether the effect of FS fixation persists in terms of chronic pain and recurrence.\nNCT00842842 (http://www.clinicaltrials.gov).", "title": "Randomized clinical trial of fibrin sealant versus titanium tacks for mesh fixation in laparoscopic umbilical hernia repair.", "date": "2011-10-04"}, {"article_id": "29643902", "content": "Chronic pain occurs in 20-30% of patients after hernia surgery. As a consequence of this chronic pain, almost one third of patients have limitations in daily activities. Frequency and severity of this pain varies with different techniques of hernia repair. The objective of this study was to compare polypropylene suture and skin staples for securing mesh in uncomplicated ventral hernioplasty in terms of acute and chronic postoperative pain and to compare the time taken for mesh fixation between polypropylene sutures and skin stapler in ventral hernioplasty.\nThis study was conducted in Surgery Department of Dow University Hospital, Dow University of Health Sciences, Ojha Campus and included 53 patients from Jan 2015 to Dec 2016, after taking informed consent. All patients were operated under general anesthesia by the same surgical team. Patients were randomized into two groups; in one group mesh fixed with 2/0 polypropylene suture while in other group mesh stapler was used. Time taken to apply mesh was noted in minutes from laying the mesh over anterior rectus sheath to completion of fixation by either method. The severity of post-operative pain was measured with VAS (1-10) after one week, one month and after one year after surgery. Data was analysed using SPSS version 17.\nPatient characteristics and operative outcome were similar in the two groups and statistically non-significant in both. Early postoperative pain was more after suture fixation but it was not statistically significant. Mean \u00b1 SD pain score was after one week 3.47\u00b12.7 after sutures while 2.91\u00b11.88 after stapler. After four weeks, 0.40\u00b10.49 after suture while 0.35\u00b10.48 after stapler fixation. In both study groups 30-34% of the patients felt some pain in follow-up after one year. Severity of pain was 0.60\u00b10.62 after suture while 1.65\u00b11.94 after stapler fixation which is statistically significant as well (p<0.007). Mean operative time was 15.33\u00b16.33 minutes for suture fixation while 1.56\u00b10.41 minutes for fixation by staples, p-value < 0.001.\nThe method of fixation does not appear to cause significant difference in early post-operative pain but chronic pain is more after stapler fixation of mesh. However, operative time was reduced significantly in staple fixation group as compared to suture fixation group.", "title": "Is there difference in chronic pain after Suture and Stapler fixation method of mesh in Ventral Hernia? Is stapler fixation method quicker? A randomized controlled trial.", "date": "2018-04-13"}, {"article_id": "23657861", "content": "Fibrin sealant for mesh fixation has significant positive effects on early outcome after laparoscopic ventral hernia repair (LVHR) compared with titanium tacks. Whether fibrin sealant fixation also results in better long-term outcome is unknown.\nWe performed a randomized controlled trial including patients with umbilical hernia defects from 1.5 to 5 cm at three Danish hernia centres. We used a 12 cm circular mesh. Participants were randomized to fibrin sealant or titanium tack fixation. Patients were seen in the outpatient clinic at 1 and 12 months follow-up.\nForty patients were included of whom 34 were available for intention to treat analysis after 1 year. There were no significant differences in pain, discomfort, fatigue, satisfaction or quality of life between the two groups at the 1-year follow-up. Five patients (26 %) in the fibrin sealant group and one (6 %) in the tack group were diagnosed with a recurrence at the 1-year follow-up (p = 0.182) (overall recurrence rate 17 %). Hernia defects in patients with recurrence were significantly larger than in those without recurrence (median 4.0 vs. 2.8 cm, p = 0.009).\nPatients with larger hernia defects and fibrin sealant mesh fixation had higher recurrence rates than expected, although the study was not powered for assessment of recurrence. There was no significant difference between groups in any parameters after the 1-year follow-up. The beneficial effects of mesh fixation with fibrin sealant on early outcome warrant further studies on optimization of the surgical technique to prevent recurrence.", "title": "Fibrin sealant for mesh fixation in laparoscopic umbilical hernia repair: 1-year results of a randomized controlled double-blinded study.", "date": "2013-05-10"}, {"article_id": "20976495", "content": "After the first report of laparoscopic incisional and ventral hernia repair (LIVHR) in 1993, several studies have proven its efficacy over open method. Among the technical issues, the technique of mesh fixation to the abdominal wall is still an area of debate. This prospective randomized study was done to compare two techniques of mesh fixation, i.e., tacker with four corner transfascial sutures versus transfascial sutures alone.\n68 patients admitted for LIVHR repair (defect size less than 25 cm2) were randomized in two groups: group I, tacker fixation (36 patients) and group II, suture fixation (32 patients). Various intraoperative variables and postoperative outcomes were recorded and analyzed.\nThe patients in the two groups were well matched in terms of age, sex, body mass index (BMI), and hernia characteristics. Mean BMI was 29.0 kg/m2. Operative time was found to be significantly higher in group II (77.5 versus 52.6 min, p=0.000). Patients in group I were found to have significantly higher pain scores at 1 h, 6 h, 24 h, 1 week, 1 month, and 3 months postoperatively. At follow-up, incidence of seromas was higher in group II but the difference was not significant (7 versus 4, p=0.219). During long-term follow-up, patients in group II were satisfied cosmetically.\nSuture fixation is a cost-effective alternative to tacker fixation, for small and medium-sized defects in anatomically accessible areas. However, suture fixation requires significantly longer operation time, but patients have statistically significantly less postoperative pain.", "title": "A prospective randomized study comparing suture mesh fixation versus tacker mesh fixation for laparoscopic repair of incisional and ventral hernias.", "date": "2010-10-27"}, {"article_id": "31236731", "content": "In open intra-peritoneal onlay mesh (IPOM) hernia repair, mesh fixation can be done by tacks, sutures or fibrin glue. There are randomized controlled trials (RCTs) on laparoscopic IPOM procedure, but no RCT so far has examined mesh fixation techniques in open IPOM repair.\nIn a single-center RCT, 48 patients undergoing open IPOM repair of an abdominal wall hernia were included. After randomization, surgery was performed in a standardized fashion. Hernia size, extent of mesh fixation, and duration of surgery were documented. The primary endpoint was postoperative pain intensity. Secondary endpoints were: complications, length of stay, quality of life, return to work, hernia recurrence. Follow-up was 1\u00a0year in all 48 patients.\nAfter using tacks, mean pain intensity was 16.9, which is slightly lower than after suture fixation (19.6, p\u2009=\u20090.20). The duration of surgery was about the same (83 vs. 85\u00a0min). When using tack fixation, significantly more fixation points were applied as compared to sutures (19 vs. 12; p\u2009=\u20090.02), although mesh size was similar. The complication rate was similar (tacks: 6/28 vs. sutures: 3/20). Cost of suture fixation was about 26 \u20ac, which is markedly lower than the 180 \u20ac associated with tacks. However, surgeons clearly preferred mesh fixation with tacks, because it is more comfortable especially in small hernias.\nThe present study failed to show an advantage of tacks over suture fixation and even there are more severe adverse events. Using tacks significantly increases the costs of hernia repair.", "title": "Mesh fixation in open IPOM procedure with tackers or sutures? A randomized clinical trial with preliminary results.", "date": "2019-06-27"}, {"article_id": "27846175", "content": "Laparoscopic incisional and ventral hernia repair (LIVHR) has been associated with a high incidence acute and chronic pain due to use of nonabsorbable tackers. Several absorbable tackers have been introduced to overcome these complications. This randomized study was done to compare 2 techniques of mesh fixation, that is, nonabsorbable versus absorbable tackers for LIVHR.\nNinety patients admitted for LIVHR repair (defect size <15 cm) were randomized into 2 groups: nonabsorbable tacker fixation (NAT group, 45 patients) and absorbable tacker fixation (AT group, 45 patients). Intraoperative variables and postoperative outcomes were recorded and analyzed.\nPatients in both the groups were comparable in terms of demographic profile and hernia characteristics. Mesh fixation time and operation time were also comparable. There was no significant difference in the incidence of immediate postoperative and chronic pain over a mean follow-up of 8.8 months. However, cost of the procedure was significantly higher in AT group (P<0.01) and NAT fixation was more cost effective as compared with AT. Postoperative quality of life outcomes and patient satisfaction scores were also comparable.\nNAT is a cost-effective method of mesh fixation in patients undergoing LIVHR with comparable early and late postoperative outcomes in terms of pain, quality of life, and patient satisfaction scores.", "title": "Comparison of Absorbable Versus Nonabsorbable Tackers in Terms of Long-term Outcomes, Chronic Pain, and Quality of Life After Laparoscopic Incisional Hernia Repair: A Randomized Study.", "date": "2016-11-16"}, {"article_id": "20652715", "content": "Mesh fixation during laparoscopic ventral hernia repair can be performed using transfascial sutures or metal tacks. The aim of the present study is to compare mesh shrinkage and pain between two different techniques of mesh fixation in a prospective randomized trial.\nA randomized trial was performed. Patients with ventral hernia of maximal diameter 8\u00a0cm were assigned to mesh fixation using either transfascial nonabsorbable sutures or metal tacks for fixation of a parietene composite mesh. The borders of the mesh were marked using clips, and radiological images in prone position were used for assessment of mesh size and location. The primary endpoint was mesh shrinkage; secondary endpoints included postoperative pain, mesh dislocation, and surgical morbidity.\nDemographic parameters were similar in both groups. A total of 40 patients were randomized, and 36 patients were available for follow-up. There was one hernia recurrence in each group. Pain was significantly higher following suture fixation after 6\u00a0weeks, but no difference was found after 6\u00a0months. Mesh shrinkage after 6\u00a0months was significantly higher using tacks for mesh fixation.\nTransfascial sutures are associated with more pain within the first 6 postoperative weeks and less mesh shrinkage after 6\u00a0months compared with mesh fixation using metal tacks.", "title": "Mesh shrinkage and pain in laparoscopic ventral hernia repair: a randomized clinical trial comparing suture versus tack mesh fixation.", "date": "2010-07-24"}, {"article_id": "29799075", "content": "The method of anchoring the mesh in laparoscopic ventral hernia repair is claimed to cause postoperative pain, affecting the quality of life of the patients. The aim of this randomized study was to compare the effect of three types of fixation devices on postoperative pain, patient quality of life, and hernia recurrence.\nPatients with ventral hernias between 2 and 7\u00a0cm were randomized into one of three mesh fixation groups: permanent tacks (Protack\u2122), absorbable tacks (Securestrap\u2122), and absorbable synthetic glue (Glubran\u2122). The primary endpoint was pain on the second postoperative day, measured on a visual analogue scale. Quality of life and recurrence rate were secondary endpoints and investigated through questionnaires and clinical examination at follow-up visits 1, 6, and 12\u00a0months after surgery.\nSeventy-five non-consecutive patients were included in the study, with 25 patients in each group. There was no significant difference between groups for unspecified pain on the second postoperative day (p\u2009=\u20090.250). The DoloTest\u2122 values were 55.3\u2009\u00b1\u200928.9\u00a0mm, 43.5\u2009\u00b1\u200928.5\u00a0mm, and 55.9\u2009\u00b1\u200926.3\u00a0mm for permanent tacks, absorbable tacks, and synthetic glue, respectively. No differences were observed between groups with respect to quality of life of the patients and hernia recurrence rate.\nIn patients with small- and medium-sized ventral hernias, the type of fixation device did not affect the immediate or long-term postoperative pain, quality of life, or recurrence rate when comparing permanent tacks, absorbable tacks, and synthetic glue for mesh fixation.\nNCT01534780.", "title": "Effect of fixation devices on postoperative pain after laparoscopic ventral hernia repair: a randomized clinical trial of permanent tacks, absorbable tacks, and synthetic glue.", "date": "2018-05-26"}]}
{"original_review": "30320433", "question_data": [{"question_id": 180, "question": "Is flexibility higher, lower, or the same when comparing exercise to usual care?", "answer": "higher", "evidence_quality": "very low", "fulltext_required": "no", "comment": "", "relevant_sources": ["17964881"]}, {"question_id": 181, "question": "Is the number of falls higher, lower, or the same when comparing exercise to usual care?", "answer": "no difference", "evidence_quality": "very low", "fulltext_required": "no", "comment": "", "relevant_sources": ["19335674"]}, {"question_id": 182, "question": "Is the rate of falls higher, lower, or the same when comparing exercise to usual care?", "answer": "insufficient data", "evidence_quality": "", "fulltext_required": "no", "comment": "", "relevant_sources": ["19335674"]}, {"question_id": 183, "question": "Is the number of fallers higher, lower, or the same when comparing exercise to usual care?", "answer": "insufficient data", "evidence_quality": "", "fulltext_required": "no", "comment": "", "relevant_sources": ["19335674"]}, {"question_id": 184, "question": "Is balance as measured by the backward walk test higher, lower, or the same when comparing exercise to usual care?", "answer": "higher", "evidence_quality": "low", "fulltext_required": "no", "comment": "", "relevant_sources": ["19949016", "19335674"]}, {"question_id": 185, "question": "Is fatigue higher, lower, or the same when comparing exercise to usual care?", "answer": "lower", "evidence_quality": "very low", "fulltext_required": "no", "comment": "", "relevant_sources": ["17964881", "19949016"]}], "sources": [{"article_id": "24113319", "content": "Long-term prostate cancer (PCa) survivors are at increased risk for comorbidities and physical deconditioning.\nTo determine the effectiveness of a year-long randomised controlled trial of exercise training in PCa survivors >5 yr postdiagnosis on physical functioning.\nBetween 2010 and 2011, 100 long-term PCa survivors from Trans-Tasman Radiation Oncology Group 03.04 Randomised Androgen Deprivation and Radiotherapy previously treated with androgen-deprivation therapy and radiation therapy were randomly assigned to 6 mo of supervised exercise followed by 6 mo of a home-based maintenance programme (n=50) or printed educational material about physical activity (n=50) for 12 mo across 13 university-affiliated exercise clinics in Australia and New Zealand.\nSupervised resistance and aerobic exercise or printed educational material about physical activity.\nThe primary end point was a 400-m walk as a measure of cardiovascular fitness. Secondary end points were physical function, patient-reported outcomes, muscle strength, body composition, and biomarkers. Analysis of covariance was used to compare outcomes for groups at 6 and 12 mo adjusted for baseline values.\nParticipants undergoing supervised exercise showed improvement in cardiorespiratory fitness performance at 6 mo (-19 s [p=0.029]) and 12 mo (-13 s [p=0.028]) and better lower-body physical function across the 12-mo period (p<0.01). Supervised exercise also improved self-reported physical functioning at 6 (p=.006) and 12 mo (p=0.002), appendicular skeletal muscle at 6 mo (p=0.019), and objective measures of muscle strength at 6 and 12 mo (p<0.050). Limitations included the restricted number of participants undertaking body composition assessment, no blinding to group assignment for physical functioning measures, and inclusion of well-functioning individuals.\nSupervised exercise training in long-term PCa survivors is more effective than physical activity educational material for increasing cardiorespiratory fitness, physical function, muscle strength, and self-reported physical functioning at 6 mo. Importantly, these benefits were maintained in the long term with a home-based programme with follow-up at 12 mo.\nThe effect of an exercise intervention on cardiovascular and metabolic risk factors in prostate cancer patients from the RADAR study, ACTRN: ACTRN12609000729224.", "title": "A multicentre year-long randomised controlled trial of exercise training targeting physical functioning in men with prostate cancer previously treated with androgen suppression and radiation from TROG 03.04 RADAR.", "date": "2013-10-12"}, {"article_id": "17964881", "content": "To show fatigue prevention and quality of life (QOL) improvement from cardiovascular exercise during radiotherapy.\nProspective enrollment (n=21), randomized to exercise (n=11) and control groups (n=10), with pre- and post-radiotherapy between- and within-group comparisons.\nAcademic medical center.\nLocalized prostate cancer patients undergoing radiotherapy.\nThe interventional group received radiotherapy plus aerobic exercise 3 times a week for 8 weeks whereas the control group received radiotherapy without exercise.\nPre- and post-radiotherapy differences in cardiac fitness, fatigue, depression, functional status, physical, social, and functional well-being, leg strength, and flexibility were examined within and between 2 groups.\nNo significant differences existed between 2 groups at pre-radiotherapy assessment. At post-radiotherapy assessment, the exercise group showed significant within group improvements in: cardiac fitness (P<.001), fatigue (P=.02), Functional Assessment of Cancer Therapy-Prostate (FACT-P) (P=.04), physical well-being (P=.002), social well-being (P=.02), flexibility (P=.006), and leg strength (P=.000). Within the control group, there was a significant increase in fatigue score (P=.004) and a decline in social well-being (P<.05) at post-radiotherapy assessment. Between-group differences at post-radiotherapy assessment were significant in cardiac fitness (P=.006), strength (P=.000), flexibility (P<.01), fatigue (P<.001), FACT-P (P=.006), physical well-being (P<.001), social well-being (P=.002), and functional well-being (P=.04).\nAn 8-week cardiovascular exercise program in patients with localized prostate cancer undergoing radiotherapy improved cardiovascular fitness, flexibility, muscle strength, and overall QOL and prevented fatigue.", "title": "Exercise prevents fatigue and improves quality of life in prostate cancer patients undergoing radiotherapy.", "date": "2007-10-30"}, {"article_id": "19949016", "content": "Androgen suppression therapy (AST) results in musculoskeletal toxicity that reduces physical function and quality of life. This study examined the impact of a combined resistance and aerobic exercise program as a countermeasure to these AST-related toxicities.\nBetween 2007 and 2008, 57 patients with prostate cancer undergoing AST (commenced > 2 months prior) were randomly assigned to a program of resistance and aerobic exercise (n = 29) or usual care (n = 28) for 12 weeks. Primary end points were whole body and regional lean mass. Secondary end points were muscle strength and function, cardiorespiratory capacity, blood biomarkers, and quality of life.\nAnalysis of covariance was used to compare outcomes for groups at 12 weeks adjusted for baseline values and potential confounders. Patients undergoing exercise showed an increase in lean mass compared with usual care (total body, P = .047; upper limb, P < .001; lower limb, P = .019) and similarly better muscle strength (P < .01), 6-meter walk time (P = .024), and 6-meter backward walk time (P = .039). Exercise also improved several aspects of quality of life including general health (P = .022) and reduced fatigue (P = .021) and decreased levels of C-reactive protein (P = .008). There were no adverse events during the testing or exercise intervention program.\nA relatively brief exposure to exercise significantly improved muscle mass, strength, physical function, and balance in hypogonadal men compared with normal care. The exercise regimen was well tolerated and could be recommended for patients undergoing AST as an effective countermeasure to these common treatment-related adverse effects.", "title": "Combined resistance and aerobic exercise program reverses muscle loss in men undergoing androgen suppression therapy for prostate cancer without bone metastases: a randomized controlled trial.", "date": "2009-12-02"}, {"article_id": "26678611", "content": "Cancer patients with chemotherapy-induced peripheral neuropathy (CIPN) have deficits in sensory and motor skills leading to inappropriate proprioceptive feedback, impaired postural control, and fall risk. Balance training programs specifically developed for CIPN patients are lacking.\nThis pilot study investigated the effect of an interactive motor adaptation balance training program based on wearable sensors for improving balance in older cancer patients with CIPN.\nTwenty-two patients (age: 70.3 \u00b1 8.7 years) with objectively confirmed CIPN [vibration perception threshold (VPT) >25 V] were randomized to either an intervention (IG) or a control (CG) group. The IG received interactive game-based balance training including repetitive weight shifting and virtual obstacle crossing tasks. Wearable sensors provided real-time visual/auditory feedback from the lower limb trajectory and allowed the perception of motor errors during each motor action. The CG received no exercise intervention and continued their normal activity. Outcome measures were changes in sway of ankle, hip, and center of mass (CoM) in both mediolateral and anteroposterior (AP) directions during 30-second balance tests with increasing task difficulty [i.e. standing in feet-closed position with eyes open (EO) and eyes closed (EC), and in semi-tandem position with EO] at baseline and after the intervention. Additionally, gait performance (speed, variability) and fear of falling [Falls Efficacy Scale-International (FES-I)] were measured.\nTraining was safe despite the participants' impaired health status, great severity of CIPN (VPT 49.6 \u00b1 26.7 V), and great fear of falling (FES-I score 31.37 \u00b1 11.20). After the intervention, sway of hip, ankle, and CoM was significantly reduced in the IG compared to the CG while standing in feet-closed position with EO (p = 0.010-0.022, except AP CoM sway) and in semi-tandem position (p = 0.008-0.035, except ankle sway). No significant effects were found for balance with EC, gait speed, and FES-I score (p > 0.05).\nThis proof-of-concept study demonstrates that older cancer patients with CIPN can significantly improve their postural balance with specifically tailored, sensor-based exercise training. The training approach has potential as a therapy for improving CIPN-related postural control deficits. However, future studies comparing the proposed technology-based training with traditional balance training are required to evaluate the benefit of the interactive joint movement feedback.", "title": "Interactive Sensor-Based Balance Training in Older Cancer Patients with Chemotherapy-Induced Peripheral Neuropathy: A Randomized Controlled Trial.", "date": "2015-12-19"}, {"article_id": "29943097", "content": "Breast cancer is the most common cancer disease of women in industrialized countries. Neurotoxic chemotherapy drugs are known to harm peripheral nerves and cause a chemotherapy-induced peripheral neuropathy (CIPN). CIPN is one of the most common adverse events associated with Paclitaxel chemotherapy and may remain present long after the termination of chemotherapy. Thus, it reduces the patients' quality of life (QoL) both during chemotherapy and onwards, and can impose a danger on breast cancer survivors due to an increased risk of falling and fall-related injuries.\nThe aim of this randomized-controlled trial (RCT) (n\u2009=\u200936) (IG: intervention group, n\u2009=\u200917) (CG: control group, n\u2009=\u200919) was to determine whether sensorimotor exercises have a positive effect on physical and psychological parameters in breast cancer patients undergoing neurotoxic chemotherapy (Paclitaxel).\nAs a result, we were able to show significant improvements in postural stability in monopedal stance [left leg 16.17\u2009\u00b1\u20093.67 vs. 21.55\u2009\u00b1\u20095.33 (p\u2009<\u20090.001) and right leg 15.14\u2009\u00b1\u20092.30 vs. 20.85\u2009\u00b1\u20095.05 (p\u2009<\u20090.001)] and in bipedal stance [T1 vs. T0, -\u20090.49 (IG) vs. +\u20091.14 (CG) p\u2009=\u20090.039].\nThese results in posturography correlate with the clinical presentation with intervention group patients scoring significantly better on the Fullerton Advanced Balance Scale [37.71\u2009\u00b1\u20092.73 vs. 34.47\u2009\u00b1\u20093.98 (p\u2009=\u20090.004)]. Moderate strength training successfully prevented a strength loss in the IG that was remarkable in the CG (-\u20091.60 vs. 0.60, p\u2009=\u20090.029). Concerning the psychological parameters assessed via EORTC- and MFI-questionnaires, no significant improvements were found.\nFuture studies should focus on the correlation of clinical and posturometry findings and subjective QOL such as the long-term-development of CIPN.", "title": "Evaluation of the effects of sensorimotor exercise on physical and psychological parameters in breast cancer patients undergoing neurotoxic chemotherapy.", "date": "2018-06-27"}, {"article_id": "23917308", "content": "Due to concerns of fragility fracture, exercise is a perceived contraindication for prostate cancer patients with bone metastases. These patients experience significant functional impairment and muscle atrophy, which may lead to an increased likelihood of skeletal complicaTIOns (i.e., pathological fracture, bone pain) and/or falls. Safe resistance exercise prescription may counteract this effect. The aim of this feasibility trial was to determine the safety and efficacy of resistance exercise by prostate cancer survivors with bone metastatic disease.\nTwenty men with established bone metastases secondary to prostate cancer were randomly assigned to a 12-week resistance exercise program in which exercise prescription was based on the location of bone lesions (n=10) or usual care (n=10). Outcomes included safety and tolerance of the exercise program, physical function, physical activity level, body composition, fatigue, quality of life and psychological distress. Outcomes were compared between groups using analysis of covariance adjusted for baseline values.\nParticipants had significant disease load with 65% of participants presenting with two or more regions affected by bone metastases and an average Gleason score of 8.2\u00b10.9. Five participants (exercise=2; usual care=3) did not complete the intervention, three of which were due to advancing disease (exercise=2; usual care=1). No adverse events or skeletal complications occurred during the supervised exercise sessions. The exercise program was well tolerated as evidenced by high attendance (83%) and compliance rates (93%), and the ability of the participants to exercise at an intensity within the target range for cancer survivors (rating of perceived exertion =13.8\u00b11.5). The change in physical function (muscle strength \u223c11%; submaximal aerobic exercise capacity \u223c5% and ambulation \u223c12%), physical activity level (\u223c24%) and lean mass (\u223c3%) differed significantly between groups following the intervention, with favorable changes in the exercise group compared with the usual care group. No significant between-group differences were observed for fatigue, quality of life or psychological distress.\nThis initial evidence involving a small sample size suggests that appropriately designed and supervised resistance exercise may be safe and well tolerated by prostate cancer patients with bone metastatic disease and can lead to improvements in physical function, physical activity levels and lean mass. Future trials involving larger sample sizes are required to expand these preliminary findings.", "title": "Safety and efficacy of resistance exercise in prostate cancer patients with bone metastases.", "date": "2013-08-07"}, {"article_id": "25964257", "content": "Survivors of breast cancer may experience deterioration of physical function. This is important because poor physical function may be associated with premature mortality, injurious falls, bone fracture, and disability. We conducted a post hoc analysis to explore the potential efficacy of slowly progressive weight lifting to reduce the incidence of physical function deterioration among survivors of breast cancer.\nBetween October 2005 and August 2008, we conducted a single-blind, 12-month, randomized controlled trial of twice-per-week slowly progressive weight lifting or standard care among 295 survivors of nonmetastatic breast cancer. In this post hoc analysis of data from the Physical Activity and Lymphedema Trial, we examined incident deterioration of physical function after 12 months, defined as a \u2265 10-point decrease in the physical function subscale of the Medical Outcomes Short-Form 36-item questionnaire.\nThe proportion of participants who experienced incident physical function deterioration after 12 months was 16.3% (24/147) in the control group and 8.1% (12/148) in the weight lifting group (relative risk, 0.49; 95% CI, 0.25 to 0.96; P = .04). No serious or unexpected adverse events occurred that were related to weight lifting.\nSlowly progressive weight lifting compared with standard care reduced the incidence of physical function deterioration among survivors of breast cancer. These data are hypothesis generating. Future studies should directly compare the efficacy of weight lifting with other modalities of exercise, such as brisk walking, to appropriately inform the development of a confirmatory study designed to preserve physical function among survivors of breast cancer.", "title": "Weight Lifting and Physical Function Among Survivors of Breast Cancer: A Post Hoc Analysis of a Randomized Controlled Trial.", "date": "2015-05-13"}, {"article_id": "25891302", "content": "The current study examined effects, moderators (for whom), and mediators (working mechanisms) of 12 months of exercise on health-related quality of life (HRQoL) in older long-term survivors of prostate cancer.\nIn total, 100 men aged 71.7 years (standard deviation, 6.4 years) were randomly assigned to 6 months of supervised aerobic and resistance exercise followed by 6 months of a home-based exercise maintenance program (EX group) or printed education material regarding physical activity for 12 months (PA group). Assessments took place at baseline and after 6 and 12 months. Generalized estimating equations were used to study the effects of EX versus PA on HRQoL at 6 and 12 months, adjusting for baseline HRQoL. The authors examined potential sociodemographic and clinical moderators by adding interaction terms, and potential physical and psychological mediators using the product-of-coefficients test.\nAt 6 months, significant beneficial effects were found for global QoL, physical function, and social function in the EX group compared with the PA group. For physical function, beneficial effects were sustained at 12 months. Moderation analyses demonstrated larger effects of EX versus PA for patients who were married, started exercising sooner after their diagnosis, and previously used bisphosphonates. Changes in lower body functional performance significantly mediated the effect of EX on global QoL, physical function, and social function. No mediating effects on HRQoL were found for aerobic fitness, physical activity, fatigue, distress, or falls self-efficacy.\nAerobic and resistance exercise appears to have beneficial effects on HRQoL among older, long-term survivors of prostate cancer. Effects were moderated by marital status, time since diagnosis, and use of bisphosphonates, and were mediated by lower body functional performance.", "title": "The effect, moderators, and mediators of resistance and aerobic exercise on health-related quality of life in older long-term survivors of prostate cancer.", "date": "2015-04-22"}, {"article_id": "20541832", "content": "Deterioration in exercise tolerance and impairment in quality of life (QoL) are common consequences of lobectomy. This study evaluates additional exercise and strength training after lung resection on QoL, exercise tolerance and muscle strength. Fifty-three (28 male) patients attending thoracotomy for lung cancer, mean age, range 64 (32-82) years; mean pack years (SD) 31.9 (26.8); BMI 25.6 (4.2); FEV1 2.0 (0.7) l were randomised to control (usual care) or intervention (twice daily training plus usual care). After discharge the intervention group received monthly home visits and weekly telephone calls, the control group received monthly telephone calls up to 12 weeks. Assessment pre-operatively, 5 day and 12 weeks post-operatively consisted of quadriceps strength using magnetic stimulation, 6 Minute Walking Distance (6MWD) and QoL-EORTC-QLQ-LC13. QoL was unchanged over 12 weeks; 6MWD showed significant deterioration at 5 days post-operatively compared with pre-operatively, mean difference (SD)-131.6 (101.8) m and -128.0 (90.7) m in active and control groups respectively (p=0.89 between groups) which returned to pre-operative levels by 12 weeks in both groups. Quadriceps strength over the 5 day in-patient period showed a decrease of -8.3 (11.3) kg in the control group compared to increase of 4.0 (21.2) kg in the intervention group (p=0.04 between groups). Strength training after thoracotomy successfully prevented the fall in quadriceps strength seen in controls, however, there was no effect on 6MWD or QoL. 6MWD returned to pre-operative levels by 12 weeks regardless of additional support offered.", "title": "Evaluation of an early exercise intervention after thoracotomy for non-small cell lung cancer (NSCLC), effects on quality of life, muscle strength and exercise tolerance: randomised controlled trial.", "date": "2010-06-15"}, {"article_id": "27161493", "content": "The aim of the study was to evaluate the effects of a combined aerobic and strength program on physiological and psychological parameters in female breast cancer survivors.\nRandomised controlled trial.\n20 patients (age: 45.6\u00a0\u00b1\u00a02.7\u00a0yrs) surgically treated for breast cancer that had completed all cancer therapies at least 6 months before and with no contraindications to physical activity, were recruited and randomly assigned to an intervention group (n\u00a0=\u00a010) and a control group (n\u00a0=\u00a010). Intervention group patients attend to a 24-week combined aerobic and strength training program. Physiological (i.e. VO2max, bioelectrical impedance test, maximal strength of principal muscular groups) and psychological (i.e. functional assessment of chronic illness therapy-fatigue: FACIT-F) parameters were assessed at baseline and after 24 weeks.\nAfter 24 weeks the intervention group showed significant improvement in VO2max (38.8%), strength of upper and lower limbs (ranging from 13 to 60%) and decrease in fat mass percentage (-6.3%). The FACIT-F showed significant increase in all of the three scores that can be derived (FACIT-F Trial outcome: 13%; FACT-G total score: 18%; FACIT-F total score: 15%) showing patient's quality of life (QOL) improvement. No significant change in all the parameters was found for the control group.\nThese results show the positive effects of a combined aerobic and strength training program on breast cancer survivors and underline the importance of the early inclusion of structured physical activity in the rehabilitation protocol.", "title": "Effects of concurrent aerobic and strength training on breast cancer survivors: a pilot study.", "date": "2016-05-11"}, {"article_id": "26715587", "content": "Prostate cancer can negatively impact quality of life of the patient and his spouse caregiver, but interventions rarely target the health of both partners simultaneously. We tested the feasibility and preliminary efficacy of a partnered strength training program on the physical and mental health of prostate cancer survivors (PCS) and spouse caregivers.\nSixty-four couples were randomly assigned to 6\u00a0months of partnered strength training (Exercising Together, N\u00a0=\u00a032) or usual care (UC, N\u00a0=\u00a032). Objective measures included body composition (lean, fat and trunk fat mass (kg), and % body fat) by DXA, upper and lower body muscle strength by 1-repetition maximum, and physical function by the physical performance battery (PPB). Self-reported measures included the physical and mental health summary scales and physical function and fatigue subscales of the SF-36 and physical activity with the CHAMPS questionnaire.\nCouple retention rates were 100\u00a0% for Exercising Together and 84\u00a0% for UC. Median attendance of couples to Exercising Together sessions was 75\u00a0%. Men in Exercising Together became stronger in the upper body (p\u00a0<\u00a00.01) and more physically active (p\u00a0<\u00a00.01) than UC. Women in Exercising Together increased muscle mass (p\u00a0=\u00a00.05) and improved upper (p\u00a0<\u00a00.01) and lower body (p\u00a0<\u00a00.01) strength and PPB scores (p\u00a0=\u00a00.01) more than UC.\nExercising Together is a novel couples-based approach to exercise that was feasible and improved several health outcomes for both PCS and their spouses.\nA couples-based approach should be considered in cancer survivorship programs so that outcomes can mutually benefit both partners.\nClinicalTrials.gov NCT00954044.", "title": "Benefits of partnered strength training for prostate cancer survivors and spouses: results from a randomized controlled trial of the Exercising Together project.", "date": "2015-12-31"}, {"article_id": "19335674", "content": "(a) to determine if 110 postmenopausal breast cancer survivors (BCS) with bone loss who participated in 24 months of strength and weight training (ST) exercises had improved muscle strength and balance and had fewer falls compared to BCS who did not exercise; and (b) to describe type and frequency of ST exercises; adverse effects of exercises; and participants' adherence to exercises at home, at fitness centers, and at 36-month follow up.\nFindings reported are from a federally funded multicomponent intervention study of 223 postmenopausal BCS with either osteopenia or osteoporosis who were randomly assigned to an exercise (n=110) or comparison (n=113) group.\nTime points for testing outcomes were baseline, 6, 12, and 24 months into intervention. Muscle strength was tested using Biodex Velocity Spectrum Evaluation, and dynamic balance using Timed Backward Tandem Walk. Adherence to exercises was measured using self-report of number of prescribed sessions attended and participants' reports of falls.\nMean adherence over 24 months was 69.4%. Using generalized estimating equation (GEE) analyses, compared to participants not exercising, participants who exercised for 24 months had significantly improved hip flexion (p=0.011), hip extension (p=0.0006), knee flexion (p<0.0001, knee extension (p=0.0018), wrist flexion (p=0.031), and balance (p=0.010). Gains in muscle strength were 9.5% and 28.5% for hip flexion and extension, 50.0% and 19.4% for wrist flexion and extension, and 21.1% and 11.6% for knee flexion and extension. Balance improved by 39.4%. Women who exercised had fewer falls, but difference in number of falls between the two groups was not significant.\nMany postmenopausal BCS with bone loss can adhere to a 24 month ST exercise intervention, and exercises can result in meaningful gains in muscle strength and balance.\nMore studies are needed for examining relationships between muscle strength and balance in postmenopausal BCS with bone loss and their incidence of falls and fractures.", "title": "An exercise intervention for breast cancer survivors with bone loss.", "date": "2009-04-02"}]}
{"original_review": "34661278", "question_data": [{"question_id": 186, "question": "Is the incidence of bronchopulmonary dysplasia (BPD) higher, lower, or the same when comparing prophylactic CPAP to very early CPAP?", "answer": "uncertain effect", "evidence_quality": "very low", "fulltext_required": "yes", "comment": "", "relevant_sources": ["23930249"]}, {"question_id": 187, "question": "Is the risk of death higher, lower, or the same when comparing prophylactic CPAP to very early CPAP?", "answer": "no difference", "evidence_quality": "very low", "fulltext_required": "no", "comment": "", "relevant_sources": ["23930249"]}, {"question_id": 188, "question": "Is the incidence of neurodevelopmental impairment higher, lower, or the same when comparing prophylactic CPAP to very early CPAP?", "answer": "insufficient data", "evidence_quality": "", "fulltext_required": "no", "comment": "", "relevant_sources": ["23930249"]}, {"question_id": 189, "question": "Is the risk of pneumothorax higher, lower, or the same when comparing prophylactic CPAP to very early CPAP?", "answer": "insufficient data", "evidence_quality": "", "fulltext_required": "yes", "comment": "", "relevant_sources": ["23930249"]}, {"question_id": 190, "question": "Is the risk of neurodevelopmental impairment higher, lower, or the same when comparing prophylactic or very early CPAP to supportive care?", "answer": "insufficient data", "evidence_quality": "", "fulltext_required": "no", "comment": "no studies report it", "relevant_sources": ["15321956", "22402568", "24554040", "3102211"]}, {"question_id": 191, "question": "Is the risk of death higher, lower, or the same when comparing prophylactic or very early CPAP to supportive care?", "answer": "no difference", "evidence_quality": "moderate", "fulltext_required": "no", "comment": "15321956 omitted with 25% weight", "relevant_sources": ["22402568", "24554040", "3102211"]}, {"question_id": 192, "question": "Is the risk of failed treatment higher, lower, or the same when comparing prophylactic or very early CPAP to supportive care?", "answer": "lower", "evidence_quality": "very low", "fulltext_required": "no", "comment": "", "relevant_sources": ["15321956", "22402568", "24554040", "3102211"]}, {"question_id": 193, "question": "Is the risk of death or bronchopulmonary dysplasia (BPD) higher, lower, or the same when comparing prophylactic or very early CPAP to mechanical ventilation?", "answer": "lower", "evidence_quality": "moderate", "fulltext_required": "no", "comment": "", "relevant_sources": ["18272893", "20472939", "22025591"]}], "sources": [{"article_id": "24725582", "content": "To explore the early childhood pulmonary outcomes of infants who participated in the National Institute of Child Health and Human Development's Surfactant Positive Airway Pressure and Pulse Oximetry Randomized Trial (SUPPORT), using a factorial design that randomized extremely preterm infants to lower vs higher oxygen saturation targets and delivery room continuous positive airway pressure (CPAP) vs intubation/surfactant.\nThe Breathing Outcomes Study, a prospective secondary study to the Surfactant Positive Airway Pressure and Pulse Oximetry Randomized Trial, assessed respiratory morbidity at 6-month intervals from hospital discharge to 18-22 months corrected age (CA). Two prespecified primary outcomes-wheezing more than twice per week during the worst 2-week period and cough longer than 3 days without a cold-were compared for each randomized intervention.\nOne or more interviews were completed for 918 of the 922 eligible infants. The incidences of wheezing and cough were 47.9% and 31.0%, respectively, and did not differ between the study arms of either randomized intervention. Infants randomized to lower vs higher oxygen saturation targets had a similar risk of death or respiratory morbidity (except for croup and treatment with oxygen or diuretics at home). Infants randomized to CPAP vs intubation/surfactant had fewer episodes of wheezing without a cold (28.9% vs 36.5%; P<.05), respiratory illnesses diagnosed by a doctor (47.7% vs 55.2%; P<.05), and physician or emergency room visits for breathing problems (68.0% vs 72.9%; P<.05) by 18-22 months CA.\nTreatment with early CPAP rather than intubation/surfactant is associated with less respiratory morbidity by 18-22 months CA. Longitudinal assessment of pulmonary morbidity is necessary to fully evaluate the potential benefits of respiratory interventions for neonates.", "title": "Respiratory outcomes of the surfactant positive pressure and oximetry randomized trial (SUPPORT).", "date": "2014-04-15"}, {"article_id": "22402568", "content": "To determine whether very low birth weight infants (VLBWIs), initially supported with continuous positive airway pressure (CPAP) and then selectively treated with the INSURE (intubation, surfactant, and extubation to CPAP; CPAP/INSURE) protocol, need less mechanical ventilation than those supported with supplemental oxygen, surfactant, and mechanical ventilation if required (Oxygen/mechanical ventilation [MV]).\nIn a multicenter randomized controlled trial, spontaneously breathing VLBWIs weighing 800-1500 g were allocated to receive either therapy. In the CPAP/INSURE group, if respiratory distress syndrome (RDS) did not occur, CPAP was discontinued after 3-6 hours. If RDS developed and the fraction of inspired oxygen (FiO(2)) was >0.35, the INSURE protocol was indicated. Failure criteria included FiO(2) >0.60, severe apnea or respiratory acidosis, and receipt of more than 2 doses of surfactant. In the Oxygen/MV group, in the presence of RDS, supplemental oxygen without CPAP was given, and if FiO(2) was >0.35, surfactant and mechanical ventilation were provided.\nA total of 256 patients were randomized to either the CPAP/INSURE group (n = 131) or the Oxygen/MV group (n = 125). The need for mechanical ventilation was lower in the CPAP/INSURE group (29.8% vs 50.4%; P = .001), as was the use of surfactant (27.5% vs 46.4%; P = .002). There were no differences in death, pneumothorax, bronchopulmonary dysplasia, and other complications of prematurity between the 2 groups.\nCPAP and early selective INSURE reduced the need for mechanical ventilation and surfactant in VLBWIs without increasing morbidity and death. These results may be particularly relevant for resource-limited regions.", "title": "Randomized trial of early bubble continuous positive airway pressure for very low birth weight infants.", "date": "2012-03-10"}, {"article_id": "22025591", "content": "We designed a multicenter randomized trial to compare 3 approaches to the initial respiratory management of preterm neonates: prophylactic surfactant followed by a period of mechanical ventilation (prophylactic surfactant [PS]); prophylactic surfactant with rapid extubation to bubble nasal continuous positive airway pressure (intubate-surfactant-extubate [ISX]) or initial management with bubble continuous positive airway pressure and selective surfactant treatment (nCPAP).\nNeonates born at 26 0/7 to 29 6/7 weeks' gestation were enrolled at participating Vermont Oxford Network centers and randomly assigned to PS, ISX, or nCPAP groups before delivery. Primary outcome was the incidence of death or bronchopulmonary dysplasia (BPD) at 36 weeks' postmenstrual age.\n648 infants enrolled at 27 centers. The study was halted before the desired sample size was reached because of declining enrollment. When compared with the PS group, the relative risk of BPD or death was 0.78 (95% confidence interval: 0.59-1.03) for the ISX group and 0.83 (95% confidence interval: 0.64-1.09) for the nCPAP group. There were no statistically significant differences in mortality or other complications of prematurity. In the nCPAP group, 48% were managed without intubation and ventilation, and 54% without surfactant treatment.\nPreterm neonates were initially managed with either nCPAP or PS with rapid extubation to nCPAP had similar clinical outcomes to those treated with PS followed by a period of mechanical ventilation. An approach that uses early nCPAP leads to a reduction in the number of infants who are intubated and given surfactant.", "title": "Randomized trial comparing 3 approaches to the initial respiratory management of preterm neonates.", "date": "2011-10-26"}, {"article_id": "23930249", "content": "This prospective study was performed to identify whether the early use of nasal continuous positive airway pressure (n CPAP) would reduce the rate of endotracheal intubation, mechanical ventilation and surfactant administration.\nThis study was conducted from June 2009 to September 2010 in the Shahid Beheshti University Hospital, Isfahan-Iran. A total of 72 preterm infants with 25-30 weeks gestation who needed respiratory support at 5 min after birth entered the study. Infants were randomly assigned to the very early CPAP (initiated 5 min after birth) or to the late CPAP (initiated 30 min after birth) treatment groups. The primary outcomes were need for intubation and mechanical ventilation during the first 48 h after birth and secondary outcomes were death, pneumothorax, intraventricular hemorrhage, duration of mechanical ventilation and bronchopulmonary dysplasia.\nThere were no significant differences between the two groups with regard to mortality rate, bronchopulmonary dysplasia and patent ductus arteriosus. The need for surfactant administration was significantly reduced in the early CPAP group (P = 0.04). Infants in the early CPAP group less frequently required intubation and mechanical ventilation.\nEarly n CPAP is more effective than late n CPAP for the treatment of respiratory distress syndrome. In addition, the early use of n CPAP would reduce the need for some invasive procedures such as intubation and mechanical ventilation.", "title": "Early versus delayed initiation of nasal continuous positive airway pressure for treatment of respiratory distress syndrome in premature newborns: A randomized clinical trial.", "date": "2013-08-10"}, {"article_id": "15321956", "content": "The role of nasal continuous positive airways pressure (nCPAP) in the management of respiratory distress syndrome in preterm infants is not completely defined.\nTo evaluate the benefits and risks of prophylactic nCPAP in infants of 28-31 weeks gestation.\nMulticentre randomised controlled clinical trial.\nSeventeen Italian neonatal intensive care units.\nA total of 230 newborns of 28-31 weeks gestation, not intubated in the delivery room and without major malformations, were randomly assigned to prophylactic or rescue nCPAP.\nProphylactic nCPAP was started within 30 minutes of birth, irrespective of oxygen requirement and clinical status. Rescue nCPAP was started when Fio2 requirement was > 0.4, for more than 30 minutes, to maintain transcutaneous oxygen saturation between 93% and 96%. Exogenous surfactant was given when Fio2 requirement was > 0.4 in nCPAP in the presence of radiological signs of respiratory distress syndrome.\nPrimary end point: need for exogenous surfactant. Secondary end points: need for mechanical ventilation and incidence of air leaks.\nSurfactant was needed by 22.6% in the prophylaxis group and 21.7% in the rescue group. Mechanical ventilation was required by 12.2% in both the prophylaxis and rescue group. The incidence of air leaks was 2.6% in both groups. More than 80% of both groups had received prenatal steroids.\nIn newborns of 28-31 weeks gestation, there is no greater benefit in giving prophylactic nCPAP than in starting nCPAP when the oxygen requirement increases to a Fio2 > 0.4.", "title": "Prophylactic nasal continuous positive airways pressure in newborns of 28-31 weeks gestation: multicentre randomised controlled clinical trial.", "date": "2004-08-24"}, {"article_id": "3102211", "content": "Application of continuous distending pressure at birth (very early CDP) should stabilize the immature airways and reduce the severity of respiratory distress syndrome (RDS) in preterm infants. Eighty-two preterm infants of less than 32 weeks gestation were randomly assigned at birth to early treatment group (TG), in which CDP of 6 cm water pressure was applied at birth by the nasopharyngeal route (NP-CDP), or to control group (CG), in which CDP was applied when indicated for established criteria (pO2 less than 50 mmHg in FiO2 greater than 0.5). Characteristics of the infants in the two groups were comparable. No statistically significant difference between the two groups was found in the incidence of RDS. The course of RDS, and oxygen and ventilatory requirements also did not appear to be changed. In blood gas parameters of most of the time frames, no significant difference was found between the two groups when the results were analyzed according to the assigned group. When the results were analyzed separately for the infants who developed RDS, infants in TG appear to have fared worse from the therapy in terms of oxygenation, as indicated by significantly higher FiO2 (P less than 0.01) and lower a/A (P less than 0.01) values on the third day of the course of RDS, as compared to infants in CG. The incidence of complications was comparable in the two groups. Four infants from TG (9.3%) and one from CG (2.6%) died (P = NS). We conclude that VECDP by nasopharyngeal route does not reduce the incidence of RDS and does not appear to improve the outcome and may worsen the severity of RDS when compared to application of CDP for established criteria.", "title": "Randomized controlled trial of very early continuous distending pressure in the management of preterm infants.", "date": "1987-01-01"}, {"article_id": "20472939", "content": "There are limited data to inform the choice between early treatment with continuous positive airway pressure (CPAP) and early surfactant treatment as the initial support for extremely-low-birth-weight infants.\nWe performed a randomized, multicenter trial, with a 2-by-2 factorial design, involving infants who were born between 24 weeks 0 days and 27 weeks 6 days of gestation. Infants were randomly assigned to intubation and surfactant treatment (within 1 hour after birth) or to CPAP treatment initiated in the delivery room, with subsequent use of a protocol-driven limited ventilation strategy. Infants were also randomly assigned to one of two target ranges of oxygen saturation. The primary outcome was death or bronchopulmonary dysplasia as defined by the requirement for supplemental oxygen at 36 weeks (with an attempt at withdrawal of supplemental oxygen in neonates who were receiving less than 30% oxygen).\nA total of 1316 infants were enrolled in the study. The rates of the primary outcome did not differ significantly between the CPAP group and the surfactant group (47.8% and 51.0%, respectively; relative risk with CPAP, 0.95; 95% confidence interval [CI], 0.85 to 1.05) after adjustment for gestational age, center, and familial clustering. The results were similar when bronchopulmonary dysplasia was defined according to the need for any supplemental oxygen at 36 weeks (rates of primary outcome, 48.7% and 54.1%, respectively; relative risk with CPAP, 0.91; 95% CI, 0.83 to 1.01). Infants who received CPAP treatment, as compared with infants who received surfactant treatment, less frequently required intubation or postnatal corticosteroids for bronchopulmonary dysplasia (P<0.001), required fewer days of mechanical ventilation (P=0.03), and were more likely to be alive and free from the need for mechanical ventilation by day 7 (P=0.01). The rates of other adverse neonatal outcomes did not differ significantly between the two groups.\nThe results of this study support consideration of CPAP as an alternative to intubation and surfactant in preterm infants. (ClinicalTrials.gov number, NCT00233324.)", "title": "Early CPAP versus surfactant in extremely preterm infants.", "date": "2010-05-18"}, {"article_id": "24554040", "content": "This study evaluated whether the use of continuous positive airway pressure (CPAP) in the delivery room alters the need for mechanical ventilation and surfactant during the first 5 days of life and modifies the incidence of respiratory morbidity and mortality during the hospital stay. The study was a multicenter randomized clinical trial conducted in five public university hospitals in Brazil, from June 2008 to December 2009. Participants were 197 infants with birth weight of 1000-1500 g and without major birth defects. They were treated according to the guidelines of the American Academy of Pediatrics (APP). Infants not intubated or extubated less than 15 min after birth were randomized for two treatments, routine or CPAP, and were followed until hospital discharge. The routine (n=99) and CPAP (n=98) infants studied presented no statistically significant differences regarding birth characteristics, complications during the prenatal period, the need for mechanical ventilation during the first 5 days of life (19.2 vs 23.4%, P=0.50), use of surfactant (18.2 vs 17.3% P=0.92), or respiratory morbidity and mortality until discharge. The CPAP group required a greater number of doses of surfactant (1.5 vs 1.0, P=0.02). When CPAP was applied to the routine group, it was installed within a median time of 30 min. We found that CPAP applied less than 15 min after birth was not able to reduce the need for ventilator support and was associated with a higher number of doses of surfactant when compared to CPAP applied as clinically indicated within a median time of 30 min.", "title": "Application of continuous positive airway pressure in the delivery room: a multicenter randomized clinical trial.", "date": "2014-02-21"}, {"article_id": "23268664", "content": "Previous results from our trial of early treatment with continuous positive airway pressure (CPAP) versus early surfactant treatment in infants showed no significant difference in the outcome of death or bronchopulmonary dysplasia. A lower (vs. higher) target range of oxygen saturation was associated with a lower rate of severe retinopathy but higher mortality. We now report longer-term results from our prespecified hypotheses.\nUsing a 2-by-2 factorial design, we randomly assigned infants born between 24 weeks 0 days and 27 weeks 6 days of gestation to early CPAP with a limited ventilation strategy or early surfactant administration and to lower or higher target ranges of oxygen saturation (85 to 89% or 91 to 95%). The primary composite outcome for the longer-term analysis was death before assessment at 18 to 22 months or neurodevelopmental impairment at 18 to 22 months of corrected age.\nThe primary outcome was determined for 1234 of 1316 enrolled infants (93.8%); 990 of the 1058 surviving infants (93.6%) were evaluated at 18 to 22 months of corrected age. Death or neurodevelopmental impairment occurred in 27.9% of the infants in the CPAP group (173 of 621 infants), versus 29.9% of those in the surfactant group (183 of 613) (relative risk, 0.93; 95% confidence interval [CI], 0.78 to 1.10; P=0.38), and in 30.2% of the infants in the lower-oxygen-saturation group (185 of 612), versus 27.5% of those in the higher-oxygen-saturation group (171 of 622) (relative risk, 1.12; 95% CI, 0.94 to 1.32; P=0.21). Mortality was increased with the lower-oxygen-saturation target (22.1%, vs. 18.2% with the higher-oxygen-saturation target; relative risk, 1.25; 95% CI, 1.00 to 1.55; P=0.046).\nWe found no significant differences in the composite outcome of death or neurodevelopmental impairment among extremely premature infants randomly assigned to early CPAP or early surfactant administration and to a lower or higher target range of oxygen saturation. (Funded by the Eunice Kennedy Shriver National Institute of Child Health and Human Development and the National Heart, Lung, and Blood Institute; SUPPORT ClinicalTrials.gov number, NCT00233324.).", "title": "Neurodevelopmental outcomes in the early CPAP and pulse oximetry trial.", "date": "2012-12-28"}, {"article_id": "18272893", "content": "Bronchopulmonary dysplasia is associated with ventilation and oxygen treatment. This randomized trial investigated whether nasal continuous positive airway pressure (CPAP), rather than intubation and ventilation, shortly after birth would reduce the rate of death or bronchopulmonary dysplasia in very preterm infants.\nWe randomly assigned 610 infants who were born at 25-to-28-weeks' gestation to CPAP or intubation and ventilation at 5 minutes after birth. We assessed outcomes at 28 days of age, at 36 weeks' gestational age, and before discharge.\nAt 36 weeks' gestational age, 33.9% of 307 infants who were assigned to receive CPAP had died or had bronchopulmonary dysplasia, as compared with 38.9% of 303 infants who were assigned to receive intubation (odds ratio favoring CPAP, 0.80; 95% confidence interval [CI], 0.58 to 1.12; P=0.19). At 28 days, there was a lower risk of death or need for oxygen therapy in the CPAP group than in the intubation group (odds ratio, 0.63; 95% CI, 0.46 to 0.88; P=0.006). There was little difference in overall mortality. In the CPAP group, 46% of infants were intubated during the first 5 days, and the use of surfactant was halved. The incidence of pneumothorax was 9% in the CPAP group, as compared with 3% in the intubation group (P<0.001). There were no other serious adverse events. The CPAP group had fewer days of ventilation.\nIn infants born at 25-to-28-weeks' gestation, early nasal CPAP did not significantly reduce the rate of death or bronchopulmonary dysplasia, as compared with intubation. Even though the CPAP group had more incidences of pneumothorax, fewer infants received oxygen at 28 days, and they had fewer days of ventilation. (Australian New Zealand Clinical Trials Registry number, 12606000258550.).", "title": "Nasal CPAP or intubation at birth for very preterm infants.", "date": "2008-02-15"}]}
{"original_review": "29969521", "question_data": [{"question_id": 194, "question": "Is the risk of wound infection in open fracture wounds higher, lower, or the same when comparing negative pressure wound therapy (NPWT) at 125 mmHg to standard care?", "answer": "uncertain effect", "evidence_quality": "very low", "fulltext_required": "no", "comment": "27022347 has fulltext but not neede", "relevant_sources": ["29896626", "19704269", "27857499", "27022347"]}, {"question_id": 195, "question": "Is the risk of wound infection in non-fracture traumatic open wounds higher, lower, or the same when comparing negative pressure wound therapy (NPWT) at 125 mmHg to NWPT at 75 mmHg?", "answer": "uncertain effect", "evidence_quality": "very low", "fulltext_required": "no", "comment": "\"downgraded the certainty of the evidence for risk of bias and imprecision\"", "relevant_sources": ["26964825"]}, {"question_id": 196, "question": "Is the risk of wound infection in non-fracture traumatic open wounds higher, lower, or the same when comparing negative pressure wound therapy (NPWT) at 75 mmHg to standard care?", "answer": "uncertain effect", "evidence_quality": "very low", "fulltext_required": "no", "comment": "\"downgraded the certainty of the evidence for risk of bias and imprecision\"", "relevant_sources": ["26964825"]}], "sources": [{"article_id": "26964825", "content": "The objectives were to investigate the emergency treatment of serious dog bite lacerations on limbs and to identify whether negative pressure wound therapy (NPWT) was beneficial in these instances.\nA total of 580 cases with serious limb lacerations due to dog bites were randomly divided into 2 groups. After thorough debridement, the limb lacerations of group A (n = 329) were left open. The remaining cases (n = 251) were randomly divided into 2 subgroups, group B and group C, which were treated with 125 and 75 mm Hg of continuous negative pressure, respectively. Antibiotics were only used in cases where there were systemic signs of wound infection, and were not given prophylactically. The infection rate, infection time, and healing time were analyzed.\nThe wound infection rates of groups A, B, and C were 9.1%, 4.1%, and 3.9%, respectively. The infection times of the 3 groups were 26.3 \u00b1 11.6, 159.8 \u00b1 13.4, and 166.4 \u00b1 16.2 hours, respectively. The recovery times of the infection patients in the 3 groups were 19.2 \u00b1 4.6, 13.2 \u00b1 2.1, and 12.7 \u00b1 2.3 days, respectively, and in the noninfection patients, the recovery times were 15.6 \u00b1 2.7, 10.1 \u00b1 2.3, and 10.5 \u00b1 1.9 days, respectively. In groups B (-125 mm Hg) and C (-75 mm Hg), the infection rate, infection time, and healing time showed no significant differences.\nPatients with serious dog bite laceration on limbs could benefit from NPWT. Compared with the traditional treatment of leaving the wounds open, NPWT reduced the infection rate and shortened recovery time. When NPWT was performed, low negative pressure (-75 mm Hg) had the same positive effects as high pressure (-125 mm Hg). Prophylactic antibiotics administration is not recommended for treating this kind of laceration.\nTherapeutic/care management, level II.", "title": "Negative pressure wound therapy for serious dog bites of extremities: a prospective randomized trial.", "date": "2016-03-12"}, {"article_id": "29896626", "content": "Open fractures of the lower limb occur when a broken bone penetrates the skin. There can be major complications from these fractures, which can be life-changing.\nTo assess the disability, rate of deep infection, and quality of life in patients with severe open fracture of the lower limb treated with negative pressure wound therapy (NPWT) vs standard wound management after the first surgical debridement of the wound.\nMulticenter randomized trial performed in the UK Major Trauma Network, recruiting 460 patients aged 16 years or older with a severe open fracture of the lower limb from July 2012 through December 2015. Final outcome data were collected through November 2016. Exclusions were presentation more than 72 hours after injury and inability to complete questionnaires.\nNPWT (n\u2009=\u2009226) in which an open-cell solid foam or gauze was placed over the surface of the wound and connected to a suction pump, creating a partial vacuum over the dressing, vs standard dressings not involving application of negative pressure (n\u2009=\u2009234).\nDisability Rating Index score (range, 0 [no disability] to 100 [completely disabled]) at 12 months was the primary outcome measure, with a minimal clinically important difference of 8 points. Secondary outcomes were complications including deep infection and quality of life (score ranged from 1 [best possible] to -0.59 [worst possible]; minimal clinically important difference, 0.08) collected at 3, 6, 9, and 12 months.\nAmong 460 patients who were randomized (mean age, 45.3 years; 74% men), 88% (374/427) of available study participants completed the trial. There were no statistically significant differences in the patients' Disability Rating Index score at 12 months (mean score, 45.5 in the NPWT group vs 42.4 in the standard dressing group; mean difference, -3.9 [95% CI, -8.9 to 1.2]; P\u2009=\u2009.13), in the number of deep surgical site infections (16 [7.1%] in the NPWT group vs 19 [8.1%] in the standard dressing group; difference, 1.0% [95% CI, -4.2% to 6.3%]; P\u2009=\u2009.64), or in quality of life between groups (difference in EuroQol 5-dimensions questionnaire, 0.02 [95% CI, -0.05 to 0.08]; Short Form-12 Physical Component Score, 0.5 [95% CI, -3.1 to 4.1] and Mental Health Component Score, -0.4 [95% CI, -2.2 to 1.4]).\nAmong patients with severe open fracture of the lower limb, use of NPWT compared with standard wound dressing did not improve self-rated disability at 12 months. The findings do not support this treatment for severe open fractures.\nisrctn.org Identifier: ISRCTN33756652.", "title": "Effect of Negative Pressure Wound Therapy vs Standard Wound Management on 12-Month Disability Among Adults With Severe Open Fracture of the Lower Limb: The WOLLF Randomized Clinical Trial.", "date": "2018-06-14"}, {"article_id": "27022347", "content": "Successful closure is a primary step of treatment in open fracture wounds. Delayed healing or complications can lead to increased treatment duration, costs and disability rates. The aim of this study was to compare Negative Pressure Wound Therapy (NPWT) and conventional wound dressings in patients with open fracture wounds.\nIn a prospective randomized clinical trial study, 90 patients with open fractures that were referred for treatment were enrolled between February 2013 to March 2015. Patients were divided into two groups. Group I underwent NPWT and group II underwent conventional wound dressing. Then patients were followed up for one month. Within the one month, the number of dressing change varied based on the extent of the wound. Duration of wound healing, presence of infection and the number of hospitalization days in these patients were recorded and compared at the end of the study between the two groups. Questionnaires and check lists were used to collect data. Analysis was done with SPSS 20, paired sample T-test, and chi-square tests. P<0.05 was considered significant.\nThere was a significant difference between the rate of wound healing in the group one or NPWT group and group II (conventional wound dressings) P<0.05. There was no significant difference between two groups in incidence of infection (P=0.6).\nUsing NPWT expedites the healing process of extremity wounds. It is more economical and can be considered as a substitute for the treatment of extremity wounds.", "title": "Comparison of negative pressure wound therapy (NPWT) &conventional wound dressings in the open fracture wounds.", "date": "2016-03-30"}, {"article_id": "27857499", "content": "Open tibial fractures are associated with a high incidence of mainly osteomyelitis. Negative pressure wound therapy (NPWT) is a novel form of treatment that uses subatmospheric pressure to effect early wound healing.\nTo determine the effect of NPWT on incidence of deep infections/osteomyelitis after open tibial fractures using a prospective randomized study design.\nNinety-three open tibial fractures were randomized into two groups receiving NPWT and the second group undergoing periodic irrigation, cleaning and debridement respectively. The wounds were closed or covered on shrinkage in size and sufficient granulation. Evidence of infection was sought during the course of treatment and follow up. Also serial cultures were sent every time the wound was cleaned.\nPatients in the control group developed a total of 11 infections (22%) as opposed to only 2 (4.6%) in the NPWT group (", "title": "Impact of negative pressure wound therapy on open diaphyseal tibial fractures: A prospective randomized trial.", "date": "2016-11-20"}, {"article_id": "19704269", "content": "To evaluate the impact of negative pressure wound therapy (NPWT) after severe open fractures on deep infection.\nProspective randomized study.\nAcademic level I trauma center.\nFifty-nine patients with 63 severe high-energy open fractures were enrolled in this study, with data available on 58 patients with 62 open fractures.\nTwenty-three patients with 25 fractures randomized to the control group and underwent initial irrigation and debridement followed by standard fine mesh gauze dressing, with repeat irrigation and debridement every 48-72 hours until wound closure. Thirty-five patients randomized to the NPWT group and had identical treatment except that NPWT was applied to the wounds between irrigation and debridement procedures until closure.\nThe presence or absence of deep wound infection or osteomyelitis, wound dehiscence, and fracture union were primary outcome measures.\nControl patients developed 2 acute infections (8%) and 5 delayed infections (20%), for a total of 7 deep infections (28%), whereas NPWT patients developed 0 acute infections, 2 delayed infections (5.4%), for a total of 2 deep infections (5.4%). There is a significant difference between the groups for total infections (P = 0.024). The relative risk ratio is 0.199 (95% confidence interval: 0.045-0.874), suggesting that patients treated with NPWT were only one-fifth as likely to have an infection compared with patients randomized to the control group. NPWT represents a promising new therapy for severe open fractures after high-energy trauma.", "title": "Negative pressure wound therapy after severe open fractures: a prospective randomized study.", "date": "2009-08-26"}, {"article_id": "26395498", "content": "Patients who sustain open lower limb fractures have reported infection risks as high as 27%. The type of dressing applied after initial debridement could potentially affect this risk. In this trial, standard dressings will be compared with a new emerging treatment, negative pressure wound therapy, for patients with open lower limb fractures.\nAll adult patients presenting with an open lower limb fracture, with a Gustilo and Anderson (G&A) grade 2/3, will be considered for inclusion. 460 consented patients will provide 90% power to detect a difference of eight points in the Disability Rating Index (DRI) score at 12\u2005months, at the 5% level. A randomisation sequence, stratified by trial centre and G&A grade, will be produced and administered by a secure web-based service. A qualitative substudy will assess patients' experience of giving consent for the trial, and acceptability of trial procedures to patients and staff. Patients will have clinical follow-up in a fracture clinic up to a minimum of 12\u2005months as per standard National Health Service (NHS) practice. Functional and quality of life outcome data will be collected using the DRI, SF12 and EQ-5D questionnaires at 3, 6, 9 and 12\u2005months postoperatively. In addition, information will be requested with regards to resource use and any late complications or surgical interventions related to their injury. The main analysis will investigate differences in the DRI score at 1\u2005year after injury, between the two treatment groups on an intention-to-treat basis. Tests will be two sided and considered to provide evidence for a significant difference if p values are less than 0.05.\nEthical approval was given by NRES Committee West Midlands-Coventry & Warwickshire on 6/2/2012 (ref: 12/WM/0001). The results of the trial will be disseminated via peer-reviewed publications and presentations at relevant conferences.\nISRCTN33756652.", "title": "Protocol for a randomised controlled trial of standard wound management versus negative pressure wound therapy in the treatment of adult patients with an open fracture of the lower limb: UK Wound management of Lower Limb Fractures (UK WOLLF).", "date": "2015-09-24"}, {"article_id": "18763197", "content": "The aim of this study was to investigate the degree of anxiety in patients in whom the vacuum-assisted closure (VAC) of wounds was used. Psychological evaluations were made on the day before VAC was applied and at the 10(th) day of treatment in 20 patients with traumatic wounds of the lower extremity. Anxiety was measured with the Hamilton Rating Scale for Anxiety and with the State Anxiety Inventory test. The same measurements were also made in 20 further patients with similar wounds but managed with classic treatment as controls. Both groups showed a significant increase in anxiety during the 10 days. The mean (SD) differences in the anxiety scores measured during the 10-day period were significantly higher in the group treated by VAC than in the control group, State Anxiety Inventory test (14.0 (2.3) compared with 2.6 (1.2), p<0.001) and Hamilton Rating Scale for Anxiety test (4.4 (0.6) compared with 1.3 (0.6), p<0.001). Although we think that VAC is an effective tool for treating lower extremity wounds, we have concerns about possible accompanying psychological effects.", "title": "Vacuum-assisted closure of wounds and anxiety.", "date": "2008-09-03"}, {"article_id": "26845802", "content": "Wounds have provided a challenge to the clinicians for centuries and this scenario persists to the 21st century. Negative pressure wound therapy (NPWT) is one of the latest additions in wound management. It has been widely adopted in developed countries with foam as the default wound dressing although it has some limitations.\nTo determine the difference in outcomes between the use of gauze versus foam as wound dressing in NPWT for the management of acute traumatic wounds with soft tissue loss.\nProspective randomised comparative interventional study.\nKenyatta National Hospital Orthopaedic and Surgical wards.\nAll patients aged above 12 years with Class III and Class IV acute traumatic wounds.\nThe main outcome measure is the time taken to achieve 100% wound granulation. Comparisons were also made on the mean pain scores during dressing change and the percentage change in wound surface area.\nWounds took an average of 8.4 days in the gauze group and 8.1 days in the foam group (p = 0.698) to achieve full granulation. The percentage change in wound surface area was 5.3 versus 5.5 (P = 0.769) in the gauze and foam groups respectively. The infection rates were comparable between the two groups (28% for gauze and 23.1% for foam, p = 0.697) and there was no significant difference in the median pain scores (gauze = 4.5, foam = 4.8 with p = 0.174). However, outcomes with gauze dressing were influenced significantly by the time to application of NPWT, initial wound surface area and wound infection while with foam dressing outcomes tended to be affected less so by the above factors.\nIn the use of NPWT for the management of acute traumatic wounds, there is no difference in terms of time to full wound granulation, change in wound surface area, wound infection and pain during dressing change whether gauze or foam is used as the wound dressing material.", "title": "OUTCOME OF FOAM VERSUS GAUZE DRESSINGS IN NEGATIVE PRESSURE WOUND THERAPY FOR THE MANAGEMENT OF ACUTE TRAUMATIC WOUNDS WITH SOFT TISSUE LOSS AT KENYATTA NATIONAL HOSPITAL.", "date": "2012-07-01"}, {"article_id": "0", "content": "As medication does not normalize outcomes of children with attention deficit hyperactivity disorder (ADHD), especially in real-life functioning, nonpharmacological methods are important to target this field. This randomized controlled clinical trial was designed to evaluate the effects of a comprehensive executive skill training program for school-aged children with ADHD in a relatively large sample.\nThe children (aged 6-12 years) with ADHD were randomized to the intervention or waitlist groups. A healthy control group was composed of gender- and age-matched healthy children. The intervention group received a 12-session training program for multiple executive skills. Executive function (EF), ADHD symptoms, and social functioning in the intervention and waitlist groups were evaluated at baseline and the end of the final training session. The healthy controls (HCs) were only assessed once at baseline. Repeated measures analyses of variance were used to compare EF, ADHD symptoms, and social function between intervention and waitlist groups.\nThirty-eight children with ADHD in intervention group, 30 in waitlist group, and 23 healthy children in healthy control group were included in final analysis. At posttreatment, intervention group showed significantly lower Behavior Rating Inventory of Executive Function (BRIEF) total score (135.89 \u00b1 16.80 vs. 146.09 \u00b1 23.92, P= 0.04) and monitoring score (18.05 \u00b1 2.67 vs. 19.77 \u00b1 3.10, P= 0.02), ADHD-IV overall score (41.11 \u00b1 7.48 vs. 47.20 \u00b1 8.47, P< 0.01), hyperactivity-impulsivity (HI) subscale score (18.92 \u00b1 5.09 vs. 21.93 \u00b1 4.93, P= 0.02), and inattentive subscale score (22.18 \u00b1 3.56 vs. 25.27 \u00b1 5.06, P< 0.01), compared with the waitlist group. Repeated measures analyses of variance revealed significant interactions between time and group on the BRIEF inhibition subscale (F = 5.06, P= 0.03), working memory (F = 4.48, P= 0.04), ADHD-IV overall score (F = 21.72, P< 0.01), HI subscale score (F = 19.08, P< 0.01), and inattentive subscale score (F = 12.40, P< 0.01). Multiple-way analysis of variance showed significant differences on all variables of BRIEF, ADHD-rating scale-IV, and WEISS Functional Impairment Scale-Parent form (WFIRS-P) among the intervention and waitlist groups at posttreatment and HCs at baseline.\nThis randomized controlled study on executive skill training in a relatively large sample provided some evidences that the training could improve EF deficits, reduce problematic symptoms, and potentially enhance the social functioning in school-aged children with ADHD.\nhttp://www.clinicaltrials.gov; NCT02327585.", "title": "Effect of an Ecological Executive Skill Training Program for School-aged Children with Attention Deficit Hyperactivity Disorder: A Randomized Controlled Clinical Trial.", "date": "2017-06-24"}]}
{"original_review": "33126293", "question_data": [{"question_id": 197, "question": "Is the risk of mortality higher, lower, or the same when comparing 1g paracetamol IV to saline IV?", "answer": "uncertain effect", "evidence_quality": "very low", "fulltext_required": "yes", "comment": "", "relevant_sources": ["26678710"]}], "sources": [{"article_id": "26678710", "content": "Strategies to prevent pyrexia in patients with acute neurological injury may reduce secondary neuronal damage. The aim of this study was to determine the safety and efficacy of the routine administration of 6 grams/day of intravenous paracetamol in reducing body temperature following severe traumatic brain injury, compared to placebo.\nA multicentre, randomised, blind, placebo-controlled clinical trial in adult patients with traumatic brain injury (TBI). Patients were randomised to receive an intravenous infusion of either 1g of paracetamol or 0.9% sodium chloride (saline) every 4 hours for 72 hours. The primary outcome was the mean difference in core temperature during the study intervention period.\nForty-one patients were included in this study: 21 were allocated to paracetamol and 20 to saline. The median (interquartile range) number of doses of study drug was 18 (17-18) in the paracetamol group and 18 (16-18) in the saline group (P = 0.85). From randomisation until 4 hours after the last dose of study treatment, there were 2798 temperature measurements (median 73 [67-76] per patient). The mean \u00b1 standard deviation temperature was 37.4\u00b10.5\u00b0C in the paracetamol group and 37.7\u00b10.4\u00b0C in the saline group (absolute difference -0.3\u00b0C; 95% confidence interval -0.6 to 0.0; P = 0.09). There were no significant differences in the use of physical cooling, or episodes of hypotension or hepatic abnormalities, between the two groups.\nThe routine administration of 6g/day of intravenous paracetamol did not significantly reduce core body temperature in patients with TBI.\nAustralian New Zealand Clinical Trials Registry ACTRN12609000444280.", "title": "The Effect of Paracetamol on Core Body Temperature in Acute Traumatic Brain Injury: A Randomised, Controlled Clinical Trial.", "date": "2015-12-19"}]}
{"original_review": "35514111", "question_data": [{"question_id": 198, "question": "Is the rate of any PCR\u2010positive SARS\u2010COV\u20102 infection higher, lower, or the same when comparing test\u2010based attendance to standard 10\u2010day self\u2010isolation?", "answer": "uncertain effect", "evidence_quality": "very low", "fulltext_required": "yes", "comment": "", "relevant_sources": ["34534517"]}, {"question_id": 199, "question": "Is the rate of COVID\u2010related absenteeism higher, lower, or the same when comparing test\u2010based attendance to standard 10\u2010day self\u2010isolation?", "answer": "no difference", "evidence_quality": "low", "fulltext_required": "no", "comment": "", "relevant_sources": ["34534517"]}, {"question_id": 200, "question": "Is the rate of symptomatic PCR\u2010positive SARS\u2010COV\u20102 infection higher, lower, or the same when comparing test\u2010based attendance to standard 10\u2010day self\u2010isolation?", "answer": "uncertain effect", "evidence_quality": "very low", "fulltext_required": "yes", "comment": "", "relevant_sources": ["34534517"]}, {"question_id": 201, "question": "Is the quality of life higher, lower, or the same when comparing test\u2010based attendance to standard 10\u2010day self\u2010isolation?", "answer": "insufficient data", "evidence_quality": "", "fulltext_required": "yes", "comment": "", "relevant_sources": ["34534517"]}, {"question_id": 202, "question": "Is SARS\u2010CoV\u20102\u2010related mortality higher, lower, or the same when comparing test\u2010based attendance to standard 10\u2010day self\u2010isolation?", "answer": "insufficient data", "evidence_quality": "", "fulltext_required": "yes", "comment": "", "relevant_sources": ["34534517"]}, {"question_id": 203, "question": "Is the rate of hospitalization higher, lower, or the same when comparing test\u2010based attendance to standard 10\u2010day self\u2010isolation?", "answer": "insufficient data", "evidence_quality": "", "fulltext_required": "yes", "comment": "", "relevant_sources": ["34534517"]}], "sources": [{"article_id": "34534517", "content": "School-based COVID-19 contacts in England have been asked to self-isolate at home, missing key educational opportunities. We trialled daily testing of contacts as an alternative to assess whether this resulted in similar control of transmission, while allowing more school attendance.\nWe did an open-label, cluster-randomised, controlled trial in secondary schools and further education colleges in England. Schools were randomly assigned (1:1) to self-isolation of school-based COVID-19 contacts for 10 days (control) or to voluntary daily lateral flow device (LFD) testing for 7 days with LFD-negative contacts remaining at school (intervention). Randomisation was stratified according to school type and size, presence of a sixth form, presence of residential students, and proportion of students eligible for free school meals. Group assignment was not masked during procedures or analysis. Coprimary outcomes in all students and staff were COVID-19-related school absence and symptomatic PCR-confirmed COVID-19, adjusted for community case rates, to estimate within-school transmission (non-inferiority margin <50% relative increase). Analyses were done on an intention-to-treat basis using quasi-Poisson regression, also estimating complier average causal effects (CACE). This trial is registered with the ISRCTN registry, ISRCTN18100261.\nBetween March 18 and May 4, 2021, 204 schools were taken through the consent process, during which three decided not to participate further. 201 schools were randomly assigned (control group n=99, intervention group n=102) in the 10-week study (April 19-May 10, 2021), which continued until the pre-appointed stop date (June 27, 2021). 76 control group schools and 86 intervention group schools actively participated; additional national data allowed most non-participating schools to be included in analysis of coprimary outcomes. 2432 (42\u00b74%) of 5763 intervention group contacts participated in daily contact testing. There were 657 symptomatic PCR-confirmed infections during 7\u2009782\u2009537 days-at-risk (59\u00b71 per 100\u2009000 per week) in the control group and 740 during 8\u2009379\u2009749 days-at-risk (61\u00b78 per 100\u2009000 per week) in the intervention group (intention-to-treat adjusted incidence rate ratio [aIRR] 0\u00b796 [95% CI 0\u00b775-1\u00b722]; p=0\u00b772; CACE aIRR 0\u00b786 [0\u00b755-1\u00b734]). Among students and staff, there were 59\u2009422 (1\u00b762%) COVID-19-related absences during 3\u2009659\u2009017 person-school-days in the control group and 51\u2009541 (1\u00b734%) during 3\u2009845\u2009208 person-school-days in the intervention group (intention-to-treat aIRR 0\u00b780 [95% CI 0\u00b754-1\u00b719]; p=0\u00b727; CACE aIRR 0\u00b761 [0\u00b730-1\u00b723]).\nDaily contact testing of school-based contacts was non-inferior to self-isolation for control of COVID-19 transmission, with similar rates of symptomatic infections among students and staff with both approaches. Infection rates in school-based contacts were low, with very few school contacts testing positive. Daily contact testing should be considered for implementation as a safe alternative to home isolation following school-based exposures.\nUK Government Department of Health and Social Care.", "title": "Daily testing for contacts of individuals with SARS-CoV-2 infection and attendance and SARS-CoV-2 transmission in English secondary schools and colleges: an open-label, cluster-randomised trial.", "date": "2021-09-18"}]}
{"original_review": "32441330", "question_data": [{"question_id": 204, "question": "Is loneliness at 6 months higher, lower, or the same when comparing video calls to usual care?", "answer": "uncertain effect", "evidence_quality": "very low", "fulltext_required": "yes", "comment": "\"We downgraded the certainty of this evidence by three levels for study limitations, imprecision and indirectness.\"", "relevant_sources": ["31992217", "22086660"]}, {"question_id": 205, "question": "Is loneliness at 1 year higher, lower, or the same when comparing video calls to usual care?", "answer": "uncertain effect", "evidence_quality": "very low", "fulltext_required": "yes", "comment": "\"We downgraded the certainty of this evidence by three levels for study limitations, imprecision and indirectness.\"", "relevant_sources": ["22086660"]}, {"question_id": 206, "question": "Is depression at 6 months higher, lower, or the same when comparing video calls to usual care?", "answer": "uncertain effect", "evidence_quality": "very low", "fulltext_required": "yes", "comment": "\"We downgraded the certainty of this evidence by three levels for study limitations, imprecision and indirectness.\"", "relevant_sources": ["31992217", "22086660"]}, {"question_id": 207, "question": "Is depression at 1 year higher, lower, or the same when comparing video calls to usual care?", "answer": "uncertain effect", "evidence_quality": "very low", "fulltext_required": "yes", "comment": "\"We downgraded the certainty of this evidence by three levels for study limitations, imprecision and indirectness.\"", "relevant_sources": ["22086660"]}, {"question_id": 208, "question": "Is physical-role quality of life higher, lower, or the same when comparing video calls to usual care?", "answer": "uncertain effect", "evidence_quality": "very low", "fulltext_required": "yes", "comment": "", "relevant_sources": ["31992217"]}], "sources": [{"article_id": "22086660", "content": "A 3-month videoconference interaction program with family members has been shown to decrease depression and loneliness in nursing home residents. However, little is known about the long-term effects on residents' depressive symptoms, social support, and loneliness.\nThe purpose of this longitudinal quasi-experimental study was to evaluate the long-term effectiveness of a videoconference intervention in improving nursing home residents' social support, loneliness, and depressive status over 1 year.\nWe purposively sampled 16 nursing homes in various areas of Taiwan. Elderly residents (N = 90) of these nursing homes meeting our inclusion criteria were divided into an experimental (n = 40) and a comparison (n = 50) group. The experimental group received at least 5 minutes/week for 3 months of videoconference interaction with their family members in addition to usual family visits, and the comparison group received regular family visits only. Data were collected in face-to face interviews on social support, loneliness, and depressive status using the Social Support Behaviors Scale, University of California Los Angeles Loneliness Scale, and Geriatric Depression Scale, respectively, at four times (baseline, 3 months, 6 months, and 12 months after baseline). Data were analyzed using the generalized estimating equation approach.\nAfter the videoconferencing program, participants in the experimental group had significantly lower mean change in instrumental social support scores at 6 months (-0.42, P = .03) and 12 months (-0.41, P = .03), and higher mean change in emotional social support at 3 (0.74, P < .001) and 12 months (0.61, P = .02), and in appraisal support at 3 months (0.74, P = .001) after adjusting for confounding variables. Participants in the experimental group also had significantly lower mean loneliness and depressive status scores at 3 months (-5.40, P < .001; -2.64, P < .001, respectively), 6 months (-6.47, P < .001; -4.33, P < .001), and 12 months (-6.27, P = .001; -4.40, P < .001) compared with baseline than those in the comparison group.\nOur videoconference program had a long-term effect in alleviating depressive symptoms and loneliness for elderly residents in nursing homes. This intervention also improved long-term emotional social support and short-term appraisal support, and decreased residents' instrumental social support. However, this intervention had no effect on informational social support.", "title": "Changes in depressive symptoms, social support, and loneliness over 1 year after a minimum 3-month videoconference program for older nursing home residents.", "date": "2011-11-17"}, {"article_id": "31992217", "content": "Smartphones can optimize the opportunities for interactions between nursing home residents and their families. However, the effectiveness of smartphone-based videoconferencing programs in enhancing emotional status and quality of life has not been explored. The purpose of this study was to evaluate of the effect of a smartphone-based videoconferencing program on nursing home residents' feelings of loneliness, depressive symptoms and quality of life.\nThis study used a quasi-experimental research design. Older residents from seven nursing homes in Taiwan participated in this study. Nursing homes (NH) were randomly selected as sites for either the intervention group (5 NH) or the control group (2 NH); NH residents who met the inclusion criteria were invited to participate. The intervention group was comprised of 32 participants; the control group was comprised of 30 participants. The intervention group interacted with their family members once a week for 6\u2009months using a smartphone and a \"LINE\" application (app). Data were collected with self-report instruments: subjective feelings of loneliness, using the University of California Los Angeles Loneliness Scale; depressive symptoms, using the Geriatric Depression Scale; and quality of life using the SF-36. Data were collected at four time points (baseline, and at 1-month, 3-months and 6-months from baseline). Data were analysed using the generalized estimating equation approach.\nAfter the intervention, as compared to those in the control group, participants in interventional group had significant decreases in baseline loneliness scores at 1\u2009months (\u03b2\u2009=\u2009-\u20093.41, p\u2009<\u20090.001), 3\u2009months (\u03b2\u2009=\u2009-\u20095.96, p\u2009<\u20090.001), and 6\u2009months (\u03b2\u2009=\u2009-\u20097.50, p\u2009<\u20090.001), and improvements in physical role (\u03b2\u2009=\u200936.49, p\u2009=\u20090.01), vitality (\u03b2\u2009=\u200913.11, p\u2009<\u20090.001) and pain scores (\u03b2\u2009=\u200916.71, p\u2009=\u20090.01) at 6\u2009months. However, changes in mean depression scores did not significantly differ between groups.\nSmartphone-based videoconferencing effectively improved residents' feelings of loneliness, and physiological health, vitality and pain, but not depressive symptoms. Future investigations might evaluate the effectiveness of other media-based technologies in nursing homes as well as their effectiveness within and between different age cohorts.", "title": "Effects of a smartphone-based videoconferencing program for older nursing home residents on depression, loneliness, and quality of life: a quasi-experimental study.", "date": "2020-01-30"}, {"article_id": "21069600", "content": "The purpose of this quasi-experimental study was to evaluate the effectiveness of a videoconference intervention program in improving nursing home residents' social support, loneliness, and depressive status.\nFourteen nursing homes were selected from various areas of Taiwan by purposive sampling. Elderly residents (N = 57) of these nursing homes, who met our inclusion criteria were divided into experimental (n = 24) and control (n = 33) groups. The experimental group received five min/week of videoconference interaction with their family members for three months, and the control group received regular care only. Data were collected through face-to face interviews on social support, loneliness, and depressive status using the Social Supportive Behavior Scale, University of California Los Angeles Loneliness Scale, and Geriatric Depression Scale, respectively, at three points (baseline, one week, and three months after baseline). Data were analyzed using the generalized estimating equation approach.\nSubjects in the experimental group had significantly higher mean emotional and appraisal social support scores at one week and three months after baseline than those in the control group. Subjects in the experimental group also had lower mean loneliness scores at one week and three months after baseline than those in the control group, and lower mean depressive status scores at three months after baseline.\nOur videoconference program alleviated depressive symptoms and loneliness in elderly residents in nursing homes. Our findings suggest that this program could be used for residents of long-term care institutions, particularly those with better ability to perform activities of daily living.", "title": "Videoconference program enhances social support, loneliness, and depressive status of elderly nursing home residents.", "date": "2010-11-12"}]}
{"original_review": "30977111", "question_data": [{"question_id": 209, "question": "Is anxiety higher, lower, or the same when comparing antidepressants to placebo?", "answer": "uncertain effect", "evidence_quality": "low", "fulltext_required": "yes", "comment": "fulltext needed for exact stats from 27664274", "relevant_sources": ["27664274", "21936344", "26600836"]}, {"question_id": 210, "question": "Is depression higher, lower, or the same when comparing antidepressants to placebo?", "answer": "uncertain effect", "evidence_quality": "low", "fulltext_required": "yes", "comment": "fulltext needed for exact stats from 27664274", "relevant_sources": ["27664274", "21936344", "26600836"]}, {"question_id": 211, "question": "Is the risk of adverse events higher, lower, or the same when comparing antidepressants to placebo?", "answer": "uncertain effect", "evidence_quality": "low", "fulltext_required": "yes", "comment": "called \"side effects\" in the original paper", "relevant_sources": ["27664274"]}, {"question_id": 212, "question": "Is maintenance of clinical remission higher, lower, or the same when comparing antidepressants to placebo?", "answer": "uncertain effect", "evidence_quality": "low", "fulltext_required": "yes", "comment": "", "relevant_sources": ["27664274"]}], "sources": [{"article_id": "27664274", "content": "Previous studies have shown that antidepressants reduce inflammation in animal models of colitis. The present trial aimed to examine whether fluoxetine added to standard therapy for Crohn's disease [CD] maintained remission, improved quality of life [QoL] and/or mental health in people with CD as compared to placebo.\nA parallel randomized double-blind placebo controlled trial was conducted. Participants with clinically established CD, with quiescent or only mild disease, were randomly assigned to receive either fluoxetine 20 mg daily or placebo, and followed for 12 months. Participants provided blood and stool samples and completed mental health and QoL questionnaires. Immune functions were assessed by stimulated cytokine secretion [CD3/CD28 stimulation] and flow cytometry for cell type. Linear mixed-effects models were used to compare groups.\nOf the 26 participants, 14 were randomized to receive fluoxetine and 12 to placebo. Overall, 14 [54%] participants were male. The mean age was 37.4 [SD=13.2] years. Fluoxetine had no effect on inflammatory bowel disease activity measured using either the Crohn's Disease Activity Index [F(3, 27.5)=0.064, p=0.978] or faecal calprotectin [F(3, 32.5)=1.08, p=0.371], but did have modest effects on immune function. There was no effect of fluoxetine on physical, psychological, social or environmental QoL, anxiety or depressive symptoms as compared to placebo [all p>0.05].\nIn this small pilot clinical trial, fluoxetine was not superior to placebo in maintaining remission or improving QoL. [ID: ACTRN12612001067864.].", "title": "Fluoxetine for Maintenance of Remission and to Improve Quality of Life in Patients with Crohn's Disease: a Pilot Randomized Placebo-Controlled Trial.", "date": "2016-11-01"}, {"article_id": "26600836", "content": "Treating inflammatory bowel disease (IBD) with antidepressants might be of utility to improve patient's condition. The aim of this study was to assess the efficacy of Duloxetine on depression, anxiety, severity of symptoms, and quality of life (QOL) in IBD patients.\nIn a randomized, double-blind, controlled clinical trial on 2013-2014, in Alzahra Hospital (Isfahan, Iran), 44 IBD patients were chosen to receive either duloxetine (60 mg/day) or placebo. They were treated in a 12 weeks program, and all of the participants also received mesalazine, 2-4 g daily. We assessed anxiety and depression with Hospital Anxiety and Depression Scale, the severity of symptoms with Lichtiger Colitis Activity Index and QOL with World Health Organization Quality of Life Instruments, before and just after the treatment. The data were analyzed using Paired sample t-test and ANCOVA.\nIn 35 subjects who completed the study, the mean (standard error [SE]) scores of depression and anxiety were reduced in duloxetine more than placebo group, significantly (P = 0.041 and P = 0.049, respectively). The mean (SE) scores of severity of symptom were also reduced in duloxetine more than the placebo group, significantly (P = 0.02). The mean (SE) scores of physical, psychological, and social dimensions of QOL were increased after treatment with duloxetine more than placebo group, significantly (P = 0.001, P = 0.038, and P = 0.015, respectively). The environmental QOL was not increased significantly (P = 0.260).\nDuloxetine is probably effective and safe for reducing depression, anxiety and severity of physical symptoms. It also could increase physical, psychological, and social QOL in patients.", "title": "Efficacy of duloxetine add on in treatment of inflammatory bowel disease patients: A double-blind controlled study.", "date": "2015-11-26"}, {"article_id": "22234954", "content": "Depression, like adverse events and psychological stress, can trigger relapse in inflammatory bowel disease (IBD); however, the effects of psychoactive drugs on disease course are unclear.\nUsing retrospective electronic case note review, after exclusion of five patients on low-dose tricyclic antidepressants we compared the course of IBD in 29 patients (14 ulcerative colitis and 15 Crohn's disease), during the years before (year 1) and after (year 2) they were started on an antidepressant for a concomitant mood disorder to that of controls matched for age, sex, disease type, medication at baseline, and relapse rate in year 1.\nPatients had fewer relapses and courses of steroids in the year after starting an antidepressant than in the year before (1 [0-4] (median [range]) vs. 0 [0-4], P = 0.002; 1 [0-3] vs. 0 [0-4], P < 0.001, respectively); the controls showed no changes between years 1 and 2 in relapses (1 [0-4] vs. 1 [0-3], respectively) or courses of steroids (1 [0-2] vs. 0 [0-3]). Although there were no differences in the use of other relapse-related medications, outpatient attendances, or hospital admissions, the number of endoscopies fell significantly in the antidepressant group in year 2 compared with year 1 (P < 0.01). No such changes were seen in the controls.\nAntidepressants, when used to treat concomitant mood disorders in IBD, seem to reduce relapse rates, use of steroids, and endoscopies in the year after their introduction. These results suggest the need for a prospective controlled trial to evaluate their effects on disease course in patients with IBD.", "title": "Do antidepressants influence the disease course in inflammatory bowel disease? A retrospective case-matched observational study.", "date": "2012-01-12"}, {"article_id": "21936344", "content": "In order to maintain ulcerative colitis (UC) in remission, chronic use of aminosalicylates is recommended. All patients have a fear of the recurrence of symptoms, which makes their mental state and quality of life worse. Because of this a number of patients are recommended to use different sorts of anxiolytic drugs and antidepressants.\nEvaluation of the influence of tianeptine (selective serotonin reuptake enhancer) on the mental and somatic status in the group of patients.\nThe research was conducted in two groups of thirty patients, with benign form of ulcerative colitis in remission, aged 24-46 years. Patients, during a period of 12 months, were given aminosalicylates in a daily doses 2 x 1.0 g and tianeptine in a doses 3 x 12.5 mg (group I) or placebo (group II). During the treatment every three months anxiety (Hamilton Anxiety Rating Scale-HARS), depression (Back Depression Inventory-BDI), The Mayo Clinic Disease Activity Index (MCDAI), hemoglobin and C-reactive protein (CRP) level were evaluated.\nAfter 12 months in a group of patients who took tianeptine decrease in anxiety (from 20.35 +/- 4.03 to 12.65 +/- 3.78 points) and depression (from 19.95 +/- 4.49 points to 9.60 +/- 2.76 points) was obtained; difference compared with placebo was statistically significant (p < 0.01). At the same time significant decrease compared with placebo (p < 0.05) of disease activity index (respectively 3.05 +/- 1.36 and 4.65 +/- 1.69), insignificantly lower level of CRP (7.00 5.65 and 9.41 +/- 10.12) and higher level of hemoglobin (11.93 +/- 0.83 and 11.0 +/- 0.70) was observed.\nTianeptine has a positive influence on mental and somatic status of patients with UC. Results give the support for tianeptine apllication in UC as adjuvant drug.", "title": "[Evaluation of the influence of tianeptine on the psychosomatic status of patients with ulcerative colitis in remission].", "date": "2011-09-23"}]}
{"original_review": "37594020", "question_data": [{"question_id": 213, "question": "Is pain relief at 48 hours higher, lower, or the same when comparing paracetamol to placebo?", "answer": "higher", "evidence_quality": "low", "fulltext_required": "no", "comment": "", "relevant_sources": ["8871138"]}, {"question_id": 214, "question": "Is pain relief at 48 hours higher, lower, or the same when comparing NSAIDs to placebo?", "answer": "higher", "evidence_quality": "low", "fulltext_required": "no", "comment": "", "relevant_sources": ["8871138"]}], "sources": [{"article_id": "18782838", "content": "To estimate the cost to the NHS and to parents and carers of treating febrile preschool children with paracetamol, ibuprofen, or both, and to compare these costs with the benefits of each treatment regimen.\nCost consequences analysis and cost effectiveness analysis conducted as part of a three arm, randomised controlled trial.\nChildren between the ages of 6 months and 6 years recruited from primary care and the community with axillary temperatures >or=37.8 degrees C and <or=41 degrees C.\nParacetamol, ibuprofen, or both drugs.\nCosts to the NHS and to parents and carers. Cost consequences analysis at 48 hours and 5 days comparing cost with children's temperature, discomfort, activity, appetite, and sleep; cost effectiveness analysis at 48 hours comparing cost with percentage of children \"recovered.\"\nDifficulties in recruiting children to the trial lowered the precision of the estimates of cost and some outcomes. At 48 hours, cost to the NHS was pound11.33 for paracetamol, pound8.49 for ibuprofen, and pound8.16 for both drugs. By day 5 these costs rose to pound19.63, pound18.36, and pound13.92 respectively. For parents and carers, the 48 hour costs were pound23.86 for paracetamol, pound20.60 for ibuprofen, and pound25.07 for both, and the day 5 costs were pound26.35, pound29.90, and pound24.02 respectively. Outcomes measured at 48 hours and 5 days were inconclusive because of lack of power; the cost effectiveness analysis at 48 hours provided little evidence that one treatment choice was significantly more cost effective than another. At 4 hours ibuprofen and the combined treatment were superior to paracetamol in terms of the trial primary outcome of time without fever; at 24 hours the combined treatment performed best on this outcome.\nThere is no strong evidence of a difference in cost between the treatments, but clinical and cost data together indicate that using both drugs together may be most cost effective over the course of the illness. This treatment option performs best and is no more expensive because of less use of healthcare resources, resulting in lower costs to the NHS and to parents.", "title": "Paracetamol plus ibuprofen for the treatment of fever in children (PITCH): economic evaluation of a randomised controlled trial.", "date": "2008-09-11"}, {"article_id": "8871138", "content": "Two hundred and nineteen children (boys: 56%, girls: 44%) were included in a randomized, double-blind, multicentre (4 centres) controlled trial designed to assess the efficacy and safety of ibuprofen (IBU) in the treatment of 1 to 6 year-old children with otoscopically proven acute otitis media (AOM), either unilateral or bilateral. They randomly received 10 mg/kg IBU (n = 71), or acetaminophen (PARA) (n = 73) or placebo (PLA) (n = 75), orally, tid, for 48 hours. All received oral cefaclor (Alfatil, Lilly, France) for seven days. They were evaluated before (D0) and at the end of treatment (D2). The main criterion of response was the aspect (landmarks and color) of the tympanic membrane assessed on a semi-quantitative scale from 0 to 6. Other criteria, assessed on semi-quantitative scales, included relief of pain (0 or 1), rectal temperature (0 to 2), and overall evaluation by parents of the improvement of quality of life on three items: appetite (0 to 2), sleep (0 to 2), and playing activity (0 to 2). The results at D2 were as follows: there was no significant difference between treatment groups as to the main criterion, but only a trend for IBU and PARA to do better than PLA but not for IBU to do better than PARA. From these data there is no argument to emphasize the utility of non-steroidal anti-inflammatory drugs (NSAIDs) in treating the inflammatory signs of the tympanic membrane in otitis. There was a statistically significant difference between treatment groups at D2 for pain, IBU being superior to PLA (P < 0.01): 7%, 10% and 25% of the children were still suffering at D2 in the IBU, PARA and PLA treatment groups, respectively. The difference between PARA and PLA for pain was not statistically significant. There was no significant difference between treatment groups for the other criteria. All treatments were well and equally tolerated. Although no significant difference was found between the treatment groups on the aspect of the tympanic membrane, the efficacy of IBU was evidenced on the relief of pain, the symptom that most disturbs the child.", "title": "A randomized, double-blind, multicentre controlled trial of ibuprofen versus acetaminophen and placebo for symptoms of acute otitis media in children.", "date": "1996-01-01"}, {"article_id": "35325846", "content": "Acute otitis media (AOM) is one of the most common childhood infections. Ear pain, the main symptom of AOM, results in parents frequently seeking medical assistance for their children. The aim of this study was to compare the effectiveness of topical 1% lidocaine ear drops administered with oral analgesics with that of oral analgesics alone.\nThis multicenter randomized, open-labeled study was conducted at 15 centers with 184 pediatric AOM patients with bilateral ear pain (aged 1-5 years) between May 1, 2016, and June 31, 2018. All patients received oral paracetamol or ibuprofen and topical 1% lidocaine, which was administered to each ear according to the randomization list. The ear pain score was evaluated within 48\u00a0h using the Face, Legs, Activity, Cry, and Consolability (FLACC) scale, and the patients were followed up for 10 days.\nThe median age was 31.8 months (min-max, 12-84.2 months). Of those patients enrolled, 22.3% received paracetamol, and 24.5% received paracetamol with lidocaine ear drops; 23.4% received ibuprofen, and 29.9% received ibuprofen with lidocaine ear drops. Lower pain scores were significantly measured at baseline and 10th minutes by a reduction 25% (RR 13.64, 95% CI 4.47-41.63, p\u00a0=\u00a00.001, RR 0.14, 95% CI 0.06-0.35, p\u00a0=\u00a00.001) and 50% (RR 4.76, 95% CI 1.63-13.87, p\u00a0=\u00a00.004, RR 0.14, 95% CI 0.05-0.4, p\u00a0=\u00a00.001) in the paracetamol and lidocaine versus paracetamol groups and the ibuprofen and lidocaine versus ibuprofen groups, respectively. No serious side effects were evident during follow-up.\nThis randomized study suggests that topical 1% lidocaine ear drops with paracetamol or ibuprofen seems to provide effective and rapid relief for children presenting with ear pain attributed to AOM.", "title": "The effectiveness of topical 1% lidocaine with systemic oral analgesics for ear pain with acute otitis media.", "date": "2022-03-25"}, {"article_id": "19454182", "content": "To establish the relative clinical effectiveness and cost-effectiveness of paracetamol plus ibuprofen compared with paracetamol and ibuprofen separately for time without fever, and the relief of fever-associated discomfort in young children who can be managed at home.\nThe trial design was a single-centre (multisite), individually randomised, blinded, three-arm trial comparing paracetamol and ibuprofen together with paracetamol or ibuprofen separately.\nThere were three recruitment settings, as follows: 'local' where research nurses were recruited from NHS primary care sites; 'remote' where NHS sites notified the study of potentially eligible children; and 'community' where parents contacted the study in response to local media advertisements.\nChildren aged between 6 months and 6 years with fever > or = 37.8 degrees C and < or = 41 degrees C due to an illness that could be managed at home.\nThe intervention was the provision of, and advice to give, the medicines for up to 48 hours: paracetamol every 4-6 hours (maximum of four doses in 24 hours) and ibuprofen every 6-8 hours (maximum of three doses in 24 hours). Every parent received two bottles, with at least one containing an active medicine. Parents, research nurses and investigators were blinded to treatment allocation by the use of identically matched placebo medicines. The dose of medicine was determined by the child's weight: paracetamol 15 mg/kg and ibuprofen 10 mg/kg per dose.\nFor additional time without fever in the first 4 hours, use of both medicines was superior to use of paracetamol alone [adjusted difference 55 minutes, 95% confidence interval (CI) 33 to 77 minutes; p < 0.001] and may have been as good as ibuprofen (adjusted difference 16 minutes, 95% CI -6 to 39 minutes; p = 0.2). Both medicines together cleared the fever 23 minutes (95% CI 2-45 minutes; p = 0.015) faster than paracetamol alone, but no faster than ibuprofen alone (adjusted difference -3 minutes, 95% CI 24-18 minutes; p = 0.8). For additional time without fever in the first 24 hours, both medicines were superior to paracetamol (adjusted difference 4.4 hours, 95% CI 2.4-6.3 hours; p < 0.001) or ibuprofen (adjusted difference 2.5 hours, 95% CI 0.6-4.5 hours; p = 0.008) alone. No reduction in discomfort or other fever-associated symptoms was found, although power was low for these outcomes. An exploratory analysis showed that children with higher discomfort levels had higher mean temperatures. No difference in adverse effects was observed between treatment groups. The recommended maximum number of doses of paracetamol and ibuprofen in 24 hours was exceeded in 8% and 11% of children respectively. Over the 5-day study period, paracetamol and ibuprofen together was the cheapest option for the NHS due to the lower use of health-care services:14 pounds [standard deviation (SD) 23 pounds] versus 20 pounds (SD 38 pounds) for paracetamol and 18 pounds (SD 40 pounds) for ibuprofen. Both medicines were also cheapest for parents because the lower use of health care services resulted in personal saving on travel costs and less time off work: 24 pounds (SD 46 pounds) versus 26 pounds (SD 63 pounds) for paracetamol and 30 pounds (SD 91 pounds) for ibuprofen. This more than compensated for the extra cost of medication. However, statistical evidence for these differences was weak due to lack of power. Overall, a quarter of children were 'back to normal' by 48 hours and one-third by day 5. Five (3%) children were admitted to hospital, two with pneumonia, two with bronchiolitis and one with a severe, but unidentified 'viral illness'.\nYoung children who are unwell with fever should be treated with ibuprofen first, but the relative risks (inadvertently exceeding the maximum recommended dose) and benefits (extra 2.5 hours without fever) of using paracetamol plus ibuprofen over 24 hours should be considered. However, if two medicines are used, it is recommended that all dose times are carefully recorded to avoid accidentally exceeding the maximum recommended dose. Manufacturers should consider supplying blank charts for this purpose. Use of both medicines should not be discouraged on the basis of cost to either parents or the NHS. Parents and clinicians should be aware that fever is a relatively short-lived symptom, but may have more serious prognostic implications than the other common symptom presentations of childhood.", "title": "Paracetamol and ibuprofen for the treatment of fever in children: the PITCH randomised controlled trial.", "date": "2009-05-21"}, {"article_id": "24162940", "content": "To assess strategies for advice on analgesia and steam inhalation for respiratory tract infections.\nOpen pragmatic parallel group factorial randomised controlled trial.\nPrimary care in United Kingdom.\nPatients aged \u2265 3 with acute respiratory tract infections.\n889 patients were randomised with computer generated random numbers in pre-prepared sealed numbered envelopes to components of advice or comparator advice: advice on analgesia (take paracetamol, ibuprofen, or both), dosing of analgesia (take as required v regularly), and steam inhalation (no inhalation v steam inhalation).\nPrimary: mean symptom severity on days 2-4; symptoms rated 0 (no problem) to 7 (as bad as it can be). Secondary: temperature, antibiotic use, reconsultations.\nNeither advice on dosing nor on steam inhalation was significantly associated with changes in outcomes. Compared with paracetamol, symptom severity was little different with ibuprofen (adjusted difference 0.04, 95% confidence interval -0.11 to 0.19) or the combination of ibuprofen and paracetamol (0.11, -0.04 to 0.26). There was no evidence for selective benefit with ibuprofen among most subgroups defined before analysis (presence of otalgia; previous duration of symptoms; temperature >37.5 \u00b0C; severe symptoms), but there was evidence of reduced symptoms severity benefit in the subgroup with chest infections (ibuprofen -0.40, -0.78 to -0.01; combination -0.47; -0.84 to -0.10), equivalent to almost one in two symptoms rated as a slight rather than a moderately bad problem. Children might also benefit from treatment with ibuprofen (ibuprofen: -0.47, -0.76 to -0.18; combination: -0.04, -0.31 to 0.23). Reconsultations with new/unresolved symptoms or complications were documented in 12% of those advised to take paracetamol, 20% of those advised to take ibuprofen (adjusted risk ratio 1.67, 1.12 to 2.38), and 17% of those advised to take the combination (1.49, 0.98 to 2.18). Mild thermal injury with steam was documented for four patients (2%) who returned full diaries, but no reconsultations with scalding were documented.\nOverall advice to use steam inhalation, or ibuprofen rather than paracetamol, does not help control symptoms in patients with acute respiratory tract infections and must be balanced against the possible progression of symptoms during the next month for a minority of patients. Advice to use ibuprofen might help short term control of symptoms in those with chest infections and in children.\nISRCTN 38551726.", "title": "Ibuprofen, paracetamol, and steam for patients with respiratory tract infections in primary care: pragmatic randomised factorial trial.", "date": "2013-10-29"}]}
{"original_review": "30039850", "question_data": [{"question_id": 215, "question": "Is the risk of cryptococcal IRIS events higher, lower, or the same when comparing early ART initiation to delayed ART initiation?", "answer": "uncertain effect", "evidence_quality": "very low", "fulltext_required": "yes", "comment": "fulltext needed for 24963568", "relevant_sources": ["24963568", "23362285"]}, {"question_id": 216, "question": "Is virological suppression at 6 months higher, lower, or the same when comparing early ART initiation to delayed ART initiation?", "answer": "uncertain effect", "evidence_quality": "very low", "fulltext_required": "yes", "comment": "23362285 omitted with 12% weight", "relevant_sources": ["24963568"]}, {"question_id": 217, "question": "Is all-cause mortality at 6-12 months higher, lower, or the same when comparing early ART initiation to delayed ART initiation?", "answer": "higher", "evidence_quality": "low", "fulltext_required": "yes", "comment": "", "relevant_sources": ["23362285", "24963568", "20415574", "19440326"]}, {"question_id": 218, "question": "Is the risk of cryptococcal meningitis relapse higher, lower, or the same when comparing early ART initiation to delayed ART initiation?", "answer": "lower", "evidence_quality": "low", "fulltext_required": "yes", "comment": "23362285 omitted with 15.9% weight", "relevant_sources": ["24963568"]}], "sources": [{"article_id": "24963568", "content": "Cryptococcal meningitis accounts for 20 to 25% of acquired immunodeficiency syndrome-related deaths in Africa. Antiretroviral therapy (ART) is essential for survival; however, the question of when ART should be initiated after diagnosis of cryptococcal meningitis remains unanswered.\nWe assessed survival at 26 weeks among 177 human immunodeficiency virus-infected adults in Uganda and South Africa who had cryptococcal meningitis and had not previously received ART. We randomly assigned study participants to undergo either earlier ART initiation (1 to 2 weeks after diagnosis) or deferred ART initiation (5 weeks after diagnosis). Participants received amphotericin B (0.7 to 1.0 mg per kilogram of body weight per day) and fluconazole (800 mg per day) for 14 days, followed by consolidation therapy with fluconazole.\nThe 26-week mortality with earlier ART initiation was significantly higher than with deferred ART initiation (45% [40 of 88 patients] vs. 30% [27 of 89 patients]; hazard ratio for death, 1.73; 95% confidence interval [CI], 1.06 to 2.82; P=0.03). The excess deaths associated with earlier ART initiation occurred 2 to 5 weeks after diagnosis (P=0.007 for the comparison between groups); mortality was similar in the two groups thereafter. Among patients with few white cells in their cerebrospinal fluid (<5 per cubic millimeter) at randomization, mortality was particularly elevated with earlier ART as compared with deferred ART (hazard ratio, 3.87; 95% CI, 1.41 to 10.58; P=0.008). The incidence of recognized cryptococcal immune reconstitution inflammatory syndrome did not differ significantly between the earlier-ART group and the deferred-ART group (20% and 13%, respectively; P=0.32). All other clinical, immunologic, virologic, and microbiologic outcomes, as well as adverse events, were similar between the groups.\nDeferring ART for 5 weeks after the diagnosis of cryptococcal meningitis was associated with significantly improved survival, as compared with initiating ART at 1 to 2 weeks, especially among patients with a paucity of white cells in cerebrospinal fluid. (Funded by the National Institute of Allergy and Infectious Diseases and others; COAT ClinicalTrials.gov number, NCT01075152.).", "title": "Timing of antiretroviral therapy after diagnosis of cryptococcal meningitis.", "date": "2014-06-26"}, {"article_id": "19440326", "content": "Optimal timing of ART initiation for individuals presenting with AIDS-related OIs has not been defined.\nA5164 was a randomized strategy trial of \"early ART\"--given within 14 days of starting acute OI treatment versus \"deferred ART\"--given after acute OI treatment is completed. Randomization was stratified by presenting OI and entry CD4 count. The primary week 48 endpoint was 3-level ordered categorical variable: 1. Death/AIDS progression; 2. No progression with incomplete viral suppression (ie HIV viral load (VL) >or=50 copies/ml); 3. No progression with optimal viral suppression (ie HIV VL <50 copies/ml). Secondary endpoints included: AIDS progression/death; plasma HIV RNA and CD4 responses and safety parameters including IRIS. 282 subjects were evaluable; 141 per arm. Entry OIs included Pneumocytis jirovecii pneumonia 63%, cryptococcal meningitis 12%, and bacterial infections 12%. The early and deferred arms started ART a median of 12 and 45 days after start of OI treatment, respectively. THE DIFFERENCE IN THE PRIMARY ENDPOINT DID NOT REACH STATISTICAL SIGNIFICANCE: AIDS progression/death was seen in 20 (14%) vs. 34 (24%); whereas no progression but with incomplete viral suppression was seen in 54 (38%) vs. 44 (31%); and no progression with optimal viral suppression in 67 (48%) vs 63 (45%) in the early vs. deferred arm, respectively (p = 0.22). However, the early ART arm had fewer AIDS progression/deaths (OR = 0.51; 95% CI = 0.27-0.94) and a longer time to AIDS progression/death (stratified HR = 0.53; 95% CI = 0.30-0.92). The early ART had shorter time to achieving a CD4 count above 50 cells/mL (p<0.001) and no increase in adverse events.\nEarly ART resulted in less AIDS progression/death with no increase in adverse events or loss of virologic response compared to deferred ART. These results support the early initiation of ART in patients presenting with acute AIDS-related OIs, absent major contraindications.\nClinicalTrials.gov NCT00055120.", "title": "Early antiretroviral therapy reduces AIDS progression/death in individuals with acute opportunistic infections: a multicenter randomized strategy trial.", "date": "2009-05-15"}, {"article_id": "23362285", "content": "The burden of Cryptococcus neoformans in cerebrospinal fluid (CSF) predicts clinical outcomes in human immunodeficiency virus (HIV)-associated cryptococcal meningitis (CM) and is lower in patients on antiretroviral therapy (ART). This study tested the hypothesis that initiation of ART during initial treatment of HIV/CM would improve CSF clearance of C. neoformans.\nA randomized treatment-strategy trial was conducted in Botswana. HIV-infected, ART-naive adults aged\u226521 years initiating amphotericin B treatment for CM were randomized to ART initiation within 7 (intervention) vs after 28 days (control) of randomization, and the primary outcome of the rate of CSF clearance of C. neoformans over the subsequent 4 weeks was compared. Adverse events, including CM immune reconstitution inflammatory syndrome (CM-IRIS), and immunologic and virologic responses were compared over 24 weeks.\nAmong 27 subjects enrolled (13 intervention and 14 control), [corrected] the median times to ART initiation were 7 (interquartile range [IQR], 5\u201310) and 32days (IQR, 28\u201336), respectively. The estimated rate of CSF clearance did not differ significantly by treatment strategy (-0.32 log10 colony-forming units [CFU]/mL/day\u00b10.20 intervention and -0.52 log10 CFUs/mL/day (\u00b10.48) control, P=.4). Two of 13 (15%) and 5 of 14 (36%) subjects died in the intervention and control arms, respectively (P=0.39). Seven of 13 subjects (54%) in the intervention arm vs 0 of 14 in the control arm experienced CM-IRIS (P=.002).\nEarly ART was not associated with improved CSF fungal clearance, but resulted in a high risk of CM-IRIS. Further research on optimal incorporation of ART into CM care is needed.\nNCT00976040.", "title": "Early versus delayed antiretroviral therapy and cerebrospinal fluid fungal clearance in adults with HIV and cryptococcal meningitis.", "date": "2013-01-31"}, {"article_id": "20415574", "content": "BACKGROUND. Cryptococcal meningitis (CM) remains a leading cause of acquired immunodeficiency syndrome-related death in sub-Saharan Africa. The timing of the initiation of antiretroviral therapy (ART) for human immunodeficiency virus (HIV)-associated CM remains uncertain. The study aimed to determine the optimal timing for initiation of ART in HIV-positive individuals with CM. METHODS. A prospective, open-label, randomized clinical trial was conducted at a tertiary teaching hospital in Zimbabwe. Participants were aged > or = 18 years, were ART naive, had received a first CM diagnosis, and were randomized to receive early ART (within 72 h after CM diagnosis) or delayed ART (after 10 weeks of treatment with fluconazole alone). Participants received 800 mg of fluconazole per day. The ART regimen used was stavudine, lamivudine, and nevirapine given twice daily. The duration of follow-up was up to 3 years. The primary end point was all-cause mortality. RESULTS. Fifty-four participants were enrolled in the study (28 in the early ART arm and 26 in the delayed ART arm). The median CD4 cell count at enrollment was 37 cells/mm(3) (interquartile range, 17-69 cells/mm(3)). The 3-year mortality rate differed significantly between the early and delayed ART groups (88% vs 54%; P < .006); the overall 3-year mortality rate was 73%. The median durations of survival were 28 days and 637 days in the early and delayed ART groups, respectively (P = .031, by log-rank test). The risk of mortality was almost 3 times as great in the early ART group versus the delayed ART group (adjusted hazard ratio, 2.85; 95% confidence interval, 1.1-7.23). The study was terminated early by the data safety monitoring committee. CONCLUSIONS. In resource-limited settings where CM management may be suboptimal, when compared with a delay of 10 weeks after a CM diagnosis, early initiation of ART results in increased mortality. Trial registration. ClinicalTrials.gov identifier: NCT00830856.", "title": "Early versus delayed initiation of antiretroviral therapy for concurrent HIV infection and cryptococcal meningitis in sub-saharan Africa.", "date": "2010-04-27"}]}
{"original_review": "28282491", "question_data": [{"question_id": 243, "question": "Is rheumatoid arthritis remission rate higher, lower, or the same when comparing tofacitinib monotherapy to placebo?", "answer": "insufficient data", "evidence_quality": "", "fulltext_required": "no", "comment": "", "relevant_sources": ["16162882", "12115176", "24972708", "23294500"]}, {"question_id": 244, "question": "Is radiographic progression higher, lower, or the same when comparing tofacitinib and MTX to MTX and placebo?", "answer": "insufficient data", "evidence_quality": "", "fulltext_required": "no", "comment": "", "relevant_sources": ["23294500"]}, {"question_id": 245, "question": "Is radiographic progression higher, lower, or the same when comparing biologic + MTX to only MTX/other traditional DMARDs?", "answer": "insufficient data", "evidence_quality": "", "fulltext_required": "no", "comment": "", "relevant_sources": ["16947627", "18625622", "19560810"]}, {"question_id": 246, "question": "Is patient function measured by HAQ higher, lower, or the same when comparing biologic monotherapy to placebo?", "answer": "insufficient data", "evidence_quality": "", "fulltext_required": "no", "comment": "", "relevant_sources": ["16162882", "12115176", "24972708"]}, {"question_id": 247, "question": "Is radiographic progression higher, lower, or the same when comparing biologic monotherapy to placebo?", "answer": "insufficient data", "evidence_quality": "", "fulltext_required": "no", "comment": "", "relevant_sources": ["16162882", "12115176", "24972708"]}, {"question_id": 248, "question": "Is cancer risk higher, lower, or the same when comparing biologic monotherapy to placebo?", "answer": "insufficient data", "evidence_quality": "", "fulltext_required": "no", "comment": "", "relevant_sources": ["16162882", "12115176", "24972708"]}, {"question_id": 249, "question": "Is improvement in ACR50 higher, lower, or the same when comparing tofacitinib monotherapy to placebo?", "answer": "insufficient data", "evidence_quality": "", "fulltext_required": "no", "comment": "", "relevant_sources": ["16162882", "12115176", "24972708", "23294500"]}], "sources": [{"article_id": "16947627", "content": "To determine the efficacy and safety of treatment with rituximab plus methotrexate (MTX) in patients with active rheumatoid arthritis (RA) who had an inadequate response to anti-tumor necrosis factor (anti-TNF) therapies and to explore the pharmacokinetics and pharmacodynamics of rituximab in this population.\nWe evaluated primary efficacy and safety at 24 weeks in patients enrolled in the Randomized Evaluation of Long-Term Efficacy of Rituximab in RA (REFLEX) Trial, a 2-year, multicenter, randomized, double-blind, placebo-controlled, phase III study of rituximab therapy. Patients with active RA and an inadequate response to 1 or more anti-TNF agents were randomized to receive intravenous rituximab (1 course, consisting of 2 infusions of 1,000 mg each) or placebo, both with background MTX. The primary efficacy end point was a response on the American College of Rheumatology 20% improvement criteria (ACR20) at 24 weeks. Secondary end points were responses on the ACR50 and ACR70 improvement criteria, the Disease Activity Score in 28 joints, and the European League against Rheumatism (EULAR) response criteria at 24 weeks. Additional end points included scores on the Functional Assessment of Chronic Illness Therapy-Fatigue (FACIT-F), Health Assessment Questionnaire (HAQ) Disability Index (DI), and Short Form 36 (SF-36) instruments, as well as Genant-modified Sharp radiographic scores at 24 weeks.\nPatients assigned to placebo (n = 209) and rituximab (n = 311) had active, longstanding RA. At week 24, significantly more (P < 0.0001) rituximab-treated patients than placebo-treated patients demonstrated ACR20 (51% versus 18%), ACR50 (27% versus 5%), and ACR70 (12% versus 1%) responses and moderate-to-good EULAR responses (65% versus 22%). All ACR response parameters were significantly improved in rituximab-treated patients, who also had clinically meaningful improvements in fatigue, disability, and health-related quality of life (demonstrated by FACIT-F, HAQ DI, and SF-36 scores, respectively) and showed a trend toward less progression in radiographic end points. Rituximab depleted peripheral CD20+ B cells, but the mean immunoglobulin levels (IgG, IgM, and IgA) remained within normal ranges. Most adverse events occurred with the first rituximab infusion and were of mild-to-moderate severity. The rate of serious infections was 5.2 per 100 patient-years in the rituximab group and 3.7 per 100 patient-years in the placebo group.\nAt 24 weeks, a single course of rituximab with concomitant MTX therapy provided significant and clinically meaningful improvements in disease activity in patients with active, longstanding RA who had an inadequate response to 1 or more anti-TNF therapies.", "title": "Rituximab for rheumatoid arthritis refractory to anti-tumor necrosis factor therapy: Results of a multicenter, randomized, double-blind, placebo-controlled, phase III trial evaluating primary efficacy and safety at twenty-four weeks.", "date": "2006-09-02"}, {"article_id": "18576334", "content": "To evaluate the efficacy and safety of treatment with 50 mg of etanercept twice a week plus weekly methotrexate (MTX; > or =15 mg) in patients with rheumatoid arthritis (RA) who had a suboptimal response to 50 mg of etanercept once a week plus weekly MTX (> or =15 mg).\nIn this multicenter, randomized, double-blind, active drug-controlled study, suboptimal responders to treatment with MTX plus etanercept 50 mg once weekly were given MTX plus etanercept 50 mg twice weekly (n = 160) or MTX plus etanercept 50 mg once weekly plus a placebo (n = 40) for 12 weeks. In a subsequent 12-week open-label period, patients who responded to etanercept 50 mg twice weekly decreased their dosage to 50 mg once weekly, those who had a partial response to etanercept 50 mg once weekly increased their dosage to 50 mg twice weekly, and those who had no response to etanercept 50 mg twice weekly were discontinued. The primary end point was the proportion of patients with a response on the Disease Activity Score 28-joint assessment (DAS28) at week 12.\nA total of 201 patients were randomized; 187 completed 12 weeks, and 102 completed 24 weeks. At week 12 (double-blind period), the DAS28 response in the 50 mg twice weekly and the 50 mg once weekly groups was not significantly different (45.6% versus 35.0%; P = 0.285), and similar proportions of patients in the groups taking 100 mg and 50 mg experienced adverse events (34.4% versus 37.5%; P = 0.711). Serious adverse events occurred in 7 of 160 of the 50 mg twice weekly group and 0 of 40 of the 50 mg once weekly group (P = 0.387), and serious infectious events occurred in 3 of 160 patients in the 50 mg twice weekly group (P = 0.884).\nEtanercept 50 mg once weekly is an optimal dosage in most patients with RA. Increasing the dosage from 50 mg once weekly to 50 mg twice weekly in suboptimal responders did not significantly improve their DAS28 responses.", "title": "Efficacy and safety of etanercept 50 mg twice a week in patients with rheumatoid arthritis who had a suboptimal response to etanercept 50 mg once a week: results of a multicenter, randomized, double-blind, active drug-controlled study.", "date": "2008-06-26"}, {"article_id": "12115176", "content": "T cells are involved in the pathogenesis of rheumatoid arthritis (RA). In animal models of autoimmune diseases, blockade of costimulatory molecules on antigen-presenting cells has been demonstrated to be effective in preventing or treating this disease by preventing T cell activation. To date, the effect of costimulatory blockade in patients with RA is unknown. The goal of this multicenter, multinational study was to determine the safety and preliminary efficacy of costimulatory blockade using CTLA-4Ig and LEA29Y in RA patients who have been treated unsuccessfully with at least 1 disease-modifying agent.\nCTLA-4Ig, LEA29Y (0.5, 2, or 10 mg/kg), or placebo was administered intravenously to 214 patients with RA. Patients received 4 infusions of study medication, on days 1, 15, 29, and 57, and were evaluated on day 85. The primary end point was the proportion of patients meeting the American College of Rheumatology 20% improvement criteria (ACR20). All patients were monitored for treatment safety and tolerability.\nCTLA-4Ig and LEA29Y infusions were well tolerated at all dose levels. Peri-infusional adverse events were carefully monitored, and showed similar incidence across all dose groups with the exception of headaches, which were slightly more frequent in the 2 treatment groups. The incidence of discontinuations due to worsening of RA was 19%, 12%, and 9% at 0.5, 2, and 10 mg/kg, respectively, in the CTLA-4Ig-treated patients and 3%, 3%, and 6% at 0.5, 2, and 10 mg/kg, respectively, in the LEA29Y-treated patients (versus 31% in the placebo group). ACR20 responses on day 85 had increased in a dose-dependent manner (23%, 44%, and 53% of CTLA-4Ig-treated patients and 34%, 45%, and 61% of LEA29Y-treated patients at 0.5, 2.0, and 10 mg/kg, respectively, versus 31% of placebo-treated patients).\nBoth of the costimulatory blocking molecules studied were generally safe and well tolerated. As compared with placebo, both CTLA-4Ig and LEA29Y demonstrated efficacy in the treatment of RA.", "title": "Costimulatory blockade in patients with rheumatoid arthritis: a pilot, dose-finding, double-blind, placebo-controlled clinical trial evaluating CTLA-4Ig and LEA29Y eighty-five days after the first infusion.", "date": "2002-07-13"}, {"article_id": "17412737", "content": "To incorporate a new trial design to examine clinical response, cytokine expression and joint imaging in patients with rheumatoid arthritis (RA) switching from etanercept to infliximab treatment.\nA randomised, open-label, clinical trial of 28 patients with an inadequate response to etanercept was conducted. Eligible patients received background methotrexate and were randomised 1:1 to discontinue etanercept and receive infliximab 3 mg/kg at weeks 0, 2, 6, 14 and 22, or to continue etanercept 25 mg twice weekly. Data were analysed for clinical response, serum biomarker levels, radiographic progression, MRI and adverse events.\nAt week 16, 62% of infliximab-treated patients achieved American College of Rheumatology 20% criteria for improvement in RA (ACR20) responses compared with 29% of etanercept-treated patients. A 30.8% decrease from baseline in Disease Activity Score 28 was observed in patients receiving infliximab, compared with a 16.0% decrease in patients receiving etanercept. ACR20 and American College of Rheumatology 50% criteria for improvement in RA responses correlated at least minimally with intracellular adhesion molecule-1 and interleukin 8 in patients receiving infliximab. 38% of patients who were switched to infliximab showed reductions in Health Assessment Questionnaire scores (>0.4), compared with 0% of patients receiving etanercept. MRI analyses were inconclusive. Both drugs were well tolerated; 54% of infliximab-treated patients and 50% of etanercept-treated patients reported adverse events.\nIn this exploratory, open-label trial (with single-blind evaluator), patients were randomised to continue with etanercept or switch to infliximab. The small sample size of this hypothesis-generating study was underpowered to show statistical differences between groups. There was a numerical trend favouring patients who switched to infliximab, therefore warranting further study with a more rigorous design.", "title": "Open-label, pilot protocol of patients with rheumatoid arthritis who switch to infliximab after an incomplete response to etanercept: the opposite study.", "date": "2007-04-07"}, {"article_id": "24972708", "content": "To study the efficacy and safety of certolizumab pegol (CZP) in patients with active rheumatoid arthritis (RA) who had discontinued an initially effective TNF inhibitor (TNF-IR).\nA randomised 12-week double-blind trial with CZP (n=27) or placebo (n=10) followed by an open-label 12 week extension period with CZP.\nBaseline characteristics of the 2 groups were similar. ACR20 response (primary end point) at week 12 was achieved in 61.5%, and none of CZP and placebo-treated patients, respectively. Weeks 12-24 showed a maximum effect for CZP at 12 weeks, and that placebo patients switched blindly to CZP attained similar results seen with CZP in weeks 0-12. Since this result was highly significant, study inclusion was terminated after entry of 33.6% of the originally planned 102 patients. Adverse events occurred in 16/27 (59.3%) CZP subjects and 4/10 (40%) placebo subjects. There were no serious adverse events, neoplasms, opportunistic, or serious infections.\nThis first, prospective, blinded trial of CZP in secondary TNF-IR shows that the ACR20 response rate observed with CZP was higher than that reported in most previous studies of TNF-IR. Additionally, CZP demonstrated good safety and tolerability. This study supports the use of CZP in RA patients who are secondary non-responders to anti-TNF therapies.", "title": "Rheumatoid arthritis secondary non-responders to TNF can attain an efficacious and safe response by switching to certolizumab pegol: a phase IV, randomised, multicentre, double-blind, 12-week study, followed by a 12-week open-label phase.", "date": "2014-06-29"}, {"article_id": "19560810", "content": "Tumour necrosis factor alpha (TNFalpha) inhibitors are frequently used to treat rheumatoid arthritis, but whether use of a different TNFalpha inhibitor can improve patient response is unknown. We assess the efficacy and safety of the TNFalpha inhibitor golimumab in patients with active rheumatoid arthritis who had previously received one or more TNFalpha inhibitors.\n461 patients with active rheumatoid arthritis from 82 sites in 10 countries were randomly allocated by interactive voice response system, stratified by study site and methotrexate use, to receive subcutaneous injections of placebo (n=155), 50 mg golimumab (n=153), or 100 mg golimumab (n=153) every 4 weeks between Feb 21, 2006, and Sept 26, 2007. Allocation was double-blind. Eligible patients had been treated with at least one dose of a TNFalpha inhibitor previously. Patients continued stable doses of methotrexate, sulfasalazine, hydroxychloroquine, oral corticosteroids, and non-steroidal anti-inflammatory drugs. The primary endpoint was achievement at week 14 of 20% or higher improvement in American College of Rheumatology criteria for assessment of rheumatoid arthritis (ACR20). At week 16, patients who had less than 20% improvement in tender and swollen joint counts were given rescue therapy and changed treatment from placebo to 50 mg golimumab, or from 50 mg to 100 mg golimumab. Drug efficacy was assessed by intention to treat and safety was assessed according to the study drug given. This study is registered with ClinicalTrials.gov, number NCT00299546.\nPatients had discontinued previous TNFalpha inhibitors because of lack of effectiveness (269 [58%] patients) or reasons unrelated to effectiveness (246 [53%] patients), such as intolerance and accessibility issues. Patients had active disease, which was indicated by a median of 14.0 (IQR 9.0-22.0) swollen and 26.0 (16.0-41.0) tender joints for the whole group. 28 (18%) patients on placebo, 54 (35%) patients on 50 mg golimumab (odds ratio 2.5 [95% CI 1.5-4.2], p=0.0006), and 58 (38%) patients on 100 mg golimumab (2.8 [1.6-4.7], p=0.0001) achieved ACR20 at week 14. Two patients were never treated, and 57 patients did not complete the study because of adverse events, unsatisfactory treatment effect, loss to follow-up, death, or other reasons. 155 patients on placebo, 153 on 50 mg golimumab, and 153 on 100 mg golimumab were assessed for drug efficacy. For weeks 1-16, serious adverse events were recorded in 11 (7%) patients on placebo, 8 (5%) on 50 mg golimumab, and 4 (3%) on 100 mg golimumab. For weeks 1-24, after some patients were given rescue therapy, serious adverse events were recorded in 15 (10%) patients on placebo, 14 (5%) on 50 mg golimumab, and 8 (4%) on 100 mg golimumab.\nGolimumab reduced the signs and symptoms of rheumatoid arthritis in patients with active disease who had previously received one or more TNFalpha inhibitors.\nCentocor Research and Development and Schering-Plough Research Institute.", "title": "Golimumab in patients with active rheumatoid arthritis after treatment with tumour necrosis factor alpha inhibitors (GO-AFTER study): a multicentre, randomised, double-blind, placebo-controlled, phase III trial.", "date": "2009-06-30"}, {"article_id": "16162882", "content": "A substantial number of patients with rheumatoid arthritis have an inadequate or unsustained response to tumor necrosis factor alpha (TNF-alpha) inhibitors. We conducted a randomized, double-blind, phase 3 trial to evaluate the efficacy and safety of abatacept, a selective costimulation modulator, in patients with active rheumatoid arthritis and an inadequate response to at least three months of anti-TNF-alpha therapy.\nPatients with active rheumatoid arthritis and an inadequate response to anti-TNF-alpha therapy were randomly assigned in a 2:1 ratio to receive abatacept or placebo on days 1, 15, and 29 and every 28 days thereafter for 6 months, in addition to at least one disease-modifying antirheumatic drug. Patients discontinued anti-TNF-alpha therapy before randomization. The rates of American College of Rheumatology (ACR) 20 responses (indicating a clinical improvement of 20 percent or greater) and improvement in functional disability, as reflected by scores for the Health Assessment Questionnaire (HAQ) disability index, were assessed.\nAfter six months, the rates of ACR 20 responses were 50.4 percent in the abatacept group and 19.5 percent in the placebo group (P<0.001); the respective rates of ACR 50 and ACR 70 responses were also significantly higher in the abatacept group than in the placebo group (20.3 percent vs. 3.8 percent, P<0.001; and 10.2 percent vs. 1.5 percent, P=0.003). At six months, significantly more patients in the abatacept group than in the placebo group had a clinically meaningful improvement in physical function, as reflected by an improvement from baseline of at least 0.3 in the HAQ disability index (47.3 percent vs. 23.3 percent, P<0.001). The incidence of adverse events and peri-infusional adverse events was 79.5 percent and 5.0 percent, respectively, in the abatacept group and 71.4 percent and 3.0 percent, respectively, in the placebo group. The incidence of serious infections was 2.3 percent in each group.\nAbatacept produced significant clinical and functional benefits in patients who had had an inadequate response to anti-TNF-alpha therapy.", "title": "Abatacept for rheumatoid arthritis refractory to tumor necrosis factor alpha inhibition.", "date": "2005-09-16"}, {"article_id": "18625622", "content": "The phase III RADIATE study examined the efficacy and safety of tocilizumab, an anti-IL-6 receptor monoclonal antibody in patients with rheumatoid arthritis (RA) refractory to tumour necrosis factor (TNF) antagonist therapy.\n499 patients with inadequate response to one or more TNF antagonists were randomly assigned to receive 8 mg/kg or 4 mg/kg tocilizumab or placebo (control) intravenously every 4 weeks with stable methotrexate for 24 weeks. ACR20 responses, secondary efficacy and safety endpoints were assessed.\nACR20 was achieved at 24 weeks by 50.0%, 30.4% and 10.1% of patients in the 8 mg/kg, 4 mg/kg and control groups, respectively (less than p<0.001 both tocilizumab groups versus control). At week 4 more patients achieved ACR20 in 8 mg/kg tocilizumab versus controls (less than p = 0.001). Patients responded regardless of most recently failed anti-TNF or the number of failed treatments. DAS28 remission (DAS28 <2.6) rates at week 24 were clearly dose related, being achieved by 30.1%, 7.6% and 1.6% of 8 mg/kg, 4 mg/kg and control groups (less than p = 0.001 for 8 mg/kg and p = 0.053 for 4 mg/kg versus control). Most adverse events were mild or moderate with overall incidences of 84.0%, 87.1% and 80.6%, respectively. The most common adverse events with higher incidence in tocilizumab groups were infections, gastrointestinal symptoms, rash and headache. The incidence of serious adverse events was higher in controls (11.3%) than in the 8 mg/kg (6.3%) and 4 mg/kg (7.4%) groups.\nTocilizumab plus methotrexate is effective in achieving rapid and sustained improvements in signs and symptoms of RA in patients with inadequate response to TNF antagonists and has a manageable safety profile.\nNCT00106522.", "title": "IL-6 receptor inhibition with tocilizumab improves treatment outcomes in patients with rheumatoid arthritis refractory to anti-tumour necrosis factor biologicals: results from a 24-week multicentre randomised placebo-controlled trial.", "date": "2008-07-16"}, {"article_id": "23294500", "content": "Rheumatoid arthritis is a heterogeneous chronic disease, and no therapeutic agent has been identified which is universally and persistently effective in all patients. We investigated the effectiveness of tofacitinib (CP-690,550), a novel oral Janus kinase inhibitor, as a targeted immunomodulator and disease-modifying therapy for rheumatoid arthritis.\nWe did a 6-month, double-blind, parallel-group phase 3 study at 82 centres in 13 countries, including North America, Europe, and Latin America. 399 patients aged 18 years or older with moderate-to-severe rheumatoid arthritis and inadequate response to tumour necrosis factor inhibitors (TNFi) were randomly assigned in a 2:2:1:1 ratio with an automated internet or telephone system to receive twice a day treatment with: tofacitinib 5 mg (n=133); tofacitinib 10 mg (n=134); or placebo (n=132), all with methotrexate. At month 3, patients given placebo advanced to either tofacitinib 5 mg twice a day (n=66) or 10 mg twice a day (n=66). Primary endpoints included American College of Rheumatology (ACR)20 response rate, mean change from baseline in Health Assessment Questionnaire-Disability Index (HAQ-DI), and rates of disease activity score (DAS)28-4(ESR) less than 2\u00b76 (referred to as DAS28<2\u00b76), all at month 3. The full analysis set for the primary analysis included all randomised patients who received at least one dose of study medication and had at least one post-baseline assessment. This trial is registered with www.ClinicalTrials.gov, number NCT00960440.\nAt month 3, ACR20 response rates were 41\u00b77% (55 of 132 [95% CI vs placebo 6\u00b706-28\u00b741]; p=0\u00b70024) for tofacitinib 5 mg twice a day and 48\u00b71% (64 of 133; [12\u00b745-34\u00b792]; p<0\u00b70001) for tofacitinib 10 mg twice a day versus 24\u00b74% (32 of 131) for placebo. Improvements from baseline in HAQ-DI were -0\u00b743 ([-0\u00b736 to -0\u00b715]; p<0\u00b70001) for 5 mg twice a day and -0\u00b746 ([-0\u00b738 to -0\u00b717]; p<0\u00b70001) for 10 mg twice a day tofacitinib versus -0\u00b718 for placebo; DAS28<2\u00b76 rates were 6\u00b77% (eight of 119; [0-10\u00b710]; p=0\u00b70496) for 5 mg twice a day tofacitinib and 8\u00b78% (11 of 125 [1\u00b766-12\u00b760]; p=0\u00b70105) for 10 mg twice a day tofacitinib versus 1\u00b77% (two of 120) for placebo. Safety was consistent with phase 2 and 3 studies. The most common adverse events in months 0-3 were diarrhoea (13 of 267; 4\u00b79%), nasopharyngitis (11 of 267; 4\u00b71%), headache (11 of 267; 4\u00b71%), and urinary tract infection (eight of 267; 3\u00b70%) across tofacitinib groups, and nausea (nine of 132; 6\u00b78%) in the placebo group.\nIn this treatment-refractory population, tofacitinib with methotrexate had rapid and clinically meaningful improvements in signs and symptoms of rheumatoid arthritis and physical function over 6 months with manageable safety. Tofacitinib could provide an effective treatment option in patients with an inadequate response to TNFi.\nPfizer.", "title": "Tofacitinib (CP-690,550) in combination with methotrexate in patients with active rheumatoid arthritis with an inadequate response to tumour necrosis factor inhibitors: a randomised phase 3 trial.", "date": "2013-01-09"}, {"article_id": "24448345", "content": "To evaluate the effect of tocilizumab (TCZ), an interleukin 6 receptor inhibitor, on humoral immune responses to immunisations in patients with rheumatoid arthritis (RA).\nPatients with RA with inadequate response/intolerance to one or more anti-tumour necrosis factor-\u03b1 agents were randomly assigned (2:1) to TCZ 8\u2005mg/kg intravenously every 4\u2005weeks plus methotrexate (MTX) or MTX alone up until week 8. Serum was collected before vaccination at week 3, antibody titres were evaluated at week 8, and then all patients received TCZ+MTX through week 20. End points included proportion of patients responding to \u22656/12 pneumococcal polysaccharide vaccine (PPV23) serotypes (primary) and proportions responding to tetanus toxoid vaccine (TTV; secondary) at week 8.\n91 patients were randomised. At week 8, 60.0% of TCZ+MTX and 70.8% of MTX patients responded to \u22656/12 PPV23 serotypes, with insufficient evidence for any difference in treatments (10.8% (95% CI -33.7 to 12.0)), and 42.0% and 39.1%, respectively, responded to TTV. Two of three TCZ+MTX patients with non-protective baseline TTV antibody titres achieved protective levels by week 8. The safety profile of TCZ was consistent with previous reports.\nShort-term TCZ treatment does not significantly attenuate humoral responses to PPV23 or TTV. To maximise vaccine response, patients should be up to date with immunisations before starting TCZ treatment.\nNCT01163747.", "title": "Humoral immune response to vaccines in patients with rheumatoid arthritis treated with tocilizumab: results of a randomised controlled trial (VISARA).", "date": "2014-01-23"}, {"article_id": "18512710", "content": "To assess the effects of treatment with rituximab plus methotrexate on patient-reported outcomes in patients with active rheumatoid arthritis (RA) who experienced inadequate response to anti-tumor necrosis factor therapy.\nPatients with active RA were randomly assigned to rituximab (1,000 mg on days 1 and 15) or placebo. The primary end point was the proportion of patients with an American College of Rheumatology 20% response at week 24. Additional goals were to assess treatment effects on pain, fatigue, functional disability, health-related quality of life, and disease activity by comparing mean changes between groups. The analysis was conducted in the intent-to-treat population. The proportion of patients who achieved the minimum clinically important difference on the Health Assessment Questionnaire (HAQ) disability index (DI), Functional Assessment of Chronic Illness Therapy-Fatigue (FACIT-F), and Short Form 36 (SF-36) was determined.\nRituximab patients had statistically significantly greater pain relief. The FACIT-F showed significantly greater improvement in rituximab patients than placebo patients from weeks 12 through 24. Mean improvement from baseline in functional disability (measured by the HAQ DI) was significantly greater in rituximab patients from weeks 8 to 24. The mean +/- SD change from baseline for the SF-36 Physical Component Score was 6.64 +/- 8.74 for rituximab patients and 1.48 +/- 7.32 for placebo patients (P < 0.0001). The mean change from baseline for the SF-36 Mental Component Score was 5.32 +/- 12.41 for rituximab patients and 2.25 +/- 12.23 for placebo patients (P = 0.0269).\nRituximab produced rapid, clinically meaningful, and statistically significant improvements in patient-reported pain, fatigue, functional disability, health-related quality of life, and disease activity. These effects were sustained throughout the study.", "title": "Improvement in patient-reported outcomes in a rituximab trial in patients with severe rheumatoid arthritis refractory to anti-tumor necrosis factor therapy.", "date": "2008-06-03"}, {"article_id": "16935912", "content": "To investigate the efficacy and safety of abatacept in combination with etanercept in patients with active rheumatoid arthritis during a 1-year, randomised, placebo-controlled, double-blind phase, followed by an open-label, long-term extension (LTE).\nPatients continued etanercept (25 mg twice weekly) and were randomised to receive abatacept 2 mg/kg (n = 85) or placebo (n = 36). As the effective dose of abatacept was established as 10 mg/kg in a separate trial, all patients received abatacept 10 mg/kg and etanercept during the LTE.\nA total of 121 patients were randomised; 80 completed double-blind treatment and entered the LTE. During double-blind treatment, the difference in the percentage of patients achieving the primary end point (modified American College of Rheumatology (ACR) 20 response at 6 months) was not significant between groups (48.2% v 30.6%; p = 0.072). At 1 year, no notable changes in modified ACR responses were observed. Subsequent to the dosing change, similar modified ACR responses were seen during the LTE. Significant improvements in quality of life were observed with abatacept and etanercept versus placebo and etanercept in five of the eight short-form 36 subscales at 1 year. More abatacept and etanercept-treated patients experienced serious adverse events (SAEs) at 1 year than patients receiving placebo and etanercept (16.5% v 2.8%), with 3.5% v 0% experiencing serious infections.\nThe combination of abatacept (at a dose of 2 mg/kg during the double-blind phase and 10 mg/kg during the LTE) and etanercept was associated with an increase in SAEs, including serious infections, with limited clinical effect. On the basis of the limited efficacy findings and safety concerns, abatacept in combination with etanercept should not be used for rheumatoid arthritis treatment.", "title": "Selective costimulation modulation using abatacept in patients with active rheumatoid arthritis while receiving etanercept: a randomised clinical trial.", "date": "2006-08-29"}]}
